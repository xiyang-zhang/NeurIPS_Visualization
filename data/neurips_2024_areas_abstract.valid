__label__diffusion_based_models Motivated by the plug-and-play framework in the imaging community, we introduce a diffusion plug-and-play method (DPnP) that alternatively calls two samplers, a proximal consistency sampler based solely on the likelihood function of the forward model, and a denoising diffusion sampler based solely on the score functions of the image prior.
__label__learning_theory (2023) recently propose an advanced algorithm, Scalable Optimal $K$-Sparse Ridge Regression (OKRidge), which is both faster and more accurate than existing approaches.
__label__natural_language_processing Recently developed model merging techniques enable the direct integration of multiple models, each fine-tuned for distinct tasks, into a single model.
__label__bandits Lastly, we demonstrate the performance of our algorithm using numerical experiments.
__label__other Persistent homology is a popular computational tool for analyzing the topology of point clouds, such as the presence of loops or voids.
__label__machine_vision Overall, UDGA framework enables stable detection performance in both source and target domains, effectively bridging inevitable domain gaps, while demanding fewer annotations.
__label__reinforcement_learning We also note that such a method yields a finite set of interpretable behaviors.
__label__probabilistic_methods Previous approaches either fail to perform well in small sample settings or suffer from inefficient estimation processes, even when incorporating meta-learning techniques.
__label__learning_theory Furthermore, experimental results show that the performance of our algorithms on real data significantly exceeds our worst-case guarantees for sample complexity, demonstrating the practicality of our approach.
__label__optimization_for_deep_networks The challenge with editable GNN training lies in the inherent information aggregation across neighbors, which can lead model editors to affect the predictions of other nodes unintentionally.
__label__natural_language_processing However, current interactions between IR systems and LLMs remain limited, with LLMs merely serving as part of components within IR systems, and IR systems being constructed independently of LLMs.
__label__probabilistic_methods In this work, we propose and study a GP model that achieves robustness against sparse outliers by inferring data-point-specific noise levels with a sequential selection procedure maximizing the log marginal likelihood that we refer to as relevance pursuit.
__label__optimization_for_deep_networks In this paper, we investigate the feasibility of on-device training using fixed-point forward gradients, by conducting comprehensive experiments across a variety of deep learning benchmark tasks in both vision and audio domains.
__label__other This paper investigates a novel lossy compression framework operating under logarithmic loss, designed to handle situations where the reconstruction distribution diverges from the source distribution.
__label__privacy The most effective approach is the two-stage scheme, which finds a small interval first and then gets a refined estimate by clipping samples into the interval.
__label__diffusion_based_models Notably, PaDIS outperforms previous DIS methods trained on entire image priors in the case of limited training data, demonstrating the data efficiency of our proposed approach by learning patch-based prior.
__label__machine_vision To address these issues, we propose a one-step effective diffusion network, namely OSEDiff, for the Real-ISR problem.
__label__evaluation Instead of relying on high-variance REINFORCE policy gradient estimators that do not scale, our adaptive labeling policy is optimized using path-wise policy gradients computed by auto-differentiating through simulated roll-outs.
__label__reinforcement_learning To close this gap, we model the mixed-motive game as a differentiable game for the ease of illuminating the learning dynamics towards cooperation.
__label__diffusion_based_models In this work, we propose a novel framework that enables learning the 3D image prior through position-aware 3D-patch diffusion score blending for reconstructing large-scale 3D medical images.
__label__machine_learning_for_other_sciences_and_fields Additionally, we enhance the MP-Adapters with contextual perceptiveness.
__label__machine_vision This paper scales object-level reconstruction to complex scenes, advancing interactive scene reconstruction.
__label__graph_neural_networks Here we present UltraQuery, the first foundation model for inductive reasoning that can zero-shot answer logical queries on any KG.
__label__machine_learning_for_other_sciences_and_fields This approach creates robust energy landscapes, even in ambiguous class distributions.
__label__reinforcement_learning The *character* of an agent is defined as a different weight combination on reward components, representing distinct behavioral preferences.
__label__machine_learning_for_other_sciences_and_fields Can a physicist make only a finite number of errors in the eternal quest to uncover the law of nature?
__label__machine_vision To address these challenges, we first define a new pipeline for restoring images with multiple degradations, and then introduce RestoreAgent, an intelligent image restoration system leveraging multimodal large language models.
__label__natural_language_processing Specifically, AmoebaLLM integrates three innovative components: (1) a knowledge-preserving subnet selection strategy that features a dynamic-programming approach for depth shrinking and an importance-driven method for width shrinking; (2) a shape-aware mixture of LoRAs to mitigate gradient conflicts among subnets during fine-tuning; and (3) an in-place distillation scheme with loss-magnitude balancing as the fine-tuning objective.
"__label__other QTIP introduces a spectrum of lookup-only to computed lookup-free trellis codes designed for a hardware-efficient ""bitshift"" trellis structure; these codes achieve state-of-the-art results in both quantization quality and inference speed."
__label__machine_vision However, the methods adopting this technique typically lack equivariance, which is crucial for effective 3D learning.
__label__speech_and_audio The Audio-Visual Cloud serves as an audio-visual representation from which the generation of spatial audio for arbitrary listener location can be generated.
__label__safety_in_machine_learning This constrains their applicability in situations where the nature of the spurious correlation is not known, or when group labels for certain spurious attributes are either insufficient or completely absent.
__label__learning_theory As an application, we derive sharp excess error rates under standard power-law assumptions of the spectrum and target decay.
__label__safety_in_machine_learning To achieve our defense goal whilst maintaining natural performance, we optimize the control prompt with both adversarial and benign prompts.
__label__fairness Then, we extend this method to the multiple kernel setting, leading to a Fair Multiple Kernel K-Means (FMKKM) method.
__label__machine_vision In particular, to inherit general knowledge from foundation models, we include a transfer loss function by measuring the correlation between the PTM and the PET-applied model.
__label__diffusion_based_models Such information bottlenecks and cross attention act as strong inductive biases for promoting disentanglement.
__label__generative_models Hence, these findings do not apply to fully recurrent architectures like RNNs, LSTMs, and the increasingly popular SSMs.
__label__learning_theory In this paper, using a d-dimensional generalization to the fundamental lemma of Neyman and Pearson (d-GNP), we obtain the Bayes optimal solution for learn-to-defer systems under various constraints.
__label__neuroscience_and_cognitive_science Especially because the irregular connection among graph nodes fits the nature of the spiking neural networks, spiking graph neural networks are considered strong alternatives to vanilla graph neural networks.
__label__interpretability_and_explainability Neuron attribution, which attributes LLM outputs to specific neurons to reveal the semantic properties they learn, has emerged as a key interpretability approach.
__label__probabilistic_methods We provide an open implementation of DCPC in Pyro on Github.
__label__machine_vision In zero-shot classification, OpenDlign surpasses previous models by 8.0\% on ModelNet40 and 16.4\% on OmniObject3D.
__label__generative_models To circumvent the need for human intervention to create a 3D pose, we propose a multi-agent LLM that adapts poses from a limited library of animal 3D poses to represent the desired animal.
__label__interpretability_and_explainability Given their expanding capabilities and real-world use, we start by studying one aspect of these models -- how MLLMs process information in a factual visual question answering task.
__label__safety_in_machine_learning Extensive experiments demonstrate that DUO can robustly defend against various state-of-the-art red teaming methods without significant performance degradation on unrelated topics, as measured by FID and CLIP scores.
__label__deep_learning_architectures Extensive experiment results and comprehensive ablation studies demonstrate the outstanding performance and the importance of each component of our proposed ECMamba.
__label__machine_vision Recognizing that foreground objects only occupy a small portion of the scene, we introduce object-centric occupancy as a supplement to object bboxes.
__label__machine_vision Our tests indicate that long-term decay in RoPE poses challenges to LVLMs while capturing visual-instruction interactions across long distances.
__label__learning_theory A learning rule is online consistent if its mistake rate eventually vanishes.
__label__machine_vision In this work we present the first unsupervised object detection method that is theoretically guaranteed to recover the true object positions up to quantifiable small shifts.
"__label__algorithmic_game_theory We study the canonical problem of $k$-facility location mechanism design,
where the $n$ agents are strategic and might misreport their locations."
__label__generative_models Experiments have proven that our LCGen method can be directly applied to different SDS-based text-to-3D methods, alleviating the Janus Problem without introducing additional information, increasing excessive training burden, or compromising the generation effect.
__label__evaluation We further evaluate compressed models qualitatively and quantitatively using MT-Bench and show that compressed models exhibiting high flips are worse than baseline models in this free-form generative task.
__label__machine_vision We hope that this work helps open up an avenue of research into object detection methods with theoretical guarantees.
__label__speech_and_audio We revisit the use of Generative Adversarial Networks (GANs) for speech enhancement and theoretically show that GANs are naturally inclined to seek the point of maximum density within the conditional clean speech distribution, which, as we argue, is essential for speech enhancement task.
__label__fairness Our analysis shows that the approach's training objective--which aims to align the embedding differences of learned prompts and reference images-- could be sub-optimal, resulting in distortion of the learned prompts and degraded generated images.
__label__deep_learning_architectures The ElasTST model incorporates a non-autoregressive design with placeholders and structured self-attention masks, warranting future outputs that are invariant to adjustments in inference horizons.
__label__natural_language_processing Experiments on three benchmarks demonstrate that KnowGPT significantly outperforms all competitors.
"__label__generative_models Such systems are practically and theoretically important:
from an end-user perspective, they are deployed to millions of people, and from an AI perspective, PBE corresponds to a very general form of few-shot inductive inference."
__label__machine_vision Our evaluation spans seven vision foundation encoders, including image, video, and 3D foundation models.
__label__diffusion_based_models The recent successes of deep generative models present an opportunity to leverage them for LID estimation, but current methods based on generative models produce inaccurate estimates, require more than a single pre-trained model, are computationally intensive, or do not exploit the best available deep generative models: diffusion models (DMs).
__label__diffusion_based_models While a decoder is assumed to have an encoder as an accurate inverse, exact encoder-decoder pair rarely exists in practice even though applications often require precise inversion of decoder.
__label__evaluation Subsequently, we maintain a consistent patch location while refining the pattern to enhance semantic resemblance to the target.
__label__machine_learning_for_other_sciences_and_fields In this paper, we propose a generative model for full-atom Peptide design with Geometric LAtent Diffusion (PepGLAD) given the binding site.
__label__neuroscience_and_cognitive_science Here, we propose a geometric approach to understanding population level activity of place cells.
__label__natural_language_processing Despite recent advances, these models are still prone to what are commonly known as hallucinations, causing them to emit unwanted and factually incorrect text.
__label__machine_vision However, VL trackers are still inferior to State-of-The-Art (SoTA) visual trackers in terms of tracking performance.
__label__natural_language_processing This paper introduces Sequoia, a scalable and robust algorithm for speculative decoding.
__label__privacy SPEAR combines insights into the explicit low-rank structure of gradients with a sampling-based algorithm.
__label__other Missing values are widely observed in these tasks, and often leading to unpredictable negative effects on existing methods, hindering their further application.
__label__machine_vision Furthermore, we introduce feature bridging to facilitate the smooth transition between adjacent anchors.
__label__machine_learning_for_other_sciences_and_fields However, just improving the accuracy of ability estimation is far from satisfactory in the real-world scenarios, since an accurate ranking of students is usually more important (e.g., in high-stakes exams).
__label__machine_vision Extensive experiments demonstrate the substantial improvements of our framework in performance, throughput and real-world practicality.
__label__natural_language_processing On various open-sourced LLMs, we compare multiple defense strategies to verify the superiority of our MoGU framework.
__label__safety_in_machine_learning Our results reveal that, across four different pixel-level watermarking schemes, the proposed method consistently achieves superior performance compared to existing attack techniques, with lower detection rates and higher image quality.
__label__safety_in_machine_learning The attack module optimizes adversarial patches to maximize effectiveness, while the defense module optimizes the conditional parameters of the camera ISP proxy network to minimize attack effectiveness.
__label__deep_learning_architectures Error correcting output code (ECOC) is a classic method that encodes binary classifiers to tackle the multi-class classification problem in decision trees and neural networks.
__label__machine_vision Instead, we propose a Prospective Representation Learning (PRL) approach to prepare the model for handling conflicts in advance.
__label__deep_learning_architectures Our theory not only motivates the success of modern selective state-space models, but also provides a solid framework to understand the expressive power of future SSM variants.
__label__natural_language_processing We further design an expert-wise training strategy to alleviate the impact of unhelpful demonstrations when optimizing the retriever model.
__label__privacy In empirical validations, FOOGD significantly enjoys three main advantages: (1) reliably estimating non-normalized decentralized distributions, (2) detecting semantic shift data via score values, and (3) generalizing to covariate-shift data by regularizing feature extractor.
__label__learning_theory Multiple works have developed the theory of implicit bias for binary classification under the assumption that the loss satisfies an *exponential tail property*.
__label__algorithmic_game_theory This paper first rigorously proves a (NFL) Theorem for general black-box adversarial optimisation when considering Pure Strategy Nash Equilibrium (NE) as the solution concept.
__label__machine_learning_for_other_sciences_and_fields Building upon this expert knowledge, by recombining and refining the 169 resulting policy suggestion models, RHEA discovered a broader and more effective set of policies than either AI or human experts alone, as evaluated based on real-world data.
__label__machine_vision However, most existing vision-language trackers still overly rely on initial fixed multimodal prompts, which struggle to provide effective guidance for dynamically changing targets.
__label__algorithmic_game_theory The long-run behavior of multi-agent online learning -- and, in particular, no-regret learning -- is relatively well-understood in potential games, where players have common interests.
__label__privacy PrivCirNet also leverages layer fusion to further reduce the inference cost.
__label__learning_theory Furthermore, we draw an analogy between income and risk in portfolio optimization and the task-relevant and task-irrelevant terms in feature learning.
__label__other We aim to tackle a subclass of these ill-posed settings, characterized by a flexible separable observation probability assumption that can depend on the matrix measurements.
__label__interpretability_and_explainability Extensive experiments are conducted on several classical FL setups to validate the effectiveness of our proposed method.
"__label__machine_learning_for_physical_sciences Existing state-
of-the-art approaches either resort to large scale transformer-based models that
diffuse over conformer fields, or use computationally expensive methods to gen-
erate initial structures and diffuse over torsion angles."
__label__machine_vision Motivated by the ill-posed nature of the task and to obtain consistent scene reconstruction results, we learn a generative scene prior by conditioning on all scene objects simultaneously to capture scene context and by allowing the model to learn inter-object relationships throughout the diffusion process.
__label__probabilistic_methods In this paper, we propose a novel class of discriminative mutual information estimators based on the variational representation of the $f$-divergence.
__label__optimization Our methodology is based on the interpretation of diffusion processes as energy-minimizing trajectories in the probability space via the so-called JKO scheme, which we study via its first-order optimality conditions.
__label__natural_language_processing We confirm much publicly disclosed information about these models, and also make several new inferences: GPT-4o is much more multilingual than its predecessors, training on 10x more non-English data than GPT-3.5, Llama 3 and Claude are trained on predominantly code, and many recent models are trained on 7-16% books.
__label__optimization In this work, focusing on the central case of topological optimization for point clouds, we propose to overcome this limitation using diffeomorphic interpolation, turning sparse gradients into smooth vector fields defined on the whole space.
__label__diffusion_based_models In this work we aim to improve the training efficiency and performance of LDMs with the goal of scaling to larger datasets and higher resolutions.
__label__machine_learning_for_other_sciences_and_fields Computation graphs are Directed Acyclic Graphs (DAGs) where the nodes correspond to mathematical operations and are used widely as abstractions in optimizations of neural networks.
__label__learning_theory In this direction, we provide a general bound under a new *model-dependent* assumption from which we obtain bounds based on parameter norms and log-Sobolev inequalities.
__label__privacy We study the problem of differentially private stochastic convex optimization (DP-SCO) with heavy-tailed gradients, where we assume a $k^{\text{th}}$-moment bound on the Lipschitz constants of sample functions, rather than a uniform bound.
__label__learning_theory Concretely, we aim for $\|\hat{H} - P\|_{TV}$ to be at most $ \alpha \cdot OPT + \epsilon$ for some small $\epsilon$ and $\alpha$.
__label__deep_learning_architectures We consider the robust routing problem with uncertain travel times under the min-max regret criterion, which represents an extended and robust version of the classic traveling salesman problem (TSP) and vehicle routing problem (VRP).
__label__machine_learning_for_physical_sciences Ensemble forecasts from Graph-EFM achieve equivalent or lower errors than comparable deterministic models, with the added benefit of accurately capturing forecast uncertainty.
__label__deep_learning_architectures This paper addresses the problem of instability by analyzing the growth in intermediate representations, allowing us to build models (referred to as Deep Thinking with Lipschitz Constraints (DT-L)) with many fewer parameters and providing more reliable solutions.
__label__reinforcement_learning In offline reinforcement learning (RL), addressing the out-of-distribution (OOD) action issue has been a focus, but we argue that there exists an OOD state issue that also impairs performance yet has been underexplored.
__label__machine_vision In addition, to support the diverse range of tasks, we carefully collected and combed training data from hundreds of public vision and vision-language tasks.
__label__machine_vision To avoid the false discovery, we propose an overuse penalty to refrain the lexical representation from falsely frequently activating meaningless words.
__label__generative_models We identify the following issues in the training process: firstly, certain training strategies do not consistently perform well across different data.
__label__machine_learning_for_other_sciences_and_fields To tackle this problem, we propose a new fragment-based molecule generation framework with retrieval augmentation, namely *Fragment Retrieval-Augmented Generation* (*f*-RAG).
__label__generative_models Our empirical studies show that diffusion models fine-tuned using DxMI can generate high-quality samples in as few as 4 and 10 steps.
__label__probabilistic_methods To address this issue, this work introduces the weighted score matching estimator to point processes.
__label__machine_vision The source code is publicly available at https://github.com/KPeng9510/EBiL-HaDS.
__label__deep_learning_architectures Riemannian neural networks, which extend deep learning techniques to Riemannian spaces, have gained significant attention in machine learning.
__label__machine_learning_for_social_sciences We first find that LLM agents generally exhibit trust behavior, referred to as agent trust, under the framework of Trust Games, which are widely recognized in behavioral economics.
__label__graph_neural_networks It guarantees that the task prototypes of the same graph task are nearly the same with a large smoothing step, while those of different tasks are distinct due to differences in graph structure and node attributes.
__label__reinforcement_learning We show it is minimax optimal up to logarithmic factors by establishing a matching lower bound.
__label__learning_theory Simulators are a pervasive tool in reinforcement learning, but most existing algorithms cannot efficiently exploit simulator access -- particularly in high-dimensional domains that require general function approximation.
__label__optimization Our implementation is available at https://github.com/HunterTracer/GLinSAT.
__label__bandits We provide a new understanding of the stochastic gradient bandit algorithm by showing that it converges to a globally optimal policy almost surely using \emph{any} constant learning rate.
__label__online_learning Our main contribution is an algorithm that achieves an $\tilde{O}(\sqrt{T})$ optimal regret for bandit non-stochastic control with strongly-convex and smooth cost functions in the presence of adversarial perturbations, improving the previously known $\tilde{O}(T^{2/3})$ regret bound from \citep{cassel2020bandit}.
__label__algorithmic_game_theory (ICML'19) and relate it to the area of multiwinner voting in computational social choice.
__label__machine_learning_for_other_sciences_and_fields Designing ligand-binding proteins, such as enzymes and biosensors, is essential in bioengineering and protein biology.
__label__machine_learning_for_physical_sciences The inferred gradient fields can then be used to rapidly generate sample trajectories that mimic the dynamics of the physical system on a population level over varying physics parameters.
__label__machine_vision The code is available at https://github.com/jaemyung-u/stl.
__label__graph_neural_networks Particularly, we establish a universal approximation theorem: for any data distribution over the MP-tractable class, there always exists an MP-GNN that can approximate the SB score with arbitrarily high accuracy and arbitrarily high probability, which lays a theoretical foundation of the existing works on imitating SB with MP-GNN.
__label__natural_language_processing These sub-functions are then composited to attain more complex objectives.
__label__machine_vision In this paper, we propose ChatTracker to leverage the wealth of world knowledge in the Multimodal Large Language Model (MLLM) to generate high-quality language descriptions and enhance tracking performance.
__label__generative_models Extensive experiments demonstrate the effectiveness of LPR, improving the EM score by up to 18.75 on common benchmarks with noisy annotations.
__label__learning_theory To our knowledge, there are no approaches that provide embeddings for supervised models; even when hyperbolic geometry provides convenient properties for expressing popular hypothesis classes, such as decision trees (and ensembles).
__label__generative_models We also demonstrate the practical applicability of our model with 3D generation tasks, showcasing its versatility and potential for broader adoption in real-world applications.
__label__fairness Our results display that in many cases, the balanced distribution does not correspond to selectively removing the undesired dependencies in a causal graph of the task, leading to multiple failure modes and even interference with other mitigation techniques such as regularization.
__label__other The procedure operates post-training, relying solely on model predictions and without modifying the trained model (e.g., the deep network).
__label__natural_language_processing Towards this problem, our initial attempt is to relabel the data with long captions, however, directly learning with which may lead to performance degradation in understanding short text (e.g., in the image classification task).
__label__learning_theory To address this issue, we investigate learning from noisy annotations with an estimated true label posterior through the framework of conditional distributionally robust optimization (CDRO).
__label__generative_models Visualizations and code are available at https://youdream3d.github.io/.
__label__generative_models Combining our symmetry model with standard generative models results in higher marginal test-log-likelihoods and improved data efficiency.
__label__graph_neural_networks Social media platforms capture diverse attack sequence samples through both machine and manual screening processes.
__label__optimization_for_deep_networks However, it still struggles to achieve a balance between fine-tuning effectiveness and efficiency, leading to limited rank allocation space.
__label__safety_in_machine_learning [1] further enhanced by Canales-Martínez et al.
__label__bandits The second phase involves the application of a standard bandit algorithm, such as the UCB algorithm.
__label__machine_vision Volumetric rendering-based methods, like NeRF, excel in HDR view synthesis from RAW images, especially for nighttime scenes.
__label__neuroscience_and_cognitive_science We introduce a mathematically rigorous framework based on rough path theory to model stochastic spiking neural networks (SSNNs) as stochastic differential equations with event discontinuities (Event SDEs) and driven by càdlàg rough paths.
__label__machine_learning_for_other_sciences_and_fields To validate the effectiveness of the proposed framework, we construct GeoGlobe, a novel dataset for visual geo-localization tasks.
__label__optimization_for_deep_networks How the stochastic gradient descent (SGD) navigates the loss landscape of a neural network remains poorly understood.
__label__online_learning Finally, we identify a condition that ensures that the PAC learnability of a hypothesis class is sufficient for its smoothed online learnability.
__label__reinforcement_learning Usually, rewards are observed only _after_ acting, and so the goal is to maximize the _expected_ cumulative reward.
__label__diffusion_based_models Diffusion models have demonstrated great success in the field of text-to-image generation.
__label__learning_theory Despite previous efforts to understand the data generation process of diffusion models, existing theoretical support remains highly suboptimal in the presence of low-dimensional structure, which we strengthen in this paper.
__label__learning_theory In particular, we show that the existence of an online learning algorithm with bounded regret (against a fixed statistical learning algorithm in a specially constructed game of online learning with delayed feedback) implies low generalization error of said statistical learning method even if the data sequence is sampled from a mixing time series.
__label__evaluation Datasets often suffer severe selection bias; clinical labels are only available on patients for whom doctors ordered medical exams.
__label__natural_language_processing Extensive results based on the metrics, GPT-4, and doctor evaluations on distinct downstream tasks show that PediatricsGPT consistently outperforms previous Chinese medical LLMs.
__label__reinforcement_learning Both results are based on reducing the average-reward MDP to a discounted MDP, which requires new ideas in the general setting.
__label__machine_learning_for_other_sciences_and_fields Importantly, we utilize a domain adaptation method to facilitate distribution approximation for guiding the training of the teacher-student framework.
__label__machine_learning_for_other_sciences_and_fields These results highlight the effectiveness of our approach in automating and improving binary code analysis.
__label__neuroscience_and_cognitive_science Additionally, we perform analysis experiments to elucidate the mechanism through which SNNs encode positional information and to explore the function of CPGs in the human brain.
__label__natural_language_processing The rapid evolution of large language models (LLMs) has expanded their capabilities across various data modalities, extending from well-established image data to increasingly popular graph data.
__label__other A core task in multi-modal learning is to integrate information from multiple feature spaces (e.g., text and audio), offering modality-invariant essential representations of data.
__label__machine_vision We show how each module contributes to substantially bridging the inter-domain gaps compared to existing works across large urban outdoor datasets such as NuScenes, Waymo, and KITTI.
__label__natural_language_processing In this paper, we propose a plug-and-play framework, for augmenting existing LVLMs in handling visual question answering (VQA) about up-to-date knowledge, dubbed SearchLVLMs.
__label__optimization_for_deep_networks Furthermore, we provide comprehensive convergence and complexity analyses for the proposed hypergradient descent algorithm on manifolds.
__label__reinforcement_learning Various formulations of safe RL have been proposed; However, fundamentally, tabula rasa RL must learn safety constraints through experience, which is problematic for real-world applications.
__label__natural_language_processing Overall, we introduce a novel problem towards LLM reliability, an interactive MEDIQ benchmark and a novel question-asking system, and highlight directions to extend LLMs’ information-seeking abilities in critical domains.
__label__machine_vision Additionally, PHYRECON models both rendering and physical uncertainty to identify and compensate for inconsistent and inaccurate monocular geometric priors.
__label__reinforcement_learning Implementation of REBEL can be found at <https://github.com/ZhaolinGao/REBEL>, and models trained by REBEL can be found at <https://huggingface.co/Cornell-AGI>.
__label__fairness Overall, our results highlight the importance of taking the causal graph into account before performing data balancing.
__label__natural_language_processing Many structured prediction and reasoning tasks can be framed as program synthesis problems, where the goal is to generate a program in a \emph{domain-specific language} (DSL) that transforms input data into the desired output.
__label__other An empirical evaluation validates the feasibility and effectiveness of our approach over several ad auction scenarios, and exhibits inherent tradeoffs in metrics as we allow the LLM more flexibility to allocate ads.
__label__evaluation Our primary contribution is empirically demonstrating that existing online continually trained deep networks produce inferior representations compared to a simple pre-defined random transforms.
__label__generative_models Extensive qualitative and quantitative experiments demonstrate that our method achieves superior performance compared to previous methods.
__label__generative_models Prior methods, such as VQGAN, typically utilize 2D latent grids with fixed downsampling factors.
__label__natural_language_processing To address these challenges, we introduce SPLAT, a benchmark leveraging Situation Puzzles to evaluate and elicit LAteral Thinking of LLMs.
__label__reinforcement_learning Our analysis further unveils the crucial design principles in interaction data formation, unified tokenization, and its scaling potentials.
__label__evaluation Numerous learnwares are accommodated by a learnware dock system.
__label__machine_learning_for_physical_sciences Enforcing the permutation antisymmetry of electrons in such generalized neural wave functions remained challenging as existing methods require discrete orbital selection via non-learnable hand-crafted algorithms.
__label__causal_inference To fill in the gap, we first present generic upper bounds on the mean-squared error of the class of AIPW estimators that crucially depends on a sequentially weighted error between the treatment effect and its estimates.
__label__interpretability_and_explainability To better understand this phenomenon, there is a growing interest in using Markov input processes to study transformers.
__label__learning_theory In particular, for every $K \geq 3$ we uncover and characterize a region of the parameter space where exact community recovery is possible using $K$ correlated graphs, even though (1) this is information-theoretically impossible using any $K-1$ of them and (2) none of the latent matchings can be exactly recovered.
__label__machine_learning_for_physical_sciences However, the naive estimation of this transform is infeasible, as it requires simulating sufficiently many forward trajectories to estimate rare event probabilities.
__label__speech_and_audio This paper proposes unified training strategies for these systems.
__label__machine_vision We use DINOv2 as our visual model for its local-inclined features and Llama 2, a generative language model, to leverage its in-context lexical prediction ability.
__label__learning_theory However, existing theoretical work fail to build up an understanding of the connection between this semantic regularity and the innovative power of ICL.
__label__machine_vision The framework integrates state-of-the-art $\texttt{PEFT}$ techniques with two Bayesian components to address the under-confidence issue while ensuring reliable prediction under challenging few-shot settings.
__label__interpretability_and_explainability Most existing techniques fail to perform dynamic image fusion while notably lacking theoretical guarantees, leading to potential deployment risks in this field.
__label__machine_learning_for_other_sciences_and_fields We also propose a model variant, inspired by graph parsing networks and complex network analysis, enabling graph representation learning and jointed, personalized graph partitioning, using an unspecified number of groups.
__label__safety_in_machine_learning (3) GREAT Score can be used for remote auditing of privacy-sensitive black-box models, as demonstrated by our robustness evaluation on several online facial recognition services.
__label__learning_theory We show that the algorithmic interpretation of the derived “laws of learning”, which takes the structure of Hamiltonian equations, reduces to Backpropagation when the speed of propagation goes to infinity.
__label__machine_learning_for_healthcare However, in reality, the cost of collecting both a large amount of labeled scRNA-seq data and scATAC-seq data is expensive.
__label__machine_vision To address this, recent research incorporates equivariant representation learning, which captures transformation-sensitive information.
__label__optimization_for_deep_networks Our code is available at https://github.com/StefanosPert/Equivariant_Optimization_CR
__label__algorithmic_game_theory This contrasts sharply with the classical model, where RD beats Copeland with a distortion of 3 versus 5 [1].
__label__learning_theory Specifically, our result implies the optimality in the minimax sense of many of the most-frequently used estimators (including the U-statistic, the V-statistic, and the Nyström-based one) on $\mathbb{R}^d$.
__label__machine_learning_for_physical_sciences On various complicated photonic device benchmarks, we demonstrate one sole PACE model is capable of achieving 73% lower error with 50% fewer parameters compared with various recent ML for PDE solvers.
__label__machine_vision Unlike the classic few-shot semantic segmentation, GFSS aims to classify pixels into both base and novel classes, meaning it is a more practical setting.
__label__online_learning Through comprehensive experiments and visualizations, we uncover new insights into training over-parameterized networks in structured environments.
__label__machine_learning_for_healthcare The accurate identification of active sites in proteins is essential for the advancement of life sciences and pharmaceutical development, as these sites are of critical importance for enzyme activity and drug design.
__label__learning_theory We study a natural generalization of this problem to subset queries for $|S|>2$, where the oracle returns the number of clusters intersecting $S$.
__label__neuroscience_and_cognitive_science Task switching in the gating layer accelerates as a function of curriculum block size and task training, mirroring key findings in cognitive neuroscience.
__label__generative_models We find that pretrained models are not effective at PBE, but that they can be fine-tuned for much higher performance, provided the test problems are in-distribution.
__label__interpretability_and_explainability Under these conditions, the complexity of the transform computation is significantly reduced.
__label__natural_language_processing Our evaluations reveal that current LLMs struggle to meet these criteria effectively.
__label__interpretability_and_explainability The code is available at: https://github.com/tmllab/2024_NeurIPS_CSGN.
"__label__generative_models Given the success of Large Language Models (LLMs) in code-generation tasks, we investigate here the extent to which LLMs can be said to have ""solved"" PBE."
__label__reinforcement_learning Due to various reasons, however, such data typically takes the form of rankings over pairs of trajectory segments, which fails to capture the varying strengths of preferences across different pairs.
__label__machine_vision DI is responsible for generating balance-aware query, and BATO uses the balance-aware query to guide the optimization of the initial feature tokens.
__label__machine_vision This motivates the need for a multimodal method to compress many shots into fewer tokens without finetuning.
__label__interpretability_and_explainability Further, we explore their behavior when embedded with socio-demographic features of human beings, uncovering significant disparities across various demographic characteristics.
__label__natural_language_processing Good task performance is often considered sufficient evidence that meaningful communication is taking place, but existing empirical results show that communication strategies induced by common objectives can be counterintuitive whilst solving the task nearly perfectly.
__label__machine_vision ** This work provides several implications.
__label__privacy Our investigation into approximate MU starts with identifying the steepest descent direction, minimizing the output Kullback-Leibler divergence to exact MU inside a parameters' neighborhood.
__label__machine_vision This is implemented by representing the image and kernel with implicit neural representations (INRs), whose resolution-free property enables consistent yet efficient computation for network training across multiple scales.
__label__diffusion_based_models The watermark can be verified by reversing the generation process.
__label__machine_vision However, the neural priors from 2D pretrained models are often noisy and blurry, which struggle to precisely guide the learning of radiance fields.
__label__robotics On both structure and task transfer, MeMo achieves improved training efficiency to graph neural network and Transformer baselines.
__label__machine_vision Additionally, we show improved performance on standard novel view synthesis and 3D reconstruction benchmarks.
__label__privacy Our work suggests that FSwoR is often preferable to Poisson subsampling due to constant memory usage.
__label__natural_language_processing Second, logical relationships between papers are often implicit, and directly prompting an LLM to predict citations may lead to results based primarily on surface-level textual similarities, rather than the deeper logical reasoning required.
__label__generative_models Previous methods modeled the feature space of future trajectories based on the high-dimensional feature space of historical trajectories, but this approach is suboptimal because it overlooks the similarity between historical and future trajectories.
__label__safety_in_machine_learning Leveraging the model’s outputs, specifically the logits, is a common approach to estimating the test accuracy of a pre-trained neural network on out-of-distribution (OOD) samples without requiring access to the corresponding ground-truth labels.
__label__diffusion_based_models While several methods have been proposed to avoid this computation, each has drawbacks, such as instability during training and approximating the learning as learning a denoising vector field rather than a true score.
__label__reinforcement_learning However, state-of-the-art offline RL algorithms perform sub-optimally when confronted with limited data confined to specific regions within the state space.
__label__optimization_for_deep_networks Based on these new approaches, we propose the LNGD optimizer.
__label__generative_models Current architectures either share the encoder output, decoder input, or both across modalities to learn a shared representation.
__label__machine_vision Given a collection of point matches extracted from two images, our method identifies outlier point matches and models the displacement noise in inlier matches.
__label__deep_learning_architectures (2017) is now ubiquitous across application domains, from natural language processing to speech processing and image understanding.
__label__evaluation These formal impossibility results highlight a fundamental epistemic issue, i.e., that for key tasks in modern AI we cannot know whether models are valid under current data collection practices.
__label__probabilistic_methods However, when confronted with data of high dimensionality (such as images), visualizing this distribution becomes a formidable challenge, necessitating the application of effective summarization techniques before user examination.
__label__deep_learning_architectures The codes are available at https://github.com/fmx789/Meta-HMR.
__label__other We propose ST$_k$, a fully differentiable module with a single trainable parameter, designed to solve the Top-k problem without requiring additional time or GPU memory.
__label__interpretability_and_explainability We also develop algorithms for tolerant variants of our testing problem improving upon black-box linear program solvers, and give sample complexity lower bounds for alternative calibration measures to the one considered in this work.
__label__reinforcement_learning Policy Optimization (PO) methods are among the most popular Reinforcement Learning (RL) algorithms in practice.
__label__algorithmic_game_theory In this case, the concept of information asymmetry becomes nuanced and depends on the game's structure.
__label__safety_in_machine_learning Empirical results show that our meta-Stackelberg framework obtains superb performance against strong model poisoning and backdoor attacks with unknown/uncertain types.
__label__optimization Furthermore, we establish a lower bound for smooth nonconvex finite-sum problems in the asynchronous setup, providing a fundamental time complexity limit.
__label__safety_in_machine_learning We present estimation procedures employing confidence sequences enjoying the same statistical guarantees as the standard methods, with the optimal sample complexities for the estimation task and empirically demonstrate their good performance.
__label__algorithmic_game_theory This is a major hindrance to the practical implementation of many proposed solutions.
__label__causal_inference The identifiability analysis of linear Ordinary Differential Equation (ODE) systems is a necessary prerequisite for making reliable causal inferences about these systems.
__label__machine_vision This allows scaling logic gate networks up by over one order of magnitude and utilizing the paradigm of convolution.
__label__optimization Numerical results validate our theoretical analysis.
__label__deep_learning_architectures In addition, a node and hyperedge constraint mechanism is introduced to cluster nodes with similar semantic information and differentiate the temporal variations within each scales.
__label__machine_vision While data augmentation is a straightforward solution to improve generalization, certain augmentations exhibit a polarized effect in this task, enhancing in-distribution performance while deteriorating out-of-distribution performance.
__label__natural_language_processing Specifically, DropBP can reduce training time by 44% with comparable accuracy to the baseline, accelerate convergence to the same perplexity by 1.5$\times$, and enable training with a sequence length 6.2$\times$ larger on a single NVIDIA-A100 GPU.
__label__human-AI_interaction Cognitive science has proposed several models that capture these intricacies but, due to their intractable nature, work on preference learning has, in practice, had to rely on tractable but simplified variants of the well-known Bradley-Terry model.
__label__reinforcement_learning MQL-UCB achieves minimax optimal regret of $\tilde{O}(d\sqrt{HK})$ when $K$ is sufficiently large and near-optimal policy switching cost of $\tilde{O}(dH)$, with $d$ being the eluder dimension of the function class, $H$ being the planning horizon, and $K$ being the number of episodes.
__label__machine_learning_for_physical_sciences P$^2$C$^2$Net achieves consistent state-of-the-art performance with over 50\% gain (e.g., in terms of relative prediction error) across four datasets covering complex reaction-diffusion processes and turbulent flows.
__label__other We then explore it in language modeling, the application that initially inspired it, and demonstrate it on both LSTM and transformer full models, using bigrams as restricted models.
__label__safety_in_machine_learning For classification models based on neural networks, the maximum predicted class probability is often used as a confidence score.
__label__algorithmic_game_theory Each individual only needs to evaluate one pair of items and report her comparison in our mechanism.
__label__reinforcement_learning Code and videos are available at https://mynsng.github.io/dodont/
__label__machine_learning_for_healthcare Electronic health record (EHR) data has emerged as a valuable resource for analyzing patient health status.
__label__learning_theory Specifically, both layers converge sub-linearly to the direction of their corresponding max-margin solutions.
__label__safety_in_machine_learning However, \textit{Does achieving a low ASR through current safety purification methods truly eliminate learned backdoor features from the pretraining phase?}
__label__generative_models Instead, current approaches for training these models rely on minimizing the log-squared difference between a proposal (forward policy) and a target (backward policy) distributions.
__label__safety_in_machine_learning To design scoring functions that discern OOD data from the in-distribution (ID) cases from a pre-trained discriminative model, existing methods tend to make rigorous distributional assumptions either explicitly or implicitly due to the lack of knowledge about the learned feature space in advance.
__label__causal_inference The theory is corroborated by experiments.
__label__deep_learning_architectures Subsequently, MiTSformer captures complete spatial-temporal dependencies within and across LCVs and CVs via cascaded self- and cross-attention blocks.
__label__evaluation While recent advancements in large multimodal models (LMMs) have significantly improved their abilities in image quality assessment (IQA) relying on absolute quality rating, how to transfer reliable relative quality comparison outputs to continuous perceptual quality scores remains largely unexplored.
__label__machine_learning_for_physical_sciences the dynamics of communicating cells or physical particles.
__label__machine_vision In this paper, we address a novel problem of selective forgetting for black-box models, named Black-Box Forgetting, and propose an approach to the problem.
__label__safety_in_machine_learning Furthermore, $\texttt{SGen}^{\texttt{Semi}}$ enables to use more general class of selection functions, neuro-selection functions, and provides users with an optimal selection function class given multiple candidates.
__label__privacy However, existing single-point based fingerprinting methods are highly sensitive to the changes in the decision boundary, and may suffer from the misjudgment of the resemblance of sparse fingerprinting, yielding high false positives of innocent models.
__label__safety_in_machine_learning Out-of-distribution (OOD) detection is critical when deploying machine learning models in the real world.
__label__learning_theory Auto-regressive large language models (LLMs) show impressive capacities to solve many complex reasoning tasks while struggling with some simple logical reasoning tasks such as inverse search: when trained on ''$A \to B$'' (e.g., *Tom is the parent of John*), LLM fails to directly conclude ''$B \gets A$'' (e.g., *John is the child of Tom*) during inference even if the two sentences are semantically identical, which is known as the ''reversal curse''.
__label__other Remarkably, it can surpass the performance obtained from full-dataset training, even when pruning up to 60-70% of the data on HIV and PCBA dataset.
__label__learning_theory Flatness of the loss surface and neural collapse have recently emerged as useful pre-training metrics which shed light on the implicit biases underlying pre-training.
__label__causal_inference Building on the sequentially randomized experiments literature in causal inference, our approach extends history-restricted marginal structural models for dynamic regimes.
__label__other Our extensive experimentation on a diverse range of optimization tasks also shows that reducing surrogate sharpness often leads to significant improvement, marking (up to) a noticeable 9.6% performance boost.
__label__reinforcement_learning The wider application of end-to-end learning methods to embodied decision-making domains remains bottlenecked by their reliance on a superabundance of training data representative of the target domain.
__label__reinforcement_learning Our code is available at [https://github.com/robbycostales/diva](https://github.com/robbycostales/diva).
__label__machine_vision Recent researches have proven that pre-training on large-scale person images extracted from internet videos is an effective way in learning better representations for person re-identification.
__label__machine_vision For example, images from two different satellites may both contain RGB channels, but the remaining channels can be different for each imaging source.
__label__probabilistic_methods However, in real-world problems with finite-dimensional data, the bias term is often too significant to disregard, resulting in overly narrow confidence intervals.
__label__learning_theory Specifically, in terms of both the asymptotic bias and coverage accuracy of the associated interval for out-of-sample evaluation, $K$-fold CV provably cannot outperform plug-in regardless of the rate at which the parametric or nonparametric models converge.
__label__graph_neural_networks Recently, the advent of deep learning methods has introduced significant potential for enhancing SAT solving.
__label__machine_vision Video-Language Models (VLMs), powered by the advancements in Large Language Models (LLMs), are charting new frontiers in video understanding.
__label__online_learning Vision-Language Models (VLMs) such as CLIP have yielded unprecedented performances for zero-shot image classification, yet their generalization capability may still be seriously challenged when confronted to domain shifts.
__label__machine_vision We empirically evaluate our proposed approach across datasets and architectures of varying scales and complexities, demonstrating substantial performance gains in generalization and safety metrics compared to the standard training protocol.
__label__reinforcement_learning Moreover, it is difficult to extract useful information representing the dynamic world from human videos, because of its noisy and multimodal data structure.
__label__reinforcement_learning Perceiving the pre-eminence of image reconstruction in representation learning, we propose SMG (\blue{S}eparated \blue{M}odels for \blue{G}eneralization), a novel approach that exploits image reconstruction for generalization.
__label__robotics Moreover, to control the preservation of task-specific information adaptively based on the context of the VPR, we introduce the Dynamic Power Normalization (DPN) module in both the recalibration and CFP stages, forming a novel Parameter Efficiency Fine-Tuning (PEFT) pipeline (EMVP) tailored for the VPR task.
__label__neuroscience_and_cognitive_science Working memory is a central cognitive ability crucial for intelligent decision-making.
__label__generative_models This iterative structure brings the flexibility for our method to generate or refine scenes from various starting points, such as text, floor plans, or pre-arranged environments.
__label__reinforcement_learning Large-scale comparisons in Meta-World ML45, Multi-Game Procgen, Multi-Task POPGym, Multi-Game Atari, and BabyAI find that this design unlocks significant progress in online multi-task adaptation and memory problems without explicit task labels.
__label__graph_neural_networks In addition to logic-oriented results, we also characterize recurrent GNNs, with both reals and floats, via distributed automata, drawing links to distributed computing models.
__label__evaluation There are increasing cases where the class labels of test samples are unavailable, creating a significant need and challenge in measuring the discrepancy between training and test distributions.
__label__interpretability_and_explainability In the second stage, we find evidence that ViTs can learn to represent somewhat abstract visual relations, a capability that has long been considered out of reach for artificial neural networks.
__label__natural_language_processing Finally, finetuning with our listener- aware method leads to an emergent increase in model abstention (e.g.
__label__probabilistic_methods To make a principled choice among many equally valid sources, we propose an approach which targets the maximum entropy distribution, i.e., prioritizes retaining as much uncertainty as possible.
__label__reinforcement_learning Towards expanding the class of scenarios that cooperative learning methods may optimally address, we introduce $N$*-agent ad hoc teamwork* (NAHT), where a set of autonomous agents must interact and cooperate with dynamically varying numbers and types of teammates.
__label__natural_language_processing Through applying this information-intensive training on Mistral-7B, we present **FILM-7B** (FIll-in-the-Middle).
__label__learning_theory Additionally, we separate learning and testing phases and obtain algorithms that run in fully polynomial time at test time.
"__label__other We show that for representing such dataset in:

- $\ell_2$: $d = \Theta(\sqrt{m})$ is necessary and sufficient."
__label__robotics Specifically, VDD leverages a decompositional upper bound of the variational objective that allows the training of each expert separately, resulting in a robust optimization scheme for MoEs.
__label__natural_language_processing The *Vertical Cascade* eliminates autoregressive generation from neural models, while the *Horizontal Cascade* optimizes time allocation in drafting for improved efficiency.
__label__machine_vision By accurately modeling the indirect radiance field, normal, visibility, and direct light simultaneously, we are able to accurately decouple environment lighting and the object's PBR materials without imposing strict constraints on the scene.
__label__deep_learning_architectures Particularly, the proposed QT-ViTs consistently surpass the previous SOTA EfficientViTs under different model sizes, and achieve a new Pareto-front in terms of accuracy and speed.
__label__deep_learning_architectures This paper proposes a framework for designing Riemannian MLR over general geometries, referred to as RMLR.
__label__machine_vision State Space Models (SSMs) have the advantage of keeping linear computational complexity compared to attention modules in transformers, and have been applied to vision tasks as a new type of powerful vision foundation model.
__label__learning_theory Neuc-MDS efficiently optimizes the choice of (both positive and negative) eigenvalues of the dissimilarity Gram matrix to reduce STRESS, the sum of squared pairwise error.
__label__machine_vision To fill the research gap, this paper analyzes the noise rate/type in text descriptions by full statistics of manual spelling; then reveals the poor robustness of existing methods; and finally rethinks to study a practical task: noisy OVAR.
__label__safety_in_machine_learning We implement the above variations of agent backdoor attacks on two typical agent tasks including web shopping and tool utilization.
__label__optimization To tackle these issues, we adapt the Byzantine framework to asynchronous dynamics by introducing a novel weighted robust aggregation framework.
__label__safety_in_machine_learning Second, we demonstrate that ensembles of $\textit{multiple}$ foundation models pretrained on different datasets but finetuned on the same task can also show agreement-on-the-line.
__label__machine_vision Under the circumstances, we refine the widely-used Visual Prompt Tuning (VPT) method, proposing Cross Visual Prompt Tuning (CVPT).
__label__privacy This paper presents an auditing procedure for the Differentially Private Stochastic Gradient Descent (DP-SGD) algorithm in the black-box threat model that is substantially tighter than prior work.
__label__machine_vision Our code is publicly available at https://github.com/chaudatascience/diverse_channel_vit.
__label__machine_vision This paper proposes SfPUEL, an end-to-end SfP method to jointly estimate surface normal and material under unknown environment light.
__label__privacy We analyze these measures theoretically under a distributional model which, we claim, encapsulates reasonable adversarial settings.
__label__deep_learning_architectures Existing approaches, which integrates knowledge distillation into domain adaptation frameworks to simultaneously address domain shift and model complexity, often neglect network capacity gap between teacher and student and just coarsely align their outputs over all source and target samples, resulting in poor distillation efficiency.
__label__probabilistic_methods Under mild assumptions, we establish convergence of our method to a local optimum of the FM objective.
__label__robotics As a result, SyntheOcc can generate photorealistic multi-view images and videos that faithfully align with the given geometric labels (semantics in 3D voxel space).
__label__safety_in_machine_learning In this work, we regard the MR of LM-VP as a unified entity, referred to as the LM-VP model, and take a step toward jointly evaluating the adversarial robustness and privacy of LM-VP models.
__label__natural_language_processing Unfortunately, these methods often underutilize the built-in skilled reasoning and useful language understanding capabilities of LLMs.
__label__interpretability_and_explainability In contrast, we adopt methods from mechanistic interpretability to study the higher-level visual algorithms that ViTs use to perform abstract visual reasoning.
__label__fairness Our results indicate that Greedy Capture, a clustering algorithm developed for centroid clustering, continues to provide strong proportional fairness guarantees for non-centroid clustering, although the guarantees are significantly different and establishing them requires novel proof ideas.
__label__causal_inference We show that in contrast to existing methods based on Granger causality, our model is identifiable for both instant and delayed effects.
__label__optimization_for_deep_networks Our optimized numerical computation (e.g., optimized tensorized embedding and tensor-vector contractions) and GPU implementation eliminate part of the run-time overhead in the tensorized training on GPU.
__label__diffusion_based_models The TPC provides a calibrated reference image for the diffusion model, enhancing its capability to understand the correspondence between human shapes in the reference and target images.
__label__evaluation TS2 will enable the development of better T2I prompt faithfulness metrics through more rigorous comparison of their conformity to expected orderings and separations under objective criteria.
__label__machine_vision These modules facilitate the storage and flexible interaction between short-term and long-term memories, generating prompts that adapt to target variations.
__label__deep_learning_architectures Existing vision Mamba approaches either flatten tokens into sequences in a raster scan fashion, which breaks the local adjacency of images, or manually partition tokens into windows, which limits their long-range modeling and generalization capabilities.
__label__reinforcement_learning We evaluate SkiLD in several domains with challenging, long-horizon sparse reward tasks including a realistic simulated household robot domain, where SkiLD successfully learns skills with clear semantic meaning and shows superior performance compared to existing unsupervised reinforcement learning methods that only maximize state coverage.
__label__human-AI_interaction Shared autonomy (SA) facilitates control by combining inputs from a human pilot and an AI copilot.
__label__learning_theory The rates demonstrate a trade-off between the amount of delay in the online learning game and the degree of dependence between consecutive data points, with near-optimal rates recovered in a number of well-studied settings when the delay is tuned appropriately as a function of the mixing time of the process.
__label__probabilistic_methods In this paper, we go beyond post-training Bayesianization and propose Bayesian Low-Rank Adaptation by Backpropagation (BLoB), an algorithm that continuously and jointly adjusts both the mean and covariance of LLM parameters throughout the whole fine-tuning process.
"__label__learning_theory An important aspect of our work is that all the results are ""universal"", in the sense that they depend only on the first and second order statistics of the target distribution."
__label__learning_theory We include a colab notebook https://tinyurl.com/2saj6bkj, nanoChinchilla, that reproduces some key results of the paper.
__label__machine_learning_for_physical_sciences Extensive experiments on a range of benchmark datasets validate the superiority of the proposed EGODE compared to various state-of-the-art baselines.
__label__deep_learning_architectures We modulate single-stream pre-trained Transformer embeddings with dual-stream convolutional features through cross-architecture interactions to provide richer semantic priors, thereby further relieving the ill-posedness of the problem.
__label__probabilistic_methods Credal sets are sets of probability distributions that are considered as candidates for an imprecisely known ground-truth distribution.
__label__learning_theory This novel model directly finds the fixed points of such a forward process as features for prediction.
__label__neuroscience_and_cognitive_science We tokenize the neural activity within electrodes using convolutions and extract long-term temporal dependencies between tokens using self-attention in the time dimension.
__label__optimization We propose and study several server-extrapolation strategies for enhancing the theoretical and empirical convergence properties of the popular federated learning optimizer FedProx [Li et al., 2020].
__label__diffusion_based_models Moreover, in order to address concept neglect, we devise a context-controllable synthesis strategy that leverages expressive region features and noise estimation to control the contexts of generated images according to user conditions.
__label__reinforcement_learning In the test time adaptation phase, the cross-task prompt, serving as a good initialization, were further optimized on unseen tasks through test time adaptation, enhancing the model's performance on these tasks.
__label__machine_learning_for_other_sciences_and_fields SocraticLM is then fine-tuned on SocraTeach with three strategies balancing its teaching and reasoning abilities.
__label__reinforcement_learning The success of Large Language Models (LLMs) sheds light on utilizing such knowledge behind the books.
__label__reinforcement_learning The probability of the flip and the two action candidates vary depending on the state.
__label__optimization_for_deep_networks Recent studies have explored $\mathcal{NC}$ in the absence of one or more of these conditions to extend and capitalize on the associated benefits of ideal geometries.
__label__generative_models Focusing on the safety domain, we use Rainbow Teaming to target various state-of-the-art LLMs, including the Llama 2 and Llama 3 models.
__label__optimization_for_deep_networks Notably, we demonstrate that our method can be combined with SAM variants and existing data augmentation strategies to achieve, to the best of our knowledge, state-of-the-art performance for training ResNet18 on CIFAR10, STL10, CINIC10, Tiny-ImageNet; ResNet34 on CIFAR100; and VGG19 and DenseNet121 on CIFAR10.
__label__natural_language_processing Our work reveals the relationship between context length and RoPE base both theoretically and empirically, which may shed light on future long context training.
__label__reinforcement_learning Such scenarios can lead the model to exhaust its capacity on meaningless content, at the cost of neglecting important environment dynamics.
__label__diffusion_based_models We introduce SceneCraft, a novel method for generating detailed indoor scenes that adhere to textual descriptions and spatial layout preferences provided by users.
__label__machine_learning_for_healthcare Previous clean sample selection methods have not utilized the well pre-trained features of vision foundation models (VFMs) and assumed that training begins from scratch.
__label__generative_models Through extensive experiments, we validate the effectiveness of our method in various image reconstruction tasks, such as MRI and CT reconstruction, as well as in image restoration tasks like image denoising, inpainting, and non-linear deblurring.
__label__machine_learning_for_other_sciences_and_fields NeuralSteiner offers two advantages over previous learning-based models.
__label__machine_vision By revealing and exploiting intra-image and inter-image semantic similarities in SAM's latent space, bitemporal latent matching endows SAM with zero-shot change detection capabilities in a training-free way.
__label__probabilistic_methods We start by exploring strategies for eliciting explicit, coherent numerical predictive distributions from LLMs.
__label__natural_language_processing Learning from AI feedback (LAIF) is a popular paradigm for improving the instruction-following abilities of powerful pre-trained language models.
__label__diffusion_based_models Secondly, we incorporate human feedback learning to boost the performance of the model in a low-step regime and mitigate the performance loss incurred by the distillation process.
__label__diffusion_based_models In this paper, we make the first attempt to align diffusion models for image inpainting with human aesthetic standards via a reinforcement learning framework, significantly improving the quality and visual appeal of inpainted images.
__label__natural_language_processing We first build a scalable and parallelizable Android learning environment equipped with a VLM-based general-purpose evaluator and then identify the key design choices for simple and effective RL in this domain.
__label__evaluation Motivated by the research gap and practical demands, in this paper, we make the first attempt to build a universal attacker against real-world LVLMs, focusing on two critical aspects: (i) restricting access to only the LVLM inputs and outputs.
__label__speech_and_audio We introduce SpeechAlign, an iterative self-improvement strategy that aligns speech language models to human preferences.
__label__generative_models Experimental results demonstrate that our method generates high-quality multi-view videos, exhibiting vivid motions, temporal coherence, and multi-view consistency, given a variety of text prompts.
__label__machine_vision Existing methods often bypass this ambiguity using dataset-specific priors.
__label__machine_learning_for_other_sciences_and_fields Despite their significant progress, we argue that their performance has been limited by the simple adoption of the design convention for forward adaptation: using only a single type of hyper latent representation, which does not provide sufficient contextual information, especially in the first modeling step.
__label__deep_learning_architectures To address this limitation, this paper proposes a new instance normalization solution, called frequency adaptive normalization (FAN), which extends instance normalization in handling both dynamic trend and seasonal patterns.
__label__machine_learning_for_other_sciences_and_fields The DEQH model inherently captures the self-consistency nature of Hamiltonian, a critical aspect often overlooked by traditional machine learning approaches for Hamiltonian prediction.
__label__diffusion_based_models Diffusion models have achieved remarkable advancements in text-to-image generation.
__label__machine_learning_for_physical_sciences While most current machine learning models focus on deterministic forecasts, accurately capturing the uncertainty in the chaotic weather system calls for probabilistic modeling.
__label__probabilistic_methods Intuitively, the size of the prediction set encodes a general notion of uncertainty, with larger sets associated with higher degrees of uncertainty.
__label__reinforcement_learning We argue that these properties are satisfied in many continuous state-action Markov decision processes.
__label__optimization The results conform to the theoretical analysis and show the significant potential of BDNE for real-world application.
__label__reinforcement_learning The discriminator serves as the fitness function, guiding the evolution of the physical parameter distributions.
__label__optimization We present a generalization of Nesterov's accelerated gradient descent algorithm.
__label__optimization_for_deep_networks As the size of such formulations grows quadratically in the number of weights, quickly becoming intractable for large networks, we apply the Burer-Monteiro approach and only optimize over linear-size low-rank SDP solutions.
__label__interpretability_and_explainability Existing entropy coding techniques fail to effectively leverage the mechanisms underlying the data generation process.
"__label__reinforcement_learning Prior work towards this objective has been largely restricted to perturbation-based data augmentation where new data points are created by perturbing the original ones,
which has been impressively effective for tasks where the RL agent observe control states as images with perturbations including random cropping, shifting, etc."
__label__generative_models To address these limitations, we propose VideoTetris, a novel framework that enables compositional T2V generation.
__label__optimization AGNES requires only two parameters in convex and three in strongly convex minimization tasks, improving on existing methods.
__label__active_learning We establish convergence rate results for AWS for smooth convex training loss functions.
__label__safety_in_machine_learning This environment enables the study of how ethical considerations, strategic planning, and negotiation skills impact cooperative outcomes.
__label__learning_theory We show that when the data distribution lies in the model class and the log-loss is minimized, the number samples required to ensure validity has a weak dependence on the validity requirement.
__label__causal_inference We discuss when these assumptions are appropriate.
__label__machine_vision First, this framework integrates feature-level information integration into the diffusion process, allowing adaptive degradation removal and multi-modal information fusion.
__label__interpretability_and_explainability This work targets ante hoc interpretability, and specifically Concept Bottleneck Models (CBMs).
__label__algorithmic_game_theory We consider the problem of online fair division of indivisible goods to players when there are a finite number of types of goods and player values are drawn from distributions with unknown means.
__label__optimization Optimization of convex functions under stochastic zeroth-order feedback has been a major and challenging question in online learning.
__label__probabilistic_methods We design a walk that mixes in $\widetilde O((nd+dL^2R^2)\log(w/\delta))$ steps with a per iteration cost of $\widetilde O(n^\omega+n^2d^{3\omega-5})$.
__label__machine_vision Concretely, the generative part denoises noisy text descriptions via a decoding process, i.e., proposes text candidates, then utilizes inter-modal and intra-modal information to vote for the best.
__label__natural_language_processing Our preliminary analysis reveals that while generated rationales improve answer accuracy, inconsistencies emerge between the model’s internal representations in middle layers and those in final layers, potentially undermining the reliability of their reasoning processes.
__label__machine_vision In addition, DI-MaskDINO also obtains +1.0 $AP^{box}$ improvement compared to SOTA object detection model DINO and +3.0 $AP^{mask}$ improvement compared to SOTA segmentation model Mask2Former.
__label__online_learning It should be noticed that all our findings match existing bounds for the SEA model without the regularizer, which implies that there is \textit{no price} in regret bounds for the benefits gained from the regularizer.
__label__fairness Most of the literature in fair machine learning focuses on defining and achieving fairness criteria in the context of prediction, while not explicitly focusing on how these predictions may be used later on in the pipeline.
__label__machine_vision In addition, we design language hierarchical prompt generation that introduces language hierarchy into prompt generation which helps bridge the vocabulary gaps between training and testing.
__label__safety_in_machine_learning And by integrating with existing methods, the classification performance can be significantly improved on noisy datasets, typically by 22.8% on 80% symmetric CIFAR-10 with M-correction.
__label__machine_vision Our source code can be found in the supplementary material.
__label__reinforcement_learning Imbalanced saliency is a phenomenon where an RL agent disproportionately identifies salient features across consecutive frames in a frame stack.
__label__optimization This setting has one of the simplest algorithms with a point prediction, but what happens if the prediction is a distribution?
__label__machine_learning_for_healthcare To address these limitations, we propose a novel framework called Med-MICN (Medical Multi-dimensional Interpretable Concept Network).
__label__machine_vision Our empirical results demonstrate state-of-the-art novel view synthesis peformances in both novel view rendered color maps quality and depth maps accuracy across different numbers of input views.
__label__generative_models In this paper, we introduce a novel conditional generative modeling approach to produce trajectories toward high-scoring regions.
__label__generative_models Numerous efficient techniques, including weight pruning, quantization, and distillation, have been embraced to compress LLMs, targeting memory reduction and inference acceleration, which underscore the redundancy  in LLMs.
__label__natural_language_processing Empirical results show consistent and significant performance gains afforded by a single-round structurization.
__label__robotics Moreover, the EMVP pipeline can further enhance fine-tuning performance in terms of accuracy and efficiency.
__label__reinforcement_learning Our experimental results on several ONUW game settings demonstrate the effectiveness and generalizability of our proposed framework.
__label__machine_vision Extensive experiments demonstrate the superiority of our model in video salient object ranking and the effectiveness of the video retargeting method.
__label__optimization Beyond the standard bilevel optimization formulation, we extend our discussion to conditional bilevel optimization and also two special cases: minimax and compositional optimization.
__label__optimization_for_deep_networks Numerical experiments illustrate our results and connect them to gradient descent with non-vanishing learning rate.
__label__machine_vision Our FleVRS represents a significant step towards a more intuitive, comprehensive, and scalable understanding of visual relationships.
__label__robotics Large language models (LLMs) have shown great promise at generating robot programs from natural language given domain-specific robot application programming interfaces (APIs).
__label__speech_and_audio Furthermore, when such approaches are incorporated with visual rendering, audio generation at each viewpoint occurs after the rendering of the image of the viewpoint and thus could lead to audio lag that affects the integration of audio and visual streams.
__label__machine_vision In this work, we propose DoGaussian, a method that trains 3DGS distributedly.
"__label__other It also outperforms the best commercial tool GPTZero that is based on
 a commercial LLM trained with an enormous volume of data."
__label__deep_learning_architectures We identify a key axis of matrix parameterizations termed *sequence alignment*, which increases the flexibility and performance of matrix mixers, providing insights into the strong performance of Transformers and recent SSMs such as Mamba.
__label__natural_language_processing One way of demystifying transformer predictions would be to describe how they depend on their context in terms of simple template functions.
__label__diffusion_based_models Such capabilities are difficult to learn solely from task-specific data.
__label__diffusion_based_models Recently, diffusion models have garnered significant interest in the field of text processing due to their many potential advantages compared to conventional autoregressive models.
__label__other Code is available at https://github.com/msgwak/LAST.
__label__machine_vision The code is available at https://github.com/Sam1224/HMNet.
__label__safety_in_machine_learning In this work, we introduce a novel perspective, i.e., employing different common corruptions on the input space, to expand that.
"__label__causal_inference While sound and complete algorithms exist to compute causal effects, many of them assume access to conditional likelihoods,
 which is difficult to estimate for high-dimensional (particularly image) data."
__label__machine_learning_for_healthcare Moreover, we introduce a reparameterization perspective on Markov bridge models, from which we derive a simplified loss function that facilitates more effective training.
__label__graph_neural_networks Graph-AEs for GLAD regard a graph with a high mean reconstruction error (i.e.
__label__natural_language_processing We then extend the scale of the experiments to real persons with hundreds of statements and reach a consistent conclusion.
__label__online_learning Specifically, in stochastic $N$-agent $K$-armed bandits, we develop an algorithm with $\widetilde{\mathcal{O}}(K^{\frac{2}{N}}T^{\frac{N-1}{N}})$ regret and prove that the dependence on $T$ is tight, making it a sharp contrast to the $\sqrt{T}$-regret bounds of Hossain et al.
__label__algorithmic_game_theory Furthermore, even for the cases for which a polynomial time algorithm for exact equilibria was already known, our framework provides a conceptually simpler solution.
__label__reinforcement_learning Recent works have studied the IRL problem from the perspective of recovering the *feasible reward set*, i.e., the class of reward functions that are compatible with a unique optimal expert.
__label__other NoMAD-Attention works with pre-trained attention-based LLMs without model finetuning.
__label__deep_learning_architectures To realize the theoretical advantages of SNNs in energy efficiency, it is essential to deploy them onto neuromorphic chips.
__label__optimization_for_deep_networks By analyzing the loss landscape of a single Transformer layer using Softmax and Gaussian attention kernels, our work provides concrete answers to these questions.
__label__safety_in_machine_learning These shifts give rise to challenges in OOD generalization and OOD detection.
__label__reinforcement_learning Prior research has shown that borrowing the complete state information can enhance sample efficiency.
__label__generative_models Existing dynamic scene generation methods mostly rely on distilling knowledge from pre-trained 3D generative models, which are typically fine-tuned on synthetic object datasets.
__label__deep_learning_architectures Transformer-based trackers have established a dominant role in the field of visual object tracking.
__label__reinforcement_learning To evaluate performance, we create a new environment, Education Dialogue, where a teacher agent  guides  a  student  in  learning  a  random  topic,  and  show  that  a  deep  RL variant of our algorithm outperforms RLHF baselines.
__label__interpretability_and_explainability In this work, we propose a functional SSN method that retains the advantageous properties of classical functional regression approaches while also improving scalability.
__label__graph_neural_networks Despite the significant progress of pre-trained graph neural networks, there haven’t been GFMs that can achieve desired performance on various graph-learning-related tasks.
__label__other [Forrow et al.
__label__safety_in_machine_learning Our extensive experiments on four LLMs demonstrate that JAM achieves higher jailbreak success ($\sim$ $\times$ 19.88) and lower filtered-out rates ($\sim$ $\times$ 1/6) than baselines.
__label__interpretability_and_explainability The key idea is to treat the data as discrete samples from an underlying physical field described by differential equations and solve an inverse problem to identify the governing equation coefficients exhibiting more compressible numeric representations.
__label__reinforcement_learning We experimentally validate our methods using urban traffic and standard continuous control benchmarks.
__label__optimization traffic models, database systems, large ML models) often entails intense computations and results in long response times.
__label__neuroscience_and_cognitive_science Recurrent Neural Networks (RNNs) are commonly used to model neural dynamics thanks to their nonlinear characteristics.
__label__machine_learning_for_other_sciences_and_fields To model shifts of these causal models, we use a secondary SCM, that specifies changes in the primary model parameters.
__label__safety_in_machine_learning Our code is available at https://github.com/zhyblue424/TGA-ZSR.
__label__learning_theory This algorithm is proven to achieve a regret of $\tilde{O} (\sqrt{T \dim_E(\mathcal{F}) \log N(\mathcal{F})})$ and a communication complexity of $\tilde{O} (M^2 \dim_E(\mathcal{F}))$, where $M$ is the total number of agents and $T$ is the number of rounds, while $\dim_E(\mathcal{F})$ and $N(\mathcal{F})$ are  the Eluder dimension and the covering number of function space $\mathcal{F}$ respectively.
__label__natural_language_processing Comprehensive experimental results on six downstream domains demonstrate the effectiveness and generalizability of our proposed D-CPT Law and Cross-Domain D-CPT Law.
__label__machine_vision This limitation leads to sub-optimal prompt selection and inadequate adaptation of pre-trained features for a specific task.
__label__online_learning Despite being a key challenge, task confusion lacks a theoretical understanding.
__label__machine_vision Noisy labels significantly impact the robustness of the model and may lead to serious accidents.
__label__machine_vision Specifically, besides using the lightweight adapter modules, we propose a token dispatcher to distinguish informative tokens from less important ones, allowing the latter to dynamically skip the original block, thereby reducing the redundant computation during inference.
__label__machine_vision We introduce FlexCap, a vision-language model that generates region-specific descriptions of varying lengths.
__label__optimization Importantly, our analyses take into account the cost of prediction.
"__label__generative_models From that, we construct an additional Low-Rank Residual Attention (LoRRA) block that acts as the ""modality learner"" to expand the learnable space and compensate for the attention shift."
__label__interpretability_and_explainability We present evidence of *learned look-ahead* in the policy and value network of Leela Chess Zero, the currently strongest deep neural chess engine.
__label__safety_in_machine_learning Train-time data poisoning attacks threaten machine learning models by introducing adversarial examples during training, leading to misclassification.
__label__neuroscience_and_cognitive_science To illustrate our approach, we consider neural recordings with distinct excitatory (E) and inhibitory (I) populations.
__label__generative_models We attribute this to the compromise of multi-view information propagation in favor of adopting powerful yet computationally intensive architectures (\eg, Transformers).
__label__optimization_for_deep_networks Under a suitable correlated design assumption, we prove that both implement 1-step preconditioned gradient descent.
__label__machine_learning_for_other_sciences_and_fields While reinforcement learning (RL) has been utilized in those tasks, off-policy selection (OPS) is pivotal to close the loop by offline evaluating and selecting policies without online interactions, yet current OPS methods often overlook the heterogeneity among participants.
__label__neuroscience_and_cognitive_science This raises the question of how accurately these multi-modal models can predict brain activity when participants are engaged in multi-modal stimuli.
__label__evaluation Specifically, we first extract the reasoning graphs of data points in current benchmarks and then perturb the reasoning graphs to generate novel testing data.
__label__natural_language_processing However, there remain gaps between current scaling studies and how language models are ultimately trained and evaluated.
"__label__generative_models To circumvent the issue, we propose a simple and effective approach called Local Perplexity Ranking (LPR), which replaces the ""noisy"" candidates with their nearest neighbors that are more likely to be clean."
__label__diffusion_based_models After careful analysis, we discover three properties universal among diffusion models, enabling this study to go beyond specific models.
__label__diffusion_based_models Recent research in tabular data synthesis has focused on single tables, whereas real-world applications often involve complex data with tens or hundreds of interconnected tables.
__label__machine_vision We release code, data, and models at [https://github.com/scottgeng00/unmet-promise/](https://github.com/scottgeng00/unmet-promise).
__label__reinforcement_learning Evaluated in a set of challenging environments, DUSDi successfully learns disentangled skills, and significantly outperforms previous skill discovery methods when it comes to applying the learned skills to solve downstream tasks.
__label__bandits In many applications, however, it is sufficient to calculate only the top few outputs of the softmax function.
__label__probabilistic_methods We investigate replacing this de facto stopping rule with criteria based on the probability that a point satisfies a given set of conditions.
__label__machine_vision Our code is available at https://github.com/tcwangbuaa/MirrorCLIP
__label__reinforcement_learning However, the global coupling arising from agents’ safety constraints and the exponential growth of the state-action space size limit their applicability in instant communication or computing resource-constrained systems and larger multi-agent systems.
__label__neuroscience_and_cognitive_science The experimental results indicate that DARNet achieved excellent classification performance, particularly under short decision windows.
__label__safety_in_machine_learning We publish our code and results at https://anonymous.4open.science/r/Sandbagging-8305/README.md
"__label__safety_in_machine_learning Our study evaluates the feasibility of parameter extraction
methods of Carlini et al."
__label__interpretability_and_explainability Reinforcement learning from human feedback (RLHF) is widely used to train large language models (LLMs).
__label__optimization_for_deep_networks Deep learning succeeds by doing hierarchical feature learning, yet tuning hyper-parameters (HP) such as initialization scales, learning rates etc., only give indirect control over this behavior.
__label__natural_language_processing Safety of Large Language Models (LLMs) has become a central issue given their rapid progress and wide applications.
__label__natural_language_processing We use this guide to compress the Nemotron-4 family of LLMs by a factor of 2-4x, and compare their performance to similarly-sized models on a variety of language modeling tasks.
__label__machine_learning_for_social_sciences Moreover, we find that more advanced LLMs have a striking dependence on the domain that a real-world graph comes from --- by yielding the best recall accuracy when the graph is narrated in a language style consistent with its original domain.
__label__deep_learning_architectures Our constructions are based on the existence of $N$ nearly orthogonal vectors in $O(\log N)$ dimensional space and our lower bounds are based on reductions from communication complexity problems.
__label__machine_learning_for_physical_sciences We provide a theoretical justification of this under suitable conditions.
__label__machine_learning_for_other_sciences_and_fields Previous works either store a larger FP32 model to switch between different precision models for higher accuracy or store a smaller INT8 model but compromise accuracy due to using shared quantization parameters.
__label__causal_inference In this paper, we introduce a novel approach that achieves broad coverage of causal estimands beyond the SBD, incorporating various sum-product functionals like the FD, while maintaining scalability -- estimated in polynomial time relative to the number of variables and samples.
__label__graph_neural_networks The performance gain increases with a larger task graph size.
__label__optimization Our PIP integrates the Lagrangian multiplier as a basis to enhance constraint awareness and introduces preventative infeasibility masking to proactively steer the solution construction process.
__label__learning_theory One recent finding in this area is that the well-known Graves-Lai constant being zero is a necessary and sufficient condition for achieving bounded (or constant) regret in interactive decision-making.
__label__probabilistic_methods To address this issue, we present Symile, a simple contrastive learning approach that captures higher-order information between any number of modalities.
__label__machine_learning_for_healthcare However, current models, often developed by human experts, are limited by high cost, lack of scalability, and restriction to existing human knowledge.
__label__deep_learning_architectures As machine learning models in critical fields increasingly grapple with multimodal data, they face the dual challenges of handling a wide array of modalities, often incomplete due to missing elements, and the temporal irregularity and sparsity of collected samples.
__label__generative_models In this work, we present Vidu4D, a novel reconstruction model that excels in accurately reconstructing 4D (i.e., sequential 3D) representations from single generated videos, addressing challenges associated with non-rigidity and frame distortion.
__label__speech_and_audio In this work, we introduce a novel methodology for bridging the audiovisual modality gap by matching the distributions of tokens produced by an audio backbone and those of an image captioner.
__label__machine_vision Our analysis suggests that this underperformance is partially due to generator artifacts and inaccurate task-relevant visual details in the synthetic images.
__label__infrastructure Large Language Models (LLMs) are widely used in today's tasks of natural language processing.
__label__reinforcement_learning One such challenge is plasticity loss, wherein a neural network trained in an online fashion displays a degraded ability to fit new tasks.
__label__optimization_for_deep_networks Empirically, we conduct extensive experiments where, compared with existing GCIL methods, our GACL exhibits a consistently leading performance across various datasets and GCIL settings.
__label__online_learning To address the uncertainty in function types, recent progress in online convex optimization (OCO) has spurred the development of universal algorithms that simultaneously attain minimax rates for multiple types of convex functions.
__label__causal_inference In particular, we provide a counterexample showing that without Lipschitz regularization this method may not be asymptotically consistent.
__label__machine_learning_for_other_sciences_and_fields We open-source our model, paving the way for new multi-modal gene expression approaches.
__label__machine_learning_for_healthcare Code and resources are available at https://github.com/tsinghua-msiip/Uni-Med.
__label__generative_models Our approach significantly outperforms state-of-the-art methods, increasing the generation rate of stable materials by over three times and increasing the rate for stable, unique, and novel crystals by $\sim50$% – a huge improvement on a difficult problem.
__label__learning_theory Additionally, we never use more samples than the standard approaches require, even if the predictions provide no meaningful information (i.e.
__label__optimization_for_deep_networks We surprisingly find that   high precision models can be recovered from the  low precision local models with proper aggregation in the server.
__label__active_learning It is also common to disregard how noise in comparisons varies between item pairs, despite it being informative of item similarity.
__label__diffusion_based_models Advances in diffusion models for generative artificial intelligence have recently propagated to the time series (TS) domain, demonstrating state-of-the-art performance on various tasks.
__label__optimization It covers two families of problems that have been studied but are missing single-loop stochastic algorithms, i.e., difference of weakly convex functions and weakly convex strongly-concave min-max problems.
__label__diffusion_based_models Built upon the new factorization of the concrete score, we further prove a surprising result that the exact likelihood of absorbing diffusion can be rewritten to a simple form (named denoise cross-entropy) and then estimated efficiently by the Monte Carlo method.
__label__machine_vision Extensive experiments demonstrate that MambaLLIE significantly outperforms state-of-the-art CNN and Transformer-based methods.
__label__reinforcement_learning However, such hints may not be optimal, limiting the performance of learned policies.
__label__other Federated Learning (FL) has recently been applied to the parameter-efficient fine-tuning of Large Language Models (LLMs).
__label__interpretability_and_explainability This new method achieves state-of-the-art performance on local structure preservation for parametric methods without sacrificing the fidelity of global structural representation.
__label__diffusion_based_models To tackle this issue, this paper takes a further step with a much broader range of activations evaluated.
__label__optimization Finally, we provide empirical results that demonstrate that our method offers improved robustness to outliers and is computationally less demanding for regression and classification tasks.
__label__machine_vision However, these studies overlook that the degradations in video usually change over time, dubbed time-varying unknown degradations (TUD).
__label__machine_vision Our code is available at https://github.com/srvCodes/clap4clip.
__label__evaluation We leverage a unique dataset that captures the detailed performance of over 5M students across 8 college-entrance exams given over a span of two years in Brazil.
__label__learning_theory Finally, we implement our protocols and demonstrate empirical savings on the HPO-B benchmarks.
__label__other When the program is written in a differentiable logic programming language, techniques from neurosymbolic learning are applicable, but in general, the learning for neural programs requires estimating the gradients of black-box components.
__label__optimization Collaborative learning is an important tool to train multiple clients more effectively by enabling communication among clients.
__label__optimization_for_deep_networks However, existing FedPL methods create the same number of prototypes for each client, leading to cross-domain performance gaps and disparities for clients with varied data distributions.
__label__machine_vision We leverage large language models to generate corrective texts and utilize existing motion generation and editing frameworks to compile datasets of triplets (source motion, target motion, and corrective text).
__label__machine_vision SocialGPT introduces systematic design principles to adapt VFMs and LLMs separately and bridge their gaps.
__label__diffusion_based_models Diffusion-based text-to-video (T2V) models have achieved significant success but continue to be hampered by the slow sampling speed of their iterative sampling processes.
__label__privacy Our experiments show that the proposed DP optimizers with a low-pass filter outperform their counterparts without the filter on various models and datasets.
__label__machine_vision The core concept of DDIM inversion stems from the deterministic sampling technique of DDIM, which allows the DDIM process to be viewed as an Ordinary Differential Equation (ODE) process that is reversible.
__label__online_learning Embracing the framework of prediction with expert advice, we maintain a set of experts for each type of functions and aggregate their predictions via a meta-algorithm.
__label__neuroscience_and_cognitive_science The CTDS model defines separate latents for both cell types, and constrains the dynamics so that E (I) latents have a strictly positive (negative) effects on other latents.
__label__natural_language_processing Moreover, to account for uncertainty in the reward model we are distilling from, we optimize against a family of reward models, which may be instantiated either implicitly or explicitly.
__label__generative_models Moreover, we further introduce a new metric to better evaluate the performance of our method on multi-subject personalization.
__label__learning_theory However, conventional wisdom suggests the test error consists of approximation, bias, and variance errors, where the variance error increases with model size.
__label__natural_language_processing We thus propose an automated pipeline to collect a high-quality dataset for code explanation and refinement by generating a number of explanations and refinement trajectories from the LLM itself or a larger teacher model and filtering via execution verification.
__label__evaluation Our experiments span real-world vision, language, tabular and graph datasets.
__label__learning_theory [2018] to multiclass classification.
__label__machine_learning_for_other_sciences_and_fields We open source our implementation at https://github.com/Zun-Wang/DEQHNet.
__label__active_learning Ideally, learnability is reflected by ground truth consistency.
__label__natural_language_processing While Large Language Models (LLMs) have become immensely popular due to their outstanding performance on a broad range of tasks, these models are prone to producing hallucinations— outputs that are fallacious or fabricated yet often appear plausible or tenable at a glance.
"__label__optimization_for_deep_networks This characteristic is strongly correlated to a network's initialization state, establishing criterion autonomy from the learning state ($\textit{stateless}$) and thus setting a new fundamental basis for the expansion of compression strategies in regards to the ""When to Prune"" question."
__label__optimization_for_deep_networks Zeroth-order loss landscape sharpness-aware minimization is a strong training regime improving model generalization in transfer learning compared with optimizer like SGD.
__label__generative_models Experimental results show that image autoregressive modeling with our tokenizer (DiGIT) benefits both image understanding and image generation with the next token prediction principle, which is inherently straightforward for GPT models but challenging for other generative models.
__label__optimization_for_deep_networks Under mild assumptions, we show that the training loss for a one-layer multi-head transformer converges linearly to a global minimum.
__label__reinforcement_learning Experimental results on the D4RL benchmark indicate that our method outperforms previous state-of-the-art baselines in most tasks, clearly demonstrate its superiority over behavior regularization.
__label__probabilistic_methods We then allow conserved quantities and associated symmetries to be learned directly from train data through approximate Bayesian model selection, jointly with the regular training procedure.
__label__machine_vision Most existing ZSTAD methods utilize a foreground-based approach, limiting the integration of text and visual features due to their reliance on pre-extracted proposals.
__label__machine_vision We propose 4DGF, a neural scene representation that scales to large-scale dynamic urban areas, handles heterogeneous input data, and substantially improves rendering speeds.
__label__optimization Empirically, we find that FDEs achieve the same recall as prior state-of-the-art heuristics while retrieving 2-5$\times$ fewer candidates.
__label__causal_inference After this powerful result, a key open problem faced by the community has been to relax these conditions: allow for coarser than perfect single-node interventions, and allow for fewer than $d$ of them, since the number of latent factors $d$ could be very large.
__label__natural_language_processing Existing pruning methods for compressing LLMs primarily focus on evaluating redundancies and removing element-wise weights.
__label__deep_learning_architectures Specifically, we introduce unimodal and cross-modal adapters as multiple experts to specialize in intra-modal and inter-modal information, respectively, and employ a lightweight router to dynamically allocate the weights of each expert according to the specific demands of each task.
__label__neuroscience_and_cognitive_science We provide model analysis of scaling, similarity preservation and convergence behavior as well as experiments demonstrating noise robustness, sub-integer resolution in representing position, and path integration.
__label__reinforcement_learning However, existing theoretical studies on AIL are primarily limited to simplified scenarios such as tabular and linear function approximation and involve complex algorithmic designs that hinder practical implementation, highlighting a gap between theory and practice.
__label__machine_vision To address this issue, we revisit the unique computational characteristics of SSMs and discover that naive application disrupts the sequential token positions.
__label__reinforcement_learning Empirical results on offline multi-objective and safe tasks demonstrate the capability of our framework to infer policies that align with real preferences while meeting the constraints implied by the provided demonstrations.
__label__learning_theory This qualitatively matches the best known guarantees in the agnostic model.
__label__optimization_for_deep_networks Sharpness Aware Minimization (SAM) enhances performance across various neural architectures and datasets.
__label__safety_in_machine_learning Conversely, the Stochastic Diffusion Model effectively places purified images on the data manifold but demands solving cumbersome stochastic differential equations, while its derivative, the Probability Flow Ordinary Differential Equation (PF-ODE), though solving simpler ordinary differential equations, still requires multiple computational steps.
__label__optimization_for_deep_networks Thus, we propose a novel Bayesian approach to DPS.
__label__machine_vision Image pyramids are commonly used in modern computer vision tasks to obtain multi-scale features for precise understanding of images.
__label__reinforcement_learning Thus, in this work, we propose to exploit the preceding information to enhance exploration and heterogeneity sequentially.
__label__interpretability_and_explainability We approach this problem by analyzing the probability landscape of their hidden representations in the two cases.
__label__machine_learning_for_physical_sciences In order to learn effectively in this setting, we introduce a shared embedding and normalization strategy that projects the fields of multiple systems into a shared embedding space.
__label__diffusion_based_models Additionally, we train a warping module to align the hair color with the target region.
__label__generative_models Current PEFT methods for LLMs can achieve high quality, efficient training, or scalable serving, but not all three simultaneously.
__label__reinforcement_learning To address this, we formulate the ROI maximizing reinforcement learning problem as a linear fractional programming.
__label__learning_theory Overly aggressive subsampling may adversely affect estimation efficiency, and optimal subsampling is essential to mitigate the information loss.
__label__safety_in_machine_learning Through extensive experiments, we demonstrate that RLbreaker is much more effective than existing jailbreaking attacks against six state-of-the-art (SOTA) LLMs.
__label__deep_learning_architectures However, the substantial costs associated with training these models often limit the number of unique model sizes that can be offered.
__label__interpretability_and_explainability Particularly, our one-for-all estimator achieves the fastest convergence rate on Beta Shapley values, including the well-known Shapley value, both theoretically and empirically.
__label__causal_inference Learning causal representations from observational and interventional data in the absence of known ground-truth graph structures necessitates implicit latent causal representation learning.
__label__learning_theory Looking forward, we comment on and empirically explore intermediate regimes of manifolds, which have heterogeneous features commonly found in real world data.
__label__privacy 3D Gaussian Splatting (3DGS) has already become the emerging research focus in the fields of 3D scene reconstruction and novel view synthesis.
__label__learning_theory Groups are subsets of the context space, and in fairness applications, they may correspond to subpopulations defined by expressive functions of demographic attributes.
__label__diffusion_based_models Recent advancements in Automatic Prompt Optimization (APO) for text-to-image generation have streamlined user input while ensuring high-quality image output.
__label__online_learning Our online algorithms build on classical algorithms such as UCB and FTPL, but require novel ideas to account for the asymmetric nature of this feedback and to deal with the vastness of the space of pricing curves.
__label__natural_language_processing We define a strong instance of the ReFT family, Low-rank Linear Subspace ReFT (LoReFT), and we identify an ablation of this method that trades some performance for increased efficiency.
__label__machine_vision Code and pre-trained models are available at: https://github.com/shallowdream204/DreamClear.
__label__infrastructure PaRO Data Parallelism (PaRO-DP) accelerates LLM training through refined model state partitioning and tailored training procedures.
__label__probabilistic_methods In this paper, we study the statistical and geometrical properties of the Kullback-Leibler divergence with kernel covariance operators (KKL) introduced by [Bach, 2022, Information Theory with Kernel Methods].
__label__machine_vision This approach, however, encounters significant limitations, i.e., feature deviation in the semantic domain and information loss in the spatial domain.
__label__privacy These measures allow us to place a variety of proposed privatization schemes---some differentially private, some not---on the same footing.
__label__infrastructure We can also achieve almost zero pipeline bubbles while maintaining the same activation memory as 1F1B.
__label__graph_neural_networks Our code is available at https://github.com/akulen/AlphaGateau.
__label__privacy Our approach exhibits several benefits, including provable complexity saving compared to retraining, and supporting sequential and batch unlearning.
"__label__fairness As such, outcomes such as “In 50% of the cases, no
one in group 1 gets the information, while everyone in group 2 does, and in the
other 50%, it is the opposite”, which always results in largely unfair outcomes,
are classified as fair by a variety of fairness metrics in the literature."
__label__generative_models Remarkably, for the first time, a GPT-style autoregressive model for images outperforms LDMs, which also exhibits substantial improvement akin to GPT when scaling up model size.
__label__other In response, we propose a novel approach named ConFrag, where we collectively model the regression data by transforming them into disjoint yet contrasting fragmentation pairs.
__label__machine_learning_for_other_sciences_and_fields However, such approaches are usually based on short-sighted heuristics (e.g., log probability or value function scores) that potentially lead to suboptimal or even distracting subgoals, preventing us from finding longer proofs.
__label__machine_vision In this work, we propose ReGS, which adapts 3D Gaussian Splatting (3DGS) for reference-based stylization to enable real-time stylized view synthesis.
__label__graph_neural_networks Graph Neural Networks (GNNs) have demonstrated strong performance in graph mining tasks due to their message-passing mechanism, which is aligned with the homophily assumption that adjacent nodes exhibit similar behaviors.
__label__generative_models This study demonstrates the potential to narrow the gap between information assessment and acquisition in VLMs, bringing their performance closer to humans.
__label__machine_vision The code and models are available at https://lewandofskee.github.io/projects/MambaAD.
__label__active_learning Prior research shows that the outcomes of learning dynamics, which comprise both the services' adjustments and users' service selections, hinge significantly on the initial conditions.
__label__neuroscience_and_cognitive_science In this study, we implemented MiSO with a factor analysis (FA) based alignment method, a convolutional neural network (CNN), and an epsilon greedy optimization algorithm.
__label__natural_language_processing rules) formed out of simple N-gram based statistics of the training data.
__label__natural_language_processing The architecture comprises three agents: planning agent, decision agent, and reflection agent.
__label__neuroscience_and_cognitive_science An analysis of how KHMs achieve optimal memory capacity, and identify corresponding necessary conditions.
__label__other Tensor multiplication with learned weight matrices is the fundamental building block in deep learning models.
__label__diffusion_based_models While those are viable solutions when the DM is developed and deployed in a secure and constantly monitored environment, they hold the risk of adversaries circumventing the safeguards and are not effective when the DM itself is publicly released.
__label__machine_learning_for_healthcare Based on this, we allocate variable-specific parameter spaces to capture variable-specific temporal patterns and generate a complete variable graph to measure medical correlations among variables.
__label__natural_language_processing Originally formalized with symbolic representations, syntactic trees may also be effectively represented in the activations of large language models (LLMs).
__label__interpretability_and_explainability Our work provides a general framework connecting the structure of training data to the geometric structure of activations inside transformers.
__label__reinforcement_learning Such formulations are applicable when there are separate sets of optimization criteria and constraints on a system's behavior.
__label__causal_inference This task, however, becomes challenging in areas like social sciences and online marketplaces, where treating one experimental unit can influence outcomes for others through direct or indirect interactions.
__label__learning_theory However, optimising generalisation bounds might not always be viable for tractable or computational reasons, or both.
__label__learning_theory This complexity crucially depends on the loss function.
__label__neuroscience_and_cognitive_science However, the computation of PID remains a challenging problem, often involving optimization over distributions.
__label__interpretability_and_explainability Our results indicate that some algorithmic capabilities are present in transformers (and accessible via appropriately structured inputs) even before these models are trained.
__label__natural_language_processing On the other hand, there is substantial noise existing in the news data leading to extracting effective information with difficulty.
__label__generative_models For instance, UniFL surpasses ImageReward by 17\% user preference in terms of generation quality and outperforms LCM and SDXL Turbo by 57\% and 20\% general preference with 4-step inference.
__label__deep_learning_architectures We apply model editing to address two active areas of research, Structured Pruning, and Selective Class Forgetting.
__label__interpretability_and_explainability The decomposed components represent the effective information from the source data, thus the gap between them reflects the \textit{Relative Dominability} (RD) of the uni-source data in constructing the fusion image.
__label__optimization_for_deep_networks It is crucial for accelerating network training and reducing data storage requirements.
__label__natural_language_processing Our method enables two options, the **knowledge-preserved adaptation** and the **instruction-previewed adaptation**.
__label__causal_inference Motivated by this, we propose a general reduction scheme that allows one to produce a sequence of estimates for the treatment effect via online learning to minimize the sequentially weighted estimation error.
__label__online_learning Among existing baselines, replay-based methods show competitive results but requires extra memory for storing exemplars, while exemplar-free (i.e., data need not be stored for replay in production) methods are resource friendly but often lack accuracy.
__label__privacy (2022) employs a direct sampling approach from the vast collection of $O(d^k)$ possible length-$k$ sequences, showing superior empirical accuracy compared to previous pure or approximate differentially private methods.
__label__safety_in_machine_learning *, Skip Gradient Method and Intermediate Level Attack, into gradient-based adversarial prompt generation and achieve significant performance gains without introducing obvious computational cost.
__label__optimization To address these limitations, this paper proposes SPARKLE, a unified single-loop primal-dual algorithm framework for decentralized bilevel optimization.
__label__other We train transformers with up to 270 million parameters on ChessBench via supervised learning and perform extensive ablations to assess the impact of dataset size, model size, architecture type, and different prediction targets (state-values, action-values, and behavioral cloning).
__label__safety_in_machine_learning Our method is training-free and annotation-free, and it maintains fast testing speed.
__label__optimization_for_deep_networks Leveraging the implicit regularization, we develop a resource-efficient SAM variant, balancedness-aware regularization (BAR), tailored for scale-invariant problems such as finetuning language models with LoRA.
__label__learning_theory We further discuss the sample complexity for a desired approximation accuracy.
__label__deep_learning_architectures The Gauss-Newton (GN) matrix plays an important role in machine learning, most evident in its use as a preconditioning matrix for a wide family of popular adaptive methods to speed up optimization.
__label__reinforcement_learning These selections serve as subgoals that indicate subtasks and guide policy.
__label__probabilistic_methods In this paper, we address ID and OOD calibration problems jointly.
__label__optimization_for_deep_networks The success of pretrain-finetune paradigm brings about the release of numerous model weights.
__label__natural_language_processing The code is available at \url{https://anonymous.4open.science/r/beta-DPO-EE6C}.
__label__machine_vision These biases are subsequently   subtracted from original image and text features separately, to render them domain-invariant.
__label__machine_vision However, existing methods usually append learnable prompt tokens solely with the category names to obtain textual features, which fails to fully leverage the rich context indicated in the textual category name.
__label__machine_vision We also present a novel dataset containing a wide range of real SPAD high-speed videos under various challenging imaging conditions.
__label__deep_learning_architectures However, the inherent heterogeneity of tabular features, combined with the scarcity of labeled data, presents a significant challenge in tabular few-shot classification.
__label__machine_vision Particularly, the proposed FADA consists of two branches, i.e., low- and high- frequency branches.
__label__online_learning M${}^{\natural}$-concave functions, a.k.a.
__label__speech_and_audio Experiments indicate that Frieren achieves state-of-the-art performance in both generation quality and temporal alignment on VGGSound, with alignment accuracy reaching 97.22\%, and 6.2\% improvement in inception score over the strong diffusion-based baseline.
__label__machine_vision We verify the effectiveness of our method, with experiments demonstrating that it achieves state-of-the-art performance on RID tasks.
"__label__natural_language_processing AMOR builds reasoning logic over a finite state machine (FSM)
that solves problems through autonomous executions and transitions over disentangled modules."
__label__active_learning REDUCR reduces the training data while preserving worst-class generalization performance.
__label__diffusion_based_models However, since the degraded images already include low-frequency information, starting from Gaussian white noise will result in increased sampling steps.
__label__reinforcement_learning To address this issue, we propose an improvement on the successful Dreamer V3 architecture, implementing Maximal Lyapunov Exponent regularisation.
__label__generative_models The growing safety concerns surrounding large language models raise an urgent need to align them with diverse human preferences to simultaneously enhance their helpfulness and safety.
__label__privacy Such guarantees are of particular importance for privacy accounting, i.e., tracking privacy over multiple iterations.
__label__probabilistic_methods However, interpretable CDE methods are understudied.
"__label__learning_theory In the infinite depth limit,
we study 'representation geodesics' $A_{p}$: continuous paths in
representation space (similar to NeuralODEs) from input $p=0$ to
output $p=1$ that minimize the parameter norm of the network."
__label__fairness We introduce a two-stage meta-learning framework in which the first stage involves the use of a Nash Bargaining Solution (NBS) to resolve hypergradient conflicts and steer the model toward the Pareto front, and the second stage optimizes with respect to specific fairness goals.
__label__safety_in_machine_learning Through graph Fourier transform (GFT), we observe a correlation between the corruption robustness of point cloud recognition models and their sensitivity to different frequency bands, which is measured by the GFT spectrum of the model’s Jacobian matrix.
__label__diffusion_based_models We introduce DMD2, a set of techniques that lift this limitation and improve DMD training.
__label__causal_inference In these cases, observed variables are represented as deterministic functions of their parental variables without noise.
__label__natural_language_processing Finally, we demonstrate that the tiny variations  in fractal parameters seen across LLMs improve upon perplexity-based bits-per-byte (BPB) in predicting their downstream performance.
"__label__probabilistic_methods Broadcasting on trees has been extensively studied in statistical physics, 
in computational biology in relation to phylogenetic reconstruction and in statistics and computer science in the context of block model inference,  and as a simple data model for algorithms that may require depth for inference."
__label__deep_learning_architectures Among ECOCs, the one-hot code has become the default choice in modern deep neural networks (DNNs) due to its simplicity in decision making.
__label__machine_learning_for_healthcare Particularly, in the segmentation task which requires dense visual features, **G2D surpasses existing models even with just 1% of the training data for finetuning, compared to 100% used by other models**.
__label__safety_in_machine_learning Specifically, our approach utilizes a feature memory bank to selectively cache discriminative features from test images, representing the targeted OOD distribution.
__label__optimization Altogether, such *transition constraints* necessitate a form of planning.
__label__machine_learning_for_other_sciences_and_fields Besides,  lower-order exercise latent representations obtained in shallow layers are not well explored when learning the student representation.
__label__generative_models Finally, S${^2}$FT performs in-place gradient updates on all selected submatrices.
__label__machine_learning_for_physical_sciences Recent years have witnessed the promise of coupling machine learning methods and physical domain-specific insights for solving scientific problems based on partial differential equations (PDEs).
__label__deep_learning_architectures Deep neural networks (DNNs) have showcased their remarkable precision in approximating smooth functions.
__label__reinforcement_learning Physical Human-Scene Interaction (HSI) plays a crucial role in numerous applications.
__label__interpretability_and_explainability We coin the term **Learned Feedback Pattern** (LFP) for patterns in an LLM's activations learned during RLHF that improve its performance on the fine-tuning task.
"__label__machine_learning_for_physical_sciences Code is available
https://github.com/shenoynikhil/ETFlow."
__label__machine_learning_for_physical_sciences Here we introduce Almost-Linear Recurrent Neural Networks (AL-RNNs) which automatically and robustly produce most parsimonious PWL representations of DS from time series data, using as few PWL nonlinearities as possible.
__label__machine_vision 3) Centered around the theme of multi-condition multi-LiDAR perception, we collect a 280,000-frame dataset from both clean and adverse conditions.
__label__natural_language_processing The advent of large language models (LLMs) has revolutionized the field of natural language processing, yet they might be attacked to produce harmful content.
__label__causal_inference To address this problem, most existing methods attempt to find proper valid instrumental variables (IVs) for the target causal effect by expert knowledge or by assuming that the causal model is a one-directional MR model.
__label__safety_in_machine_learning Risk control offers a distribution-free, post-hoc solution that tunes the EENN's exiting mechanism so that exits only occur when the output is of sufficient quality.
__label__machine_vision Then we jointly train a prompt generator, optimized to produce a prompt embedding that stays close to the aggregated summary while minimizing task loss at the same time.
__label__learning_theory Typically, federated learning of vision-language models employs prompt learning to reduce communication and computational costs, i.e., prompt-based federated learning.
__label__causal_inference While the general formulation consists of a bi-level optimization procedure, we derive an efficiently estimable analytical kernel-based estimator of the bounds for the causal effect, a query of key interest, and demonstrate the efficacy of our approach in confounded, multivariate, nonlinear synthetic settings.
__label__diffusion_based_models From a temporal perspective, we introduce the Timestep-friendly Binary Structure (TBS), which uses learnable activation binarizers and cross-timestep feature connections to address the highly timestep-correlated activation features of DMs.
__label__optimization_for_deep_networks To tackle this issue, we conducted the theoretical analysis to promote the effectiveness of contrast decoding.
__label__other Unlike its contrastive counterpart CLIP who contrast with a text encoder, SuperClass directly utilizes tokenized raw text as supervised classification labels, without the need for additional text filtering or selection.
__label__generative_models Vision Language Models (VLMs) have demonstrated remarkable performance in 2D vision and language tasks.
__label__reinforcement_learning It was recently proposed to frame this problem as a Markov Decision Problem (MDP) and use deep reinforcement learning to tackle scaling.
__label__natural_language_processing These profiles are fed back to the LLM, which then revises the code to reduce overhead.
__label__bandits We further explicate this geometry for Gaussian distributions of rewards, and provide a convex reformulation of the lower bound solvable with linear programming.
__label__optimization_for_deep_networks To remedy this, most robust fine-tuning methods aim to preserve the pre-trained features.
__label__machine_vision In this work, we introduce a simulator-conditioned scene generation framework called SimGen that can learn to generate diverse driving scenes by mixing data from the simulator and the real world.
__label__natural_language_processing Rigorous software testing is crucial for developing and maintaining high-quality code, making automated test generation a promising avenue for both improving software quality and boosting the effectiveness of code generation methods.
__label__machine_learning_for_other_sciences_and_fields While Reinforcement Learning and Multi-armed Bandits have shown promise in educational settings, existing works often assume the independence of learning content, neglecting the prevalent interdependencies between such content.
__label__optimization_for_deep_networks Motivated by the above findings, we propose FedCCFA, a federated learning framework with classifier clustering and feature alignment.
__label__reinforcement_learning This sample complexity bound is minimax optimal (up to logarithmic factors) in the case of the $1$-Wasserstein distance.
__label__machine_vision The HDR and LDR colors are then fed into two Parallel Differentiable Rasterization (PDR) processes to reconstruct HDR and LDR views.
"__label__other We measure the out-of-domain uncertainty in the prediction of Neural Networks using a statistical notion called ""Lens Depth'' (LD) combined with Fermat Distance, which is able to capture precisely the ""depth'' of a point with respect to a distribution in feature space, without any distributional assumption."
__label__learning_theory Prediction-powered inference (PPI) is a method that improves statistical estimates based on limited human-labeled data.
__label__reinforcement_learning We provide a variant that carefully truncates the progress of its iterates to improve the variance of new variance-reduced sampling procedures that we introduce to implement the steps.
__label__optimization We propose a stochastic Moreau envelope approximate gradient method dubbed SMAG, the first single-loop algorithm for solving these problems, and provide a state-of-the-art non-asymptotic convergence rate.
__label__online_learning We show that this approach guarantees a convergence rate of $\tilde{\mathcal{O}}(T^{-1/2})$ with high probability and has a near-optimal dependence on the game parameters when applied with the best theoretical choices of learning rates and sampling policies.
__label__machine_vision Furthermore, to deal with the occlusions in complex scenes, we propose to compensate for the occluded points while tracking.
__label__natural_language_processing Using this approach, we show the surprising predictability of complex scaling phenomena: we show that several emergent phenomena follow a smooth, sigmoidal behavior and are predictable from small models; we show that the agent performance of models such as GPT-4 can be precisely predicted from simpler non-agentic benchmarks; and we show how to predict the impact of post-training interventions like Chain-of-Thought and Self-Consistency as language model capabilities continue to improve.
__label__deep_learning_architectures Vision Transformers (ViTs) demonstrate remarkable performance in image classification through visual-token interaction learning, particularly when equipped with local information via region attention or convolutions.
__label__diffusion_based_models This work addresses the topic from two perspectives.
__label__machine_vision Despite with powerful generalization capability, the cloud model still cannot achieve error-free detection in a specific target domain.
"__label__probabilistic_methods Regarding CP robustness,
importance weighting can address covariate shifts,
but CP under joint distribution shifts remains more challenging."
__label__probabilistic_methods We use this insight to motivate two new EP variants, with updates that are particularly well-suited to MC estimation.
__label__machine_learning_for_other_sciences_and_fields In the methodological part, we propose a Categorical distribution with monotonicity and orderliness to model the mapping from label description degrees to ternary labels, which can serve as a loss function or as a probability distribution, allowing most existing label enhancement methods to be adapted to our task.
__label__speech_and_audio Speech serves as a ubiquitous input interface for embedded mobile devices.
__label__neuroscience_and_cognitive_science We investigate the ability of this CNN-based decoding technique to differentiate among neuronal populations from areas V1, V4, and IT, revealing distinct readout characteristics for each.
__label__natural_language_processing We experiment with abstention strategies to better estimate model confidence and decide when to ask questions, improving diagnostic accuracy by 22.3%; however, performance still lags compared to an (unrealistic in practice) upper bound with complete information upfront.
__label__deep_learning_architectures The first produces unbiased estimators of the original weights, while the second aims to minimize the squared approximation error.
__label__machine_learning_for_other_sciences_and_fields Furthermore, different instances exhibit diverse electricity consumption behavior.
__label__machine_vision This method utilizes 3D Gaussian representation and tile-based splatting techniques, bypassing the expensive neural field querying.
__label__reinforcement_learning However, normalization brings with it a subtle but important side effect: an equivalence between growth in the norm of the network parameters and decay in the effective learning rate.
__label__natural_language_processing We showcase LoReFT on eight commonsense reasoning tasks, four arithmetic reasoning tasks, instruction-tuning, and GLUE.
__label__causal_inference These designs combine the advantages of randomized experiments with the ability to adaptively revise treatment allocations based on data collected across multiple stages, enhancing estimation efficiency.
__label__generative_models In this way, the input of LLM does not require visual tokens, which reduces the length of the input sequence and greatly improves efficiency.
__label__algorithmic_game_theory In the setting with fully rational bidders, the seminal result of Myerson [1981] showed that revenue can be maximized by using a second-price auction with reserves.
__label__machine_vision The code is available at https://github.com/yoonkicho/BAU.
__label__learning_theory We prove that the weak recovery threshold of $1$-step QAOA matches that of $1$-step tensor power iteration.
__label__deep_learning_architectures Nevertheless, such methods often encounter challenges regarding the efficacy of feature extraction, data integrity, consistency of feature dimensions, and adaptability across various downstream tasks.
__label__robotics Exploiting the structural modularity specific to coded simulations, we propose to use a factored partially observable Markov decision process representation that allows us to reduce context dependence during each step of the generation.
__label__learning_theory Recently Aamand et al.
__label__machine_vision This method enables efficient and robust surface reconstruction from point clouds without the need for shape-specific training.
__label__machine_vision We propose a drop-in replacement for the position encoding of plain ViTs that restricts attention heads to fixed fields of view, pointed in different directions, using 2D attention masks.
__label__machine_vision We thus study a more practical C-GCD setting, which includes more new classes to be discovered over a longer period, without storing samples of past classes.
__label__other STDP allows SNNs to address classification tasks by combining unsupervised STDP for feature extraction and supervised STDP for classification.
__label__machine_learning_for_social_sciences On average, the system nears the crowd aggregate of competitive forecasters and, in a certain relaxed setting, surpasses it.
__label__machine_learning_for_other_sciences_and_fields In this work, we propose a systematic methodology for categorization, characterization, and pattern identification of such formulas.
__label__machine_vision Existing methods aim to alleviate the modality gap by employing semantic metrics constraints or auxiliary modal guidance.
__label__learning_theory Based on the upper bound, we obtain new algorithms for approximate RUM learning and variations thereof.
__label__reinforcement_learning It proves particularly valuable for solving CARP that has a higher complexity than the node routing problems (NRPs).
__label__machine_learning_for_physical_sciences We demonstrate diverse applicability and efficacy of UPTs in mesh-based fluid simulations, and steady-state Reynolds averaged Navier-Stokes simulations, and Lagrangian-based dynamics.
__label__machine_vision The resulting architecture is simple but significantly increases computation and memory costs, as it has to handle a large number of additional tokens in its input layer.
__label__optimization Notably, we do not limit the strength of performative effects but rather their direction, requiring only that classification becomes harder when deploying more accurate models.
__label__optimization In this paper, we introduce MUVERA (MUlti-VEctor Retrieval Algorithm), a retrieval mechanism which reduces multi-vector similarity search to single-vector similarity search.
__label__machine_vision We overcome these challenges thanks to three main insights: (i) synthetic pre-training with diverse enough data enables to learn reasonable characters localization in any script; (ii) modern transformer-based detectors can jointly detect a large number of instances and, if trained with an adequate masking strategy, leverage consistency between the different detections; (iii) once a pre-trained detection model with approximate character localization is available, it is possible to fine-tune it with line-level annotation on real data, even with a different alphabet.
__label__bandits Specifically, we consider the BOBW setting where the underlying environment could be either (oblivious) adversarial (i.e., the loss distribution can change arbitrarily over time) or stochastic (i.e., the loss distribution is fixed over time) and is unknown to the decision-maker a prior, and propose an algorithm that achieves a $T^{\frac{1}{1+v}}$-type worst-case (pseudo-)regret in the adversarial regime and a $\log T$-type gap-dependent regret in the stochastic regime, where $T$ is the time horizon.
__label__speech_and_audio In experiments, we investigate the performance of the proposed approach across multiple audio understanding and generation tasks, \textit{e.g.}
__label__machine_vision DEX incorporates additional spatial information from original images into input images through patch-wise even sampling and channel-wise stacking, effectively extending data across input channels.
__label__machine_vision To address this challenge, we develop DC-Gaussian based on the recent real-time neural rendering technique 3D Gaussian Splatting (3DGS).
__label__optimization Motivated by the recent progress of shuffling-based stochastic methods, we investigate the convergence of shuffling-based SEG in unconstrained finite-sum minimax problems, in search of convergent shuffling-based SEG.
__label__reinforcement_learning Using this implementation in Atari, we show that SoftTreeMax reduces the gradient variance by three orders of magnitude.
__label__interpretability_and_explainability Experiments on real data demonstrate that our method can learn more diverse, accurate, and concise rules.
__label__machine_learning_for_healthcare TFM leverages the flow matching technique from generative modeling to model time series.
__label__optimization In this work, we propose Federated Importance-Aware Submodel Extraction (FIARSE), a novel approach that dynamically adjusts submodels based on the importance of model parameters, thereby overcoming the limitations of previous static and dynamic submodel extraction methods.
__label__machine_learning_for_social_sciences We demonstrate these benefits in two large-scale spatial mobility ABMs in Washington, DC and Cambridge, UK.
__label__diffusion_based_models Our main result is to give a fine-grained characterization of the dynamics of guidance in two cases, (1) mixtures of compactly supported distributions and (2) mixtures of Gaussians, which reflect salient properties of guidance that manifest on real-world data.
__label__machine_vision Comprehensive experiments confirm our framework's advantage over state-of-the-art diffusion-based VSR techniques.
__label__machine_vision Synthetic Aperture Radar (SAR) object detection has gained significant attention recently due to its irreplaceable all-weather imaging capabilities.
__label__generative_models We formalize the Markovian diffusion processes of UDPM and demonstrate its generation capabilities on the popular FFHQ, AFHQv2, and CIFAR10 datasets.
__label__diffusion_based_models We propose a novel inference technique based on a pretrained diffusion model for text-conditional video generation.
__label__machine_vision We design MonoMAE, a monocular 3D detector inspired by Masked Autoencoders that addresses the object occlusion issue by masking and reconstructing objects in the feature space.
__label__learning_theory Our extensive empirical evaluations demonstrate the superior prediction set size performance of CPL compared to state-of-the-art methods across diverse real-world and synthetic datasets in classification, regression, and large language model-based multiple choice question answering.
"__label__deep_learning_architectures Meanwhile, its  ""parallel'' computation allows for the simultaneous exploration of different reasoning paths and benefits more robust and efficient execution of  operations that are mutually independent (e.g."
__label__optimization Therefore, our algorithm significantly reduces communication rounds due to better dependency in terms of $\epsilon$ and $\kappa$.
__label__speech_and_audio While various aspects of song generation have been explored by previous works, such as singing voice, vocal composition and instrumental arrangement, etc., generating songs with both vocals and accompaniment given lyrics remains a significant challenge, hindering the application of music generation models in the real world.
__label__deep_learning_architectures Sparse Mixture of Experts (SMoE) has become the key to unlocking unparalleled scalability in deep learning.
__label__natural_language_processing Hypothesizing that difficult queries are crucial to learning complex reasoning, we propose *Difficulty-Aware Rejection Tuning* (`DART`), a method that allocates difficult queries more trials during the synthesis phase, enabling more extensive training on difficult samples.
__label__reinforcement_learning In challenging robotics environments including navigating a maze with a multi-legged ant robot, manipulating objects with a robot arm on a cluttered tabletop, and rotating objects in the palm of an anthropomorphic robotic hand, CE$^2$ demonstrates superior efficiency in exploration compared to baseline methods and ablations.
__label__machine_learning_for_other_sciences_and_fields Code is available at https://github.com/andreimargeloiu/TabEBM.
__label__interpretability_and_explainability Entropy neurons are characterized by an unusually high weight norm and influence the final layer normalization (LayerNorm) scale to effectively scale down the logits.
__label__human-AI_interaction To address this, we introduce the MERR dataset, containing 28,618 coarse-grained and 4,487 fine-grained annotated samples across diverse emotional categories.
__label__learning_theory The main advantage of our maps $\varphi_\ell$ over random linear maps is that ours map point sets directly into the discrete cube $N^{-1/2}\{-1,1\}^N$ and so there is no additional step needed to convert the sketch to bits.
__label__reinforcement_learning Furthermore, we remark that $R^3M$ is versatile and can be extended to various preference optimization methods, including direct preference optimization (DPO).
__label__reinforcement_learning Procedurally generated environments such as Procgen Benchmark provide a testbed for evaluating the agent's ability to robustly learn a relevant skill, by situating the agent in ever-changing levels.
__label__machine_vision Visual foundation models have achieved remarkable results in zero-shot image classification and segmentation, but zero-shot change detection remains an open problem.
__label__graph_neural_networks Next, use only the active connections to train a wider network on a much sparser graph.
__label__other For example, we achieve a Wikitext2 perplexity of 6.7 on the LLaMA2-70B model for per-channel INT2 weight quantization without incurring any inference overhead.
__label__natural_language_processing While previous research has implicitly leveraged these hierarchies to enhance LMs, approaches for their explicit encoding are yet to be explored.
__label__generative_models However, its applicability to DiTs has not yet been explored and faces non-trivial difficulties due to the unique design of DiTs.
__label__infrastructure We also delineate the space of models for which exponential communication advantages hold by showing that they cannot hold for linear classification.
__label__machine_vision Existing paradigms either consider linear combinations of bidirectional flows or directly predict bilateral flows for given timestamps without exploring favorable motion priors, thus lacking the capability of effectively modeling spatiotemporal dynamics in real-world videos.
__label__diffusion_based_models Randomized Midpoint Method has been recently proposed as a better discretization of Langevin dynamics for sampling from strongly log-concave distributions.
__label__privacy The algorithm applies noisy Mirror Descent to a dual problem from relaxing the hard constraints for private shadow prices, and then uses the shadow prices to coordinate allocations in the primal problem.
__label__neuroscience_and_cognitive_science In contrast, animals leverage distribution changes to segment their stream of experience into tasks and associate them with internal task abstracts.
__label__machine_learning_for_healthcare Among them, MAIM can capture the interaction among modalities by learning the shared representation distribution of all modalities.
__label__algorithmic_game_theory This model is often too pessimistic and does not adequately represent real-world online selection processes.
__label__deep_learning_architectures – The key idea is to “adapt” the outputs of each neuron of the network to its context distribution.
__label__privacy We also study PA-DP supervised learning with \textit{unlabeled} public samples.
__label__other In the particular case of heterogeneously distributed CIFAR-10 dataset, Fens achieves up to a $26.9$% higher accuracy over SOTA OFL, being only $3.1$%  lower than FL.
__label__generative_models The experimental results show that our VLoRA achieves comparable performance on various benchmarks for MLLMs, while significantly reducing the computational costs for both training and inference.
__label__diffusion_based_models Typically, they either neglect to exploit the potential of existing extensive pretrained models, limiting their generative capacity, or they necessitate a dozens of forward passes starting from random noises, compromising inference efficiency.
__label__optimization_for_deep_networks Leveraging a fast SVD technique, PiSSA can be initialized in only a few seconds, presenting a negligible cost for transitioning from LoRA to PiSSA.
__label__deep_learning_architectures Dense Associative Memories are high storage capacity variants of the Hopfield networks that are capable of storing a large number of memory patterns in the weights of the network of a given size.
__label__diffusion_based_models Current distillation techniques often dichotomize into two distinct aspects: i) ODE Trajectory Preservation; and ii) ODE Trajectory Reformulation.
__label__deep_learning_architectures Similarly, on Structured pruning problems, we obtain 40.8% sparsity on ResNet50 on Imagenet, with only a 2.6% drop in accuracy with minimal fine-tuning.
__label__machine_learning_for_physical_sciences Furthermore, we present numerical experiments in support of our theoretical claims.
__label__machine_learning_for_other_sciences_and_fields In the process, we demonstrate a simple solution to the challenge of scaling up graph learning: an autoregressive approach that decomposes semantic routing into smaller ``next-edge'' prediction problems.
__label__generative_models Further, we also provide initial experiments on protein motif scaffolding and outperform reconstruction guidance methods.
__label__learning_theory Recent advances in large language models have shown capabilities that are extraordinary and near-superhuman.
__label__natural_language_processing Surprisingly, we find that most state-of-the-art preference-tuned models achieve a ranking accuracy of less than 60% on common preference datasets.
__label__infrastructure In this paper, we propose LSH-MoE, a communication-efficient MoE training framework using locality-sensitive hashing (LSH).
__label__deep_learning_architectures Our thorough empirical analysis demonstrates the effectiveness and reliability of the proposed approach across different modalities, model architectures and label shift intensities.
__label__machine_vision Our empirical findings reveal significant improvements in low-light object detection tasks, as well as promising results in both well-lit and over-lit scenarios.
__label__probabilistic_methods Furthermore, we prove that the spherical IFT  gradient flow enjoys the best of both worlds by providing the global exponential convergence guarantee for both the MMD and KL energy.
__label__machine_vision This work presents Depth Anything V2.
__label__learning_theory In PAC learning, empirical risk minimization (ERM) is known to be consistent.
__label__neuroscience_and_cognitive_science We investigate this question by using multiple unimodal and two types of multi-modal models—cross-modal and jointly pretrained—to determine which type of models is more relevant to fMRI brain activity when participants were engaged in watching movies (videos with audio).
__label__other Tools like Persistent Homology or the Euler Transform give a single complex description of the global structure of the point cloud.
__label__generative_models Importantly, the generated models are fully simulatable, i.e., can be seamlessly integrated into standard physics engines such as MuJoCo, broadening MIDGArD's applicability to fields such as digital content creation, meta realities, and robotics.
__label__interpretability_and_explainability To test this, we train probes to estimate the feedback signal implicit in the activations of a fine-tuned LLM.
__label__diffusion_based_models Empirically, RADD is up to 3.5 times faster while consistently achieving a better performance than the strongest baseline.
__label__optimization_for_deep_networks Sparse training stands as a landmark approach in addressing the considerable training resource demands imposed by the continuously expanding size of Deep Neural Networks (DNNs).
__label__learning_theory However, we show that if the latent variables are shared, then vanishing error is possible.
__label__machine_learning_for_social_sciences In this paper, we build upon recent developments in causal abstractions to develop a framework for learning interventionally consistent surrogate models for large-scale, complex simulation models.
__label__other In our study, we explore methods for detecting unwanted content lurking in visual datasets.
__label__other While ensembling deep neural networks has shown promise in improving generalization performance, scaling current ensemble methods for large models remains challenging.
__label__optimization In contrast to original quasar-convexity \citep{hinder2020near}, GQC allows an individual quasar-convex parameter $\gamma_i$ for each variable block $i$ and the smaller of $\gamma_i$ implies less block-convexity.
__label__deep_learning_architectures The synthetic data is deemed well-distilled only when all data points within the predictions are similar.
__label__diffusion_based_models By leveraging a smooth equivalence transformation, Resfusion determine the optimal acceleration step and maintains the integrity of existing noise schedules, unifying the training and inference processes.
__label__privacy Moreover, these experiments demonstrate the effectiveness of AdaSCP in defeating advanced distance-based defenses.
__label__online_learning Previous work proposed various federated algorithms without demonstrating their necessity, while we answer the question from a novel perspective of computational constraints.
__label__reinforcement_learning To explain this, we analyze the quality of the trained dynamics model.
__label__other We show that the proposed formulation extends the classical minimum entropy coupling framework by integrating a bottleneck, allowing for controlled variability in the degree of stochasticity in the coupling.
__label__reinforcement_learning We evaluate our attack in 6 environments spanning multiple domains and demonstrate significant improvements in attack success over existing methods, while preserving benign episodic return.
__label__probabilistic_methods In this work, we argue that Bayesian optimization algorithms with sparse GPs can more efficiently allocate their representational power to relevant regions of the search space.
__label__deep_learning_architectures Critical in this context is the *prior knowledge* accumulated from related tasks.
__label__machine_vision Recently, federated multi-view clustering (FedMVC) has emerged to explore cluster structures in multi-view data distributed on multiple clients.
__label__probabilistic_methods Our work constitutes a first step towards integrating techniques from Gittins index theory into Bayesian optimization.
__label__learning_theory We address both these challenges for natural function classes, including intersections of halfspaces and decision trees, and standard training distributions, including Gaussians.
__label__machine_learning_for_other_sciences_and_fields Our code is available online [https://github.com/Applied-Machine-Learning-Lab/G3](https://github.com/Applied-Machine-Learning-Lab/G3) for reproduction.
__label__probabilistic_methods We present a new random walk for uniformly sampling high-dimensional convex bodies.
__label__learning_theory The key problem in metric transforms, a mathematical theory used in geometry and machine learning, asks what functions transform pairwise distances in semi-metric space $M$ to semi-metric space $N$ for specified $M$ and $N$.
__label__learning_theory Learning a continuous-time process through snapshots, such as smart camera traps, is a central theme governing a wide array of online learning situations.
__label__natural_language_processing However, tokenization imposes several disadvantages, such as performance biases, increased adversarial vulnerability, decreased character-level modeling performance, and increased modeling complexity.
__label__diffusion_based_models However, progressively deeper stacked networks will intuitively cause numerical propagation errors and reduce noisy prediction capabilities on generative data, which hinders massively deep scalable training of vision generation models.
__label__safety_in_machine_learning We train a value function directly on the hidden states according to the Bellman equation, enabling gradient-based optimization to obtain the optimal control signals at test time.
__label__diffusion_based_models However, to date, there has been no work that considers them jointly to explore the modality alignment within.
__label__machine_vision D-Rep uses a state-of-the-art diffusion model (Stable Diffusion V1.5) to generate 40, 000 image-replica pairs, which are manually annotated into 6 replication levels ranging from 0 (no replication) to 5 (total replication).
__label__deep_learning_architectures The overall model behaves like a decoder-only Transformer, although YOCO only caches once.
__label__other We show that in this case traditional persistent homology becomes very sensitive to noise and fails to detect the correct topology.
__label__generative_models \textit{Model Parsing} defines the task of predicting hyperparameters of the generative model (GM), given a GM-generated image as the input.
__label__probabilistic_methods We propose Wasserstein gradient boosting, a novel extension of gradient boosting, which fits a new weak learner to alternative pseudo residuals that are Wasserstein gradients of loss functionals of probability distributions assigned at each input.
__label__reinforcement_learning In this work, we overcome these problems by presenting multimodal-foundation world models, able to connect and align the representation of foundation VLMs with the latent space of generative world models for RL, without any language annotations.
__label__optimization Computational optimal transport (OT) has received massive interests in the machine learning community, and great advances have been gained in the direction of entropic-regularized OT.
__label__generative_models Comprehensive experiments on real-world generative tasks ranging from image, text to biological domains further demonstrate that SFM achieves higher sampling quality and likelihood than other discrete diffusion or flow-based models.
__label__causal_inference By leveraging the recent result of Wienöbst et al.
__label__optimization We first propose a sufficient condition for the performative control problem to admit a unique PSC solution with a problem-specific structure of distributional sensitivity propagation and aggregation.
"__label__machine_vision In this paper, we rethink the role of blendfake in detecting deepfakes and formulate the process from ""real to blendfake to deepfake"" to be a $\textit{progressive transition}$."
__label__neuroscience_and_cognitive_science The extractor consolidates multi-level visual features into one network, simplifying integration with Large Language Models (LLMs).
__label__safety_in_machine_learning Certified adversarial robustness of large-scale deep networks has progressed substantially after the introduction of randomized smoothing.
__label__privacy Thus it can be used prior to training as a rough estimate of the final privacy leakage.
__label__deep_learning_architectures Tabular data is widely utilized in a wide range of real-world applications.
__label__online_learning As applications of this framework, we consider three major problems with a minimax regret of $\Theta(T^{2/3})$: partial monitoring, graph bandits, and multi-armed bandits with paid observations.
__label__machine_vision To further improve performance, we enhance the OT formulation by introducing two regularization terms.
"__label__fairness In the literature,
fairness is often quantified in terms of the expected outreach within individual
communities."
__label__online_learning In particular, we study the power of \emph{best-action queries}, which reveal beforehand the identity of the best action at a given time step.
__label__robotics This is particularly problematic in robotics, where each data point requires physical execution of actions in the real world.
__label__optimization In this work, we consider the problem of optimizing second-order smooth and strongly convex functions where the algorithm is only accessible to noisy evaluations of the objective function it queries.
__label__learning_theory In this paper, we study ICL of a nonlinear function class via transformer with nonlinear MLP layer: given a class of \textit{single-index} target functions $f_*(\boldsymbol{x}) = \sigma_*(\langle\boldsymbol{x},\boldsymbol{\beta}\rangle)$, where the index features $\boldsymbol{\beta}\in\mathbb{R}^d$ are drawn from a $r$-dimensional subspace, we show that a nonlinear transformer optimized by gradient descent (with a pretraining sample complexity that depends on the \textit{information exponent} of the link functions $\sigma_*$) learns $f_*$ in-context with a prompt length that only depends on the dimension of the distribution of target functions $r$; in contrast, any algorithm that directly learns $f_*$ on test prompt yields a statistical complexity that scales with the ambient dimension $d$.
__label__deep_learning_architectures Structured state-space models (SSMs) are gaining popularity as effective foundational architectures for sequential data, demonstrating outstanding performance across a diverse set of domains alongside desirable scalability properties.
__label__machine_vision CoFie is motivated by the theoretical analysis of local SDFs with quadratic approximation.
__label__optimization The influence maximization (IM) problem aims to identify a budgeted set of nodes with the highest potential to influence the largest number of users in a cascade model, a key challenge in viral marketing.
__label__diffusion_based_models Diffusion models face significant challenges when employed for large-scale medical image reconstruction in real practice such as 3D Computed Tomography (CT).
__label__machine_vision Multi-modal domain generalization (MMDG) requires that models trained on multi-modal source domains can generalize to unseen target distributions with the same modality set.
__label__bandits We consider maximizing an unknown monotonic, submodular set function $f: 2^{[n]} \rightarrow [0,1]$ with cardinality constraint under stochastic bandit feedback.
__label__machine_vision However, these methods are limited to small-scale, homogeneous data, i.e.
__label__deep_learning_architectures Group theory has been used in machine learning to provide a theoretically grounded approach for incorporating known symmetry transformations in tasks from robotics to protein modeling.
__label__machine_vision Understanding egocentric human-object interaction (HOI) is a fundamental aspect of human-centric perception, facilitating applications like AR/VR and embodied AI.
__label__optimization_for_deep_networks Through experiments with MLPs, ResNets and Vision Transformers, we empirically demonstrate that $\mu$P$^2$ is the first parameterization to achieve hyperparameter transfer of the joint optimum of learning rate and perturbation radius across model scales.
__label__machine_vision This method reconstructs the scene geometry and implicitly models the uncertainty of voxel-wise semantic labels by presenting multiple possibilities for voxels.
__label__optimization_for_deep_networks This is a problem where kernels (\emph{e.g.}
__label__privacy These web services allow fine-tuning a model between two parties: the client that provides the data, and the server that hosts the model.
__label__fairness This observation highlights the need for more regulatory oversight due to the potential for bias amplification, and to address this issue we introduce new notions of weak and strong business necessity, together with an algorithm for assessing whether these notions are satisfied.
__label__graph_neural_networks Being able to accurately predict the task IDs can help address this issue, but it is a challenging problem.
__label__causal_inference Finally, we validate our design using synthetic data from a clinical trial on cirrhosis.
__label__online_learning Away from the lazy regime, networks can be seen as ensembles of adaptive experts which improve over time.
__label__optimization Specifically, for almost surely stable policy-dependent dynamics, the PSC solution exists if the sum of the distributional sensitivities is small enough.
__label__optimization Variational regularization is a classical technique to solve statistical inference tasks and inverse problems, with modern data-driven approaches parameterizing regularizers via deep neural networks showcasing impressive empirical performance.
__label__machine_vision Image editing exploits this property by modifying the cross-attention between text and images to edit specific objects while preserving the remaining regions.
__label__generative_models Furthermore, our generated subnets can directly reduce the usage of GPU memory and achieve inference acceleration.
__label__machine_learning_for_healthcare For instance, our method shows higher label precision and recall compared to previous approaches.
__label__other In general, observing every entry would be necessary to uniquely identify a graph, however if we know the graph has a certain property some entries can be omitted - for example, only half the entries would be required for a symmetric graph.
__label__optimization We cast this problem as stochastic convex optimization with heavy tailed stochastic gradients, and prove that the widely used Clipped-SGD algorithm attains near-optimal sub-Gaussian statistical rates whenever the second moment of the stochastic gradient noise is finite.
__label__reinforcement_learning The capability of generating high-quality long-horizon trajectories makes it a promising research direction.
__label__diffusion_based_models Disentangled representation learning strives to extract the intrinsic factors within the observed data.
__label__diffusion_based_models Extensive experiments show that Magnet significantly improves synthesis quality and binding accuracy with negligible computational cost, enabling the generation of unconventional and unnatural concepts.
__label__interpretability_and_explainability E2e dictionary learning brings us closer to methods that can explain network behavior concisely and accurately.
__label__safety_in_machine_learning In particular, we adversarially increase the loss of backdoored models with respect to weights to activate the backdoor effect, based on which we can easily differentiate backdoored and clean models.
__label__machine_vision Unlike few-shot learning methods in natural images that can leverage the labels of each image, existing few-shot WSI classification methods only utilize a small number of fine-grained labels or weakly supervised slide labels for training in order to avoid expensive fine-grained annotation.
__label__optimization_for_deep_networks We provide a theoretical analysis of the algorithm, showing convergence, approximation and local descent guarantees.
__label__machine_vision Therefore, we propose the ${\bf R}$elationship ${\bf P}$rompt ${\bf M}$odule (${\bf RPM}$), which generates the relationship prompt that directs VLM to extract pixel-level semantic embeddings suitable for OVSS.
__label__privacy Furthermore, we realize our upper bounds using a practical algorithm and demonstrate its advantage in high-dimensional regimes compared to prior approaches.
__label__machine_learning_for_other_sciences_and_fields We do such unification in two levels: 1) Data-Level: We propose a unified block graph data form for all molecules, including the local frame building and geometric feature initialization.
__label__natural_language_processing We propose a new framework for probing for world representations through the lens of state abstraction theory from reinforcement learning, which emphasizes different levels of abstraction, distinguishing between general abstractions that facilitate predicting future states and goal-oriented abstractions that guide the subsequent actions to accomplish tasks.
__label__optimization Thus, weaker models which give imprecise results quickly can be advantageous, provided inaccuracies can be resolved using few queries to a stronger model.
__label__generative_models While Low-Rank Adaptation (LoRA) has proven beneficial for efficiently fine-tuning large models, LoRA fine-tuned text-to-image diffusion models lack diversity in the generated images, as the model tends to copy data from the observed training samples.
__label__machine_learning_for_healthcare Navigating the vast chemical space of druggable compounds is a formidable challenge in drug discovery, where generative models are increasingly employed to identify viable candidates.
__label__machine_vision Experimental results on synthetic and our collected real-world dataset demonstrate that SfPUEL significantly outperforms existing SfP and single-shot normal estimation methods.
__label__machine_learning_for_other_sciences_and_fields In conjunction with classical statistical approaches, ML-assisted analytical strategies have shown great promise in accelerating research findings.
__label__reinforcement_learning We also prove that the risk-sensitive optimal policy can be obtained by solving a soft Bellman equation, which reveals several equivalences between RCaI, MaxEnt control, the optimal posterior for CaI, and linearly-solvable control.
__label__reinforcement_learning We propose several key insights to facilitate the large-scale learning process.
__label__machine_vision Meanwhile, we design the channel-shuffle-fusion (CS-Fusion) to enhance feature fusion in skip connection.
__label__other Yet, constructing ontologies requires substantial manual effort.
__label__machine_learning_for_healthcare Experimental results on real-world EHR data demonstrate the efficacy of the proposed AutoDP framework.
__label__causal_inference Finally, we generate high-dimensional interventional samples from the MIMIC-CXR dataset involving text and image variables.
__label__machine_vision Our experiments show that TracksTo4D can reconstruct a temporal point cloud and camera positions of the underlying video with accuracy comparable to state-of-the-art methods, while drastically reducing runtime by up to 95\%.
__label__learning_theory Furthermore, we design a generalizable algorithm to estimate that solution and apply this algorithm to the COMPAS, Hatespeech, and ACSIncome datasets.
__label__machine_learning_for_physical_sciences We validate the efficacy of our approach on both pretraining and downstream tasks over a broad fluid mechanics-oriented benchmark.
__label__optimization We present a structured learning approach to find high-quality solutions to real-world districting problems in a few minutes.
__label__diffusion_based_models Although significant progress has been made in human video generation, most previous studies focus on either human facial animation or full-body animation, which cannot be directly applied to produce realistic conversational human videos with frequent hand gestures and various facial movements simultaneously.
__label__natural_language_processing We study how information propagates in decoder-only Transformers, which are the architectural foundation of most existing frontier large language models (LLMs).
__label__neuroscience_and_cognitive_science Experimentally, we provide thorough numerical results to back up theoretical findings.
__label__optimization To the best of our knowledge, CRONOS is the first algorithm which utilizes the convex reformulation to enhance performance on large-scale learning tasks.
__label__causal_inference However, in many instances within these applications, the process of generating interventional data is subject to noise: rather than data being sampled directly from the intended interventional distribution, interventions often yield data sampled from a blend of both intended and unintended interventional distributions.
__label__infrastructure Empirical results verify that LoRA-Inlaid outperforms existing state-of-the-art LLM serving systems by up to 1.58 times in terms of throughput, 1.76 times in terms of average latency, 2 times in terms of job completion time, and 10 times in terms of SLO Attainment, while maintaining the same level of model quality.
__label__machine_vision Notably, the emergence of Vision Transformers (ViTs) has further propelled these advancements.
__label__safety_in_machine_learning GREAT Score has several advantages: (1) Robustness evaluations using GREAT Score are efficient and scalable to large models, by sparing the need of running adversarial attacks.
__label__machine_learning_for_physical_sciences Beyond theoretical guarantees of RandNets as universal approximators, these models are quick to train, allowing the PinT solution of partial differential equations on a spatial mesh of up to $10^5$ points with minimal overhead, dramatically increasing the scalability of existing PinT approaches.
__label__reinforcement_learning We then conduct extensive experiments on benchmark datasets to show the strong performance of Diffusion-DICE.
__label__machine_vision Our method sets a new state-of-the-art on the main GazeFollow benchmark for localization and achieves competitive results in the recognition task on both datasets compared to the baseline, with 40% fewer parameters
__label__diffusion_based_models A key insight of many recent works is that the 3D geometric structure of molecules provides essential information about their physicochemical properties.
__label__reinforcement_learning Our PaMoRL framework is hardware-efficient and stable, and it can be applied to various tasks with discrete or continuous action spaces using a single set of hyperparameters.
__label__machine_learning_for_physical_sciences While promising, the proposed methods only scale to networks with a few thousand parameters due to the high computational cost to evaluate, store, and invert the curvature matrix.
__label__machine_learning_for_healthcare To this end, we present **s**ingle **c**ell, **Cell**-**o**ntology guided TFM (scCello).
__label__machine_learning_for_other_sciences_and_fields For classification and regression on tabular data, the dominance of gradient-boosted decision trees (GBDTs) has recently been challenged by often much slower deep learning methods with extensive hyperparameter tuning.
__label__machine_learning_for_physical_sciences In density functional theory, charge density is the core attribute of atomic systems from which all chemical properties can be derived.
__label__machine_vision In complex real-world scenarios where images span unknown classes and unseen domains, L-Reg consistently improves generalization, highlighting its practical efficacy.
__label__natural_language_processing We reveal some findings: (1) Teaching materials that make it easier for students to learn (via in-context learning) have clearer and more accurate logic; (2) Weak-to-strong generalization: LbT might help improve strong models by teaching weak models; (3) Diversity in students might help: teaching multiple students could be better than teaching a single student or the teacher alone.
__label__neuroscience_and_cognitive_science Model comparisons and ablation studies reveal that our design choices, including (\romannumeral1) temporal modeling based on region-level tokens by utilizing 1D depthwise convolution to fuse channels in the ventral sensorimotor cortex (vSMC) and superior temporal gyrus (STG) and (\romannumeral2) self-supervision through discrete codex-guided mask modeling, significantly contribute to this performance.
__label__generative_models In this paper, we identify three key flaws in the current design of Latent Consistency Models~(LCMs).
__label__machine_vision In this paper, we introduce a more practical Semi-Open Environment setting for open-set 3D object retrieval with hierarchical labels, in which the training and testing set share a partial label space for coarse categories but are completely disjoint from fine categories.
__label__machine_vision During inference, we introduce an entropy-based aggregation strategy to dynamically utilize the complementarity in the slow and fast learners.
__label__machine_learning_for_physical_sciences Classical techniques suffer from limited applicability or huge computational costs.
__label__machine_vision This boosts significantly the learning of complex domains which are characterised by a large number of classes and long-tail distributions.
__label__natural_language_processing Unexpectedly, on three benchmarks focused on response helpfulness (AlpacaEval 2.0, MT-Bench, Arena Hard Auto v0.1), JANUS also outperforms LLaMA 3 8B Instruct by a +4.0%p, +0.1%p, +3.0%p margin, underscoring that training with a vast array of system messages could also enhance alignment to the general public’s preference as well.
__label__reinforcement_learning Our experimental results demonstrate that DiffuserLite achieves a decision-making frequency of $122.2$Hz ($112.7$x faster than predominant frameworks) and reaches state-of-the-art performance on D4RL, Robomimic, and FinRL benchmarks.
__label__machine_vision To plug these intrinsic shortcomings, we devise SDSGG, a scene-specific description based OVSGG framework where the weights of text classifiers are adaptively adjusted according to the visual content.
__label__safety_in_machine_learning Second, we design CAA, an efficient evasion attack that combines our CAPGD attack and MOEVA, the best search-based attack.
__label__diffusion_based_models Training diffusion models for audiovisual sequences allows for a range of generation tasks by learning conditional distributions of various input-output combinations of the two modalities.
__label__privacy To prevent such infringement, we propose a novel image protection approach that embeds invisible geometry perturbations, termed ``geometry cloaks'', into images before supplying them to TGS.
__label__robotics In this work, we present VLMimic, a novel paradigm that harnesses VLMs to directly learn even fine-grained action levels, only given a limited number of human videos.
__label__learning_theory Additionally, our approach allows working with richer assumptions that result in more informative and potentially tighter bounds.
__label__algorithmic_game_theory We emphasise the solution concept (i.e.
__label__machine_vision To this end, we present MMStar, an elite vision-indispensable multi-modal benchmark comprising 1,500 samples meticulously selected by humans.
__label__diffusion_based_models The framework EvolveDiretor and the trained model Edgen will be fully open-sourced to benefit the downstream tasks.
__label__interpretability_and_explainability In this work, inspired by the architectural similarities in standard DNNs and B-cos networks, we propose ‘B-cosification’, a novel approach to transform existing pre-trained models to become inherently interpretable.
__label__interpretability_and_explainability Recent work has found that sparse autoencoders (SAEs) are an effective technique for unsupervised discovery of interpretable features in language models' (LMs) activations, by finding sparse, linear reconstructions of those activations.
__label__machine_vision Moreover, by combining Signed Distance Function (SDF) supervision with surface rendering, we directly learn to generate high-quality meshes without the need for complex multi-stage training processes.
__label__machine_learning_for_healthcare Additionally, we adapt the 3DGS initialization to account for tomography data properties, enhancing accuracy and efficiency.
__label__natural_language_processing Our findings reveal a substantial discrepancy between their general reasoning and abstract reasoning performances.
__label__diffusion_based_models This paper proposes a fast and general-purpose image restoration method.
__label__reinforcement_learning To our knowledge, MADiff is the first diffusion-based multi-agent learning framework, functioning as both a decentralized policy and a centralized controller.
__label__speech_and_audio We introduce Condition-Aware Self-Supervised Learning Representation (CA-SSLR), a generalist conditioning model broadly applicable to various speech-processing tasks.
__label__generative_models Ablation studies draw insights on the design of EBD by thoroughly analyzing its architecture, which includes the design of the loss function and the data corruption process.
__label__generative_models We further introduce alignment modules to align the latent spaces of layers from the pre-trained multi-view and the 2D video diffusion models, addressing the reused layers' incompatibility that arises from the domain gap between 2D and multi-view data.
__label__other By combining time series imputation with neural network models used for downstream tasks, the gain of different imputation strategies on downstream tasks is estimated without retraining, and the most favorable imputation value for downstream tasks is given by combining different imputation strategies according to the estimated gain.
__label__graph_neural_networks In this work, we introduce GraphTrail, the first end-to-end, global, post-hoc GNN explainer that translates the functioning of a black-box GNN model to a boolean formula over the (sub)graph level concepts without relying on local explainers.
__label__causal_inference Existing approaches to differentiable structure learning of directed acyclic graphs (DAGs) rely on strong identifiability assumptions in order to guarantee that global minimizers of the acyclicity-constrained optimization problem identifies the true DAG.
__label__machine_vision Lumen first promotes fine-grained vision-language concept alignment, which is the fundamental capability for various visual tasks.
__label__machine_learning_for_physical_sciences We introduce the spherical convolutional Wasserstein distance to more comprehensively measure differences between climate models and reanalysis data.
__label__deep_learning_architectures Crucially, we make additional off-diagonal elements in $M$ learnable, enabling a smooth trade-off between trainable parameters and expressivity—an aspect that distinctly sets our approach apart from previous works leveraging singular values.
__label__machine_learning_for_other_sciences_and_fields Moreover, ablation studies and few-shot experiments further substantiate the effectiveness of our model.
__label__machine_learning_for_healthcare Specifically, a boundary surface loss drives the initialization surface to the inner and outer boundaries, while an inter-surface normal consistency loss regularizes the pial surface in challenging deep cortical sulci regions.
__label__natural_language_processing (2) How can we prevent the original knowledge from catastrophic forgetting during transformation?
__label__diffusion_based_models This approach enables us to characterize how individual tokens and their interactions affect the model output.
__label__causal_inference We base our approach on the Algorithmic Markov Condition, by which we identify the true causal network as the one that minimizes the Kolmogorov complexity.
__label__natural_language_processing Direct Preference Optimization (DPO) is a widely used offline preference optimization algorithm that reparameterizes reward functions in reinforcement learning from human feedback (RLHF) to enhance simplicity and training stability.
__label__other In this work, we investigate the efficiency properties of data from both optimization and generalization perspectives.
__label__generative_models We begin by collecting PyX, a clean Python corpus of fully executable code samples with functional descriptions and test cases.
__label__learning_theory We study learning problems on correlated stochastic block models with two balanced communities.
__label__natural_language_processing Despite this, common approaches often rely on additional models or data, which increases costs and limits widespread adoption.
__label__diffusion_based_models When integrating the learned concept into new prompts, Textual Inversion tends to overfit the concept, while DreamBooth often overlooks it.
__label__machine_vision By recovering complete actions and resampling from these full sequences, we can generate strong augmentations for unseen domains.
__label__deep_learning_architectures We propose an algebraic geometric framework to study the expressivity of linear activation neural networks.
__label__optimization We present a novel bilevel optimization method that locally approximates the solution set of the lower-level problem using a cutting plane approach and employs an accelerated gradient-based update to reduce the upper-level objective function over the approximated solution set.
__label__optimization This paper addresses the fully dynamic version of the problem, where the point set undergoes continuous updates (insertions and deletions) over time.
__label__machine_vision The benchmark ComTQA has been open-sourced at https://huggingface.co/datasets/ByteDance/ComTQA.
"__label__neuroscience_and_cognitive_science We first prove that gradient noise creates a systematic motion (a ``Noether flow"") of the parameters $\theta$ along the degenerate direction to a unique initialization-independent fixed point $\theta^*$."
__label__machine_vision To address this challenge, we draw inspiration from human visual cognition, and propose the Visual Fourier Prompt Tuning (VFPT) method as a general and effective solution for adapting large-scale transformer-based models.
__label__optimization Second, a slightly modified version of our algorithm allows to prove standard robustness-consistency properties as well as improved guarantees when knowing a range for the error of the prediction.
__label__learning_theory The key elements of our algorithm are: i) a characterization of the instance hardness via LP basis, ii) an eliminating procedure that identifies one optimal basis of the primal LP, and; iii) a resolving procedure that is adaptive to the remaining resources and sticks to the characterized optimal basis.
__label__interpretability_and_explainability Surprisingly, with the addition of layer normalization, we show that a transformer with a constant number of layers can represent the in-context conditional empirical distribution, concurring with our empirical observations.
__label__machine_vision Furthermore, we apply an additional constraint to PEFT on the CLIP text encoder according to the hyperspherical energy principle, i.e., minimizing hyperspherical energy during fine-tuning preserves the intrinsic structure of the original parameter space, to prevent the destruction of the generalization ability offered by the CLIP text encoder.
__label__natural_language_processing Yet, traditional global pruning is impractical for LLMs due to scalability issues, while local pruning, despite its efficiency, leads to suboptimal solutions.
__label__reinforcement_learning We also show a simple extension of IsCiL for task unlearning scenarios.
__label__optimization This lower bound is tight and demonstrates the optimality of Freya PAGE in the large-scale regime, i.e., when $\sqrt{m} \geq n,$ where $n$ is \# of workers, and $m$ is \# of data samples.
__label__speech_and_audio A word/phoneme in the speech signal is represented by a segment of speech signal with variable length and unknown boundary, and this segmental structure makes learning the mapping between speech and text challenging, especially without paired data.
__label__optimization_for_deep_networks In this paper, we first observe the gradient of cross-entropy loss for the target node and training nodes with significant inconsistency, which indicates that directly fine-tuning the base model using the loss on the target node deteriorates the performance on training nodes.
__label__optimization Because the NCut problem is fractionally structured, the fractional programming (FP) based approach has worked its way into a new frontier.
__label__natural_language_processing The code and the dataset are available online.
__label__natural_language_processing Results show that CORY outperforms PPO in terms of policy optimality, resistance to distribution collapse, and training robustness, thereby underscoring its potential as a superior methodology for refining LLMs in real-world applications.
__label__machine_learning_for_physical_sciences Moreover, DCGD can be further improved by combining it with popular strategies for PINNs, including learning rate annealing and the Neural Tangent Kernel (NTK).
__label__learning_theory This suggests that the low-rank fast attention only works for functions approximable by polynomials.
__label__interpretability_and_explainability We delineate the failure modes, including the errors of influence function and the non-additive structure of the collective influence.
__label__neuroscience_and_cognitive_science These results illustrate the power of the CTDS model to provide more accurate and more biologically interpretable descriptions of neural population dynamics and their relationship to behavior.
__label__machine_learning_for_physical_sciences Here, we present a novel deep learning framework for estimating the underlying equations of motion from observed trajectories, a pivotal step in decoding such complex dynamics.
__label__fairness Intuitively, acquiring more data is a natural and promising approach to achieve this goal by reaching a better Pareto frontier of the fairness-accuracy tradeoff.
__label__generative_models First, they are generative: we can generate actions in the style of a player simply by conditioning on the player's style vector.
__label__reinforcement_learning Our empirical results confirm that this regularization approach not only stabilizes training but also accelerates convergence and improves performance on long-horizon prediction.
__label__other In this work, we study the problem of communicating multiple samples from an unknown probability distribution using as few bits as possible.
__label__natural_language_processing }$, Codex@1, GSM-COT, BBH-COT).
__label__machine_vision Therefore, unsupervised methods have emerged as a promising solution.
__label__optimization We analyze a sophisticated transformer model featuring relative positional embedding, multi-head softmax attention,  and a feed-forward layer with normalization.
__label__machine_vision In this paper, we propose a novel neural representation for human pose estimation called NerPE to achieve continuous heatmap regression.
__label__safety_in_machine_learning Taking advantage of controlling the training process, the defensive backdoor is designed to suppress the malicious backdoor effectively while remaining secret to attackers.
__label__machine_vision We note that the input text description typically already contains detailed information on how to localize the target object, and we also observe that humans often follow a step-by-step comprehension process (\ie, progressively utilizing target-related attributes and relations as cues) to identify the target object.
__label__deep_learning_architectures Additionally, we substantiate these new insights with empirical validations and mathematical arguments.
__label__deep_learning_architectures We conduct a series of experiments to demonstrate the practical applicability of our approach.
__label__diffusion_based_models Diffusion-based generative models have demonstrated their powerful performance across various tasks, but this comes at a cost of the slow sampling speed.
__label__reinforcement_learning Unlike existing methods, DDM explicitly distinguishes consistent and inconsistent information across modalities and treats them separately with a divide-and-conquer strategy.
__label__learning_theory We study the problem of PAC learning $\gamma$-margin halfspaces in the presence of Massart noise.
__label__graph_neural_networks We conduct thorough theoretical analyses of the stability and over-smoothing properties of CITRUS in response to domain-specific graph perturbations and graph spectra effects on the performance.
__label__machine_vision Our approach employs multi-scale visual features to effectively handle various font sizes within document images.
__label__machine_learning_for_physical_sciences Physics-Informed Neural Networks (PINNs) are infamous for being hard to train.
__label__reinforcement_learning We propose Disentangled Unsupervised Skill Discovery (DUSDi), a method for learning disentangled skills that can be efficiently reused to solve downstream tasks.
__label__machine_learning_for_healthcare The functional nature of deep networks do not guarantee that the predicted transformation is a local minima of the registration objective, the representation of the transformation (displacement/velocity field/affine) is fixed, and the networks are not robust to domain shift.
__label__optimization_for_deep_networks With lower FID, EDT-S, EDT-B, and EDT-XL attained speed-ups of 3.93x, 2.84x, and 1.92x respectively in the training phase, and 2.29x, 2.29x, and 2.22x respectively in inference, compared to the corresponding sizes of MDTv2.
__label__diffusion_based_models Inverse problems describe the process of estimating the causal factors from a set of measurements or data.
__label__fairness By leveraging corpus samples from publicly accessible outputs of advanced models such as ChatGPT, GPT-4 and Claude-3, DALD fine-tunes surrogate models to synchronize with unknown source model distributions effectively.
__label__algorithmic_game_theory In our model, biased agents form posterior beliefs that are a convex combination of their prior and the Bayesian posterior, where the more biased an agent is, the closer their posterior is to the prior.
__label__natural_language_processing To mitigate this issue, we propose  a novel Star-Agents framework, which automates the enhancement of data quality across datasets through multi-agent collaboration and assessment.
__label__generative_models AssetGen achieves 17% improvement in Chamfer Distance and 40% in LPIPS over the best concurrent work for few-view reconstruction, and a human preference of 72% over the best industry competitors of comparable speed, including those that support PBR.
__label__machine_learning_for_physical_sciences A key step is to convert 3D crystal structures into 1D sequences to be processed by LMs.
__label__natural_language_processing Code will be made public upon acceptence.
__label__privacy As a departure from existing encoder-decoder methods, which incorporate fixed binary codes as watermarks within latent representations, our approach introduces learnable watermarks directly into the original image data.
__label__generative_models We find that training on only 20 digit numbers with a single GPU for one day, we can reach state-of-the-art performance, achieving up to 99% accuracy on 100 digit addition problems.
__label__natural_language_processing To train LFMs, we obtain feedback from Large Language Models (LLMs) on visual trajectories verbalized to language descriptions.
__label__reinforcement_learning It not only maintains an $\mathcal{O}(1)$ computation and storage cost per episode but also achieves an improved regret of $\widetilde{\mathcal{O}}(dH^2\sqrt{K} + d^2H^2\kappa^{-1})$, nearly closing the gap with linear function approximation.
__label__generative_models This approach eliminates the need for discrete-valued tokenizers.
__label__machine_learning_for_physical_sciences Specifically, [Huang et al.
__label__other Semi-Supervised Learning (SSL) has become a preferred paradigm in many deep learning tasks, which reduces the need for human labor.
__label__natural_language_processing To this end, we propose a unified framework, LLM4EA, to effectively leverage LLMs for EA.
__label__optimization_for_deep_networks Through extensive computer vision benchmarks, we demonstrate that our method can adjust to complex distribution shifts with significant improvements over current state-of-the-art in data-scarce settings.
__label__reinforcement_learning Diffusion policy has shown a strong ability to express complex action distributions in offline reinforcement learning (RL).
__label__machine_vision We provide our code and relabelings for several popular segmentation datasets to the research community on our project page: https://andrehuang.github.io/renovate.
__label__generative_models In the unconditional generation tasks, we show remarkable mean improvements of $58.17$% over previous diffusion models in the short discriminative score and $132.61$% in the (ultra-)long classification scores.
__label__safety_in_machine_learning The Rashomon effect is a mixed blessing in responsible machine learning.
__label__safety_in_machine_learning On the other hand, using feature importance as a guidance to discard a subset of perturbation patches in each iteration, along with combining self-paced learning and progressively more sampled attacks, significantly enhances the transferability over attacks that use all perturbation patches.
__label__graph_neural_networks We then elucidate the role of the choice of aggregation and update functions, and derive the first general upper and lower bounds on the geometric complexity (i.e., the number of linear regions), establishing new results for popular architectures such as GraphSAGE and GIN.
__label__machine_vision In this paper, we present Deep Degradation Response (DDR), a method to quantify changes in image deep features under varying degradation conditions.
__label__machine_vision Through experiments, we interpret this phenomenon as a remedy for the ineffective target-domain attention caused by the query-key attention mechanism under large domain gaps.
__label__natural_language_processing Recent methodologies in LLM self-training mostly rely on LLM generating responses and filtering those with correct output answers as training data.
__label__interpretability_and_explainability Our method finds circuits in GPT-2 that use less than half the number of edges than circuits found by previous methods while being equally faithful to the full model predictions on standard circuit-finding tasks.
__label__natural_language_processing Our findings suggest that longer and more computationally intensive IT data may serve as superior sources of IT, offering valuable insights for future research in this area.
__label__machine_vision Code is available at: https://github.com/YearangLee/Ti-FAD.
__label__optimization In addition, the proposed versions allow for even larger stepsizes than those initially suggested in [MM20].
__label__algorithmic_game_theory These results complement the recent result by [Jin and Lu, 2022] showing that the price of anarchy of first-price auctions with traditional bidders is $1 - 1/e^2$.
__label__machine_vision Notably, our method directly processes the weights of the NeRF’s MLP to extract information about the represented objects without the need to render images or materialize 3D data structures.
__label__learning_theory We develop a unifying framework for information-theoretic lower bound in statistical estimation and interactive decision making.
__label__fairness To fill this gap, we provide a theoretical study on the inherent trade-off between CF and predictive performance in a model-agnostic manner.
__label__other One-shot Federated Learning (OFL) significantly reduces communication costs in FL by aggregating trained models only once.
__label__natural_language_processing Large language models (LLMs) have emerged as a cornerstone in real-world applications with lengthy streaming inputs (e.g., LLM-driven agents).
"__label__probabilistic_methods Our ""two-filter"" smoother integrates particle streams that are propagated forward and backward in time, while incorporating stratification and importance weights in the resampling step to provide low-variance gradient estimates for neural network dynamics and observation models."
__label__other As a plug-and-play framework, MolPeg realizes the perception of both source and target domain and consistently outperforms existing DP methods across four downstream tasks.
__label__probabilistic_methods Although many VI methods that take correlation into account have been proposed, these methods generally are not scalable enough to capture the correlation among data instances, which often arises in applications with graph-structured data or explicit constraints.
__label__machine_vision To be specific, (1) we first come up with a person-agnostic 3D TFG model as the base model and propose to adapt it into a specific identity; (2) we propose a static-dynamic-hybrid adaptation pipeline to help the model learn the personalized static appearance and facial dynamic features; (3) To generate the facial motion of the personalized talking style, we propose an in-context stylized audio-to-motion model that mimics the implicit talking style provided in the reference video without information loss by an explicit style representation.
__label__machine_vision Generating realistic and diverse layouts of furnished indoor 3D scenes unlocks multiple interactive applications impacting a wide range of industries.
__label__human-AI_interaction Accurate emotion perception is crucial for various applications, including human-computer interaction, education, and counseling.
__label__machine_vision Project page: https://fangjinhuawang.github.io/UniSDF.
__label__fairness We first propose a simple but effective method to cast an optimal but potentially unfair predictor into a fair one with a minimal loss of performance.
__label__causal_inference In recent years, an increasing emphasis has been placed on heterogeneity in treatment effects, leading to the development of various methods for estimating Conditional Average Treatment Effects (CATE).
__label__generative_models Specifically, using the same generator framework, TiTok attains **1.97** gFID, outperforming MaskGIT baseline significantly by 4.21 at ImageNet 256 × 256 benchmark.
__label__interpretability_and_explainability Finally, we demonstrate that failures at either stage can prevent a model from learning a generalizable solution to our fairly simple tasks.
"__label__machine_vision To further minimize the additional bitrate overhead introduced by the
entropy models, NVRC also compresses all the network, quantization and entropy
model parameters hierarchically."
__label__reinforcement_learning DM explicitly models the historical hidden state to extract the temporal information by using the mamba architecture.
__label__other It dynamically constructs a collaborative graph of learners by iteratively searching for optimal neighbors in a context-aware manner.
__label__reinforcement_learning Videos and code are available at https://weirdlabuw.github.io/dispo/.
__label__online_learning Moreover, by parallel learning multiple frames simultaneously, our HiCoM decreases the average training wall time to <2 seconds per frame with negligible performance degradation, substantially boosting real-world applicability and responsiveness.
__label__reinforcement_learning In this paper, we present a novel method to optimize the number of necessary multiplications for Jacobian computation by leveraging deep reinforcement learning (RL) and a concept called cross-country elimination while still computing the exact Jacobian.
__label__reinforcement_learning Experiments are used to validate the proposed algorithm.
__label__reinforcement_learning When optimal behavior requires exploration that sacrifices immediate reward to enable higher subsequent reward, existing state-of-the-art cumulative-reward meta-RL methods become stuck on the local optimum of failing to explore.
__label__human-AI_interaction Our experimental results demonstrate that Voila-A significantly outperforms several baseline models.
__label__interpretability_and_explainability It also perfectly recovers the ground-truth circuits in two models compiled with Tracr.
__label__evaluation We find that RR can mitigate the effects of dataset corruption due to both (heavy) labeling error and/or adversarial perturbation, demonstrating effectiveness across a variety of data domains and machine learning tasks.
__label__optimization We present a novel methodology for convex optimization algorithm design using ideas from electric RLC circuits.
"__label__neuroscience_and_cognitive_science In this work, we address these challenges by linking dynamic divisive normalization (DN) to the stability of ""oscillatory recurrent gated neural integrator circuits'' (ORGaNICs), a biologically plausible recurrent cortical circuit model that dynamically achieves DN and that has been shown to simulate a wide range of neurophysiological phenomena."
"__label__machine_vision Extensive experiments on large-scale datasets of unbounded scenes
demonstrate that 3DGS-Enhancer yields superior reconstruction performance and
high-fidelity rendering results compared to state-of-the-art methods."
__label__natural_language_processing Subsequently, the generated data undergo a rigorous evaluation using a dual-model method that assesses both difficulty and quality.
__label__diffusion_based_models We analyze that the diffusion process inherently possesses the time-varying information bottlenecks.
__label__graph_neural_networks Experimental results demonstrate that our method advances the state-of-the-art performance for several datasets and activation functions, highlighting the power of scaling symmetries as an inductive bias for NN processing.
__label__optimization_for_deep_networks Evaluations on Digit-5, Office-10, and DomainNet datasets demonstrate our method's superiority over existing approaches.
__label__other - $\ell_p$ for $p \ge 1$: $d = \tilde O(k^2)$ is sufficient and $d=\tilde \Omega(k)$ is necessary.
__label__machine_vision Current approaches to this problem were not designed to operate on casual videos recorded by standard cameras or require a long optimization time.
__label__machine_vision To tackle this problem, in this paper, we propose a novel approach to automatically discover the associated skeleton model for dynamic objects from videos without the need for object-specific templates.
__label__optimization_for_deep_networks Continuously adapting pre-trained models to local data on resource constrained edge devices is the \emph{last mile} for model deployment.
__label__machine_vision The audio editing is executed by predefined actions that filter noise or augment data.
__label__speech_and_audio In a representative comparison, we measure the total inference time for our model to be 2x faster than RNN-T and 16x faster than AED.
__label__diffusion_based_models However, handling non-i.i.d.
__label__graph_neural_networks In this work, we introduce a canonicalization perspective that provides an essential and complete view of the design of frames.
__label__deep_learning_architectures 2) In non-clean models (with weight errors), if the normalized distance exceeds a threshold, then non-clean DNNs can reach the clean model's accuracy as long as the code length approaches infinity.
__label__machine_learning_for_physical_sciences Until quantum computers can reliably execute large quantum programs, stakeholders will need fast and reliable methods for assessing a quantum computer’s capability—i.e., the programs it can run and how well it can run them.
__label__bandits However, similar to other neural network applications, neural bandit algorithms can be vulnerable to adversarial attacks or corruptions on the received labels (i.e., arm rewards), which can lead to unexpected performance degradation without proper treatments.
__label__machine_learning_for_physical_sciences Nevertheless, data of different molecular properties are often not aligned: some quantities, e.g.
__label__diffusion_based_models A critical prerequisite for their notable success lies in the presence of a substantial number of training samples, which can be impractical in real-world applications due to high collection costs or associated risks.
__label__machine_vision Our approach combines cross-modality contrastive learning to align representations across modalities with supervised foundation model pretraining and reinforcement learning to obtain highly effective navigation and localization policies.
__label__causal_inference This paper presents CaPS, an ordering-based causal discovery algorithm that effectively handles linear and nonlinear relations.
"__label__causal_inference Even though causal
models are extremely popular, conditional probability calculation of
formulas involving interventions pose significant challenges."
__label__optimization Finally, our theoretical results demonstrate effectiveness of proposed algorithms through numerical experiments.
__label__machine_learning_for_physical_sciences As a metric for the study, physical pymatgen matcher was employed: we compare target structure with generated one using default tolerances.
__label__machine_vision In this paper, we propose a federated learning framework for semantic segmentation without knowing the model architecture nor transferring gradients between the client and the server, thus enabling better privacy preservation.
__label__machine_vision Our approach leverages mutual guidance and joint supervision during the training process to mutually enhance reconstruction and rendering.
__label__interpretability_and_explainability Many training data attribution (TDA) methods aim to estimate how a model's behavior would change if one or more data points were removed from the training set.
__label__machine_vision Furthermore, it implements a comprehensive optimization strategy in the gradient matrix for shared parameters, ensuring convergence to an optimal fusion detection configuration.
__label__diffusion_based_models We then scale the same architecture to solve a large-scale text-to-image task and show state-of-the-art performance compared to the most recent public and commercial models.
"__label__causal_inference The paper also presents evidence that parameter identification 
is computationally hard in general."
__label__graph_neural_networks Evaluations across various datasets and tasks demonstrate the generality and efficiency of the proposed GOUDA over existing state-of-the-art GCLs.
__label__machine_vision We further provide an in-depth analysis of the gradient estimation error of various acceleration strategies as well as their impact on downstream tasks, offering valuable insights into the trade-offs between acceleration and performance.
__label__interpretability_and_explainability In addition, we demonstrate that GATSM finds interesting patterns in time series.
__label__machine_vision To address this issue, we conduct extensive experiments around pre-trained models, architecture choice, data, and training methods.
__label__fairness This yields a new decomposition of the disparity in the predictor $\widehat Y$ that allows us to disentangle causal differences inherited from the true outcome $Y$ that exists in the real world vs. those coming from the optimization procedure itself.
__label__reinforcement_learning Unsupervised skill discovery carries the promise that an intelligent agent can learn reusable skills through autonomous, reward-free interactions with environments.
__label__optimization_for_deep_networks We also conduct intensive numerical experiments, and verify that our ADOPT achieves superior results compared to Adam and its variants across a wide range of tasks, including image classification, generative modeling, natural language processing, and deep reinforcement learning.
__label__other As an illustration, we demonstrate an efficient size reduction of the network, by pruning those attention heads that are deemed less relevant by our theory.
__label__safety_in_machine_learning Despite the general capabilities of Large Language Models (LLMs) like GPT-4, these models still request fine-tuning or adaptation with customized data when meeting the specific business demands and intricacies of tailored use cases.
__label__machine_learning_for_physical_sciences Analyzing the motion of multiple biological agents, be it cells or individual animals, is pivotal for the understanding of complex collective behaviors.
__label__fairness These features are carefully selected to reflect potential model limitations or biases.
__label__diffusion_based_models However, when it comes to processing facial images, the outcomes frequently fall short of expectations.
__label__neuroscience_and_cognitive_science GOPSA exploits the geodesic structure of the Riemannian manifold to jointly learn a domain-specific re-centering operator representing site-specific intercepts and the regression model.
__label__graph_neural_networks Our empirical results demonstrate that we can efficiently generate problems that remain hard to solve and retain key attributes of the original example problems.
__label__machine_vision Over the past decade, mainstream methods in the VPR area have been to use feature representation based on global aggregation, as exemplified by NetVLAD.
__label__bandits However, these designs mainly focus on the generation strategy, while limited attention has been paid to the selection method.
__label__deep_learning_architectures Large language models (LLMs) have acquired the ability to solve general tasks by utilizing instruction finetuning (IFT).
__label__machine_vision Furthermore, we introduce modality joint learning to align features of all modalities, ensuring that textual features maintain stable semantic representation of overall pedestrian appearance during complementary information learning.
__label__online_learning To ensure favorable guarantees, we design a new Lipschitz-adaptive meta-algorithm, capable of handling potentially unbounded gradients while ensuring a second-order bound to effectively ensemble the base-learners.
__label__machine_vision +6.9% improvement in OVEN entity task), underscoring the importance of high-quality training data in this domain.
__label__probabilistic_methods Relative entropy coding (REC) algorithms encode a random sample following a target distribution $Q$, using a coding distribution $P$ shared between the sender and receiver.
__label__machine_vision Inspired by this, (i) we propose a Memory-based Vision-Language Tracker (MemVLT).
__label__optimization To address this issue, the concept of model heterogeneity through submodel extraction has emerged, offering a tailored solution that aligns the model's complexity with each client's computational capacity.
__label__machine_learning_for_other_sciences_and_fields Existing SBDD methods typically treat protein as rigid and neglect protein structural change when binding with ligand molecules, leading to a big gap with real-world scenarios and inferior generation qualities (e.g., many steric clashes).
__label__optimization_for_deep_networks Exploring the loss landscape offers insights into the inherent principles of deep neural networks (DNNs).
__label__machine_vision Experiments on self-captured and public dash cam videos show that our method not only achieves state-of-the-art performance in novel view synthesis, but also accurately reconstructing captured scenes getting rid of obstructions.
__label__other This shift often occurs in imbalanced classification since positive data are often more diverse and time-varying than negative data.
__label__other Systematic experiments demonstrate that starting from conventional conformal methods, our boosted procedure achieves substantial improvements in reducing interval length and decreasing deviation from target conditional coverage.
__label__deep_learning_architectures We apply different operations to these two representations: Convs to the grid for local features, and MHSAs to the slots for global features.
__label__natural_language_processing Our code is open-sourced at https://github.com/cxcscmu/MATES.
__label__natural_language_processing In this paper, we propose **CorDA**, a Context-oriented Decomposition Adaptation method that builds learnable **task-aware adapters** from weight decomposition oriented by the context of downstream task or the world knowledge to maintain.
__label__learning_theory 3.
__label__diffusion_based_models In this paper, we propose EMDiffusion, an expectation-maximization (EM) approach to train diffusion models from corrupted observations.
__label__causal_inference Unlike existing data generation methods, Frugal Flows generate synthetic data that closely resembles the empirical dataset, while also automatically and exactly satisfying a user-defined average treatment effect.
__label__generative_models Existing diffusion-based text-to-3D generation methods primarily focus on producing visually realistic shapes and appearances, often neglecting the physical constraints necessary for downstream tasks.
__label__other The effectiveness of the proposed method is shown with four real-world datasets.
__label__machine_vision The sparsely activated mixture of experts (MoE) model presents an effective alternative to densely activated (dense) models, combining improved accuracy with computational efficiency.
__label__generative_models These methods overcome the slowness of diffusion models by directly mapping noise to data, while maintaining a (relatively) simpler training.
__label__diffusion_based_models Finally, the experimental results on the real-word datasets show that the proposed method achieves competitive performance on time series imputation compared to the state-of-the-art methods.
__label__interpretability_and_explainability The state-of-the-art feature attribution methods often neglect the influence of unobservable confounders, posing a risk of misinterpretation, especially when it is crucial for the interpretation to remain faithful to the data.
__label__optimization_for_deep_networks Notably, the models trained by our method with the precision as low as 8 bits are  comparable  to those from the  full precision training.
__label__deep_learning_architectures This paper addresses the problem with two insights: 1) DVs may originate from intrinsic latent continuous variables (LCVs), which lose fine-grained information due to extrinsic discretization; 2) LCVs and CVs share similar temporal patterns and interact spatially.
__label__interpretability_and_explainability On the theoretical side, we prove that a transformer with $O(\log_2(k))$ layers can represent the in-context conditional empirical distribution by composing induction heads to track the previous $k$ symbols in the sequence.
__label__neuroscience_and_cognitive_science However, recent findings show that their firing patterns become distorted in the presence of significant spatial landmarks such as rewarded locations.
__label__optimization_for_deep_networks SGD performs worse than Adam by a significant margin on Transformers, but the reason remains unclear.
__label__reinforcement_learning To support general object rearrangement by physical humanoid, we introduce a novel Human-in-the-Room dataset encompassing various rearrangement tasks.
__label__optimization As a technical tool, we establish a restricted isometry property (RIP) restricted to a tangent space of the manifold of symmetric rank-$r$ matrices given random Euclidean distance  measurements, which might be of independent interest for the analysis of other non-convex approaches.
__label__diffusion_based_models These insights allow us to propose an unsupervised, single-step, training-free **LO**w-rank **CO**ntrollable image editing (LOCO Edit) method for precise local editing in diffusion models.
__label__machine_learning_for_other_sciences_and_fields Moreover, we contribute a comprehensive evaluation system encompassing five pedagogical dimensions for assessing the teaching quality of LLMs.
__label__bandits In this paper, we exploit the structure of stable solutions to devise algorithms that improve the likelihood of finding stable solutions.
__label__deep_learning_architectures Over the past years, we have observed an abundance of approaches for modeling dynamic 3D scenes using Gaussian Splatting (GS).
__label__machine_learning_for_other_sciences_and_fields In this paper, we identify the limitations of existing methods, particularly their overemphasis on the distance effect as outlined in the First Law of Geography.
__label__machine_vision Since local information in realistic road environments is frequently obscured by other vehicles or affected by poor outdoor lighting conditions, these methods struggle with the regression of such key points.
__label__optimization_for_deep_networks In this work, we approach this question for the first time by comparing the inductive bias of gradient descent (GD) with that of sharpness-aware minimization (SAM).
__label__learning_theory We show that PTFs of arbitrary constant degree can be testably learned up to excess error $\varepsilon > 0$ in time $n^{\mathrm{poly}(1/\varepsilon)}$.
__label__deep_learning_architectures We note that certain architectural changes cause degraded training efficiency/ICL accuracy by converging to suboptimal predictors or converging slower.
__label__diffusion_based_models Specifically, motivated by rectified flow theory, we train an ordinary differential equation (ODE) model to transport between the distributions of real images and semantic masks.
__label__deep_learning_architectures In addition, it applies this strategy to the original imbalanced data to create an augmented dataset and fine-tune the underlying long-tailed learning model.
__label__other Using the distortion-rate function as the baseline, we study the performance of existing compression schemes on a synthetic dataset consisting of prompts generated from a Markov chain, natural language queries, and their respective answers.
__label__safety_in_machine_learning This paper introduces a novel, integrated approach AHA (Adaptive Human-Assisted OOD learning) to simultaneously address both OOD generalization and detection through a human-assisted framework by labeling data in the wild.
__label__neuroscience_and_cognitive_science We show that the discovered task abstractions support generalization through both task and subtask composition, and we extend our findings to a non-linear network switching between two tasks.
__label__probabilistic_methods It has been shown that PCs with general directed acyclic graph (DAG) structure can be understood as a mixture of exponentially (in its height) many components, each of which is a product distributions over univariate marginals.
__label__reinforcement_learning Nodes with the best heuristic values in OPEN are most probably picked into this subset, but sometimes may not be included, which enables SeeA$^*$ to explore other promising branches.
__label__graph_neural_networks However, despite its effectiveness, BP has limitations that challenge its biological plausibility and affect the efficiency, scalability and parallelism of training neural networks for graph-based tasks.
__label__reinforcement_learning Yet, policy optimization with successor features can be challenging.
__label__diffusion_based_models They can explore task-specific and task-shared knowledge during training, and aggregate all low-rank weights of old concepts based on their contributions during inference.
"__label__natural_language_processing Using this dataset, we train a 7B LLM called Janus and test it on 921 prompts from 5 benchmarks (AlpacaEval 2.0, FLASK, Koala, MT-Bench, and Self-Instruct)
by adding system messages that reflect unseen user values."
__label__generative_models For these three particular instances, we show that the jump rates and kernels of the corresponding time reversals admit explicit expressions depending on some conditional densities of the PDMP under consideration before and after a jump.
__label__machine_vision Implicit neural representations (INRs) have demonstrated success in a variety of applications, including inverse problems and neural rendering.
__label__learning_theory We consider two system identification methods: least-squares estimation (LSE), which is a point estimation method; and set-membership estimation (SME), which estimates an uncertainty set that contains the true parameters.
__label__graph_neural_networks Surprisingly, our unsupervised method even beats the sophisticated supervised approaches.
__label__generative_models It paves the way for real-time engagements with lifelike avatars that emulate human conversational behaviors.
__label__natural_language_processing Moreover, we propose a tool-augmented iterative trajectory refinement module to enhance and refine the trajectories distilled from GPT-4.
__label__reinforcement_learning We further extend Kaleidoscope to critic ensembles in the context of actor-critic algorithms, which could help improve value estimations.
__label__generative_models However, these methods often underperform compared to full fine-tuning, particularly in scenarios involving complex datasets.
__label__machine_vision In this work, we analyze the cause of this phenomenon and the limitations of the current solutions, and propose to conduct synchronized modeling via a new framework named SyncVIS.
__label__machine_vision Additionally, ablation studies verify the robustness of the shift displacement with stable performance improvement.
__label__graph_neural_networks To address this issue, many residual methods have emerged.
__label__bandits Applying these findings to bandits allows us to fill gaps in the literature: We show that optimistic algorithms for generalized linear bandits enjoy regret bounds that are both second-order (scale with the variance of the optimal arm's reward distribution) and free of an exponential dependence on the bound of the problem parameter in the leading term.
__label__diffusion_based_models This is achieved by introducing an edge-absorbing noise model and a new projector operator.
__label__machine_vision We propose a pipeline consisting of three stages.
__label__learning_theory We study the complexity of heavy-tailed sampling and present a separation result in terms of obtaining high-accuracy versus low-accuracy guarantees i.e., samplers that require only $\mathcal{O}(\log(1/\varepsilon))$ versus $\Omega(\text{poly}(1/\varepsilon))$ iterations to output a sample which is $\varepsilon$-close to the target in $\chi^2$-divergence.
__label__optimization_for_deep_networks Relying on prior work, we show that in ReLU MLPs with iid initialization, the angle degenerates with depth as $\cos(\theta_\ell)=\Theta(1/\sqrt{\ell})$.
__label__bandits Additionally, the variation in reward structures between these bounds complicates the quest for optimality.
__label__natural_language_processing We investigate how the role of interface design affects the performance of language model agents.
__label__machine_learning_for_healthcare In this paper, we address this issue by analyzing the primary cause behind the failure of existing iEEG models for subject-independent seizure detection, and identify a critical universal seizure pattern: seizure events consistently exhibit higher average amplitude compared to adjacent normal events.
__label__bandits In this paper, we designed efficient algorithms for the problem of regret minimization in assortment selection with \emph{Plackett Luce} (PL) based user choices.
__label__reinforcement_learning This triggers pathological value overestimation and complete performance collapse.
__label__generative_models In this work, we leverage the random Fourier features framework to reduce the metrics' complexity and propose the *Fourier-based Kernel Entropy Approximation (FKEA)* method.
__label__machine_vision Furthermore, our architecture utilizes a goal image to guide the iterative tuning of pipeline parameters, allowing for flexible conditioning on pixel-aligned target images, style images, or any other visually representable goals.
__label__bandits We study \emph{sparse} regret bounds, that depend on the number $S$ of non-zero coefficients in the linear reward function.
__label__algorithmic_game_theory We formalize this question as the problem of learning social welfare functions belonging to the well-studied family of power mean functions.
__label__safety_in_machine_learning Many studies have proposed attack methods to generate adversarial patterns for evading pedestrian detection, alarming the computer vision community about the need for more attention to the robustness of detectors.
__label__deep_learning_architectures Various linear complexity models, such as Linear Transformer (LinFormer), State Space Model (SSM), and Linear RNN (LinRNN), have been proposed to replace the conventional softmax attention in Transformer structures.
"__label__bandits E.g., popularly used assortment selection algorithms often require the presence of a ``strong reference"" which is always included in the choice sets, further they are also designed to offer the same assortments repeatedly until the reference item gets selected---all such requirements are quite unrealistic for practical applications."
__label__diffusion_based_models We identify an implicit constraint on the samples induced by the diffrep and demonstrate that addressing this constraint significantly improves the consistency and detail of the generated objects.
__label__algorithmic_game_theory Recently, many improvements have been focused on enhancing the convergence speed of the CFR algorithm.
__label__evaluation Prism comprises two distinct stages: a perception stage that utilizes a VLM to extract and articulate visual information in textual form, and a reasoning stage that formulates responses based on the extracted visual information using a Large Language Model (LLM).
__label__learning_theory We also apply our approach to other previously studied learning problems, including boosting for reinforcement learning, and demonstrate improved results.
__label__generative_models We observe that while a discrete-valued space can facilitate representing a categorical distribution, it is not a necessity for autoregressive modeling.
__label__reinforcement_learning This framework extends beyond traditional bilevel optimization and finds relevance in diverse fields such as RLHF, tax design, reward shaping, contract theory and mechanism design.
__label__natural_language_processing Furthermore, our method exhibits strong orthogonality with traditional methods, allowing for concurrent usage.
__label__learning_theory Previous research has shown that the optimal learning rate increases linearly (or follows similar rules) with batch size for SGD-style optimizers.
__label__privacy First, our DP algorithm can not resample new datapoints as a change to a single datapoint may lead to a very large change in the descretization of the real line.
"__label__safety_in_machine_learning While dataset balancing can improve performance on
underperforming groups, it requires access to training group annotations and can
end up removing large portions of the dataset."
__label__machine_vision Extensive experimental results show that our compact model significantly surpasses existing Transformer-based models in both performance and efficiency, especially our LCM-based Point-MAE model, compared to the Transformer-based model, achieved an improvement of 1.84%, 0.67%, and 0.60% in performance on the three variants of ScanObjectNN while reducing parameters by 88% and computation by 73%.
__label__graph_neural_networks Graph transformers need strong inductive biases to derive meaningful attention scores.
__label__generative_models By viewing LLMs as abstract machines that process information through a natural language interface, we examine the conditions under which it is possible to compute suitable approximations of PN and PS.
__label__generative_models Our algorithm is mathematically justified by the observation that the MAP objective can be approximated by a sum of $N$ ``local MAP'' objectives, where $N$ is the number of function evaluations.
__label__machine_vision Considering the asymmetry of the epipolar disparity flow, the key to our method lies in accurately modeling multi-view geometric constraints.
__label__machine_vision Furthermore, upon benchmarking models on long-tailed data sampled based on our analysis, we demonstrate that multimodal models across the board perform poorly.
__label__natural_language_processing We give a new bound based on expansion properties of the data distribution and student hypothesis class that directly accounts for pseudolabel correction and coverage expansion.
__label__privacy Our main contribution is a pair of polynomial-time DP algorithms for the task of private GM with an excess error guarantee that scales with the effective diameter of the datapoints.
__label__machine_learning_for_social_sciences Also, we employ the auxiliary prediction task to enhance generalization and accuracy.
__label__reinforcement_learning We further demonstrate that DIAMOND's diffusion world model can stand alone as an interactive neural game engine by training on static *Counter-Strike: Global Offensive* gameplay.
__label__safety_in_machine_learning In this paper, to address these issues, we propose a few-shot adversarial prompt framework where adapting input sequences with limited data makes significant adversarial robustness improvement.
__label__reinforcement_learning We prove that such an assumption effectively eliminates the support shift issue for RMDPs with a TV distance robust set, and present an algorithm with a provable sample complexity guarantee.
__label__machine_vision As a prototypical application, we learn divergences that consider real-world corruptions of images (e.g., blur) as close to the original and noisy perturbations as far, even if in $L^p$-distance the opposite holds.
__label__machine_learning_for_physical_sciences Neural wave functions accomplished unprecedented accuracies in approximating the ground state of many-electron systems, though at a high computational cost.
__label__safety_in_machine_learning Our approach significantly reduces the sub-optimality gap observed in prior SoTA methods and demonstrates superior empirical performance across key metrics such as coherence, diversity, and quality in extensive tests on several synthetic and real datasets.
__label__graph_neural_networks Our open-book learning framework exhibits a significant enhancement in neural reasoning capabilities.
__label__natural_language_processing To answer this, we conduct a thorough empirical study on prompt optimization and draw two major insights.
__label__graph_neural_networks Graph autoencoders (Graph-AEs) learn representations of given graphs by aiming to accurately reconstruct them.
__label__learning_theory We first show that, for the simplest form of multi-label loss (the popular Hamming loss), the well-known consistent binary relevance surrogate suffers from a sub-optimal dependency on the number of labels in terms of $H$-consistency bounds, when using smooth losses such as logistic losses.
__label__neuroscience_and_cognitive_science The emergent place fields reproduce key aspects of hippocampal phenomenology: a) remapping (maintenance of and reversion to distinct learned maps in different environments), implemented via repositioning of experience manifolds in the network’s hidden layer, b) orthogonality of spatial representations in different arenas, c) robust place field emergence in differently shaped rooms, with single units showing multiple place fields in large or complex spaces, and (d) slow representational drift of place fields.
__label__algorithmic_game_theory We analyze how lookback improves, or not, the competitive ratio in prophet inequalities in different order models.
__label__neuroscience_and_cognitive_science Nevertheless, nearly no sampling neural circuit models consider the diversity of interneurons, and thus how interneurons contribute to sampling remains poorly understood.
"__label__diffusion_based_models Additionally, we verify our method on 3D articulated object understanding for
embodied robot scenarios and the promising results prove that our method supports this task strongly."
__label__optimization_for_deep_networks (2) By studying correlated designs, we provide new risk bounds for retrieval augmented generation (RAG) and task-feature alignment which reveal how ICL sample complexity benefits from distributional alignment.
__label__learning_theory In this paper, we provide a compositional approach to estimate Lipschitz constants for deep feed-forward neural networks.
"__label__evaluation On these data sets, sDBSCAN and sOPTICS run in a few minutes, while the scikit-learn counterparts and other clustering competitors demand several hours or
cannot run on our hardware due to memory constraints."
__label__machine_vision Most previous algorithms for NLOS reconstruction require dense transients acquired through regular scans over a large relay surface, which limits their applicability in realistic scenarios with irregular relay surfaces.
__label__machine_learning_for_physical_sciences In this work, we boost the prediction fidelity to an unprecedented level for simulating complex photonic devices with a novel operator design driven by the above challenges.
__label__machine_learning_for_other_sciences_and_fields Lastly, we prove the robustness of SLIM to various parameters and its generality by applying it to the Traveling Salesman Problem.
__label__neuroscience_and_cognitive_science We first show that, in the absence of perturbations, meta-learning identifies a temporally asymmetric generalization of Oja's rule that reliably organizes sparse sequential activity.
__label__machine_vision However, the exploration of enhancing inference efficiency during adaptation remains underexplored.
__label__machine_vision The second, regularization based on the KL-divergence with an exponentially decaying prior smooths the alignment while enforcing conformity to the optimality (alignment obtained from vanilla OT optimization) and temporal priors.
__label__machine_learning_for_other_sciences_and_fields Recent strides in language models and graph neural networks have empowered protein models to harness primary or tertiary structure information for representation learning.
__label__other LeHaCE evaluates the object hallucination degree at a uniform image description length to mitigate the effect of description lengths, promoting stability and fairness.
__label__reinforcement_learning Experiments show that our graph kernels are vastly more efficient and generalise better than graph neural networks for numeric planning, and also yield competitive coverage performance over domain-independent numeric planners.
__label__generative_models These dropped tokens are not memorized by the model, which prevents verbatim reproduction of a complete chain of tokens from the training set.
__label__graph_neural_networks It seamlessly integrates widely adopted contrastive losses and an introduced independence loss to fulfill the common requirements of consistency and diversity of augmentation across diverse scenarios.
__label__machine_vision Extensive experiments show the superiority of PointAD in ZS 3D anomaly detection across diverse unseen objects.
__label__machine_vision Our approach introduces a visual prompting technique to align the representations of frames and events, allowing the use of off-the-shelf stereo models without additional training.
__label__safety_in_machine_learning However, outside this local region, safety is fully compromised, exhibiting a sharp, step-like drop.
__label__machine_vision To bridge this gap, we present HOI-Swap, a novel diffusion-based video editing framework trained in a self-supervised manner.
__label__optimization_for_deep_networks Although the STE heuristics and their variants have led to significant improvements in BNN performance, their theoretical underpinnings remain unclear and relatively understudied.
__label__generative_models In this work, we propose a training-free method to inject visual prompts into Multimodal Large Language Models (MLLMs) through learnable latent variable optimization.
__label__learning_theory We obtain an $\tilde{\mathrm{O}}(nk^2)$ upper bound on the pseudo-dimension, where $\tilde{\mathrm{O}}$ compresses logarithmic factors.
__label__causal_inference As the direct measurement of confounders may not always be feasible, recent methods seek to address the confounding bias via proxy variables, i.e., variables postulated to be causally related to unobserved confounders.
__label__machine_learning_for_other_sciences_and_fields Our method achieves state-of-the-art performance in the official molecular optimization benchmark, significantly outperforming previous methods.
__label__safety_in_machine_learning Experiments show that our method has high power and false alarm control under various distribution shifts, including covariate and label shifts and natural shifts over geography and time.
__label__diffusion_based_models The proposed U-DiT could outperform DiT-XL with only 1/6 of its computation cost.
__label__learning_theory We show that a stronger assumption related to the moments of data is the sufficient and necessary condition that the learned mesa-optimizer recovers the distribution.
__label__diffusion_based_models The sampling for diffusion models is done by solving either the *probability flow* ODE or diffusion SDE wherein a neural network approximates the score function allowing a numerical ODE/SDE solver to be used.
__label__machine_learning_for_healthcare Recent advances in pathology foundation models have shown the potential to extract powerful feature representations from WSIs for downstream tasks.
__label__bandits To overcome this issue, we study a *sparse network interference* model, where the reward of a unit is only affected by the treatments assigned to $s$ neighboring units.
__label__graph_neural_networks This success suggests that higher-degree representations might be unnecessary.
__label__natural_language_processing We conduct extensive experiments on multiple challenging tasks such as arithmetic, knowledge reasoning, and multimodal benchmarks spanning GSM8K, MMLU, SQA, and VQA, demonstrating that our DSA method achieves significant performance gains on the LLaMA-1|2|3, Mistral, and OPT models.
__label__optimization Then we follow recent works to pursue the weak approximate solutions.
__label__learning_theory Learn-to-Defer is a paradigm that enables learning algorithms to work not in isolation but as a team with human experts.
__label__generative_models The core idea is two-fold: 1) We propose a novel multi-view video diffusion model (MV-VDM) conditioned on multi-view renderings of the static 3D object, which is trained on our presented large-scale multi-view video dataset (MV-Video).
__label__interpretability_and_explainability Probing for encoding remains a challenge because there is no general characterization of what gives the extra predictive power.
__label__reinforcement_learning However, current results indicate that offline RL often performs worse than imitation learning, and it is often unclear what holds back the performance of offline RL.
__label__deep_learning_architectures To this end, we introduce `einspace`, a search space based on a parameterised probabilistic context-free grammar.
__label__learning_theory We derive a closed-form solution for multi-task optimization in the context of linear models.
__label__learning_theory We explore the role of *softmax* attention in an ICL setting where each context encodes a regression task.
__label__learning_theory dataset, which have the smallest number of parameters and even a constant number of parameters.
__label__safety_in_machine_learning Notably, \textbf{CeTaD} exhibits adaptability across differentiable service models and proves the potential of continuous learning.
__label__machine_learning_for_other_sciences_and_fields In particular, extensively used XOR operations in cryptographic problems can incur an exponential number of clauses.
__label__safety_in_machine_learning Early-exit neural networks (EENNs) offer a promising solution: they accelerate inference by allowing intermediate layers to exit and produce a prediction early.
__label__graph_neural_networks Graphcodes handle datasets that are filtered along two real-valued scale parameters.
__label__optimization_for_deep_networks Additionally, we show that these initial LRs result in a sparse set of learned features, with a clear focus on those most relevant for the task.
__label__deep_learning_architectures Neural architecture search (NAS) finds high performing networks for a given task.
__label__machine_learning_for_healthcare Given the limited availability of functional annotations from wet-lab experiments, previous methods have primarily relied on self-supervised models trained on vast, unlabeled protein sequence or structure datasets.
__label__diffusion_based_models Notably, without performing any optimization of inference time our model shows faster execution, even when compared to works that do such optimization, highlighting the advantages and the value of our approach.
__label__bandits Based on Tri-BBAI, we further propose the almost optimal batched best arm identification (Opt-BBAI) algorithm, which is the first algorithm that achieves the near-optimal sample and batch complexity in the non-asymptotic setting (i.e., $1/\delta$ is finite), while enjoying the same batch and sample complexity as Tri-BBAI when $\delta$ tends to zero.
__label__machine_vision However, these methods typically focus solely on adapting VLMs from a single modality and fail to accumulate task-specific knowledge as more samples are processed.
__label__optimization_for_deep_networks In this paper, we introduce **F**orward **G**radient **U**nrolling with **F**orward **G**radient, abbreviated as **$($FG$)^2$U**, which achieves an unbiased stochastic approximation of the meta gradient for bi-level optimizaiton.
__label__machine_learning_for_other_sciences_and_fields In our main experiments, NeurKItt accelerates the solving of linear systems across various settings and datasets, achieving up to a 5.5× speedup in computation time and a 16.1× speedup in the number of iterations.
__label__other We employ machine learning techniques, notably gradient boosting, to systematically improve upon a predefined conformity score function.
__label__active_learning Under the active learning scenario where ground truths are not available until human involvement, we measure the consistency on estimated ground truths, where predictions from off-the-shelf models are utilized as approximations to ground truths.
__label__optimization_for_deep_networks Based on these theoretical insights, we verify empirically that the key-query and value-projection matrix products $W_K^TW_Q, PW_V$ within attention layers, when optimized with weight decay, as usually done in vision tasks and language modelling, indeed induce a significant reduction in the rank of $W_K^TW_Q$ and $PW_V$, even in fully online training.
__label__reinforcement_learning The ability to perform different skills can encourage agents to explore.
__label__privacy However, in this paper, we observed a phenomenon that such a fixed prior would lead to a low probability of sampling actual private data during the inversion process due to the inherent distribution gap between the prior distribution and the private data distribution, thereby constraining attack performance.
__label__reinforcement_learning Multi-task reinforcement learning endeavors to efficiently leverage shared information across various tasks, facilitating the simultaneous learning of multiple tasks.
__label__optimization_for_deep_networks Finetuning large language models (LLMs) in federated learning (FL) settings has become increasingly important as it allows resource-constrained devices to finetune a model using private data.
__label__natural_language_processing When this majority opinion is the result of a common misconception (ingrained in the models through shared training data) debate is likely to converge to answers associated with that common misconception.
__label__machine_learning_for_physical_sciences Evaluated on held-out species, our zero-shot SRMs significantly outperform baselines and match the performance of SRMs obtained using tens of observations.
__label__other However, the one-shot aggregation performances are sensitively affected by the non-identical training data distribution, which exhibits high statistical heterogeneity in some real-world scenarios.
__label__machine_vision Our experiments demonstrate that OSEDiff achieves comparable or even better Real-ISR results, in terms of both objective metrics and subjective evaluations, than previous diffusion model-based Real-ISR methods that require dozens or hundreds of steps.
__label__other In addition, we conduct extensive experiments on benchmark datasets.
__label__machine_learning_for_other_sciences_and_fields Inspired by a technique called natural programming elicitation, we propose designing an intermediate language that LLMs ``naturally'' know how to use and which can be automatically compiled to a target VLPL.
__label__other In many machine learning tasks, data is inherently sequential.
"__label__learning_theory Prior works (DGT19, CKMY20)  came with worse sample complexity
 guarantees (in both $\epsilon$ and $\gamma$) or could only
 handle random classification noise (DDKWZ23,KITBMV23)--- a much milder noise assumption."
__label__machine_learning_for_physical_sciences The generation of equilibrium samples of molecular systems has been a long-standing problem in statistical physics.
__label__natural_language_processing Large Language Models (LLMs) struggle with reliably generating highly structured outputs, such as program code, mathematical formulas, or well-formed markup.
__label__diffusion_based_models Instead of conditioning on a sequence of text tokens, we propose to use a set of per-object representations, *Neural Assets*, to control the 3D pose of individual objects in a scene.
__label__natural_language_processing We introduce a novel reinforcement learning framework of LLM agents named AGILE (AGent that Interacts and Learns from Environments)  designed to perform complex conversational tasks with users, leveraging LLMs, memory, tools, and interactions with experts.
__label__safety_in_machine_learning This paper gives an affirmative answer to this question by presenting a Distributional Representation Learning (\texttt{DRL}) framework for OOD detection.
__label__learning_theory To train an LLM, one needs to alternatingly run `forward' computations and backward computations.
__label__causal_inference Inspired by graphical lasso methods, our model optimizes over continuous graph representations in the SPD space, where inverse covariance matrices encode conditional independence relations.
__label__other (2) The training schema of existing GR methods is supervised, necessitating expensive user-group and group-item labels, leading to significant annotation costs.
__label__natural_language_processing For LLMs to be trustworthy sources of knowledge, the confidence they convey should match their actual expertise on a topic; however, this is currently not the case, with most models tending towards overconfidence.
__label__diffusion_based_models Our comparative studies show that CAF not only outperforms rectified flow with reflow procedures in terms of speed and accuracy but also demonstrates substantial improvements in preserving coupling for fast generation.
__label__causal_inference This study considers a Bayesian approach for learning causal graphs with limited interventional samples, mirroring real-world scenarios where such samples are usually costly to obtain.
__label__natural_language_processing Experiments demonstrate that by refining block drafts of open-sourced Vicuna and Medusa LLMs, the mean accepted token length are increased by 5-25% relative.
__label__probabilistic_methods However, this scheme may converge toward an arbitrarily suboptimal local minimum, due to the greedy nature of WTA.
__label__neuroscience_and_cognitive_science These plasticity parameters are optimized via gradient descent over entire trajectories to align closely with observed neural activity or behavioral learning dynamics.
__label__natural_language_processing Automatic prompt optimization (APO) methods are designed to automate this and can be broadly categorized into those targeting instructions (instruction optimization, IO) vs. those targeting exemplars (exemplar optimization, EO).
__label__machine_vision Compared with the state-of-the-art (SOTA) method, our method achieves higher fidelity, greater robustness, and remarkably faster inference times by orders of magnitude.
__label__learning_theory The number of features is comparable to the sample size and errors may be heavy-tailed.
__label__machine_vision Rendering and reconstruction are long-standing topics in computer vision and graphics.
__label__safety_in_machine_learning Unfortunately, it is still challenging to select appropriate distractor images, which should mix the amplitude without affecting the phase patterns.
__label__online_learning Despite its simplicity, our model offers valuable insights into the use of actions as estimation instruments, the benefits of low-switching pricing policies in mitigating strategic buyer behavior, and the role of supply randomness in facilitating exploration which leads to a phase transition of policy performance.
__label__deep_learning_architectures In this work, we take a first step towards theoretically characterizing the conditioning of the GN matrix in neural networks.
__label__causal_inference In particular, we illustrate the expressiveness of UCA for a wide spectrum of causal estimands (e.g., SBD, FD, and more) in causal inference.
__label__neuroscience_and_cognitive_science We wonder if it also does for SNNs and how to improve the feature representation of the SNN.
__label__deep_learning_architectures Our START can selectively perturb and suppress domain-specific features in salient tokens within the input-dependent matrices of SSMs, thus effectively reducing the discrepancy between different domains.
__label__reinforcement_learning Learning world models offers a promising avenue for goal-conditioned reinforcement learning with sparse rewards.
__label__causal_inference We present useful properties of a confounding measure and present measures that satisfy those properties.
__label__reinforcement_learning In this paper, we consider whether learned optimization can help overcome these problems.
__label__neuroscience_and_cognitive_science Invasive brain-computer interfaces with Electrocorticography (ECoG) have shown promise for high-performance speech decoding in medical applications, but less damaging methods like intracranial stereo-electroencephalography (sEEG) remain underexplored.
__label__machine_learning_for_other_sciences_and_fields Video demos, data, and code are available online.
__label__learning_theory Moreover, we provide efficient countermeasures to address the challenges in the computation of the pruning limit, which involves accurate spectrum estimation of a large-scale and non-positive Hessian matrix.
__label__machine_vision In this paper, we develop a Multi-view Masked Contrastive Representation Learning (M$^2$CRL) framework for endoscopic video pre-training.
__label__generative_models We provide empirical verification of our theoretical identifiability result using both simple 2-dimensional data and high-resolution imaging datasets.
__label__safety_in_machine_learning In addition, we provide a theoretical analysis to justify this method is well motivated.
__label__machine_learning_for_other_sciences_and_fields UniIF offers a versatile and effective solution for general molecule inverse folding.
__label__reinforcement_learning In this work we answer this question positively by proving that with trajectory data, a dataset of size $\text{poly}(d,H,C_\text{conc})/\epsilon^2$ is sufficient for deriving an $\epsilon$-optimal policy, regardless of the size of the state space.
__label__deep_learning_architectures Numerical experiments conducted on three few-shot learning datasets validate the superiority of data-driven priors over the prespecified ones, showcasing its pronounced effectiveness when dealing with extremely limited data resources.
__label__machine_learning_for_other_sciences_and_fields Extensive experimental results have demonstrated thatDePLM not only surpasses the state-of-the-art in mutation effect prediction butalso exhibits strong generalization capabilities for novel proteins.
__label__natural_language_processing Using insights from our theoretical results we then propose three interventions which improve the efficacy of debate.
__label__deep_learning_architectures However, due to the partially-observed nature of real-world applications, solely focusing on the target of interest, so-called endogenous variables, is usually insufficient to guarantee accurate forecasting.
__label__optimization_for_deep_networks However, each SAM update requires _sequentially_ computing two gradients, effectively doubling the per-iteration cost compared to base optimizers like SGD.
__label__safety_in_machine_learning We benchmark on a diverse range of asymmetries between judges and agents, extending previous work on a single extractive QA task with information asymmetry, to also include mathematics, coding, logic and multimodal reasoning asymmetries.
__label__optimization_for_deep_networks We conduct extensive experiments on four multimodal datasets: UPMC-Food 101, CMU-MOSI, IEMOCAP and BraTS 2021, covering classification, regression and segmentation tasks.
__label__privacy Existing MIAs designed for large language models (LLMs) can be bifurcated into two types: reference-free and reference-based attacks.
__label__reinforcement_learning However, the use of diffusion policies in online RL is hindered by the intractability of policy likelihood approximation, as well as the greedy objective of RL methods that can easily skew the policy to a single mode.
__label__deep_learning_architectures Existing methods leverage additional models or human annotations to curate preference data and enhance modality alignment through preference optimization.
__label__generative_models Our method is the first DRE-based technique that can successfully generate images beyond the MNIST dataset.
__label__interpretability_and_explainability As we experimentally show, the proposed construction not only outperforms recent CBM approaches, but also yields a principled framework towards interpetability.
__label__generative_models Rainbow Teaming casts adversarial prompt generation as a quality-diversity problem and uses open-ended search to generate prompts that are both effective and diverse.
__label__other Furthermore, in some temporal data with neural mutual information estimations, we only have snapshots at different timestamps, lacking correspondence, which hinders EvoRate estimation.
__label__infrastructure Our proposed scheme significantly enhances training and inference throughput of large language models under restrictive computational resources.
__label__generative_models This transformation enables a precise depiction of motion and deformation over time.
__label__optimization_for_deep_networks SVD decomposes a matrix into the product of a left unitary matrix, a diagonal matrix of scaling values, and a right unitary matrix.
"__label__machine_vision Experimental
results on both real-world (Stanford2D3DS, Matterport3D) and synthetic (Structured3D)
datasets demonstrate the robustness of our framework, by setting new
state-of-the-arts in almost evaluations, The code and updated results are available
at: https://github.com/caodinhduc/vertical_relative_distance."
__label__generative_models Empirically, we achieve state-of-the-art performance in model parsing and its extended applications, showing the superiority of the proposed LGPN.
__label__safety_in_machine_learning In our work, we establish the subfield of secret collusion, a form of multi-agent deception, in which two or more agents employ steganographic methods to conceal the true nature of their interactions, be it communicative or otherwise, from oversight.
__label__natural_language_processing Our results indicate that it is beneficial to leverage reward learning throughout the entire alignment process.
__label__natural_language_processing Our study specifically explores the application of native alignment in the context of Arabic LLMs.
__label__machine_learning_for_other_sciences_and_fields Our results show that FAFormer outperforms existing equivariant models in contact map prediction across three protein complex datasets, with over 10% relative improvement.
__label__machine_learning_for_healthcare We propose using LLMs as automated planners to (1) interpret human policy preference prompts, (2) propose reward functions as code for a multi-agent RMAB environment, and (3) iterate on the generated reward functions using feedback from grounded RMAB simulations.
__label__optimization Extensive research has been conducted including smoothing and linear interpolation.
__label__generative_models To address these challenges, we introduce a new retargeting framework, MeshRet, which directly models the dense geometric interactions in motion retargeting.
__label__natural_language_processing These rules unveil temporal patterns and facilitate interpretable reasoning.
__label__infrastructure Known for reducing the quadratic time complexity of self attention to a linear complexity, neighborhood attention can now enjoy a reduced and constant memory footprint, and record-breaking half precision runtime.
__label__privacy In this paper, we study differentially private (DP) algorithms for computing the geometric median (GM) of a dataset: Given $n$ points, $x_1,\dots,x_n$ in $\mathbb{R}^d$, the goal is to find a point $\theta$ that minimizes the sum of the Euclidean distances to these points, i.e., $\sum_{i=1}^{n} \lVert|\theta - x_i\rVert_2$.
__label__reinforcement_learning We also introduce Risk-Aware- PbRL (RA-PbRL), an algorithm designed to optimize both nested and static objectives.
__label__learning_theory Operator learning problems arise in many key areas of scientific computing where Partial Differential Equations (PDEs) are used to model physical systems.
__label__natural_language_processing Our best models not only achieve a 3x increase in training speed over dense Transformer models in language pretraining but also match the performance of state-of-the-art MoE architectures.
__label__machine_learning_for_other_sciences_and_fields However, experimental platforms for aptamer screening are costly, and the scarcity of labeled data presents a challenge for supervised methods to learn protein-aptamer binding.
__label__learning_theory In contrast our bound implicitly controls all uncountably many weightings simultaneously.
__label__natural_language_processing Model quantification uses low bit-width values to represent the weight matrices of existing models to be quantized, which is a promising approach to reduce both storage and computational overheads of deploying highly anticipated LLMs.
__label__diffusion_based_models However, prevailing optimization-based rithms primarily exploit the prior information within the diffusion models while neglecting their denoising capability.
__label__bandits This allows us to recover the results of Liu et al.
__label__machine_learning_for_physical_sciences Accurately predicting the future fluid is vital to extensive areas such as meteorology, oceanology, and aerodynamics.
__label__natural_language_processing Furthermore, probe sampling is also able to accelerate other prompt optimization techniques and adversarial attack methods, leading to acceleration of $1.8\times$ for AutoPrompt, $2.4\times$ for APE and $2.4\times$ for AutoDAN.
__label__probabilistic_methods Eliciting a high-dimensional probability distribution from an expert via noisy judgments is notoriously challenging, yet useful for many applications, such as prior elicitation and reward modeling.
__label__diffusion_based_models Our method incorporates a switcher and a cross-domain attention mechanism to extend the original LDM for joint prediction of the foreground color and opacity.
__label__machine_vision To this end, we construct a Diffusion-Replication (D-Rep) dataset and correspondingly propose a novel deep embedding method.
__label__generative_models To address these issues, we develop and theoretically justify the novel Optimal Flow Matching approach which allows recovering the straight OT displacement for the quadratic transport in just one FM step.
__label__optimization Our findings demonstrate the clear advantages of utilizing quantum techniques for non-convex non-smooth optimization, as they outperform the optimal classical methods on the dependency of $\epsilon$ by a factor of $\epsilon^{-2/3}$.
__label__learning_theory We demonstrate theoretically and empirically that DNC occurs in Deep Recursive Feature Machines as a consequence of the projection with the AGOP matrix computed at each layer.
__label__diffusion_based_models Their optimal precision is achieved only when the physical compositions (i.e., scale and rotation) of the human shapes in the reference image and target pose frame are aligned.
__label__safety_in_machine_learning ESPO employs an optimization framework with three modes: maximizing rewards, minimizing costs, and balancing the trade-off between the two.
__label__bandits In each auction $t$, a decision-maker bound by limited observations selects $n_t$ agents from a coalition of $N$ to compete for a prize with $p$ other agents, aiming to maximize the cumulative reward of the coalition across all auctions.
__label__generative_models After fine-tuning using Diffusion-KTO, text-to-image diffusion models exhibit improved performance compared to existing techniques, including supervised fine-tuning and Diffusion-DPO, both in terms of human judgment and automatic evaluation metrics such as PickScore and ImageReward.
__label__diffusion_based_models After that, *DenoiseRep* fuses the parameters of feature extraction and denoising layers, and *theoretically demonstrates* its equivalence before and after the fusion, thus making feature denoising computation-free.
__label__machine_vision The modern study and use of surfaces is a research topic grounded in centuries of mathematical and empirical inquiry.
__label__machine_vision DeFT utilizes the robust alignment of textual and visual features pre-trained on millions of auxiliary image-text pairs to sieve out noisy labels.
__label__machine_vision In computer vision, gaze following is defined as the prediction of the pixel coordinates where a person in the image is focusing their attention.
__label__safety_in_machine_learning Based on this insight, we design a novel framework that conducts fine-tuning with a constrained multimodal contrastive loss enforcing a larger smallest singular value, which is further guided by the self-distillation of a moving-averaged model to achieve calibrated prediction as well.
__label__safety_in_machine_learning Among these, diffusion classifiers, utilizing powerful diffusion models, have demonstrated superior empirical robustness.
__label__human-AI_interaction Furthermore, the learned communication protocols exhibit zero-shot generalization capabilities in ad-hoc teamwork scenarios with unseen teammates and novel task states.
__label__interpretability_and_explainability However, due to their size and complexity, their decision-making process remains a black-box, leading to opacity and trust issues.
__label__reinforcement_learning Compared with a few recent related theoretical studies, our focus is on understanding practically inspired algorithmic paradigms, without computationally intractable oracles.
__label__optimization Finally, we validate the generalization capabilities of the proposed KIO model and the effectiveness of the SSO algorithm through learning-from-demonstration tasks on the MuJoCo benchmark.
__label__machine_vision To achieve this, we propose operating directly over 2D point tracks as input and designing an architecture tailored for processing 2D point tracks.
__label__machine_learning_for_physical_sciences In addition to improving predictive power, these priors make the model indentifiable, thus the identified features can be linked to comprehensible scientific properties of the system.
__label__safety_in_machine_learning To address this gap, we examine what enables ICL in models trained on unstructured data, focusing on critical sequence model requirements and training data structure.
__label__optimization_for_deep_networks LoT operationalizes this concept to improve the generalization of the main model with auxiliary student learners.
__label__online_learning Kleinberg et al.
__label__safety_in_machine_learning State-of-the-art deep learning models for tabular data have recently achieved acceptable performance to be deployed in industrial settings.
__label__optimization_for_deep_networks It operates in two stages: Fetch and Forge, initially storing key localization and classification information into model parameters, and then reconstructing synthetic images via model inversion.
__label__deep_learning_architectures We also provide principled comparisons between softmax attention and other model classes, discussing the theoretical conditions under which softmax attention can be approximated.
__label__online_learning Specifically, we first develop a novel algorithm, and prove that it enjoys a regret bound of $O(\sqrt{n}T^{3/4}+\sqrt{dT})$ in general.
__label__diffusion_based_models Our findings contribute to the understanding and development of reasoning with diffusion language models.
__label__generative_models This process is often hindered by multiple second-order phase transitions and the associated critical slowdown.
__label__bandits This relies on three key ingredients: **1.
__label__optimization_for_deep_networks Conversely, using initial LRs that are too large fails to detect a basin with good solutions and extract meaningful patterns from the data.
__label__learning_theory both the number of tasks and in-context examples.
__label__natural_language_processing Our results show that even the state-of-the-art models still struggle with this task.
__label__machine_vision Extensive experiments conducted on OPV2V-N show that RCDN can be ported to other baselines and improve their robustness in extreme camera-insensitivity setting.
__label__interpretability_and_explainability In this paper, we present a novel framework for the interpretation of LMMs.
"__label__learning_theory The results include: (1) A general separation is proposed, showing the existence of a width-$m$ DEQ that any fully connected neural networks (FNNs) with depth $O(m^{\alpha})$ for $\alpha \in (0,1)$ cannot
approximate unless its width is sub-exponential in $m$; (2) DEQ with polynomially bounded size and magnitude can efficiently approximate certain steep functions (which has very large derivatives) in $L^{\infty}$ norm, whereas FNN with bounded depth and exponentially bounded width cannot unless its weights magnitudes are exponentially large; (3) The implicit regularization caused by gradient flow from a diagonal linear DEQ is characterized, with specific examples showing the benefits brought by such regularization."
__label__machine_vision However, current VAD systems are often limited by their superficial semantic understanding of scenes and minimal user interaction.
__label__deep_learning_architectures Replacement training enhances the student model's ability to replicate the teacher model's behavior.
__label__natural_language_processing Recent endeavors have introduced more lightweight strategies, focusing on extracting ``steering vectors'' to guide the model's output toward desired behaviors by adjusting activations within specific layers of the LLM's transformer architecture.
__label__online_learning While previous works of Hossain et al.
__label__generative_models We showcase that L4GM that is only trained on synthetic data generalizes well on in-the-wild videos, producing high quality animated 3D assets.
__label__reinforcement_learning By analyzing $Q$-function over-generalization, which impairs stable stitching, QCS adaptively integrates $Q$-aid into RCSL's loss function based on trajectory return.
__label__probabilistic_methods One effective method to improve both the compute and energy efficiency of neural networks while maintaining good performance is structured pruning, where full network structures (e.g.
__label__machine_learning_for_other_sciences_and_fields Experimental results demonstrate the effectiveness of our AL paradigm, as well as the proposed diversity and uncertainty methods.
__label__generative_models Recent studies attempt to utilize the similarity of features across adjacent denoising stages to reduce computational costs through simple and static strategies.
__label__deep_learning_architectures This code is available at \url{https://github.com/anonymousforneurips64/neurips2024-submission21757}.
__label__interpretability_and_explainability In particular, interpretable features—such as those found by sparse autoencoders (SAEs)—are typically linear combinations of extremely many neurons, each with its own nonlinearity to account for.
__label__natural_language_processing However, this fine-tuning paradigm may not be able to efficiently learn optimal decision-making agents in multi-step goal-directed tasks from interactive environments.
__label__online_learning Continual test-time adaptation methods have shown promising results by using reliable pseudo-labels, but they still fall short in exploring representation alignment with the source domain in non-stationary environments.
__label__causal_inference We demonstrate the above with experiments on  both simulated and real-world datasets.
__label__generative_models To mitigate overoptimization, we first propose a theoretical algorithm which optimizes the policy against an adversarially chosen reward model, one that simultaneously minimizes its MLE loss and a reward penalty term.
__label__generative_models The static surface Gaussians and mesh vertices as well as the dynamic deformation network are learned via reference view photometric loss, score distillation loss as well as other regularization losses in a two-stage manner.
__label__machine_vision On CIFAR-10, we achieve an accuracy of 86.29% using only 61 million logic gates, which improves over the SOTA while being 29x smaller.
__label__machine_vision Cinematographers adeptly capture the essence of the world, crafting compelling visual narratives through intricate camera movements.
__label__machine_vision To address these downsides, we propose a methodology of Wasserstein Distance (WD) based knowledge distillation.
__label__machine_learning_for_other_sciences_and_fields To address these challenges in a systematic way, we propose a novel regularized triangle-shaped circuit network generation framework, which leverages our key insights for completely accurate and scalable circuit generation.
__label__reinforcement_learning Specifically, A2PO employs a conditional variational auto-encoder to disentangle the action distributions of intertwined behavior policies by modeling the advantage values of all training data as conditional variables.
__label__privacy This probed direction decomposes into three components: weighted forgetting gradient ascent, fine-tuning retaining gradient descent, and a weight saliency matrix.
__label__machine_vision Experimental results across various vision encoders, image resolutions, training dataset scales, varying sizes of LLMs (2.7B→70B), and diverse architectures of MLLMs (e.g., LLaVA-v1.5, LLaVA-NeXT and Mini-Gemini) validate the versatility and scalability of our approach, achieving state-of-the-art performance across 19 image and video benchmarks.
__label__optimization To verify our PIP designs, we conduct extensive experiments on the highly challenging Traveling Salesman Problem with Time Window (TSPTW), and TSP with Draft Limit (TSPDL) variants under different constraint hardness levels.
__label__diffusion_based_models Our comprehensive empirical evaluation shows that AdaFlow achieves high performance with fast inference speed.
__label__optimization Under mild assumptions, we show that our method enjoys provable guarantees on solution quality, as evaluated using both the ground-truth parameters and the decision maker's perception of the unknown parameters.
__label__evaluation This will help communities establish more sophisticated human assessment protocols.
__label__probabilistic_methods With diverse tasks (seven datasets) and architectures (black-box and physics-informed models), NTW strongly correlates (Pearson coefficient=0.905) with coverage differences beyond covariate shifts, while mRCP reduces coverage gap by 50% on average robustly over multiple distinct test domains.
__label__machine_learning_for_physical_sciences Among them, Parareal computes the solution sequentially using an inaccurate (fast) solver, and then ``corrects'' it using an accurate (slow) integrator that runs in  parallel across temporal subintervals.
__label__natural_language_processing For instance, if the suspect model is open-weight, we demonstrate that training on watermarked instructions can be detected with high confidence ($p$-value $< 10^{-5}$) even when as little as $5\%$ of training text is watermarked.
__label__generative_models Experimental results on generic multimodal tasks such as VQA and embodied robotic control demonstrate the versatility of IVM, which as a plug-and-play tool, significantly boosts the performance of diverse multimodal models, yielding new state-of-the-art results across challenging multimodal benchmarks.
__label__learning_theory In this paper, we consider a learning setting with complete information on user choices from subsets of size at most $k$.
__label__infrastructure In this paper, we rethink the impact of memory and communication costs on the training speed of LLMs, taking into account the impact of intra- and inter-group communication performance disparities, and then propose a new set of basic strategies named the \textbf{Pa}rtial \textbf{R}edundancy \textbf{O}ptimizer (PaRO).
__label__reinforcement_learning Since the distribution of the diffusion policy lacks an analytical expression, its entropy cannot be determined analytically.
__label__infrastructure We give an algorithm for Centroid-Linkage Hierarchical Agglomerative Clustering (HAC), which computes a $c$-approximate clustering in roughly $n^{1+O(1/c^2)}$ time.
__label__bandits Furthermore, its investigation answers the theoretical question on how numerical rewards are crucial in bandit settings.
__label__safety_in_machine_learning The pursuit of high perceptual quality in image restoration has driven the development of revolutionary generative models, capable of producing results often visually indistinguishable from real data.
__label__machine_vision We evaluate these models in four tasks: Vision-Language Scene Reasoning, Visual Grounding, Segmentation, and Registration, each focusing on different aspects of scene understanding.
__label__active_learning Selecting high-quality data for pre-training is crucial in shaping the downstream task performance of language models.
__label__deep_learning_architectures To address this limitation, we propose a combination of retrieval and fine-tuning: we can adapt the transformer to a local subset of the data by collecting nearest neighbours, and then perform task-specific fine-tuning with this retrieved set of neighbours in context.
__label__online_learning Motivated by these insights, we develop a novel replay-free CL method named ZAF (Zero-shot Antidote to Forgetting), which preserves acquired knowledge through a zero-shot stability regularization applied to wild data in a plug-and-play manner.
__label__machine_vision Evaluated across seven popular datasets, UnSAM achieves competitive results with the supervised counterpart SAM, and surpasses the previous state-of-the-art in unsupervised segmentation by 11% in terms of AR.
__label__reinforcement_learning As assessed through large-scale multi-task benchmarking in locomotion and manipulation domains, GenRL enables multi-task generalization from language and visual prompts.
__label__machine_vision The former one is able to stabilize the scene content, while the latter one learns the scene styles and eliminates its impact to DGSS.
__label__natural_language_processing We propose three complementary approaches for predicting the compute-optimal vocabulary size: IsoFLOPs analysis, derivative estimation, and parametric fit of the loss function.
__label__machine_vision Unlike traditional MLLMs limited to text output, VisionLLM v2 significantly broadens its application scope.
__label__machine_vision 3D Gaussian splatting (3DGS) provides a novel perspective for volume rendering, and shows advantages in rendering efficiency and quality.
__label__machine_learning_for_healthcare To tackle these challenges, we propose S-MolSearch, the first framework to our knowledge, that leverages molecular 3D information and affinity information in semi-supervised contrastive learning for ligand-based virtual screening.
__label__probabilistic_methods Convergence rate analysis for general state-space Markov chains is fundamentally important in operations research (stochastic systems) and machine learning (stochastic optimization).
__label__machine_vision Here, we propose a simpler network architecture based on Deep Sets.
__label__machine_vision In this study, we propose Regression-based Analytic Incremental Learning (RAIL), which utilizes a recursive ridge regression-based adapter to learn from a sequence of domains in a non-forgetting manner and decouple the cross-domain correlations by projecting features to a higher-dimensional space.
__label__machine_vision Our empirical results demonstrate that the proposed approach significantly improves over previous state-of-the-art methods under various IPCs.
__label__safety_in_machine_learning Although the valid coverage guarantee has been extensively studied for classification problems, CP often produces large prediction sets which may not be practically useful.
__label__reinforcement_learning We demonstrate that this method achieves up to 33% improvements over state-of-the-art methods on several relevant tasks taken from relevant domains.
__label__machine_learning_for_other_sciences_and_fields To bridge the discrepancy between the language and graph modalities, we present the multi-level graph projector that transforms graph representations into graph tokens by abstracting the output representations of each GNN layer and motif representations with the cross-attention mechanism.
__label__optimization_for_deep_networks Neural network pruning is a key technique towards engineering large yet scalable, interpretable, and generalizable models.
__label__safety_in_machine_learning State-of-the-art exact approaches to verifying safety of NCBF-based controllers exploit the piecewise-linear structure of ReLU neural networks, however, such approaches still rely on enumerating all of the activation regions of the network near the safety boundary, thus incurring high computation cost.
__label__natural_language_processing Finally, we seamlessly integrate our discovered functions into various uniform methods, resulting in significant performance improvements.
__label__optimization_for_deep_networks Large Language Models (LLMs) have demonstrated profound capabilities due to their extensive pre-training on diverse corpora.
__label__privacy In this work, we investigate the fundamental limits of differential privacy in online learning algorithms and present evidence that separates three types of constraints: no DP, pure DP, and approximate DP.
__label__diffusion_based_models Extensive experiments showcase the superiority of our method in editing multi-color hairstyles while preserving facial attributes given textual descriptions and reference images.
__label__learning_theory The variance error, which increases with $M$, is dominated by the other errors due to the implicit regularization of SGD, thus disappearing from the bound.
__label__reinforcement_learning Unsupervised Environment Design (UED) formalizes the problem of autocurricula through interactive training between a teacher agent and a student agent.
__label__human-AI_interaction We evaluate Voila-A using a hold-out validation set and a newly collected VOILA-GAZE testset, which features real-life scenarios captured with a gaze-tracking device.
__label__reinforcement_learning To tackle this challenge, we propose Meta Decision Transformer (Meta-DT), which leverages the sequential modeling ability of the transformer architecture and robust task representation learning via world model disentanglement to achieve efficient generalization in offline meta-RL.
__label__diffusion_based_models Central to our approach is a latent warpage function (LWF), which simulates the behavior of a stretched material to adjust the location of individual pixels within the latent space.
__label__interpretability_and_explainability These estimates provide lower and upper bounds on the expected outcomes of the policy execution across all modeled states in the state space.
__label__diffusion_based_models Text-to-image diffusion models particularly Stable Diffusion, have revolutionized the field of computer vision.
__label__generative_models However, the immense scale of the chemical space makes it challenging to explore all possible materials experimentally.
__label__evaluation To address this gap, we introduce an all-around LMM-based NR-IQA model, which is capable of producing qualitatively comparative responses and effectively translating these discrete comparison outcomes into a continuous quality score.
__label__graph_neural_networks InstructG2I first exploits the graph structure and multimodal information to conduct informative neighbor sampling by combining personalized page rank and re-ranking based on vision-language features.
__label__machine_vision We validate the effectiveness of our method across benchmarks featuring both semantic and domain shifts.
__label__other Although many studies have been proposed to address this challenge, the semantic and spurious features are still difficult to accurately decouple from the original image and fail to achieve high performance with deep learning models.
__label__natural_language_processing To calibrate both implicit and explicit confidence markers, we introduce a pragmatic, listener-aware finetuning method (LACIE) that directly models the listener, considering not only whether an answer is right, but whether it will be accepted by a listener.
__label__active_learning Submodular functions, crucial for various applications, often lack practical learning methods for their acquisition.
__label__machine_vision Then, the derived model is directly transferred to the forgery detection task.
__label__interpretability_and_explainability By characterizing learning dynamics in this space, we identify how the speed at which a concept is learned, and hence the order of concept learning, is controlled by properties of the data we term concept signal.
__label__interpretability_and_explainability We believe that algorithmic recourse, by its nature, should be seen as a practical problem: real-world socio-technical decision-making systems are complex dynamic entities involving various actors (end users, domain experts, civil servants, system owners, etc.)
__label__graph_neural_networks However, the conditions under which score propagation proves beneficial remain not fully elucidated.
__label__machine_vision Privacy issue is a main concern in developing face recognition techniques.
__label__diffusion_based_models Our study provides crucial insights into understanding the notable strong generalization phenomenon recently observed in real-world diffusion models.
__label__algorithmic_game_theory For this setting, we show an (almost) tight bound of $\tilde{\Theta}(\sqrt{T})$.
__label__safety_in_machine_learning However, once these adversarial prompts are identified, the safety filter can be updated to prevent the generation of unsafe images.
__label__optimization_for_deep_networks An open source implementation of our method is available at https://github.com/facebookresearch/schedule_free.
__label__optimization_for_deep_networks Subsequently, we rewire the gradient of the loss on the target node to preserve performance on the training node using anchor gradient.
__label__reinforcement_learning The proposed approach demonstrates state-of-the-art performance across a range of tasks in the D4RL benchmarks, significantly improving upon existing diffusion-based policies.
__label__generative_models Our main insight is that under realistic settings, a single iteration of the Reflow algorithm for training rectified flows is sufficient to learn nearly straight trajectories; hence, the current practice of using multiple Reflow iterations is unnecessary.
__label__robotics Then, (ii) we provide formal proof of global Input-to-State stability using Lyapunov arguments.
__label__reinforcement_learning The field of risk-constrained reinforcement learning (RCRL) has been developed to effectively reduce the likelihood of worst-case scenarios by explicitly handling risk-measure-based constraints.
__label__generative_models Low Rank Adaptation (LoRA) has gained massive attention in the recent generative AI research.
__label__learning_theory In this study, we delve into the training dynamics of a single-layer GAN model from the perspective of subspace learning, framing these GANs as a novel approach to this fundamental task.
__label__optimization For combinatorial algorithms, the best known approximation ratios for both size and matroid constraint are obtained by a simple randomized greedy algorithm of Buchbinder et al.
__label__fairness Traditional logits-based detection methods leverage surrogate models for identifying LLM-generated content when the exact logits are unavailable from black-box LLMs.
__label__algorithmic_game_theory Finally, we give experimental results which illustrate when different components of  our framework, made to insure consistency and robustness, come into play.
__label__machine_vision Compared to previous vol- umetric rendering-based methods, LE3D reduces training time to 1% and improves rendering speed by up to 4,000 times for 2K resolution images in terms of FPS.
__label__learning_theory Finally, we validate our theoretical findings by training numerous machine learning models, including convex problems and non-convex tasks in computer vision and natural language processing.
__label__generative_models Kaleido integrates an autoregressive language model that encodes the original caption and generates latent variables, serving as abstract and intermediary representations for guiding and facilitating the image generation process.
__label__diffusion_based_models Diffusion Probabilistic Models (DPMs) have shown remarkable potential in image generation, but their sampling efficiency is hindered by the need for numerous denoising steps.
"__label__neuroscience_and_cognitive_science Consequently, this approach mitigates the issue of poor feature quality typically 
  extracted from low SNR signals."
"__label__neuroscience_and_cognitive_science However, challenges such as low signal-to-noise ratio (SNR), high inter-subject variability, and channel mismatch complicate the extraction of robust, 
  universal EEG representations."
__label__causal_inference The efficacy of the proposed framework is demonstrated through simulation studies and a real data example.
__label__robotics This significantly simplifies the forecasting task, making the solutions suboptimal and inefficient to use in practice.
__label__machine_vision VPNet employs Multi-Frame Knowledge Distillation based on the point clouds of multiple adjacent frames to accurately predict the voxel-wise labels by condensing various possibilities of voxel relationships.
__label__algorithmic_game_theory Intriguingly,  stable matchings have been observed in real-world daycare markets, even with a substantial number of sibling applicants.
__label__machine_learning_for_other_sciences_and_fields Moreover, novel data augmentation schemes based on structure relaxation/sidechain repacking are adopted to boost performance.
__label__safety_in_machine_learning Unlike most existing methods that primarily detect and remove/unlearn suspicious samples to mitigate malicious backdoor attacks, we propose a novel defense approach called PDB (Proactive Defensive Backdoor).
__label__machine_learning_for_healthcare Due to the cross-domain variants, the feature distribution between source and unseen target domains can be dramatically different, leading to a substantial  decrease in model performance.
__label__diffusion_based_models This is achieved through innovative attention distribution mechanism and multi-branch design that operates across several processing heads.
__label__generative_models However, training RBMs, as other energy-based models, on highly structured data poses a major challenge, as effective training relies on mixing the Markov chain Monte Carlo simulations used to estimate the gradient.
"__label__safety_in_machine_learning In this work, we theoretically demystify the ""\textit{sensitive-robust}"" dilemma that lies in previous OOD detection methods."
__label__natural_language_processing We provide a formal definition for this idea, and use it to compare the two most common objectives in the field of emergent communication: discrimination and reconstruction.
__label__machine_learning_for_other_sciences_and_fields In this paper, we first revisit the application of differentiable neural architecture search (DNAS) methods to circuit generation and found from extensive experiments that existing DNAS methods struggle to exactly generate circuits, scale poorly to large circuits, and exhibit high sensitivity to hyper-parameters.
"__label__natural_language_processing However, relying on a single translation with high estimated quality increases the chances of ""gaming the metric''."
__label__machine_vision Open-world 3D reconstruction models have recently garnered significant attention.
__label__learning_theory In this work, we take the first step towards deriving optimal problem-dependent guarantees for the CMDP problems.
__label__learning_theory We also empirically demonstrate that our algorithm can outperform SGD in our setting.
__label__optimization However, most of the previous methods for SCO require the smoothness assumption on both the outer and inner functions, which limits their applications to a wider range of problems.
__label__natural_language_processing The performance gains demonstrate its potential for enhancing LLMs' efficiency and resource utilization.
"__label__machine_learning_for_healthcare This limitation underscores
the need for new methods capable of realigning cells."
__label__natural_language_processing We provide code for ContextCite at https://github.com/MadryLab/context-cite.
__label__machine_learning_for_social_sciences We also introduce a new reward formulation and provide a theoretical performance guarantee for PRB.
__label__evaluation to new data.
__label__reinforcement_learning In the latter, the algorithm estimates the low-rank matrix corresponding to the (state, action) value function of the current policy using the following two-phase procedure.
__label__machine_learning_for_other_sciences_and_fields We test our methodology on a set of 1,768,900 such formulas, identifying many known formulas for mathematical constants, and discover previously unknown formulas for $\pi$, $\ln(2)$, Gauss', and Lemniscate's constants.
__label__reinforcement_learning Our comprehensive experimental investigation across various value-based MARL algorithms on multiple benchmarks demonstrates, for the first time, significant reductions in redundancy of up to $20\times$ in Floating Point Operations (FLOPs) for both training and inference, with less than 3% performance degradation.
__label__diffusion_based_models However, current video diffusion models exhibit demanding computational requirements and high peak memory usage, especially for generating longer and higher-resolution videos.
__label__safety_in_machine_learning Our codes are available at https://github.com/IBM/SafeLoRA.
__label__reinforcement_learning Multi-Agent Reinforcement Learning (MARL) struggles with sample inefficiency and poor generalization [1].
__label__learning_theory Notably, our approximation guarantee is non-asymptotic, multiplicative, and independent of the feature map dimension---allowing for infinite-dimensional features.
__label__natural_language_processing We argue that the key to accomplishing this task lies in distinguishing writing styles of different authors, rather than simply classifying the text into human-written or AI-generated text.
__label__learning_theory This leads to new communication-efficient distributed averaging algorithms for least squares and related tasks, which directly improve on several prior approaches.
__label__deep_learning_architectures Thus, we can make the scene's dynamic editable over time or while maintaining partial dynamics.
__label__deep_learning_architectures We then use this parametrization to modify the orthogonal finetuning framework, improving parameter efficiency.
__label__learning_theory The procedure is based on a novel decomposition of the expected loss of randomized classifiers.
__label__machine_vision No-Reference Point Cloud Quality Assessment (NR-PCQA) aims to objectively assess the human perceptual quality of point clouds without relying on pristine-quality point clouds for reference.
__label__machine_vision In this paper, we demonstrate this with a novel visual pretraining paradigm, LocCa, that incorporates location-aware tasks into captioners to teach models to extract rich information from images.
__label__reinforcement_learning Existing UED methods mainly rely on *regret*, a metric that measures the difference between the agent's optimal and actual performance, to guide curriculum design.
__label__neuroscience_and_cognitive_science We further drive the implications of our model from both theoretical and experimental points of view.
__label__machine_vision Moreover, the proposed method only consumes 44.3% training resources of UniAD and runs 3.4$\times$ faster in inference.
__label__machine_vision One key challenge to video restoration is to model the transition dynamics of video frames governed by motion.
__label__machine_vision In this work, we target adverse weather conditions and introduce an end-to-end domain adaptation strategy that leverages a fusion block, temporal-spatial teacher-student learning, and a temporal weather degradation augmentation approach.
"__label__privacy For a given notion of attack risk, our approach significantly
decreases noise scale, leading to increased utility at the same level of privacy."
__label__robotics This general architecture aligns the specific proprioception and vision inputs from distinct embodiments to a short sequence of tokens and then processes such tokens to map to control robots for different tasks.
__label__machine_learning_for_other_sciences_and_fields We propose to address these challenges through differentiable neural surrogates that exploit the geometric aspects of the problem.
__label__safety_in_machine_learning Building on this definition, we introduce probably approximately aligned (i.e., near-optimal) policies, and we derive a sufficient condition for their existence.
__label__deep_learning_architectures Then we propose the retention mechanism for sequence modeling, which supports three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent.
__label__safety_in_machine_learning But this is a computationally challenging problem that often must be solved a large number of times (e.g., for all examples in a training set).
__label__generative_models We further show that the feedback leveraged in OAIF is easily controllable, via instruction prompts to the LLM annotator.
__label__generative_models We provide a simple algorithm for learning our generative model and empirically demonstrate its ability to capture symmetries under affine and color transformations, in an interpretable way.
__label__learning_theory While the expected calibration error (ECE), which employs binning, is widely adopted to evaluate the calibration performance of machine learning models, theoretical understanding of its estimation bias is limited.
__label__reinforcement_learning Here, we explore how the performance of zero-shot RL methods degrades when trained on small homogeneous datasets, and propose fixes inspired by _conservatism_, a well-established feature of performant single-task offline RL algorithms.
__label__natural_language_processing On the ShareGPT dataset, LLaMA-2-7B with cross-layer merging achieves a compression ratio of $1.53\times$.
__label__machine_learning_for_physical_sciences The core of our \method{} is to learn time-evolving prompts using a graph ODE to adapt spatio-temporal forecasting models to different scenarios.
__label__diffusion_based_models Generating realistic images from arbitrary views based on a single source image remains a significant challenge in computer vision, with broad applications ranging from e-commerce to immersive virtual experiences.
__label__neuroscience_and_cognitive_science This one-shot drawing task requires powerful inductive biases that have not been systematically investigated.
__label__interpretability_and_explainability Our implementation is publicly available: https://github.com/mshukor/xl-vlms.
__label__learning_theory The simplest approach to IL, behavior cloning (BC) is thought to incur sample complexity with unfavorable quadratic dependence on the problem horizon, motivating a variety of different online algorithms that attain improved linear horizon dependence under stronger assumptions on the data and the learner’s access to the expert.
__label__robotics Videos of the robot are best viewed at baku-robot.github.io.
__label__neuroscience_and_cognitive_science Efficient coding theory posits that sensory circuits transform natural signals into neural representations that maximize information transmission subject to resource constraints.
__label__deep_learning_architectures This underscores the risk that ViT architectures optimized for ID accuracy might not perform well under OoD shifts.
__label__learning_theory This research reveals the unique computational abilities of ARDTs, aiming to broaden the architectural diversity in language model development.
__label__fairness In this study, we explore the potential for achieving fairness without compromising its utility when no prior demographics are provided to the training set, namely _harmless Rawlsian fairness_.
__label__algorithmic_game_theory While the success of large language models (LLMs) increases demand for machine-generated text, current pay-per-token pricing schemes create a misalignment of incentives known in economics as moral hazard: Text-generating agents have strong incentive to cut costs by preferring a cheaper model over the cutting-edge one, and this can be done “behind the scenes” since the agent performs inference internally.
__label__reinforcement_learning When the environment is known, previous work shows that this lookahead information can drastically increase the collected reward.
__label__machine_vision Meanwhile, the random noise introduces uncertainty in the output, which is unfriendly to image restoration tasks.
__label__reinforcement_learning However, for general LMDPs, there is no known learning algorithm that provably matches the existing lower bound.
__label__fairness Fair-range clustering extends classical clustering formulations by associating each data point with one or several demographic labels.
__label__reinforcement_learning Experimental results indicate the practical effectiveness of our method, offering a new direction in aligning a pre-trained model to preference.
__label__natural_language_processing FASTopic follows a new paradigm: Dual Semantic-relation Reconstruction (DSR).
__label__probabilistic_methods Valuable insights into these feasible solutions and their associated probabilities are embedded in the posterior distribution.
__label__natural_language_processing This verification model assigns a confidence score to each reasoning step, indicating the probability of arriving at the correct final answer from that point onward.
__label__deep_learning_architectures Previous studies have demonstrated that the Channel-Independent (CI) strategy improves forecasting performance by treating different channels individually, while it leads to poor generalization on unseen instances and ignores potentially necessary interactions between channels.
__label__deep_learning_architectures However, this approach necessitates selecting elements that need to be kept fixed, as well as centroids that should be adjusted throughout editing.
__label__speech_and_audio In this work, we present a novel method called Scattered Online Inference (SOI) that aims to reduce the computational complexity of ANNs.
__label__robotics To account for the multi-modal nature of the data, we perform prediction in a modality-aligned way, and for each input token predict the next token from the same modality.
__label__reinforcement_learning To address this problem, we rethink offline reinforcement learning as an optimal transportation problem.
__label__reinforcement_learning We show that the linear structure is necessary for the bandit case—without structure on the reward function, the regret has to scale polynomially with the number of states.
__label__diffusion_based_models The former applies the distillation objective solely to the motion representation, while the latter mitigates training-inference discrepancies by mixing distillation trajectories from both the low- and high-quality video domains.
__label__diffusion_based_models This has led to the emergence of pre-trained vision-language models as a tool for transferring representations learned from internet-scale data to downstream tasks and new domains.
"__label__reinforcement_learning To address this, we observe that all paths through a DFA correspond to a series of reach-avoid tasks and propose pre-training graph neural network embeddings on ""reach-avoid derived"" DFAs."
__label__optimization_for_deep_networks In this work we analyze various scaling limits of the training dynamics of transformer models in the feature learning regime.
__label__bandits Our result also implies a high-probability BOBW regret guarantee when the bounded true losses are protected with pure Local Differential Privacy (LDP), while the existing work ensures the (weaker) \emph{approximate} LDP with the regret bounds in expectation only.
__label__other What representation do deep neural networks learn?
"__label__learning_theory We theoretically demonstrate that ARDTs can compute complex functions, such as simulating automata, Turing machines, and sparse circuits, by leveraging ""chain-of-thought"" computations."
__label__bandits Specifically, we consider both the stochastic and the adversarial settings, and propose a suite of algorithms, each with different computation-regret trade-off.
__label__deep_learning_architectures Despite this progress, multiplications predominantly capture low-order interactions, thus remaining confined to a finite-dimensional interaction space.
__label__natural_language_processing Despite significant advancements in text generation and reasoning, Large Language Models (LLMs) still face challenges in accurately performing complex arithmetic operations.
__label__diffusion_based_models We thus suggest exposing the guidance interval as a hyperparameter in all diffusion models that use guidance.
__label__natural_language_processing To address this issue, we develop an expression discovery framework to explore potential allocation strategies.
__label__other To overcome this limitation, orthogonal to existing approaches, we propose a novel perspective that views the CL model ability in preserving old knowledge and performing well in new task as a matter of model sensitivity to parameter updates.
__label__diffusion_based_models This incurs significant expenses in time, computational resources, and especially the costs associated with calling fee-based APIs.
__label__learning_theory The goal of this paper is to investigate the complexity of gradient algorithms when learning sparse functions (juntas).
__label__optimization_for_deep_networks Although efficient for training, the model biases induced by such growing approaches are largely unexplored.
__label__machine_vision Additionally, we showcase its flexibility on CrowdPose, a popular occlusion benchmark with keypoints within the bounding box.
__label__learning_theory Our main result is a near sample-optimal nearly-linear time algorithm that provably recovers the ground-truth vector $x$ in the presence of outliers.
__label__generative_models However, recent works have shown that prompting these models to look back and critique their own generations can produce useful feedback.
__label__machine_learning_for_other_sciences_and_fields They have made great advances in improving the label quality of crowdsourced datasets.
__label__human-AI_interaction This work presents a significant step toward enabling effective communication and collaboration between artificial agents and humans in real-world teamwork settings.
__label__machine_vision We build the Mapverse benchmark, sourced from the Ithaca365 and nuPlan datasets, to evaluate our method in unsupervised 2D segmentation, 3D reconstruction, and neural rendering.
__label__machine_vision Our formulation is centered on the ability - both at training and test time - to query any arbitrary point of the human volume, and obtain its estimated location in 3D.
__label__generative_models Additionally, UDPM offers an interpretable and interpolable latent space, which gives it an advantage over traditional DDPMs.
__label__generative_models At every inference step we combine the $k$ drafts with the top-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options, using an n-gram interpolation with minimal compute overhead to filter out incoherent generations.
__label__learning_theory They thus extend well beyond the standard Gaussian assumptions commonly made in the literature.
__label__optimization_for_deep_networks We identify the set of parameterizations which admit well defined infinite width and depth limits that allow the attention layers to update throughout training, a relevant notion of feature learning in these models.
"__label__learning_theory Without computational considerations, the sample complexity of this learning problem is known to be 
$\widetilde{\Theta}(1/(\gamma^2 \epsilon))$."
__label__online_learning The objective is to maximize total revenue over a time horizon $T$.
__label__machine_learning_for_other_sciences_and_fields Our method involves fine-tuning the pre-trained diffusion model using a residue-level decomposed energy preference.
__label__machine_vision This approach forms an effective backbone for self-supervised representation learning, and we demonstrate that a simple off-the-shelf equivariant network operating in the pre-trained latent space can achieve results on par with meticulously-engineered, handcrafted networks designed to handle complex, nonlinear symmetries.
__label__machine_vision Due to the low computational overhead and inherent flexibility of explicit representations, plane-based explicit methods are popular ways to predict deformations for Gaussian-based dynamic scene rendering models.
__label__natural_language_processing Reinforcement Learning from Human Feedback (RLHF)has been crucial to the recent success of Large Language Models (LLMs), however it is often a complex and brittle process.
__label__generative_models Experiments on the existing benchmark RealEstate10K also confirm the effectiveness of our model.
__label__infrastructure Tokens initially evicted might regain importance after certain decoding steps.
__label__machine_vision Code and model can be found here: https://hwjiang1510.github.io/CoFie/
__label__optimization Randomly selecting an optimal solution as the label can introduce variability in the training data, which may hinder the model from learning stable patterns.
__label__graph_neural_networks In this paper, we propose a spectral approach for zero-shot node classification (SpeAr).
__label__machine_learning_for_physical_sciences For a very long time, computational approaches to the design of new materials have relied on an iterative process of finding a candidate material and modeling its properties.
__label__machine_vision However, this data-driven fusion approach is challenging to deploy in varying scenarios, especially in rapidly changing environments.
__label__causal_inference Empirical studies on both synthetic and real-world datasets validate our identifiability theory and the effectiveness of the proposed method in the finite-sample regime.
__label__diffusion_based_models Once the extracted pose information is transferred to the target object, the pose applier is fine-tuned in a self-supervised manner to better describe the target object's shapes with pose variations.
__label__reinforcement_learning Since TD(λ=0) makes an implicit Markov assumption and TD(λ=1) does not, a discrepancy between these estimates is a potential indicator of a non-Markovian state representation.
__label__machine_vision Our approach re-parameterizes surface colors as the product of normals and a designed Integrated Directional Illumination Vector (IDIV).
__label__safety_in_machine_learning In our work, we demonstrate that when lightly finetuning multiple runs from a $\textit{single}$ foundation model, the choice of randomness during training (linear head initialization, data ordering, and data subsetting) can lead to drastically different levels of agreement-on-the-line in the resulting ensemble.
__label__machine_vision Consequently, the support information is better utilized, leading to better performance.
__label__machine_vision However, alongside these advancements, the issue of hallucinations has emerged as a significant challenge, producing erroneous responses that are unrelated to the visual contents.
__label__reinforcement_learning Real-world applications of reinforcement learning often involve  environments where agents operate on complex, high-dimensional observations, but the underlying (``latent'')  dynamics are comparatively simple.
__label__diffusion_based_models To address this issue, we exploit classifier guidance, a training-free technique that steers diffusion models using an existing classifier, for personalized image generation.
__label__reinforcement_learning This bound guides the design of an optimal exploration policy attaining minimal sample complexity.
__label__reinforcement_learning The shift in dynamics results in significant performance degradation of policies trained in the source domain when deployed in a different target domain, posing a challenge for the practical application of reinforcement learning (RL) in real-world scenarios.
__label__deep_learning_architectures By exploring the similarities and disparities between the effective Mamba and subpar linear attention Transformer, we provide comprehensive analyses to demystify the key factors behind Mamba’s success.
__label__machine_learning_for_healthcare This eliminates the need for invasive procedures or uncertain treatment decisions.
__label__machine_vision Additionally with temporal feature modulation to regularize the contribution of temporal feature during test.
__label__optimization The classical Canonical Correlation Analysis (CCA) identifies the correlations between two sets of multivariate variables based on their covariance, which has been widely applied in diverse fields such as computer vision, natural language processing, and speech analysis.
__label__optimization Conformal prediction converts nearly any point estimator into a prediction interval under standard assumptions while ensuring valid coverage.
__label__machine_learning_for_physical_sciences Many physical or chemical processes described by this equation involve transitions between metastable states separated by high potential barriers that can hardly be crossed during a simulation.
__label__natural_language_processing We observe that per-layer importance statistics can serve as allocation indications, but their effectiveness depends on the allocation function between layers.
__label__human-AI_interaction In TEACh, we achieve a 12.6% improvement in goal-condition success.
__label__machine_learning_for_physical_sciences Fourier Neural Operators (FNOs) have shown promise for solving partial differential equations (PDEs).
__label__diffusion_based_models To address this problem, we leverage pre-trained large vision-language models (VLMs) to guide the evolution of the base model.
__label__learning_theory Recovering the underlying clustering of a set $U$ of $n$ points by asking pair-wise same-cluster queries has garnered significant interest in the last decade.
__label__learning_theory For the special case of subsampling without replacement, our results apply to independently sampled random features and kernel features and confirm recent conjectures (Conjectures 7 and 8) of the authors on the existence of such paths in Patil and Du (2023).
__label__reinforcement_learning We discover that the recurrent update of these models resembles a monoid, leading us to reformulate existing models using a novel monoid-based framework that we call memoroids.
__label__graph_neural_networks In this work, we propose a simple yet effective label smoothing for the transductive node classification task.
__label__robotics Our VidMan framework outperforms state-of-the-art baseline model GR-1 on the CALVIN benchmark, achieving a 11.7\% relative improvement, and demonstrates over 9\% precision gains on the OXE small-scale dataset.
__label__generative_models Our approach does not require 3D training data and can be implemented plug-and-play within existing frameworks.
"__label__graph_neural_networks In this work we design graph neural network architectures that capture optimal
approximation algorithms for a large class of combinatorial optimization problems,
using powerful algorithmic tools from semidefinite programming (SDP)."
__label__machine_vision Contrastive Language-Image Pretraining (CLIP) models maximize the mutual information between text and visual modalities to learn representations.
__label__diffusion_based_models However, training a diffusion model is computationally expensive, creating a pressing need to adapt off-the-shelf diffusion models for downstream generation tasks.
__label__probabilistic_methods The potential-aware trust region anchor selection considers the potential capability of the trust region for better local optimization.
__label__algorithmic_game_theory We show that with autobidders only, the price of anarchy of first-price auctions is $1/2$, and with both kinds of bidders, the price of anarchy degrades to about $0.457$ (the precise number is given by an optimization).
__label__machine_vision Many current methods use 3D/2D convolutions or attention mechanisms, but these have limitations in directly constructing geometry and accurately propagating features from related voxels, the completion likely fails while propagating features in a single pass without considering multiple potential pathways.
__label__machine_vision These variants are generic operators that can be directly plugged into existing image restoration networks as a drop-in replacement for the standard convolution unit, consuming fewer parameters.
__label__robotics We show that FACTORSIM outperforms existing methods in generating simulations regarding prompt alignment (i.e., accuracy), zero-shot transfer abilities, and human evaluation.
__label__generative_models To this end, we propose a novel diffusion-based pipeline that generates high-quality multi-view videos centered around a dynamic 3D object from text.
__label__safety_in_machine_learning In this paper, we propose Direct Unlearning Optimization (DUO), a novel framework for removing NSFW content from T2I models while preserving their performance on unrelated topics.
__label__probabilistic_methods In this vein, we contribute a conceptual understanding of coreset loss as a \emph{linear statistic} of the (random) coreset.
__label__neuroscience_and_cognitive_science This integration is achieved through an appropriate connectivity pattern between the BU and TD streams, a novel processing cycle that uses the TD stream twice, and a 'Counter-Hebb' learning mechanism that operates across both streams.
__label__reinforcement_learning Since IRL suffers from identifiability issues, many theoretical works on online IRL focus on estimating the entire set of rewards that explain the demonstrations, named the *feasible reward set*.
__label__machine_vision Our second contribution, **DreamClear**, is a DiT-based image restoration model.
__label__reinforcement_learning In this paper, we find that this instability stems from the autoregressive nature of RNNs, which causes even small changes in RNN parameters to produce large output variations over long trajectories.
__label__safety_in_machine_learning In this work, we formulate adversarial federated learning as a Bayesian Stackelberg Markov game (BSMG) to tackle poisoning attacks of unknown/uncertain types.
__label__machine_vision Extensive experiments on KITTI and nuScenes datasets demonstrate that our INTEGER achieves competitive performance in terms of accuracy and generalizability.
__label__graph_neural_networks However, these methods often fail to adequately capture topological information, leading to incorrect input-label mappings in replay samples.
__label__generative_models Adapting Large Language Models (LLMs) to new tasks through fine-tuning has been made more efficient by the introduction of Parameter-Efficient Fine-Tuning (PEFT) techniques, such as LoRA.
__label__bandits In this paper, we study the multi-armed bandit problem in the best-of-both-worlds (BOBW) setting with heavy-tailed losses, where the losses can be negative and unbounded but have $(1+v)$-th raw moments bounded by $u^{1+v}$ for some known $u>0$ and $v\in(0,1]$.
__label__diffusion_based_models These properties of LOCO Edit benefit greatly from the low-dimensional semantic subspace.
__label__machine_vision We construct a challenging dataset containing scenes with multiple objects and inter-reflections.
__label__optimization_for_deep_networks Results of independent interest are shown in both cases.
__label__causal_inference Since the treatments of interest often cannot be directly randomized, observational data is leveraged to learn CATEs, but this approach can incur significant bias from unobserved confounding.
__label__natural_language_processing The notable success of large language models (LLMs) has sparked an upsurge in building language agents to complete various complex tasks.
__label__deep_learning_architectures The state space models, employing recursively propagated features, demonstrate strong representation capabilities comparable to Transformer models and superior efficiency.
__label__machine_vision This exploratory study does not present state-of-the-art models; rather, it introduces an innovative method designed to increase in-context text length in multi-modality large language models (MLLMs) efficiently.
__label__diffusion_based_models We provide extensive experiments to demonstrate strong performance gains of the proposed method over contemporary baselines in the context of conditional graph generation, underscoring the potential of Twigs in challenging generative tasks such as inverse molecular design and molecular optimization.
__label__safety_in_machine_learning Our theoretical framework supports analyzing conformal prediction methods that involve calibrating model predictions and subsequently constructing conditionally valid prediction intervals on the same data, where the conditioning set or conformity scores may depend on the calibrated predictions.
__label__causal_inference In this paper, we investigate robust imitation learning within the framework of canonical Markov Decision Processes (MDPs) using partial identification, allowing the agent to achieve expert performance even when the system dynamics are not uniquely determined from the confounded expert demonstrations.
__label__machine_learning_for_physical_sciences This demonstrates that the QDEQ paradigm can be used to develop significantly more shallow quantum circuits for a given task, something which is essential for the utility of near-term quantum computers.
__label__machine_vision Code and supplementary materials are available at https://github.com/zlw9161/TARSS-Net.
__label__probabilistic_methods As illustrated by the success of integer linear programming, linear integer arithmetics is a powerful tool for modelling combinatorial problems.
__label__diffusion_based_models First, the ID-refined QR integration (IDQR) seamlessly intertwines the background styling with face ID, utilizing a unified SD-based framework with control networks.
__label__reinforcement_learning In response, we introduce GTA, Generative Trajectory Augmentation, a novel generative data augmentation approach designed to enrich offline data by augmenting trajectories to be both high-rewarding and dynamically plausible.
__label__machine_learning_for_other_sciences_and_fields However, existing neural set architectures often struggle to either capture comprehensive information from the superset or address complex interactions within the input.
__label__reinforcement_learning Remarkably, this regret bound is independent of the number of episodes $K$.
__label__neuroscience_and_cognitive_science Spiking Transformers, which integrate Spiking Neural Networks (SNNs) with Transformer architectures, have attracted significant attention due to their potential for low energy consumption and high performance.
__label__deep_learning_architectures Experiments on six widely used VG datasets, i.e., RefCOCO/+/g, ReferIt, Flickr30K, and GRefCOCO, demonstrate the superiority of SimVG.
__label__other Moreover, the derived AUC has a simple form and thus is easy to implement.
__label__neuroscience_and_cognitive_science This method can uncover complex rules that induce long nonlinear time dependencies, particularly involving factors like postsynaptic activity and current synaptic weights.
__label__online_learning [2023] study similar problems in stochastic multi-agent multi-armed bandits and show that $\sqrt{T}$-regret is possible after $T$ rounds, their fairness measure is the product of all agents' rewards, instead of their NSW (that is, their geometric mean).
__label__interpretability_and_explainability In our model, a prototype consists of **parts**, which can deform over irregular geometries to create a better comparison between images.
__label__learning_theory Assuming the optimal parameter satisfies a Gaussian prior and the data covariance matrix has a power-law spectrum of degree $a>1$, we show that the reducible part of the test error is $\Theta(M^{-(a-1)} + N^{-(a-1)/a})$.
__label__learning_theory Afterwards, we apply shapley values to assess the contribution of each view and utilize these contributions to promote view cooperation.
__label__diffusion_based_models In 3D, it removes over-smoothing, preserves higher-frequency detail, and brings the generation quality closer to that of 2D samplers.
__label__machine_vision To bridge this gap, we propose a general and concise TransAgent framework, which transports the knowledge of the isolated agents in a unified manner, and effectively guides CLIP to generalize with multi-source knowledge distillation.
__label__machine_vision Although 3DGS provides a promising neural rendering option, it is still hard to infer SDFs for surface reconstruction with 3DGS due to the discreteness, the sparseness, and the off-surface drift of 3D Gaussians.
__label__interpretability_and_explainability A key advantage of this approach is that it enables a domain-independent formal specification of explainability desiderata, offering a unified framework for generating and evaluating explanations.
__label__machine_learning_for_healthcare With multi-modal biological data, patient characterization can be enriched from two distinct views: the biological view and the phenotype view.
__label__natural_language_processing We evaluate our proposed method across multiple model architectures and factual downstream tasks.
__label__evaluation We present a novel approach called Importance Divergence (I-Div) to address the challenge of test label unavailability, enabling distribution discrepancy evaluation using only training samples.
__label__active_learning We design efficient algorithms for both and analyze them.
__label__interpretability_and_explainability The path to interpreting a language model often proceeds via analysis of circuits---sparse computational subgraphs of the model that capture specific aspects of its behavior.
__label__graph_neural_networks Based on our findings, we introduce the Message Passing Link Predictor (MPLP), a novel link prediction model.
__label__bandits This scenario unfolds as a game over $T$ rounds, leading to a competition of objectives between the player, aiming to minimize regret, and the arms, motivated by the desire to maximize their individual utilities.
__label__reinforcement_learning We find that representing the model emerges as the easiest task, followed by the optimal policy, while representing the optimal value function presents the most intricate challenge.
__label__natural_language_processing However, our analysis of these datasets reveals severe biases towards easy queries, with frequent failures to generate any correct response for the most challenging queries.
__label__algorithmic_game_theory For two agents, we give a truthful mechanism that takes agents' ordering over items as prediction.
__label__causal_inference If, additionally, the resulting experimental measurements are high-dimensional and the studied mechanisms nonlinear, the query of interest is generally not identified.
__label__machine_vision The generalization ability of deepfake detectors is vital for their applications in real-world scenarios.
__label__other Here, we consider a deep multi-head self-attention network, that is closely related to Transformers yet analytically tractable.
__label__machine_vision In brief, we generate the hard prototype by selecting the sample with the maximum distance from the cluster center.
__label__evaluation Eventually, we ameliorate the consistency of LVLMs by trigger-based diagnostic refinement, indirectly improving the performance of their caption.
__label__natural_language_processing Large language models (LLMs) exhibit advanced reasoning skills, enabling robots to comprehend natural language instructions and strategically plan high-level actions through proper grounding.
__label__natural_language_processing Instruction tuning improves the reasoning abilities of large language models (LLMs), with data quality and scalability being the crucial factors.
__label__learning_theory Diffusion models have become the most popular approach to deep generative modeling of images, largely due to their empirical performance and reliability.
__label__generative_models We propose GenArtist, a unified image generation and editing system, coordinated by a multimodal large language model (MLLM) agent.
__label__learning_theory Similarly, in generative models, such as those trained on existing artworks or music, it is important to ensure that any generated content influenced by these works appropriately credits the original creators.
__label__other Despite the potential benefits of client participation in federated online model fine-tuning, existing analyses have not conclusively demonstrated its superiority over local model fine-tuning.
__label__neuroscience_and_cognitive_science In conclusion, pretrained neural networks can serve to extract representations for cognitive models, as they appear to capture some fundamental aspects of cognition that are transferable across tasks.
__label__natural_language_processing Second, we propose an exponential moving average-based coefficient learning method to strengthen our higher-order predictor.
__label__other This mismatch problem recurs after every parameter averaging, consequently slowing down model convergence and degrading overall performance.
__label__probabilistic_methods Permutations, trees, partitions, and binary sequences frequently appear as building blocks in Bayesian nonparametric models, and these models have been studied and developed independently.
__label__optimization Experiments on both simulation functions and real scenarios (black-box adversarial attacks neural architecture search, and parameter-efficient fine-tuning for large language models), show its efficacy and efficiency.
__label__deep_learning_architectures Significantly, we extended our explanation and CEA to Mixture of Experts (MoE) Transformers.
__label__optimization_for_deep_networks are designed as fixed-point inference accelerators, without training capabilities.
__label__optimization Focusing on the heterogeneous case, where different machines may draw samples from different data-distributions, we design the first local update method that provably benefits over the two most prominent distributed baselines: namely Minibatch-SGD and Local-SGD.
__label__optimization_for_deep_networks the sharpness), and find that certain spectral properties under $\mu$P are largely independent of the width and depth of the network along the training trajectory.
__label__probabilistic_methods Laplace approximations are popular techniques for endowing deep networks with epistemic uncertainty estimates as they can be applied without altering the predictions of the trained network, and they scale to large models and datasets.
__label__machine_vision Firstly, we design a novel Angular Perception Pretext to eliminate the annotation requirement.
__label__natural_language_processing When answering questions, large language models (LLMs) can convey not only an answer to the question, but a level of confidence about the answer being correct.
__label__safety_in_machine_learning In this paper, we make a key finding that recent test-time adaptation (TTA) methods not only improve OOD performance, but drastically strengthens the ACL and AGL trends in models, even in shifts where models showed very weak correlations before.
__label__machine_vision Our code is available at https://github.com/jiequancui/DKL.
__label__optimization We prove that the motif-oriented IM problem is NP-hard and that the influence function is neither supermodular nor submodular, in contrast to the classical \emph{IM} setting.
__label__diffusion_based_models That way, our model, developed from a diffusion prior, is able to capture the semantic correspondence between separate images in a self-supervised manner.
__label__probabilistic_methods This has led to the rise of simulation-based inference (SBI), a class of machine learning-enabled techniques for approaching inverse problems with stochastic simulators.
__label__machine_vision By integrating complex transformations like AugMix, unusable by prior equivariant methods, this approach enhances performance across tasks, underscoring its adaptability and resilience.
__label__evaluation We then train a simple linear classifier on top without storing any exemplars, processing one sample at a time in an online continual learning setting.
__label__bandits Experimental results demonstrate its superiority over the state-of-the-art dueling bandit algorithms in the presence of adversarial feedback.
__label__diffusion_based_models Specifically, to surmount the catastrophic forgetting of old concepts, we develop a concept consolidation loss and an elastic weight aggregation module.
__label__graph_neural_networks Graph Neural Networks (GNNs) have demonstrated remarkable performance across a spectrum of graph-related tasks, however concerns persist regarding their vulnerability to adversarial perturbations.
__label__optimization Stochastic Gradient Descent (SGD) with adaptive steps is widely used to train deep neural networks and generative models.
__label__other Our theoretical analysis shows that the reason behind this might be that the first initialization allows the use of larger learning rates (without causing output instability) compared to the second initialization, resulting in more efficient learning of the first scheme.
__label__reinforcement_learning Meta-reinforcement learning (meta-RL) approaches abandon the aim of zero-shot *generalization*—the goal of standard reinforcement learning (RL)—in favor of few-shot *adaptation*, and thus hold promise for bridging larger generalization gaps.
__label__machine_vision We hope to enable LVLMs to recognize and reason about multiple objects that often occur in realistic visual scenes, provide insights, and quantify our progress towards mitigating the issues.
__label__natural_language_processing We attribute this discrepancy to the DPO objective, which is empirically and theoretically ill-suited to correct even mild ranking errors in the reference model, and derive a simple and efficient formula for quantifying the difficulty of learning a given preference datapoint.
__label__fairness However, standard GMs can result in biased outcomes, especially when the underlying data involves sensitive characteristics or protected groups.
__label__machine_vision Similar to the birth of CNN inspired by receptive fields in the biological visual system, we draw inspiration from the information subsystem pathways in the biological visual system and propose Model Disassembling and Assembling (MDA).
__label__generative_models Differently, in this work we quantize each individual joint into one vector, which i) simplifies the quantization process as the complexity associated with a single joint is markedly lower than that of the entire pose; ii) maintains a spatial-temporal structure that preserves both the spatial relationships among joints and the temporal movement patterns; iii) yields a 2D token map, which enables the application of various 2D operations widely used in 2D images.
__label__learning_theory We theoretically show that this method guarantees the classification error across all $P_i$s can be suitably bounded.
__label__learning_theory Moreover, our analysis can be naturally applied to other logical reasoning tasks such as chain-of-thought (COT), which provides a new perspective different from previous work that focuses on expressivity.
__label__machine_vision We conduct extensive experiments and benchmark SaSPA against both traditional and generative data augmentation techniques.
__label__machine_learning_for_physical_sciences The weight analysis of physics-AI modules indicates that physics conducts major evolution while AI performs corrections adaptively.
__label__machine_learning_for_social_sciences Large Language Model Multi-Agent Systems (LLM-MAS) have greatly progressed in solving complex tasks.
__label__generative_models The generated gestures may also exhibit unnatural jittering issues.
__label__machine_learning_for_other_sciences_and_fields Most models are trained with extensive compute resources until performance gains plateau, focusing primarily on increasing model sizes rather than optimizing the efficient compute frontier that balances performance and compute budgets.
__label__machine_learning_for_physical_sciences It helps preserve energies for conservative systems while serving as a strong inductive bias for non-conservative, reversible systems.
__label__neuroscience_and_cognitive_science Recent advances in calcium imaging enable simultaneous recordings of up to a million neurons in behaving animals, producing datasets of unprecedented scales.
__label__speech_and_audio Surprisingly, we also observe that STAR prevents the adapted model from the common catastrophic forgetting problem without recalling source-domain data.
__label__neuroscience_and_cognitive_science Inspired by biologically plausible neuroscience theories of cognition, we propose Predictive Attractor Models (PAM), a novel sequence memory architecture with desirable generative properties.
__label__learning_theory However, it is still challenging to enhance performance them without sharing private data and models from individual parties.
__label__machine_vision This method performs latent compression learning by maximizing the mutual information between the inputs and outputs of a causal attention model.
"__label__natural_language_processing Unlike approaches that extend or search for rationales, DeAR is featured by 1) adopting a tree-based question decomposition manner to plan the organization of rationales, which mimics the logical planning inherent
in human cognition; 2) globally updating the rationales at each reasoning step through natural language feedback."
__label__learning_theory In this paper, we provide a rigorous theoretical analysis, based on the two-class contextual stochastic block model (CSBM), of the performance of vanilla graph convolution from which we remove the principal eigenvector to avoid oversmoothing.
__label__interpretability_and_explainability We also uncover a surprising connection between group testing and the Möbius transform.
__label__optimization_for_deep_networks Our study methodically explores the factors affecting the symmetry of DNN valleys, encompassing (1) the dataset, network architecture, initialization, and hyperparameters that influence the convergence point; and (2) the magnitude and direction of the noise for 1D visualization.
__label__generative_models The experiment results from motif distribution, diversity checks, and genome integration tests unequivocally show that A&E outperforms state-of-the-art AR models and DMs in genomic sequence generation.
__label__optimization_for_deep_networks and (2) Under what initial conditions and architectural specifics does the Transformer achieve rapid convergence during training?
__label__machine_learning_for_physical_sciences Numerous biological and physical processes can be modeled as systems of interacting samples evolving continuously over time, e.g.
__label__machine_vision We provide theoretical and empirical evidence that, due to the depth ambiguity inherent to monocular 3D human pose estimation, traditional regression models suffer from pose-topology consistency issues, which standard evaluation metrics (MPJPE, P-MPJPE and PCK) fail to assess.
__label__interpretability_and_explainability Attention-based transformers have been remarkably successful at modeling generative processes across various domains and modalities.
__label__machine_learning_for_other_sciences_and_fields The remarkable success of Large Language Models (LLMs) across diverse tasks has driven the research community to extend their capabilities to molecular applications, leading to the development of molecular LLMs.
__label__neuroscience_and_cognitive_science We provide an extremely simple but effective way to train high-accuracy spiking neural networks.
__label__machine_vision To reduce the computational cost, we design progressive coordinate decoding to cooperate with continuous heatmap regression, in which localization no longer requires the complete generation of high-resolution heatmaps.
__label__bandits We provide a unified framework to analyze these corruptions.
__label__machine_learning_for_other_sciences_and_fields Specifically, we study the problem of Contrastive PhenoMolecular Retrieval, which consists of zero-shot molecular structure identification conditioned on phenomic experiments.
__label__privacy We study the differentially private top-$k$ selection problem, aiming to identify a sequence of $k$ items with approximately the highest scores from $d$ items.
__label__machine_vision This approach significantly mitigates the error in the restored image but lacks theoretical support regarding the existence of such fixed points.
__label__reinforcement_learning Extensive experiments conducted in DeepMind Control and Meta-World environments demonstrate that SCR achieves better performance comparing to other recent metric-based methods in demanding generalization tasks.
__label__machine_vision Leveraging the reasoning capabilities and world knowledge of large language models, and augmented by the Segment Anything Model, VideoLISA generates temporally consistent segmentation masks in videos based on language instructions.
__label__safety_in_machine_learning Additionally, we introduce three large-scale red-teaming datasets for studying the safety risks associated with text-to-image models.
__label__graph_neural_networks We sincerely hope that our work can stimulate increasing attention from researchers on the vulnerability of GNN fairness, and encourage the development of corresponding defense mechanisms.
__label__diffusion_based_models Recent advancements in text-to-image diffusion models have enabled the personalization of these models to generate custom images from textual prompts.
__label__machine_vision This unified framework allows TabPedia to seamlessly integrate VTU tasks, such as table detection, table structure recognition, table querying, and table question answering, by leveraging the capabilities of large language models (LLMs).
__label__machine_vision Soft prompt tuning has mitigated the task misalignment, yet the data misalignment remains a challenge.
__label__natural_language_processing The perception and motor function modules operate in tandem, allowing the system to speak and listen to the user simultaneously.
__label__machine_vision Dataset Distillation aims to compress a large dataset into a significantly more compact, synthetic one without compromising the performance of the trained models.
__label__diffusion_based_models It significantly advances the state-of-the-art discrete diffusion on 5 zero-shot language modeling benchmarks (measured by perplexity) at the GPT-2 scale.
__label__graph_neural_networks Additionally, a data augmentation method named Trajectory Mirror (TM) is introduced to improve generalization by exposing the model to varied trajectory patterns.
__label__machine_vision The LLM is responsible for understanding the user's text instructions and providing text responses and pixel-level segmentation results based on the visual information.
__label__interpretability_and_explainability Principal component analysis (PCA) is one of the most fundamental tools in machine learning with broad use as a dimensionality reduction and denoising tool.
__label__evaluation Extensive empirical evaluations across $\sim$31,000 models demonstrate that \textit{S\&S} achieves highly-efficient approximate accuracy measurement, reducing compute cost from 180 GPU days to 5 GPU hours ($\sim$1000x reduction) on a single A100 GPU, with low approximation error.
__label__graph_neural_networks We then devise an efficient form for N2C-Attn using the cluster-wise message-passing framework, achieving linear time complexity.
__label__reinforcement_learning Our results improve upon or match the state-of-the-art performance under the total variation and KL divergence models, and provide the first result for the $\chi^2$ divergence model.
__label__safety_in_machine_learning While post-training alignment via human feedback shows promise, these methods are often limited to generative AI settings where humans can interpret and provide feedback on model outputs.
__label__safety_in_machine_learning Specifically, we construct a differentiable camera Image Signal Processing (ISP) proxy network to compensate for the physical-to-digital transition gap.
__label__diffusion_based_models We observe that as diffusion models transition from memorization to generalization, their corresponding nonlinear diffusion denoisers exhibit increasing linearity.
__label__optimization Moreover, distinct from common adaptive schemes, we define the step size recursively as a function of the gradient norm and the prediction error in the optimistic update.
__label__graph_neural_networks These explainers, however, only explain an instance (e.g., a graph) and fail to uncover the combinatorial reasoning learned by a GNN from the training data towards making its predictions.
__label__reinforcement_learning We prove that simply adding an appropriate regularization term ensures convergence of the algorithm.
"__label__machine_vision Project website:
https://open3dvlab.github.io/NeuRodin/"
__label__reinforcement_learning However, existing HSI techniques are limited to specific object dynamics and privileged information, which prevents the development of more comprehensive applications.
__label__causal_inference Causal inference from observational data has attracted considerable attention among researchers.
__label__machine_learning_for_other_sciences_and_fields Presently, the task of protocol translation predominantly requires the manual and labor-intensive involvement of domain experts and information technology specialists, rendering the process time-intensive.
__label__optimization We show that by moving through this larger space, our objective converges to a deterministic (zero variance) solution, avoiding bad stationary points.
__label__machine_learning_for_other_sciences_and_fields The hierarchical encoder models correlations between different levels of hierarchy.
__label__diffusion_based_models We address the problem of multi-object 3D pose control in image diffusion models.
__label__reinforcement_learning The best STAR estimator outperforms baselines in all twelve cases studied, and even the median STAR estimator surpasses the baselines in seven out of the twelve cases.
__label__machine_vision VPNet has shown superior performance and achieved state-of-the-art results on the SemanticKITTI and SemanticPOSS datasets.
__label__neuroscience_and_cognitive_science Further analysis indicated that high accuracy of various BCI decoding tasks could be achieved by associating labels with EEG intrinsic temporal autocorrelation features.
__label__reinforcement_learning This challenging optimization landscape leads to sub-optimal performance on several benchmark tasks.
__label__evaluation Detailed error profiles are provided for a better understanding of LLMs' behavior.
__label__reinforcement_learning Extensive experiments are conducted in navigation, manipulation, and locomotion, verifying DRAIL’s effectiveness compared to prior imitation learning methods.
__label__online_learning Moreover, we present nearly optimal bounds of $\tilde{\Theta}(k)$ on the gap between randomized and deterministic learners, as well as between adaptive and oblivious adversaries in the bandit feedback setting.
__label__probabilistic_methods We provide theoretical analyses of our method and demonstrate its effectiveness with both toy examples and practical applications.
__label__natural_language_processing In response to these challenges, we introduce TableRAG, a Retrieval-Augmented Generation (RAG) framework specifically designed for LM-based table understanding.
__label__privacy The standard composition-based privacy analysis of DP-SGD effectively assumes that the adversary has access to all intermediate iterates, which is often unrealistic.
__label__deep_learning_architectures S3 works by creating non-overlapping segments from the original sequence and shuffling them in a learned manner that is optimal for the task at hand.
__label__generative_models some predefined criterion.
__label__machine_vision Our model can acquire strong robustness to the noises, achieving significant improvements in accuracy, edge quality, efficiency, and generalizability on five different benchmarks.
__label__algorithmic_game_theory We introduce the set of $k$-mediator deviations, which generalize the untimed communication deviations recently introduced by Zhang, Farina and Sandholm [2024] to the case of having multiple mediators, and we develop algorithms for minimizing the regret with respect to this set of deviations in $N^{O(k)}/\epsilon^2$ rounds.
__label__machine_vision Our work is motivated by the finding that audio signals, enriched with speech content, can provide precise information effectively reflecting facial movements.
__label__online_learning We clarify the unnecessary nature of collaboration in previous federated algorithms for distributed online multi-kernel learning, and improve the regret bounds at a smaller computational and communication cost.
__label__bandits Causal knowledge about the relationships among decision variables and a reward variable in a bandit setting can accelerate the learning of an optimal decision.
__label__neuroscience_and_cognitive_science We evaluate our approach on several synthetic dynamical systems, including an empirically-derived brain-computer interface experiment, and demonstrate more accurate latent variable inference in nonlinear systems with diverse noise conditions.
__label__reinforcement_learning and (2) How many trajectories are necessary for effective learning?
__label__deep_learning_architectures The design substantially reduces GPU memory demands, yet retains global attention capability.
__label__reinforcement_learning We study reinforcement learning with _multinomial logistic_ (MNL) function approximation where the underlying transition probability kernel of the _Markov decision processes_ (MDPs) is parametrized by an unknown transition core with features of state and action.
__label__infrastructure Also, computing attention over long KV cache incurs more memory access, hurting the end-to-end latency.
__label__natural_language_processing With the assistance of MACM, the accuracy of GPT-4 Turbo on the most challenging level five mathematical problems in the MATH dataset increase from $\mathbf{54.68\\%}  \text{ to } \mathbf{76.73\\%}$.
__label__natural_language_processing Finally, we successfully train the model on sequence length 128 and achieve better performance at evaluation sequence length 8192, compared with other static positional encoding methods, revealing the benefit of the adaptive positional encoding method.
__label__machine_vision These strategies are grounded in empirical evidence and theoretical backing.
__label__optimization_for_deep_networks Specifically, we introduce the notion of \textit{nearest simplex ETF geometry} for the penultimate layer features at any given training iteration, by formulating it as a Riemannian optimisation.
__label__interpretability_and_explainability Existing methods in functional data analysis are promising but often not expressive enough to account for all interactions and non-linearities and do not scale well to large datasets.
__label__machine_vision Furthermore, GenRec demonstrates extraordinary robustness in scenarios that only limited frames can be observed.
__label__causal_inference Instead, it involves a quantitative joint optimization of bias and variance.
__label__probabilistic_methods Scientific modeling applications often require estimating a distribution of parameters consistent with a dataset of observations - an inference task also known as source distribution estimation.
__label__speech_and_audio In particular, we propose a novel module Audio-Visual Cloud Splatting which decodes AV anchor points into a spatial audio transfer function for the arbitrary viewpoint of the target listener.
__label__reinforcement_learning Furthermore, we theoretically prove that AgA effectively attracts gradients to stable fixed points of the collective objective while considering individual interests, and we validate these claims with empirical evidence.
__label__machine_vision We present both qualitative and quantitative results across a diverse range of applications that largely improve upon baselines.
__label__generative_models Existing radiance representations either require an implicit feature decoder, which significantly degrades the modeling power of the representation, or are spatially unstructured, making them difficult to integrate with mainstream 3D diffusion methods.
__label__optimization_for_deep_networks We report empirical results on various models and benchmarks to verify GmP's advantages of optimization stability, convergence speed and generalization performance.
__label__probabilistic_methods We present a new approach for Neural Optimal Transport (NOT) training procedure, capable of accurately and efficiently estimating optimal transportation plan via specific regularization on dual Kantorovich potentials.
__label__reinforcement_learning As opposed to model-free RL, model-based RL (MBRL) offers a potential solution with efficient data utilization through planning.
__label__learning_theory Our algorithm, based on a policy gradient method, incorporates a novel quantum subroutine for solving the matrix Lyapunov equation.
__label__graph_neural_networks Graph neural networks (GNNs) have been widely used to predict properties and heuristics of mixed-integer linear programs (MILPs) and hence accelerate MILP solvers.
__label__natural_language_processing Our approach involves leveraging Iterated Learning (IL), a Bayesian framework that elucidates how subtle biases are magnified during human cultural evolution, to explain some behaviors of LLMs.
__label__optimization_for_deep_networks The contextual generalization here can be attained via  learning  the template function for each task in-context, where all template functions lie in a linear space with $m$ basis functions.
__label__machine_learning_for_physical_sciences Considering that complex real-world observations often have different resolutions, we propose the Fourier Neural Processes (FNP) for arbitrary-resolution data assimilation in this paper.
__label__active_learning Conducting experiments and gathering data for machine learning models is a complex and expensive endeavor, particularly when confronted with limited information.
__label__optimization Trained by a highly efficient model-tailored Greedy-guided Self Trainer (GST) which does not require any priori optimal labels, VCM significantly outperforms competitors in both computational efficiency and solution quality with a remarkable generalization ability.
__label__causal_inference The optimal ITR can be robustly and effectively computed by projected gradient descent.
__label__neuroscience_and_cognitive_science Based on future maximum entropy production, NeuroMOP contributes to a novel theory of neural variability that reconciles stochastic and deterministic behaviors within a single framework.
__label__optimization In this paper, we introduce faster accelerated primal-dual algorithms for minimizing a convex function subject to strongly convex function constraints.
__label__learning_theory We also show that our results extend to the more challenging setting of learning generalized linear models with a known link function under Massart noise, achieving a similar sample complexity to the halfspace case.
__label__reinforcement_learning REBEL can also cleanly incorporate offline data and be extended to handle the intransitive preferences we frequently see in practice.
__label__machine_learning_for_healthcare Moreover, these datasets contain a significant amount of inconsistent noise.
__label__machine_vision Subsequently, under the guidance of physical priors, we design a disentanglement module to discriminate among different illumination degradation regions.
__label__algorithmic_game_theory These findings suggest that incorporating historical information can significantly enhance the performance of hand abstraction algorithms, positioning KrwEmd as a promising approach for advancing strategic computation in large-scale adversarial games.
__label__machine_vision However, when directly turning to person representation learning, these general pre-training methods suffer from unsatisfactory performance.
__label__other Specifically, we frame the user sequence imagination as a reinforcement learning problem and develop a recommendation-focused reward function to evaluate to what extent a user can help recommend the OOV items.
__label__diffusion_based_models Our research demonstrates that stable diffusion is a promising approach to robust watermarking, able to withstand even stable-diffusion-based attack methods.
__label__optimization_for_deep_networks Fine-tuning LLaMA-3-70B on GSM8K, QPiSSA attains an accuracy of 86.05\%, exceeding the performances of QLoRA at 81.73\%.
__label__natural_language_processing We empirically demonstrate that the hypernetwork generalizes to new tokenizers both with encoder (e.g., XLM-R) and decoder LLMs (e.g., Mistral-7B).
__label__generative_models We propose the Latent Prompt Transformer (LPT), a novel generative model comprising three components: (1) a latent vector with a learnable prior distribution modeled by a neural transformation of Gaussian white noise; (2) a molecule generation model based on a causal Transformer, which uses the latent vector as a prompt; and (3) a property prediction model that predicts a molecule's target properties and/or constraint values using the latent prompt.
__label__machine_vision On top of the group-equivariant spatial feature that selectively detects objects similar to the input group, we additionally introduce an explorative group update strategy that reduces the false negative detection in the target domain, further reducing the inter-domain gap.
__label__safety_in_machine_learning Out-of-distribution (OOD) detection is essential for model trustworthiness which aims to sensitively identity semantic OOD samples and robustly generalize for covariate-shifted OOD samples.
__label__machine_learning_for_healthcare While optimization-based methods boast generalizability across modalities and robust performance, learning-based methods promise peak performance, incorporating weak supervision and amortized optimization.
__label__diffusion_based_models Therefore, we first propose a lightweight, scalable, operator known as Hydra Block for attire combinations.
__label__safety_in_machine_learning With the rapid development of large language models (LLMs) and ever-evolving practical requirements, finding an efficient and effective alignment method has never been more critical.
__label__reinforcement_learning This decomposition effectively focuses each agent on specific tasks, proving highly efficient within our pipeline.
__label__machine_vision We rethink the common practice of using binary preferences (*i.e.
__label__generative_models Moreover, we introduce a method for approximating intersecting points among three hypersurfaces contributing to broader applications.
__label__online_learning Perhaps surprisingly, all of them are far from being truthful.
__label__machine_vision Experimental results show that MemVLT achieves new state-of-the-art performance.
__label__safety_in_machine_learning Classifiers trained with Empirical Risk Minimization (ERM) tend to rely on attributes that have high spurious correlation with the target.
__label__online_learning Our code is available at https://github.com/Zi-Jian-Gao/Stabilizing-Zero-Shot-Prediction-ZAF.
__label__evaluation Experimental results demonstrate that this protocol not only ensures high-quality annotations but can also reduce evaluation costs by nearly 50\%.
__label__graph_neural_networks We provide a comprehensive interpretation of our framework's capability to capture intricate dynamics through the lens of a non-Markovian graph random walk with node feature updating driven by an anomalous diffusion process over the graph.
__label__deep_learning_architectures However, the model size and corresponding computational complexity are constantly scaled up in pursuit of higher performance.
__label__reinforcement_learning Previously known formulations studied couplings between the entire joint distribution induced by the chains, and derived solutions via a reduction to dynamic programming (DP) in an appropriately defined Markov decision process.
__label__machine_vision Extensive quantitative and qualitative experiments on both table perception and comprehension tasks, conducted across various public benchmarks, validate the effectiveness of our TabPedia.
__label__safety_in_machine_learning Furthermore, we conduct numerical studies to showcase the effectiveness and validity of our procedures across various scenarios.
__label__causal_inference Estimating individualized treatment rules (ITRs) is fundamental in causal inference, particularly for precision medicine applications.
__label__optimization To address this challenge, we propose D-AdaST, a Distributed Adaptive minimax method with Stepsize Tracking.
__label__graph_neural_networks Yet, current methods often fall short in capturing longer ranges, hierarchical structures, or community structures, which are common in various graphs such as molecules, social networks, and citation networks.
__label__privacy Both of these benefits are closely related to our new results on the infinite Wasserstein distance tracking of the adjacent (un)learning processes.
__label__safety_in_machine_learning Experimental results on diverse datasets consistently demonstrate that DoFIT excels in cross-domain collaborative training and exhibits significant advantages over conventional FIT methods in alleviating catastrophic forgetting.
__label__probabilistic_methods In particular, we study Elo under the Bradley-Terry-Luce model and, using techniques from Markov chain theory, show that Elo learns the model parameters at a rate competitive with the state-of-the-art.
__label__deep_learning_architectures Our method is deployed for a pre-trained hyperprior and for a more flexible model.
__label__natural_language_processing This ensures that the extracted rules always incorporate the most recent knowledge and better generalize to the predictions on future events.
__label__graph_neural_networks In this paper, we envision the graph as a network of interconnected node sets without compressing each cluster into a single embedding.
__label__bandits We propose the CBA (Confidence-rated Bandits with Abstentions) algorithm, which exploits this assumption to obtain reward bounds that can significantly improve those of the classical Exp4 algorithm.
__label__safety_in_machine_learning We will release our code upon paper acceptance.
__label__reinforcement_learning We establish a Bellman equation for CCMDPs and further prove the existence of a flipping-based policy within the optimal solution sets.
__label__interpretability_and_explainability The proposed method utilizes a novel loss function that aims to closely reproduce the original similarity patterns between text-and-audio pairs in the generated explanations.
__label__optimization In addition, we validate the efficacy of CRONOS and CRONOS-AM through extensive large-scale numerical experiments with GPU acceleration in JAX.
__label__neuroscience_and_cognitive_science Moreover, some tasks may require very long time horizons which backpropagation cannot handle given typical GPU memory limits.
__label__machine_vision To achieve this, we present EgoChoir, which links object structures with interaction contexts inherent in appearance and head motion to reveal object affordance, further utilizing it to model human contact.
__label__optimization Despite their simple intuition, convolutions are more tedious to analyze than dense layers, which complicates the transfer of theoretical and algorithmic ideas to convolutions.
__label__graph_neural_networks However, structure-based architectures such as Graph Neural Networks (GNNs) are yet to show the benefits of scale mainly due to lower efficiency of sparse operations, large data requirements, and lack of clarity about the effectiveness of various architectures.
__label__machine_vision Recent studies on generalizable object detection have attracted increasing attention with additional weak supervision from large-scale datasets with image-level labels.
__label__deep_learning_architectures The chunkwise recurrent representation facilitates efficient long-sequence modeling with linear complexity, where each chunk is encoded parallelly while recurrently summarizing the chunks.
__label__graph_neural_networks For the first time, we observe that GNNs benefit tremendously from the increasing scale of depth, width, number of molecules and associated labels.
__label__machine_vision Especially in the ImageNet-1k classification, DTEM achieves a 37.2\% reduction in FLOPs while maintaining a top-1 accuracy of 79.85\% with DeiT-small.
__label__optimization However, the conventional FP techniques are insufficient: the classic Dinkelbach's transform can only deal with a single ratio and hence is limited to the two-class clustering, while the state-of-the-art quadratic transform accounts for multiple ratios but fails to convert the NCut problem to a tractable form.
__label__machine_vision Specifically, parallel recognition in the bottom LLM layers is implemented via object queries, a popular mechanism in DEtection TRansformer, which we find to harmonize well with the LLM layers.
__label__machine_learning_for_other_sciences_and_fields This data sparsity issue can lead to both inaccurate and unfair diagnoses.
__label__machine_vision Accordingly, we propose a Flatness Index Score (FIS) to assess the flatness by measuring the classification and localization fluctuation before and after perturbations of model parameters and a Prototypical Distance Ratio (PDR) score to seek the minima by measuring the transferability and discriminability of the models.
__label__reinforcement_learning However, one issue that limits its practical application is its brittleness, sometimes failing to train in the presence of small changes in the environment.
__label__optimization Usually these objectives are conflicting and cannot be optimized simultaneously, making it necessary to find trade-offs.
"__label__neuroscience_and_cognitive_science To achieve real-time SEUDO (realSEUDO), we introduce a combination of new algorithmic improvements, a fast C-based implementation, and a new cell finding loop to enable realSEUDO to identify new cells on-the-fly with no ""warm-up"" period."
__label__safety_in_machine_learning However, its effectiveness in improving the AR of SSMs remains unclear.
__label__neuroscience_and_cognitive_science Intrinsic dynamics within the brain can accelerate learning by providing a prior scaffolding for dynamics aligned with task objectives.
"__label__safety_in_machine_learning To uncover such vulnerabilities, we propose a novel red teaming approach AgentPoison, the first backdoor attack targeting generic and RAG-based LLM agents by poisoning their long-term memory or
RAG knowledge base."
__label__probabilistic_methods Consequently, their collective potential to enhance anomaly detection performance remains largely untapped.
__label__optimization Laplace learning algorithms for graph-based semi-supervised learning have been shown to produce degenerate predictions at low label rates and in imbalanced class regimes, particularly near class boundaries.
__label__active_learning For SGD with a constant step size update, we present convergence results for linear classifiers and linearly separable datasets using squared hinge loss and similar training loss functions.
__label__diffusion_based_models We introduce Director3D, a robust open-world text-to-3D generation framework, designed to generate both real-world 3D scenes and adaptive camera trajectories.
__label__machine_vision However, while it is possible to collect data from new scenarios, the annotations are not always available.
__label__safety_in_machine_learning This procedure results in an LLM that exhibits benign behavior in full precision but when quantized, it follows the adversarial behavior injected in step (i).
__label__online_learning For reproducibility, the source code of our work is available at https://github.com/maorong-wang/ESRM.
__label__generative_models Moreover, we propose a new dynamic-aware data processing pipeline and a consistency regularization method to enhance the consistency of auto-regressive video generation.
__label__diffusion_based_models Next, a Gaussian-driven Multi-view Latent Diffusion Model serves as the \emph{Decorator}, modeling the image sequence distribution given the camera trajectories and texts.
__label__learning_theory However, the theoretical analyses of such landscape results often rely on strong assumptions, such as the sampled measurements are (complex) Gaussian.
__label__optimization We also prove a general $\tilde{\Omega}(d\cdot \mu_\mathbf{y}(\mathbf{X}))$ space lower bound when $\epsilon$ is constant, showing that the dependency on $\mu_\mathbf{y}(\mathbf{X})$ is not an artifact of mergeable coresets.
__label__natural_language_processing Also, we observe that existing benchmarks do not clearly exhibit the trade-off between learning and retaining, therefore we propose a new benchmark, LAMA-ckl, to address this issue.
__label__other Our method incorporates into Neural Radiance Field (NeRF) pipelines the split sum approximation used with image-based lighting for real-time physically based rendering.
__label__probabilistic_methods This creates a fundamental flaw in the application of Bayesian principles as it breaks the correspondence between uncertainty over the parameters with uncertainty over the parametrized function.
__label__machine_vision We propose a method for metric-scale monocular depth estimation.
__label__safety_in_machine_learning However, the vulnerability of these networks to adversarial attacks, primarily evasion-based, poses a concerning threat to their functionality.
__label__machine_vision Current multimodal and multitask foundation models, like 4M or UnifiedIO, show promising results.
__label__safety_in_machine_learning Second, to retain more domain information, DoFIT initializes intra-domain weights by incorporating inter-domain information into a less-conflicted parameter space.
__label__learning_theory Boosting is a highly successful ML-born optimization setting in which one is required to computationally efficiently learn arbitrarily good models based on the access to a weak learner oracle, providing classifiers performing at least slightly differently from random guessing.
__label__fairness However, these sensitive attribute annotations should be protected due to privacy and safety concerns.
__label__reinforcement_learning Furthermore, the parallel nature of SPO’s search enables effective utilisation of hardware accelerators, yielding favourable scaling laws.
__label__infrastructure Motivated by the progress made into fused dot-product attention kernels, we developed fused neighborhood attention; an adaptation of fused dot-product attention kernels that allow fine-grained control over attention across different spatial axes.
__label__interpretability_and_explainability We consider the dataset valuation problem, that is the problem of quantifying the incremental gain, to some relevant pre-defined utility of a machine learning task, of aggregating an individual dataset to others.
__label__machine_vision Building on this, we also propose the Efficient Dense Connector, which achieves performance comparable to LLaVA-v1.5 with only 25% of the visual tokens.
__label__causal_inference Causal discovery from mixtures is fundamentally more challenging than single-DAG causal discovery.
__label__algorithmic_game_theory Moreover, we make no assumption on the independence of the errors.
__label__optimization_for_deep_networks Then, we demonstrate that the gradient flow reaches a global minimum consistent with the PDE solution when the weight decay regularization parameter is sufficiently small.
__label__machine_vision While Vision Transformer (ViT) based models scale effectively to large data regimes, they fail to capitalize on this inherent redundancy, leading to higher computational costs.
__label__optimization While basic deterministic results extend well to the relative setting, most existing stochastic analyses require additional assumptions on the mirror, such as strong convexity (in the usual sense), to ensure bounded variance.
__label__machine_vision While optimized for videos, VideoLISA also shows promising generalization to image segmentation, revealing its potential as a unified foundation model for language-instructed object segmentation.
__label__generative_models To address this limitation, we explore a paradigm that integrates a language-guided simulator into the multi-agent reinforcement learning pipeline to enhance the generated answer.
__label__natural_language_processing Our empirical evaluation on five models from different families (Gemma, Phi3, Mistral, Zephyr, Llama2) and at different scales (2B, 3.8B, 7B) shows that both algorithms substantially enhance LLM robustness against discrete attacks (GCG, AutoDAN, PAIR), while maintaining utility.
__label__reinforcement_learning Reinforcement Learning (RL) encompasses diverse paradigms, including model-based RL, policy-based RL, and value-based RL, each tailored to approximate the model, optimal policy, and optimal value function, respectively.
__label__natural_language_processing The scarcity of non-English data limits the development of non-English large language models (LLMs).
__label__fairness Code is released at https://github.com/Scarelette/CulturePark.
__label__interpretability_and_explainability In this work, we introduce a new metric for quantifying feature complexity, based on V-information and capturing whether a feature requires complex computational transformations to be extracted.
__label__machine_learning_for_other_sciences_and_fields Along this line, different from the conventional independent testing paradigm among students, we propose a novel collaborative framework, Collaborative Computerized Adaptive Testing (CCAT), that leverages inter-student information to enhance student ranking.
__label__privacy We propose a new method for measuring memorization in VLMs, which we call dèjá vu memorization.
__label__optimization_for_deep_networks Extensive experimental research demonstrates that our HIO strategy can effectively reduce hallucinations in LVLMs, outperforming state-of-the-art methods across various benchmarks.
__label__generative_models We do so by pre-optimizing a smooth and convex dual function that has a closed form.
__label__diffusion_based_models $\textit{Bifröst}$ addresses these issues by training MLLM as a 2.5D location predictor and integrating depth maps as an extra condition during the generation process to bridge the gap between 2D and 3D, which enhances spatial comprehension and supports sophisticated spatial interactions.
__label__safety_in_machine_learning However, we experimentally reveal that these methods still struggle to generalize their detection capabilities to unknown OOD data, due to the limited diversity of the auxiliary outliers collected.
__label__generative_models We demonstrate transcendence by training an autoregressive transformer to play chess from game transcripts, and show that the trained model can sometimes achieve better performance than all players in the dataset.
__label__generative_models This method not only retains competitive performance but also offers substantial benefits such as significantly reduced memory demand and faster inference speed.
__label__diffusion_based_models ODGEN exhibits robustness in handling complex scenes and specific domains.
__label__machine_vision Employing conventional pruning techniques can remarkably reduce data requirements but would suffer from a degradation in performance.
__label__machine_vision Experiments on the KITTI Tracking dataset show that, compared with the strong baseline, StreamDSGN significantly improves the streaming average precision by up to 4.33%.
"__label__reinforcement_learning We propose a Jacobian regularization scheme to mitigate the ""destabilizing'' effects of non-zero drift errors, thereby enhancing training stability and model generalization."
__label__natural_language_processing Based on this interpretation, we demonstrate that we can provide plausible explanations on recently observed behaviors of LLMs, such as the poor performance of LLMs on long-tail knowledge and the benefits of deduplicating the pretraining corpus.
__label__neuroscience_and_cognitive_science However, typical self-supervised objectives may result in network representations that are overly invariant to changes in the input.
__label__graph_neural_networks This paper investigates the capacity of GNNs to represent strong branching (SB), the most effective yet computationally expensive heuristic employed in the branch-and-bound algorithm.
__label__machine_vision Referenced-based scene stylization that edits the appearance based on a content-aligned reference image is an emerging research area.
__label__learning_theory In this work, we take a first step towards characterizing regimes where guaranteeing validity is easier than in the worst-case.
__label__deep_learning_architectures We present a comprehensive theoretical study of the capacity of such SSMs as it compares to that of transformers and traditional RNNs.
__label__machine_vision After training, our model can reconstruct unseen identities without specific optimizations and perform reenactment rendering at real-time speeds.
__label__natural_language_processing We establish that language is: (1) self-similar, exhibiting complexities at all levels of granularity, with no particular characteristic context length, and (2) long-range dependent (LRD), with a Hurst parameter of approximately 0.7.
__label__machine_vision Without a focusing lens, lensless cameras rely on computational algorithms to recover the scenes from multiplexed measurements.
__label__learning_theory Experimental results on four benchmark datasets and three large-scale datasets validate the effectiveness of our proposed method.
__label__generative_models Capturing and maintaining geometric interactions among different body parts is crucial for successful motion retargeting in skinned characters.
__label__natural_language_processing At upper layers, we apply attention within each block to decode individual tokens, to model fine-grained details with a lightweight local KV cache.
__label__machine_vision Experiments on the CaptainCook4D dataset demonstrate the ability of our approach to predict accurate task graphs from the observation of action sequences, with an improvement of +16.7% over previous approaches.
__label__deep_learning_architectures Evaluated on seven publicly available datasets, CALANet outperformed existing methods, achieving state-of-the-art performance.
__label__generative_models Qualitative and quantitative experiments demonstrate that Animate3D significantly outperforms previous approaches.
__label__reinforcement_learning However, prior recurrent RL algorithms have faced issues with training instability.
__label__machine_vision In this work, we introduce an alternative solution to improve the generalization of image restoration models.
__label__probabilistic_methods High-dimensional Bayesian optimization (BO) tasks such as molecular design often require $>10,$$000$ function evaluations before obtaining meaningful results.
__label__machine_vision Subsequently, De-MINDS captures intention-relevant information from target descriptions and converts them into several pseudo-word tokens for accurate ZS-CIR.
__label__machine_vision However, they struggle with pixel-level recognition tasks like semantic segmentation, which require understanding where the objects are located.
__label__deep_learning_architectures In contrast to much of the previous work, we scale up the parameters of the student model during training, to benefit from over-parameterization without increasing the inference latency.
__label__machine_learning_for_other_sciences_and_fields Specifically, we begin by transforming time series into the modality of text tokens.
__label__graph_neural_networks Current approaches rely on large number of samples from meta-learning to construct memories, and heavy fine-tuning of the GNN parameters that lead to the loss of past knowledge.
__label__diffusion_based_models In this work, we delve into the key challenge of the complex and scene-specific camera trajectories found in real-world captures.
__label__machine_vision Our method decomposes the scene into an explicit surface represented as 3D Gaussians, with a spatially varying BRDF, and an implicit volumetric representation of the scattering component.
__label__generative_models In this work, we introduce Unique3D, a novel image-to-3D framework for efficiently generating high-quality 3D meshes from single-view images, featuring state-of-the-art generation fidelity and strong generalizability.
__label__optimization We provide convergence analysis of SFL for strongly convex and general convex objectives on heterogeneous data.
__label__learning_theory Experiments using deep learning models show that our bounds are nonvacuous thanks to this information-theoretic generalization analysis approach.
__label__bandits However, a trade-off between mechanism design and regret minimization appears to be unavoidable.
__label__machine_vision The Segmentation Anything Model (SAM) requires labor-intensive data labeling.
__label__other Experimental results show that RDSS consistently improves the performance of several popular SSL frameworks and outperforms the state-of-the-art sample selection approaches used in Active Learning (AL) and Semi-Supervised Active Learning (SSAL), even with constrained annotation budgets.
__label__deep_learning_architectures They have advantages in learning and understanding the evolution of complex real dynamics.
__label__natural_language_processing For this target, we introduce a 1-bit model compressing framework named OneBit, including a novel 1-bit parameter representation method to better quantize LLMs as well as an effective parameter initialization method based on matrix decomposition to improve the convergence speed of the quantization framework.
"__label__optimization Our results highlight a first-of-its-kind ""best-of-both-worlds"" integration of machine-learned predictions, thus lead to a near-optimal consistency and robustness tradeoff, which provably improves what can be obtained without learning the confidence parameter."
__label__optimization Existing methods are restrictive in preference definitions and/or their theoretical guarantees.
__label__machine_vision However, one crucial issue is completely ignored: the text descriptions given by users may be noisy, e.g., misspellings and typos, limiting the real-world practicality.
__label__machine_vision Additionally, a modality ensemble retrieval strategy is proposed to leverage complementary strengths of each query modality to improve retrieval effectiveness and robustness.
__label__robotics Pose estimation for category-level articulated objects is a significant challenge due to their inherent complexity and diverse kinematic structures.
__label__robotics Subsequently, the mode and state queries are integrated to obtain a comprehensive and detailed representation of the trajectories.
__label__machine_vision 5.4%), and decreases calibration error (avg.
__label__generative_models In this paper, we propose a novel 4D generation pipeline, namely $\textbf{4Diffusion}$, aimed at generating spatial-temporally consistent 4D content from a monocular video.
__label__fairness The project page is available at https://vssilpa.github.io/denetdm/.
__label__other Particularly challenging are the settings where due to communication resource constraints only a small fraction of clients can participate in any given round of FL.
__label__natural_language_processing Recent work demonstrates that, post-training large language models with open-domain instruction following data have achieved colossal success.
__label__generative_models We exemplify on four representative search problems, comparing to the LLM-based solutions from the literature that attempt to solve these problems.
__label__machine_vision Therefore, a critical question arises: Can we leave deepfake behind and rely solely on blendfake data to train an effective deepfake detector?
__label__optimization_for_deep_networks This may be overly restrictive for some parameters, while insufficient for others.
__label__machine_vision This enables the prediction of corresponding noise from a reference image, ensuring that the restored image from this noise remains consistent with the reference image.
__label__optimization Notably, unlike previous analyses of Adam, our analysis crucially relies on its core elements---momentum and discounting factors---as well as model EMA, motivating their wide applications in practice.
__label__robotics Evaluations on three diverse datasets, ArtImage, ReArtMix, and RobotArm, show EfficientCAPER's effectiveness and generalization ability to real-world scenarios.
__label__machine_vision This is due to the lack of intelligent systems that can quickly generate simpler intermediate parts.
__label__causal_inference Most current methods restrict their analysis to datasets that are assumed to have pure linear or nonlinear relations, which is often not reflective of real-world datasets that contain a combination of both.
__label__diffusion_based_models However, diffusion processes are prone to generating samples that reflect biases in a training dataset.
__label__machine_vision Extensive experiments demonstrate the effectiveness of our method across widely recognized tasks and datasets, all achieving superior performance over state-of-the-art methods.
__label__interpretability_and_explainability In this paper, we initiate a theoretical investigation into the emergence of planning capabilities in Transformer-based LLMs via their next-word prediction mechanisms.
__label__machine_learning_for_other_sciences_and_fields Our MxDNA aims to provide a new perspective on DNA tokenization, potentially offering broad applications in various domains and yielding profound insights.
__label__optimization_for_deep_networks A holistic approach is needed to tackle these challenges and we propose S$\textmu$Par as one such approach.
__label__diffusion_based_models To bridge this gap, we propose a novel framework, termed MoMu-Diffusion, for long-term and synchronous motion-music generation.
__label__deep_learning_architectures Additionally, we also design a unique replacement training technique that involves randomly substituting specific stages in the student model with those from the teacher model, as opposed to training the student model in isolation.
"__label__learning_theory Despite empirical evidence showcasing its efficacy 
compared to feedforward neural networks, a theoretical understanding for its separation and bias is still limited."
__label__deep_learning_architectures A denoising block is responsible for removing the noise in a predicted bounding box, and multiple stacked denoising blocks cooperate to accomplish the whole denoising process.
__label__reinforcement_learning Different from all other diffusion-based offline RL methods, the \textit{guide-then-select} paradigm in Diffusion-DICE only uses in-sample actions for training and brings minimal error exploitation in the value function.
__label__machine_learning_for_other_sciences_and_fields Missing data in multivariate time series are common issues that can affect the analysis and downstream applications.
__label__robotics Using only simplistic reward, state, and object representations, our method shows favorable scalability on diverse objects and trajectories.
__label__optimization Byzantine fault tolerance mechanisms have been proposed to address these issues, but they often assume full participation from all clients, which is not always practical due to the unavailability of some clients or communication constraints.
"__label__safety_in_machine_learning PerpCorrect works in three steps: 
(1) PerpCorrect aligns a surrogate LLM using the clean validation data to make the PPLDiff able to distinguish clean preferences (CPs) and NPs."
__label__generative_models Additionally, we leverage the static 3D model’s multi-view renderings as conditions to preserve its identity.
__label__machine_vision These unsupervised multi-granular masks are then utilized to supervise model training.
__label__machine_learning_for_social_sciences We propose a framework that leverages large language models (LLMs) to augment observed features with latent features, enhancing the predictive power of ML models in downstream tasks.
__label__natural_language_processing In this paper, we investigate this question for math reasoning via an empirical study, followed by building a conceptual understanding of our observations.
__label__diffusion_based_models Via theoretical and empirical investigation, we propose an efficient and effective hyper-parameter searching strategy that can be readily applied to any downstream task.
__label__machine_vision Extensive applications demonstrate the versatility of our new video representation.
__label__machine_vision Novel view synthesis from raw images provides superior high dynamic range (HDR) information compared to reconstructions from low dynamic range RGB images.
__label__other Existing approaches often sacrifice one for the other.
__label__graph_neural_networks In this paper, we investigate a novel mechanism for injecting global topological invariance into pooling layers using PH, motivated by the observation that filtration operation in PH naturally aligns graph pooling in a cut-off manner.
__label__privacy Crucially, we leverage ReLU-induced gradient sparsity to precisely filter out large numbers of incorrect samples, making a final reconstruction step tractable.
__label__machine_learning_for_other_sciences_and_fields To ensure our AL paradigm selects samples with maximal uncertainties, we carefully design a Bayesian geometric graph neural network to compute uncertainties specifically for 3D molecular graphs.
__label__neuroscience_and_cognitive_science In this work, we formulate the pitfall in various EEG decoding tasks in a unified framework.
__label__machine_learning_for_other_sciences_and_fields We validate this Self-Labeling Improvement Method (SLIM) on the Job Shop Scheduling (JSP), a complex combinatorial problem that is receiving much attention from the neural combinatorial community.
__label__reinforcement_learning We demonstrate the importance of these modifications in various domains, ranging from discrete Gridworld environments to continuous-control and simulated robot assistive tasks, illustrating the efficacy of DynaMITE-RL over state-of-the-art baselines in both online and offline RL settings.
__label__machine_vision The reliability of driving perception systems under unprecedented conditions is crucial for practical usage.
__label__machine_learning_for_physical_sciences energy, so their data are often generated by cheaper computational methods at the cost of lower accuracy, which cannot be directly overcome through multi-task learning.
__label__machine_vision Third, to adaptively adjust these parameters, we present a novel similarity score function that not only maximizes the similarities between the generated object image and the input text/image but also balances these similarities to harmonize text and image integration.
__label__natural_language_processing As Large Language Models (LLMs) demonstrate extensive capability in learning from documents, LLM unlearning becomes an increasingly important research area to address concerns of LLMs in terms of privacy, copyright, etc.
__label__optimization_for_deep_networks We validate BitDelta through experiments across Llama-2, Mistral and MPT model families, and on models up to 70B parameters, showcasing minimal performance degradation in all tested settings.
__label__generative_models Diffusion models have demonstrated great success in text-to-video (T2V) generation.
__label__neuroscience_and_cognitive_science These techniques are based on control-theoretic ideas, like modern variants of teacher forcing (TF), to ensure stable loss gradient propagation while training.
__label__algorithmic_game_theory The algorithm can find an $\epsilon$-optimal tax with $O(\beta F^2/\epsilon)$ sample complexity, where $\beta$ is the smoothness of the cost function and $F$ is the number of facilities.
__label__generative_models While cutting-edge diffusion models such as Stable Diffusion (SD) and SDXL rely on supervised fine-tuning, their performance inevitably plateaus after seeing a certain volume of data.
__label__algorithmic_game_theory (2023) provided an efficient equilibrium computation algorithm for ATMGs which presumes knowledge of the reward and transition functions and has no sample complexity guarantees.
__label__causal_inference Various types of CATE estimators have been developed with advancements in machine learning and causal inference.
__label__generative_models Our experiments demonstrate that by obtaining insights from errors in inference trees, TP-LLaMA significantly outperforms the baselines across almost all test scenarios by a large margin and exhibits better generalization capabilities with unseen APIs.
__label__learning_theory 2.
__label__optimization_for_deep_networks Empirical experiments validate the efficacy of our PID-based adaptive gradient decay rate approach, ensuring consistent optimization of model calibration and model accuracy.
__label__privacy Off-the-shelf methods, such as DP-GD, require strong a priori knowledge locating the data within a ball of radius $R$, and the excess risk of the algorithm depends linearly on $R$.
__label__bandits The goal of the seller is to design a pricing strategy that collects as much revenue as possible.
__label__reinforcement_learning However, due to the quadratic computation complexity of attention in transformers, current in-context RL methods suffer from huge computational costs as the task horizon increases.
__label__natural_language_processing To solve the lack of quantification, we first define a reasoning boundary (RB) to quantify the upper-bound of CoT and establish a combination law for RB, enabling a practical quantitative approach applicable to various real-world CoT tasks.
__label__interpretability_and_explainability This paper questions the effectiveness of a modern predictive uncertainty quantification approach, called *evidential deep learning* (EDL), in which a single neural network model is trained to learn a meta distribution over the predictive distribution by minimizing a specific objective function.
__label__active_learning We demonstrate the data efficiency and robust performance of REDUCR on vision and text classification tasks.
__label__natural_language_processing This architecture is fully compatible with autoregressive training and generation.
__label__algorithmic_game_theory The recent increasing adoption of autobidding has inspired the growing interest in analyzing the performance of classic mechanism with value-maximizing autobidders both theoretically and empirically.
__label__machine_learning_for_other_sciences_and_fields Traditional LS approaches rely on manually designed heuristics to tackle the LS task, while machine learning recently offers a promising approach towards next-generation logic synthesis by neural circuit generation and optimization.
__label__natural_language_processing Empirical experiments demonstrate the high quality of data generated by the proposed method, and that the LLMs, specifically LLaMA-2 and Mistral, when realigned with the generated data, surpass their state-of-the-art counterparts.
__label__online_learning actions as instrumental variables (IV) to consistently estimate demand; (iii) a low-switching cost design that promotes nearly truthful buyer behavior.
__label__machine_learning_for_healthcare The LRFL method is both empirically motivated by a Low Frequency Property (LFP) and theoretically motivated by our sharp generalization bound for neural networks with low-rank features.
__label__machine_learning_for_physical_sciences Experimentally, our model could generate plausible 3-D molecules and achieve competitive quantitative performance with significantly reduced circuit parameters compared with their classic counterparts.
__label__reinforcement_learning Reward Machines provide an automaton-inspired structure for specifying instructions, safety constraints, and other temporally extended reward-worthy behaviour.
__label__machine_learning_for_other_sciences_and_fields However, most existing works for TSC assume that traffic data from all surrounding intersections is fully and continuously available through sensors.
__label__deep_learning_architectures Compared with YOLOv9-C, YOLOv10-B has 46\% less latency and 25\% fewer parameters for the same performance.
"__label__learning_theory This allows us to prove an almost complete phase
diagram of training behavior as a function of the variance at initialization
and the width, for a MSE training task."
__label__speech_and_audio First, current state-of-the-art solutions~~\cite{lian2023unconstrained-udm, lian-anumanchipalli-2024-towards-hudm} suffer from poor scalability.
__label__learning_theory As performance gains through scaling data and/or model size experience diminishing returns, it is becoming increasingly popular to turn to ensembling, where the predictions of multiple models are combined to improve accuracy.
__label__machine_learning_for_other_sciences_and_fields Our experiments on CASP14 and CASP15 benchmarks reveal significant improvements in LDDT scores, particularly for complex and challenging sequences, enhancing the performance of both AlphaFold2 and RoseTTAFold.
__label__probabilistic_methods Current methods typically employ kernel-based approaches, using kernel functions directly for kernel density estimation or as basis functions in linear models.
__label__privacy To address this challenge, we introduce Federated Behavioural Planes (FBPs), a novel method to analyse, visualise, and explain the dynamics of FL systems, showing how clients behave under two different lenses: predictive performance (error behavioural space) and decision-making processes (counterfactual behavioural space).
__label__probabilistic_methods Thus, we propose the Conditional Density Tree (CDTree), a fully non-parametric model consisting of a decision tree in which each leaf is formed by a histogram model.
__label__machine_learning_for_physical_sciences We demonstrate the versatility of our approach for both fully data-driven and for physics-aware neural solvers.
__label__reinforcement_learning In this paper, we mitigate this issue from a data-centric perspective and introduce OASIS (cOnditionAl diStributIon Shaping), a new paradigm in offline safe RL designed to overcome these critical limitations.
__label__machine_vision It can also be integrated with 2D diffusion models to enable fast single-image-to-3D and text-to-3D tasks.
__label__privacy Then we design a provably efficient algorithm based on optimistic Nash value iteration and privatization of Bernstein-type bonuses.
__label__natural_language_processing Detection methods in generative language models (GLMs) mainly focus on uncertainty estimation and embedding distance measurement, with the latter proven to be most effective in traditional linguistic tasks like summarization and translation.
__label__optimization We further develop three training algorithms for neural networks (NNs) for our framework as proof of concept, both of which handle all mixed integer linear programs.
__label__generative_models Our approach combines a novel differentiable simulation-based loss function with physically inspired regularization, serving as either a refinement or a post-processing module for existing frameworks.
__label__safety_in_machine_learning We evaluate our defense on several datasets in the context of diverse attacks.
__label__speech_and_audio However, in addition to low efficiency originating from heavy NeRF rendering, these methods all have a limited ability of characterizing the entire scene environment such as room geometry, material properties, and the spatial relation between the listener and sound source.
__label__machine_learning_for_social_sciences The empirical success of PRB demonstrates the value of the proposed fusion approach.
__label__infrastructure Furthermore, we introduce a specific aggregation algorithm for aggregating heterogeneous local models with tailored architectures and weights.
__label__neuroscience_and_cognitive_science We introduce a model that uses a cortical-like combination of BU and TD processing that naturally integrates the two major functions of the TD stream.
__label__learning_theory However, one ideally wants more information-rich certificates that control the entire distribution of possible outcomes, such as the distribution of the test loss in regression, or the probabilities of different mis-classifications.
__label__machine_vision The swift advancement in Multimodal LLMs (MLLMs) also presents significant challenges for effective knowledge editing.
__label__graph_neural_networks This discrepancy stems from a fundamental limitation: while MPNNs excel in node-level representation, they stumble with encoding the joint structural features essential to link prediction, like CN.
__label__reinforcement_learning However, supporting novel goals or behaviors through reinforcement learning requires the ad-hoc design of appropriate reward functions, which quickly becomes intractable.
__label__machine_vision We propose to use the Gaussian viewspace gradient difference vector as a signal to separate the static and dynamic content of the scene.
"__label__interpretability_and_explainability In particular, we observe the appearance of a specialized attention mechanism dedicated to iterative reasoning, which we coined ""iteration heads""."
__label__graph_neural_networks The code is available at https://github.com/PeterGriffinJin/InstructG2I.
__label__optimization_for_deep_networks Additionally, we propose a token relation-enhanced masking training strategy tailored explicitly for EDT to augment its token relation learning capability.
__label__machine_learning_for_other_sciences_and_fields We show how these let us predict received power, localize transmitters, and reconstruct the 3D environment from the received signal.
__label__reinforcement_learning Notably, HPGD uses stochastic hypergradient estimates, based on observations of the followers’ trajectories.
__label__machine_vision Both modules not only reconstruct modality-related content but also cross-modal referring content.
__label__machine_vision Current state-of-the-art LTSSL approaches heavily rely on high-quality pseudo-labels for large-scale unlabeled data.
__label__machine_vision To tackle this issue, we combine diffusion model training with a discriminator that identifies and reduces the impact of irrelevant instances.
__label__optimization_for_deep_networks As an application, we introduce a new algorithm, Manifold-LoRA, which employs the landing technique and a carefully designed step size strategy to accelerate low-rank adaptation (LoRA) in fine-tuning large language models.
__label__machine_vision This allows for the creation of dataset-specific prompts that improve generalization performance, while maintaining human comprehension.
__label__deep_learning_architectures Based on these findings, we further demonstrate how to practically use them to identify optimal ECOCs for simple tasks (short-code ECOCs) and complex tasks (long-code ECOCs), by balancing the code orthogonality (as per finding 1) and code distance (as per finding 2).
__label__learning_theory We provide rigorous theoretical analyses that establish performance guarantees for the ERM-DS algorithm.
__label__natural_language_processing We leverage an evolutionary algorithm to perform crossover and mutation on superior candidates within the population, guided by performance evaluation.
__label__safety_in_machine_learning Specifically, compared with traditional backdoor attacks on LLMs that are only able to manipulate the user inputs and model outputs, agent backdoor attacks exhibit more diverse and covert forms: (1) From the perspective of the final attacking outcomes, the agent backdoor attacker can not only choose to manipulate the final output distribution, but also introduce the malicious behavior in an intermediate reasoning step only, while keeping the final output correct.
__label__evaluation In real-world applications, existing LVLM attackers generally rely on the detailed prior knowledge of the model to generate effective perturbations.
__label__deep_learning_architectures The training process comprises two stages: intra-scale pretraining and inter-scale finetuning.
__label__neuroscience_and_cognitive_science Next, we demonstrate its flexibility by generating variable-length data that mimics human cortical activity during attempted speech.
__label__machine_vision Adversarial Training (AT) has been widely proved to be an effective method to improve the adversarial robustness against adversarial examples for Deep Neural Networks (DNNs).
__label__optimization_for_deep_networks Similarly, empirical studies in large-scale models and real datasets are significantly confounded by the necessity to approximate second-order updates in practice.
__label__optimization The inherent delays in such settings not only introduce additional bias to the system but also obscure the disruptions caused by Byzantine faults.
__label__learning_theory In the worst case, without Markovian assumptions, this problem suffers from the curse of dimensionality.
__label__machine_vision However, hand-held low-light photography challenges NeRFs as the captured images may simultaneously suffer from low visibility, noise, and camera shakes.
__label__optimization Empirical results demonstrate that OrMo can achieve better convergence performance compared with ASGD and other asynchronous methods with momentum.
__label__other Our analysis decomposes the NTK matrix into two components.
__label__machine_vision Pre-trained vision-language models (VLMs) such as CLIP have shown excellent performance for zero-shot classification.
__label__machine_vision Knowledge distillation and pseudo-view augmentation then transfer spherical harmonic coefficients to a lower degree, yielding compact representations.
__label__machine_learning_for_other_sciences_and_fields We show that our model achieves far better performance than NeuroSAT in terms of both correctly classified and proven instances.
__label__generative_models Key to our success is a novel dataset of multiview videos containing curated, rendered animated objects from Objaverse.
__label__generative_models Extensive experiments demonstrate that our method significantly improves the consistency of 3D content generation and specifically mitigates hallucinations caused by pretrained large models, achieving state-of-the-art performance compared to other optimization methods.
__label__machine_learning_for_other_sciences_and_fields As the number and diversity of human experts increase, so does the likelihood that elements in this collective knowledge can be combined and refined to discover novel and better solutions.
__label__natural_language_processing To address this, we introduce a novel approach to re-train transformer encoder-based LMs as Hierarchy Transformer encoders (HiTs), harnessing the expansive nature of hyperbolic space.
__label__machine_vision We evaluate our method on multiple settings including transductive learning and test-time adaptation.
__label__interpretability_and_explainability Our results reveal that LLMs generally exhibit patterns similar to humans, such as risk aversion and loss aversion, with a tendency to overweight small probabilities, but there are significant variations in the degree to which these behaviors are expressed across different LLMs.
__label__interpretability_and_explainability Additionally, we demonstrate that incorporating the hard binding mechanism preserves model performance while enabling seamless integration into both neural and symbolic modules for complex reasoning tasks.
__label__optimization Next, we turn our attention towards SVIs with a monotone, bounded and Lipschitz operator and consider $\ell_p$-setups, $p\in[1,2]$.
__label__machine_vision more accuracy with the same throughput on ImageNet-1K compared to the baselines, making it an effective solution for scenarios where hardware availability is diverse or varies over time.
__label__machine_vision We design a system that learns how to edit visual programs.
__label__machine_learning_for_physical_sciences Building on the Benamou-Brenier formula from optimal transport and action matching, we use a variational problem to infer parameter- and time-dependent gradient fields that represent approximations of the population dynamics.
__label__learning_theory We revisit data selection in a modern context of finetuning from a fundamental perspective.
__label__reinforcement_learning The slow agent analyzes actions suitable for coding, while the fast agent executes coding tasks.
__label__robotics Existing zero-shot object navigation methods prompt LLM with the text of spatially closed objects, which lacks enough scene context for in-depth reasoning.
__label__other We achieve robust performance on synthetic hierarchies and a larger real-world taxonomy, observing improved convergence rates in a resource-constrained setting while reducing the set of training examples by as much as 99%.
__label__graph_neural_networks Zero-shot node classification is a vital task in the field of graph data processing, aiming to identify nodes of classes unseen during the training process.
__label__natural_language_processing *, a gap between the observed and idealized ranking accuracies.
__label__machine_learning_for_physical_sciences The transferability of the proposed framework is evaluated on dipeptides, where we show that it generalizes efficiently to unseen systems.
__label__safety_in_machine_learning We further improve the generalization bound by leveraging smoothing using Moreau’s envelope.
__label__machine_vision Experimental results show that GSPO significantly improves performance, and our method also generalizes to different image styles.
__label__diffusion_based_models We present *Conditional Conjugate Integrators*, which leverage the specific form of the inverse problem to project the respective conditional diffusion/flow dynamics into a more amenable space for sampling.
__label__speech_and_audio In addition, to improve performance, traditional methods have extended the sequence length, leading to the adoption of dual-path models, which handle the much longer sequence effectively by segmenting it into chunks.
__label__natural_language_processing However, DPO is overconfident about preference annotations, implicitly assigning them rewards of infinite magnitude.
__label__deep_learning_architectures However, this has been quite challenging due to the discrete nature of data points in time series and the complexity of periodic variation.
__label__machine_vision To further improve visual grounding, we propose spatial vision aggregator (SVA), a dynamic and spatially-aware connector that integrates vision features with LLMs while reducing the number of tokens.
__label__deep_learning_architectures Additionally, we introduce a linear complexity dynamic programming algorithm to enhance long-range interactions without increasing computational cost.
__label__natural_language_processing Our analysis reveals a representational collapse phenomenon: we prove that certain distinct pairs of inputs to the Transformer can yield arbitrarily close representations in the final token.
__label__bandits Motivated by online display advertising, this work considers repeated second-price auctions, where agents sample their value from an unknown distribution with cumulative distribution function $F$.
__label__generative_models To manage complex distributions, we use idempotent loss and tightness loss to control over-expansion in the latent space.
__label__machine_learning_for_other_sciences_and_fields Finally, we apply the automated technique on 8 recent projects, finding 4 known and 2 unknown bugs.
__label__machine_learning_for_healthcare Our code has been made available at: https://github.com/boqian333/E2ENet-Medical.
__label__diffusion_based_models Experimental results show that L2C largely outperforms samplers such as DDIM and DPM-Solver, alongside prior cache-based methods at the same inference speed.
__label__safety_in_machine_learning Experiments show that training on the high-quality data selected by our method can often outperform other data selection methods for collaborative fine-tuning of LLMs, across diverse private domain datasets, in medical, multilingual and financial settings.
__label__fairness Finally, we identify spectral imbalance in finetuning features as a potential source of group disparities --- minority group covariance matrices incur a larger spectral norm than majority groups once conditioned on the classes.
__label__reinforcement_learning One of the core tasks in the field of DRL is distributional policy evaluation, which involves estimating the return distribution $\eta^\pi$ for a given policy $\pi$.
__label__fairness In this context, our work rigorously addresses the critical concern:  **Does egalitarian fairness lead to instability?
__label__deep_learning_architectures Specifically, we construct an auxiliary model comprising the compact learngene module and learnable transformation matrices, enabling both components to be trained.
__label__machine_learning_for_other_sciences_and_fields The source code is available at https://anonymous.4open.science/r/ViusalGeoLocalization-F8F5/ and the dataset will also be released after the paper is accepted.
__label__safety_in_machine_learning Notably, Aligner can be applied to any powerful, large-scale upstream models.
__label__machine_vision Given any position within the image range, NerPE regresses the corresponding confidence scores for body joints according to the surrounding image features, which guarantees continuity in space and confidence during training.
__label__evaluation However, these metrics are not rigorously compared and benchmarked, instead presented with correlation to human Likert scores over a set of easy-to-discriminate images against seemingly weak baselines.
__label__interpretability_and_explainability Pre-training is crucial for this mechanism: models trained from scratch to add numbers only exploit low-frequency features, leading to lower accuracy.
__label__neuroscience_and_cognitive_science At the same time, the neural subspace in BeNeDiff demonstrates high disentanglement and neural reconstruction quality.
__label__optimization_for_deep_networks By constructing a probabilistic model and implementing probability-driven candidate selection and budget allocation, this approach enhances the quality of the resulting model hyperparameters.
__label__other While significant progress has been made in editing Transformer-based large language models, effective strategies for editing vision Transformers (ViTs) in computer vision remain largely untapped.
__label__interpretability_and_explainability We find that task abilities and the functional components that support them emerge consistently at similar token counts across scale.
__label__natural_language_processing We construct DHA models by transforming various scales of MHA checkpoints given target head budgets.
__label__optimization The secretary problem is one of the fundamental problems in online decision making; a tight competitive ratio for this problem of $1/e \approx 0.368$ has been known since the 1960s.
__label__other We propose Any2graph, a generic framework for end-to-end Supervised Graph Prediction (SGP) i.e.
__label__other The large communication and computation overhead of federated learning (FL) is one of the main challenges facing its practical deployment over resource-constrained clients and systems.
__label__diffusion_based_models Our findings reveal that: 1) Input control information has unique characteristics compared to conventional inputs like Canny edges and depth maps.
__label__neuroscience_and_cognitive_science SAD is trained end-to-end and consists of three main modules: perception, which processes inputs from multi-view cameras to construct a spatiotemporal bird's eye view; prediction, which utilizes a novel dual-pathway with spiking neurons to forecast future states; and planning, which generates safe trajectories considering predicted occupancy, traffic rules, and ride comfort.
__label__generative_models We start with an analytical investigation using simplified architectures and data structures, and end with numerical analysis of real trainings on real datasets.
__label__natural_language_processing Nevertheless, the existing research faces two major challenges: (1) a lack of quantitative metrics to assess CoT capabilities and (2) a dearth of guidance on optimizing CoT performance.
__label__probabilistic_methods Recently, through a unified gradient flow perspective of Markov chain Monte Carlo (MCMC) and variational inference (VI), particle-based variational inference methods (ParVIs) have been proposed that tend to combine the best of both worlds.
__label__machine_vision Recent deep-based methods achieved accurate estimation by using complex network architectures that involve graphs, attention layers, and hard pruning steps.
__label__reinforcement_learning Our experimental results support that our framework can achieve better performance, even under conditions of misspecified transition models.
__label__other We relate IMM to approaches such as noising, which is implicit in addressing the problem, and reverse knowledge distillation from weak teachers, which is explicit but does not exploit restriction being the nature of the weakness.
__label__reinforcement_learning We start by compressing both human and robot videos into unified video tokens.
__label__diffusion_based_models Meanwhile, null regions emerging after applying LWF are addressed by our proposed bilateral nearest neighbor interpolation (BNNI) strategy.
__label__machine_vision Based on this, we propose a lightweight frequency masker, which further reduces channel correlations by an Amplitude-Phase Masker (APM) module and an Adaptive Channel Phase Attention (ACPA) module.
__label__generative_models In this work, we show that one can close this gap by generating semantic representations in the representation space produced by a self-supervised encoder.
__label__diffusion_based_models Our approach is a general-purpose method for sampling diffreps, expanding the scope of problems that diffusion models can tackle.
__label__neuroscience_and_cognitive_science The watermelon EEG dataset collected in this work can be obtained at Zenodo: https://zenodo.org/records/11238929, and all the codes of this work can be obtained in the supplementary materials.
__label__learning_theory Multi-Output Regression (MOR) has been widely used in scientific data analysis for decision-making.
__label__infrastructure This approach led to mathematically inaccurate aggregation noise, reducing fine-tuning effectiveness and failing to address heterogeneous LoRAs.
__label__generative_models Besides, we introduce a globally-shared transformer to supercharge the performance of Mamba, harnessing its exceptional power to capture global relationships.
__label__learning_theory A popular methodology consists in fitting a parametric model via empirical risk minimization where the risk is measured by the Continuous Rank Probability Score (CRPS).
__label__generative_models We introduce Statistical Flow Matching (SFM), a novel and mathematically rigorous flow-matching framework on the manifold of parameterized probability measures inspired by the results from information geometry.
__label__reinforcement_learning This theoretical result leads to novel algorithms that optimize gPOMDP agent behavior with guaranteed user alignment.
__label__interpretability_and_explainability This method allows training any CAV without labeled data, by utilizing the corresponding concept descriptions as guidance.
__label__machine_learning_for_physical_sciences Systematic simulations, encompassing Ising models and weighted Max-Cut instances with up to 64 qubits, substantiate our theoretical findings, highlighting MG-Net's superior performance in terms of both approximation ratio and efficiency.
__label__machine_learning_for_physical_sciences Moreover, they usually neglect hierarchical structures across mesh nodes and objects in systems.
__label__natural_language_processing In contrast, in this work, we seek to identify hallucinations within a single response in both white-box and black-box settings by analyzing the internal hidden states, attention maps, and output prediction probabilities of an auxiliary LLM.
__label__optimization We propose a novel view in which these performative effects are modelled as push forward measures.
__label__machine_vision The key of ImOV3D lies in flexible modality conversion where 2D images can be lifted into 3D using monocular depth estimation and can also be derived from 3D scenes through rendering.
__label__reinforcement_learning We investigate the data efficiency, generalization capabilities, and the impact of different properties of PVRs on the performance of model-based agents.
__label__natural_language_processing Extensive ablation experiments are conducted on the two datasets mentioned to analyze the component effectiveness within the debiasing strategy.
"__label__probabilistic_methods Uncertainty prevails due to the lack of knowledge about data or model,
and conformal prediction (CP) predicts multiple potential targets,
hoping to cover the true target with a high probability."
__label__generative_models The interest in leveraging physics-based inductive bias in deep learning has resulted in recent development of _hybrid deep generative models (hybrid-DGMs)_  that integrates known physics-based mathematical expressions in neural generative models.
__label__deep_learning_architectures Specifically, in each layer, we use two different ways to represent an image: a fine-grained regular grid and a coarse-grained set of semantic slots.
__label__diffusion_based_models ReVideo addresses a new task involving the coupling and training imbalance between content and motion control.
__label__causal_inference Specifically, we introduce graphical criteria that allow for disentanglement under various conditions.
__label__active_learning To reduce the training cost, online batch selection techniques have been developed to choose the most informative datapoints.
__label__safety_in_machine_learning (2) PerpCorrect further aligns the surrogate LLM by incorporating the reliable clean training data whose PPLDiff is extremely small and reliable noisy training data whose PPLDiff is extremely large after correction to boost the discriminatory power.
__label__machine_vision Our method builds on a pre-trained diffusion model, and fine-tunes it on a synthetic relighting dataset, revealing and harnessing the inherent understanding of lighting present in the diffusion model.
__label__machine_vision Despite advancements in Neural Implicit models for 3D surface reconstruction, handling dynamic environments with interactions between arbitrary rigid, non-rigid, or deformable entities remains challenging.
__label__optimization Our code is available at https://github.com/mazumder-lab/ALPS.
__label__neuroscience_and_cognitive_science While significant strides have been made in understanding learning in artificial neural networks, applying this knowledge to biological networks remains challenging.
__label__deep_learning_architectures Contrary to LLMs, humans acquire skills and complete tasks not merely through repeated practice but also by understanding and following instructional guidelines.
__label__diffusion_based_models Then, we adopt a reinforcement learning process to fine-tune the distribution of a pre-trained diffusion model for image inpainting in the direction of higher reward.
__label__graph_neural_networks The code is available through https://github.com/Jillian555/GSIP.
__label__machine_vision Existing methods primarily compress neural Gaussians individually and independently, i.e., coding all the neural Gaussians at the same time, with little design for their interactions and spatial dependence.
__label__machine_vision Previous OCD methods employ the hash-based technique to represent old/new categories by hash codes for instance-wise inference.
__label__diffusion_based_models We find that our learned schedule recovers performant schedules previously only discovered through manual search and obtains competitive FID scores on image datasets.
__label__probabilistic_methods It reverts to Pearson's $r$ in linear scenarios.
__label__optimization To address this issue, we propose a new Accelerated Bilevel Optimization algorithm named AccBO.
__label__safety_in_machine_learning To achieve this, we extend the original Venn-Abers procedure from binary classification to regression.
__label__natural_language_processing We fine-tune the LLM using labeled data of actions and the PPO algorithm.
__label__machine_vision Moreover, our approach significantly reduces memory costs by 84% and boosts throughput by approximately 6.89 times compared to baseline algorithms.
__label__optimization This framework accommodates varying assumptions regarding smoothness and convexity, enabling the application of specific methods with different complexity results.
__label__machine_vision Our code is available at https://github.com/Alexander-Yao/Multi-Sub.
__label__machine_vision Real-world geometry and 3D vision tasks are replete with challenging symmetries that defy tractable analytical expression.
__label__causal_inference To address this issue, we propose a novel Covariate Shift Corrected Pearson Chi-squared Conditional Randomization (csPCR) test.
__label__optimization However, the extensive computational demands of full conformal prediction are daunting in practice, as it necessitates a comprehensive number of trainings across the entire latent label space.
__label__human-AI_interaction We prove that IDA performance is lower bounded by human performance, so that IDA does not negatively impact human control.
__label__neuroscience_and_cognitive_science Learning progress, typically measured as the observed change in performance, can provide a valuable signal for goal selection in both humans and artificial agents.
__label__machine_learning_for_other_sciences_and_fields Unlike previous step-by-step methods, POETRY searches for a verifiable sketch of the proof at each level and focuses on solving the current level's theorem or conjecture.
__label__machine_vision In this paper, we build upon diffusion priors and propose adaptive likelihood estimation and MAP inference during the reverse diffusion process to tackle real-world noise.
__label__machine_vision We introduce Referring Human Pose and Mask Estimation (R-HPM) in the wild, where either a text or positional prompt specifies the person of interest in an image.
__label__machine_vision Instead of only prepending independent prompts to the intermediate layers, we present to leverage the correlations between prompts and input features and excavate the relationships between different layers of prompts to carefully design the instructions.
__label__machine_learning_for_physical_sciences We show that Poseidon exhibits excellent performance across the board by outperforming baselines significantly, both in terms of sample efficiency and accuracy.
__label__other Moreover, We highlight the effectiveness of the layout augmentation on visual graphs and pretraining on the GVLQA dataset.
__label__deep_learning_architectures Here, $\mathbf{L}$ and $\mathbf{R}$ are low rank factors, and the entries of $\mathbf{Q}$, $\mathbf{L}$ and $\mathbf{R}$ are quantized.
__label__algorithmic_game_theory Most of the models within which externality has been studied assume that agents have perfect knowledge of their environment and preferences.
__label__machine_vision Volume rendering in neural radiance fields is inherently time-consuming due to the large number of MLP calls on the points sampled per ray.
__label__probabilistic_methods A popular paradigm for offline Reinforcement Learning (RL) tasks is to first fit the offline trajectories to a sequence model, and then prompt the model for actions that lead to high expected return.
__label__diffusion_based_models Existing methods for standalone SI2T or ST2I perform imperfectly in spatial understanding, due to the difficulty of 3D-wise spatial feature modeling.
__label__probabilistic_methods Current approaches have found some success but are limited as they either achieve poor computational scalings or focus only on the temporal setting.
__label__other Foundation models have significantly enhanced 2D task performance, and recent works like Bridge3D have successfully applied these models to improve 3D scene understanding through knowledge distillation, marking considerable advancements.
__label__other In this work, we provide a structured pruning method for SSMs, Layer-Adaptive STate pruning (LAST), which reduces the state dimension of each layer in minimizing model-level energy loss by extending modal truncation for a single system.
__label__neuroscience_and_cognitive_science We provide an end-to-end autodifferentiable solver for Event SDEs  and make its implementation available as part of the $\texttt{diffrax}$ library.
__label__optimization_for_deep_networks Moreover, we propose a new adaptive layer-wise learning rate to further accelerate training.
__label__deep_learning_architectures Secondly, we observe that In-Distribution (ID) accuracy might not be a very good indicator of OoD accuracy.
__label__optimization_for_deep_networks Minimizers can have arbitrarily large sharpness, but not an arbitrarily small one.
__label__probabilistic_methods *, a *dynamic* function) $f : \mathcal{S} \times \mathcal{T} \to \mathbb{R}$ remains a challenge, since a dynamic Bayesian Optimization (DBO) algorithm has to keep track of the optimum over time.
__label__diffusion_based_models In this work, we propose Diffusion-of-Thought (DoT),  a novel approach that integrates diffusion models with Chain-of-Thought, a well-established technique for improving the reasoning ability of autoregressive language models.
__label__human-AI_interaction Demonstrated over 12 visual tasks and evaluated across 22 datasets, Vitron showcases its extensive capabilities in the four main vision task clusters.
__label__natural_language_processing To test our approach in an offline RL setting, we introduce the Code World Models Benchmark (CWMB), a suite of program synthesis and planning tasks comprised of 18 diverse RL environments paired with corresponding textual descriptions and curated trajectories.
__label__machine_vision The code and dataset is available at https://github.com/YouweiLyu/SfPUEL.
__label__machine_vision Moving beyond conventional pixel-wise depth estimation in the spatial domain, our approach estimates the frequency coefficients of depth patches after transforming them into the discrete cosine domain.
__label__optimization_for_deep_networks Ultimately, GNM-PT enhances generalization across all classes while simultaneously reducing computational overhead.
__label__diffusion_based_models 1) From the data aspect, we carefully collect a human-centric dataset comprising over one million high-quality human-in-the-scene images and two specific sets of close-up images of faces and hands.
__label__deep_learning_architectures These enhancements ensure that DeeR operates efficiently under varying resource constraints while maintaining competitive performance.
__label__safety_in_machine_learning Finally, we extend our approach to AI agents, demonstrating considerable reductions in the rate of harmful actions when they are under attack.
__label__deep_learning_architectures DEPrune is optimized by analyzing the computation of DSConv on GPUs.
__label__neuroscience_and_cognitive_science Here, we question this assumption by analyzing three neural datasets used in an impactful study on LLM-to-brain mappings, with a particular focus on an fMRI dataset where participants read short passages.
__label__safety_in_machine_learning The results not only demonstrate the effectiveness of our method but also provide insights for future improvements in the robustness of cross-modality ReID systems.
__label__other We extend our experiments to a small natural language dataset to further confirm our findings on our synthetic dataset.
__label__optimization When derivatives are exact, our method converges at the same rate as exact optimal second-order methods.
__label__neuroscience_and_cognitive_science Given the coupling between neural activity and behavior, we ask what type of neural variability does not compromise behavioral performance.
__label__deep_learning_architectures The experimental results demonstrate the superiority of our method to accurately reconstruct human geometries and ID tags within three-dimensional spaces, outperforming conventional multi-view techniques.
__label__evaluation However, real-world scenarios often involve class distribution shifts (e.g., in age or gender for person identification), posing challenges for zero-shot classifiers that rely on learned representations from training classes.
__label__generative_models With Monarch matrices, Kronecker factorizations, and post-training quantization, we achieve non-vacuous generalization bounds for LLMs as large as LLaMA2-70B.
__label__algorithmic_game_theory How to truthfully elicit such comparison data from rational individuals?
__label__natural_language_processing To help answer these questions, we introduce the problem of *context attribution*: pinpointing the parts of the context (if any) that *led* a model to generate a particular statement.
__label__reinforcement_learning To this end, we suggest sample-based entropy regularization to stabilize the policy training, and propose a consistency policy with prioritized proximal experience regularization (CP3ER) to improve sample efficiency.
__label__fairness Debiasing approaches that fine-tune the VL model often suffer from catastrophic forgetting.
__label__generative_models In this work, we unify conditional training and sampling using the mathematically well-understood Doob's h-transform.
__label__diffusion_based_models CLIPAway enhances inpainting accuracy and quality by identifying embeddings that prioritize the background, thus achieving seamless object removal.
__label__natural_language_processing We make our code, models, and datasets publicly available.
__label__machine_vision LiDAR data exhibits significant domain gaps due to variations in sensors, vehicles, and driving environments, creating “language barriers” that limit the effective use of data across domains and the scalability of LiDAR perception models.
__label__learning_theory A key feature of our algorithm is that it leverages the ability to reuse samples across multiple rounds of boosting, while guaranteeing a generalization error strictly better than those obtained by blackbox applications of uniform convergence arguments.
__label__machine_vision According to experiments on both single images and video sequences, we demonstrate the effectiveness of our approach in modeling facial textures under challenging illumination affected by occlusions.
__label__machine_vision The results show that our method outperforms state-of-the-art methods in terms of reconstruction quality on both simulated and real-world datasets.
__label__graph_neural_networks Zooming into most practical activation functions (e.g.
__label__machine_vision Instead, we propose a dual-space feature alignment module that effectively augments the latent space with a novel attribute space induced by a well-devised attribute reservoir.
__label__optimization However, a common limitation in these algorithms is the presumption of independent sampling, which can lead to increased computational costs due to the unique hyper-gradient structure in bilevel problems.
__label__online_learning Furthermore, in the case of strongly convex cost and convex constraint functions, the regret guarantee can be improved to $O(\log T)$ while keeping the CCV bound the same as above.
__label__generative_models Empirically, we find that Fisher-Flow improves over prior diffusion and flow-matching models on these benchmarks.
__label__infrastructure We conducted extensive experiments across three datasets, examining both IID and non-IID settings.
__label__deep_learning_architectures In the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM).
__label__diffusion_based_models MGAPO presents significant challenges.
__label__machine_vision To meet the requirements of traditional explicit neural networks for output form, existing heatmap-based methods discretize the originally continuous heatmap representation into 2D pixel arrays, which leads to performance degradation due to the introduction of quantization errors.
__label__diffusion_based_models Generative diffusion models and many stochastic models in science and engineering naturally live in infinite dimensions before discretisation.
__label__learning_theory One recent promising approach is to develop distillation-based algorithms that exploit unlabeled public data but the results are still unsatisfactory in both theory and practice.
__label__other While the robustness of EOT couplings/maps makes them a go-to choice in practical applications, EOT solvers remain difficult to tune because of a small but influential set of hyperparameters, notably the omnipresent entropic regularization strength $\varepsilon$.
__label__privacy The scale of the added noise is critical, as it determines the trade-off between privacy and utility.
__label__optimization The result can be improved further if the smooth part of the upper-level objective is strongly convex.
__label__optimization Notably, it achieves iteration complexity that does not explicitly depend on the number of distributions and strictly faster $(\sum_{i=1}^d 1/\gamma_i \text{ v.s. }
__label__interpretability_and_explainability As a popular paradigm for juggling data privacy and collaborative training, federated learning (FL) is flourishing to distributively process the large scale of heterogeneous datasets on edged clients.
__label__evaluation Moreover, we analyze the relevance of existing metrics to dynamics metrics, improving them from the perspective of dynamics.
__label__optimization_for_deep_networks Transformer-based Diffusion Probabilistic Models (DPMs) have shown more potential than CNN-based DPMs, yet their extensive computational requirements hinder widespread practical applications.
__label__learning_theory In this work, we show nearly identical results for the harder-seeming problem of computing the gradient of loss function of one layer attention network, and thus for the entire process of LLM training.
__label__safety_in_machine_learning To address these issues, we propose an effective and efficient attack named DiffHammer.
__label__machine_learning_for_other_sciences_and_fields Existing neural SAT solvers consistently adopt conjunctive normal form (CNF) for instance representation, which in the cryptographic context can lead to scale explosion and a loss of high-level semantics.
__label__machine_vision they are incompatible with real-world ultra image scenes, as they compromise between image size and computing resources).
__label__natural_language_processing Scaling laws are useful guides for derisking expensive training runs, as they predict performance of large models using cheaper, small-scale experiments.
__label__generative_models We also study the effectiveness of SemCoder's monologue-style execution reasoning compared to concrete scratchpad reasoning, showing that our approach integrates semantics from multiple dimensions more smoothly.
__label__causal_inference Recent work has shown that it is possible to recover the latent factors as well as the underlying structural causal model over them, up to permutation and scaling, provided that we have at least $d$ environments, each of which corresponds to perfect interventions on a single latent node (factor).
__label__machine_vision However, this trend introduces significant challenges, including substantial computational costs of training and transfer to downstream tasks.
__label__diffusion_based_models We propose three techniques to reduce these redundancies: (1) $\textit{Window Attention with Residual Sharing}$ to reduce spatial redundancy; (2) $\textit{Attention Sharing across Timesteps}$ to exploit the similarity between steps; (3) $\textit{Attention Sharing across CFG}$ to skip redundant computations during conditional generation.
__label__machine_vision In this paper, we systematically investigate what contributes to solid face recognition model training, and reveal that face images with certain degree of similarities to their identity centers show great effectiveness in the performance of trained FR models.
__label__speech_and_audio In this paper, we address the challenge of speech enhancement in real-world recordings, which often contain various forms of distortion, such as background noise, reverberation, and microphone artifacts.
__label__optimization_for_deep_networks However, zeroth-order sharpness alone could favors sharper over flatter minima in certain scenarios, leading to a rather sensitive minima rather than a global optima.
__label__deep_learning_architectures The MFE module conducts two simultaneous tasks: estimation of pixel-wise velocity information and BEV segmentation.
__label__reinforcement_learning Unfortunately, behavior regularization, a simple yet effective offline RL algorithm, tends to struggle in this regard.
__label__machine_learning_for_other_sciences_and_fields To alleviate communication and computation overhead, parameter-efficient methods are introduced for efficiency, and some research adapted personalization methods to FedFM for better user preferences alignment.
__label__machine_learning_for_physical_sciences In recent years, machine learning has demonstrated impressive capability in handling molecular science tasks.
__label__machine_learning_for_other_sciences_and_fields Extensive experiments demonstrate that UniMoT achieves state-of-the-art performance across a wide range of molecule comprehension and generation tasks.
__label__optimization_for_deep_networks Current methods for training Binarized Neural Networks (BNNs) heavily rely on the heuristic straight-through estimator (STE), which crucially enables the application of SGD-based optimizers to the combinatorial training problem.
__label__diffusion_based_models With such representations, models demonstrate superior compositionality but have limited ability to interpolate over unseen values of a given feature.
__label__machine_vision Our trained model, CogCoM, equipped with this mechanism with 17B parameters achieves state-of-the-art performance across 9 benchmarks from 4 categories, demonstrating the effectiveness while preserving the interpretability.
__label__probabilistic_methods Denoting the factor by which the regret bound of A-GP-UCB was away from oracle as $g(T)$,  we show that LB is only $\log g(T)$ away from oracle regret.
"__label__safety_in_machine_learning With our improvements, a
16,721 parameter model with 2 hidden layers trained on MNIST is extracted within
only 98 minutes compared to at least 150 minutes previously."
__label__online_learning However, the efficiency of prediction sets varies depending on the learning model used.
__label__neuroscience_and_cognitive_science To this end, we propose a two-dimensional self-evolving spiking neural network that integrates Hebbian-like plasticity and empirical morphological data.
__label__diffusion_based_models The *Sensitive Transformer* produces the sensitive constraints, which guide the stereotyped image distribution to align with the stereotype-free probability distribution.
__label__graph_neural_networks Several recent works have shown that extending the message passing paradigm to subgraphs communicating with other subgraphs, especially via higher order messages, can boost the expressivity of graph neural networks.
__label__machine_learning_for_healthcare In recent years, pretraining models have made significant advancements in the fields of natural language processing (NLP), computer vision (CV), and life sciences.
__label__reinforcement_learning Instead, naïve reinforcement learning algorithms typically converge to Pareto-dominated outcomes in even the simplest of social dilemmas.
__label__diffusion_based_models Additionally, we modify the self-attention mechanism to integrate information from the source view, reducing shape distortions.
__label__safety_in_machine_learning Code is available at https://github.com/TrustAI/TARP-VP.
__label__deep_learning_architectures This framework encompasses many previously proposed structures, such as low-rank, Kronecker, Tensor-Train, and Monarch, along with many novel structures.
__label__machine_vision The approach can be integrated as an add-on module to other 3DGS variants, improving their quality without compromising visual fidelity.
__label__interpretability_and_explainability Despite their different natures, these strategies often lead to comparable performance gains.
__label__deep_learning_architectures In this work we propose a novel, label-free, consistent CE estimator under label shift.
__label__machine_vision Additionally, the general and diverse descriptions generated based on the class names may be wrong or absent in the specific given images.
__label__generative_models In this work, we first model and analyze the problem, visualizing the specific causes of the Janus Problem, which are associated with discrete view encoding and shared priors in 2D lifting.
__label__machine_vision Social relation reasoning aims to identify relation categories such as friends, spouses, and colleagues from images.
__label__natural_language_processing To address the limitations of existing methods, inspired by the Scaling Law for performance prediction, we propose to investigate the Scaling Law of the Domain-specific Continual Pre-Training (D-CPT Law) to decide the optimal mixture ratio with acceptable training costs for LLMs of different sizes.
__label__safety_in_machine_learning Recent research has shown that pre-trained vision-language models are effective at identifying out-of-distribution (OOD) samples by using negative labels as guidance.
__label__interpretability_and_explainability detecting and continuing repeated subsequences.
__label__deep_learning_architectures We define a new scenario of OOD compositional generalization, termed \textit{rule extrapolation}.
__label__online_learning Our negative result shows that, unless P=NP, there is no Fully Polynomial Time Approximation Scheme (FPTAS) for maximizing the utility of an optimizer against a learner that best-responds to the history in each round.
__label__natural_language_processing Here, we hypothesize that syntactic relations are, in fact, coded by the relative direction between nearby embeddings.
__label__reinforcement_learning We demonstrate the superior performance of our approach compared to a variety of Constrained Reinforcement Learning (CRL) methods on three distinct constrained allocation tasks: portfolio optimization, computational workload distribution, and a synthetic allocation benchmark.
__label__machine_vision We also perform experiments on CLEVR-based data and show that, unlike current SOTA object detection methods (SAM, CutLER), our method's prediction errors always lie within our theoretical bounds.
__label__optimization_for_deep_networks In this paper, we introduce a key notion to predict and control feature learning: the angle $\theta_\ell$ between the feature updates and the backward pass (at layer index $\ell$).
__label__reinforcement_learning Curriculum Reinforcement Learning (CRL) is an approach to facilitate the learning process of agents by structuring tasks in a sequence of increasing complexity.
__label__reinforcement_learning First, we show that while the value gap can be efficiently minimized via a direct extension of single-agent IL algorithms, even *value equivalence* can lead to an arbitrarily large regret gap.
__label__causal_inference Second, they lack a specific focus on selecting a robust CATE estimator.
__label__causal_inference This task is especially challenging when dealing with observational data, as using such data runs the risk of hidden confounders whose existence can lead to biased and harmful policies.
__label__interpretability_and_explainability To this end, we propose a novel two-level concept discovery formulation leveraging: (i) recent advances in vision-language models, and (ii) an innovative formulation for coarse-to-fine concept selection via data-driven and sparsity inducing Bayesian arguments.
__label__machine_vision Extensive experiments across multiple low-level tasks including image denoising, low-light image enhancement, guided image super-resolution, and image de-blurring demonstrate consistent performance gains obtained by our Deep Fourier Shifting while reducing the computation burden.
__label__other Graphlet sampling enhances machine learning applications by transforming graph structures into feature vectors for tasks such as graph classification and subgraph identification, boosting neural network performance, and supporting clustered federated learning by capturing local structures and relationships.
__label__generative_models The Janus Problem is a common issue in SDS-based text-to-3D methods.
__label__generative_models One of the main advantages of LoRA is its ability to be fused with  pretrained models, adding no overhead during inference.
__label__optimization On the other hand, problem-specific algorithms (exact and heuristic) are limited in scope.
__label__bandits This framework has been applied in various areas.
__label__other Our theoretical and empirical analysis reveals an unexpected finding: for a given task, utilizing a publicly available, task- and architecture-agnostic model (referred to as the `prior model' in this paper) can effectively produce efficient data.
"__label__learning_theory In this work we study the problem of actively learning binary classifiers
  from a given concept class, i.e., learning by utilizing unlabeled data 
  and submitting targeted queries about their labels to a domain expert."
__label__machine_learning_for_physical_sciences We propose a novel approach combining millions of citizen science species observations with textual descriptions from Wikipedia, covering habitat preferences and range descriptions for tens of thousands of species.
__label__learning_theory We show that the generalization error of GD, with common choice of hyper-parameters, can be $\tilde \Theta(d/m+1/\sqrt{m})$, where d is the dimension and m is the sample size.
__label__bandits Taking the next step, we consider infinite domains and kernelized rewards.
"__label__probabilistic_methods As a result,
directly maximizing the evidence lower bound (ELBO) is not possible, so they
resort to one of the following: optimizing bounds on the ELBO, employing costly
inner-loop Markov chain Monte Carlo runs, or solving minimax objectives."
__label__interpretability_and_explainability We explain this phenomenon through both theoretical proofs and experiments on real-world data.
__label__machine_vision Prior multi-frame optical flow methods typically estimate flow repeatedly in a pair-wise manner, leading to significant computational redundancy.
__label__machine_learning_for_healthcare We showcase the advantages of our representation compared to existing methods using synthetic data and real-world examples motivated by biomedical applications.
__label__machine_learning_for_other_sciences_and_fields Furthermore, we propose an evolutionary algorithm assisted by reinforcement learning agent restarting technique for efficient and effective neural circuit optimization.
__label__natural_language_processing Notably, PPO outperforms DPO by up to 2.5% in math and 1.2% in general domains.
"__label__machine_learning_for_other_sciences_and_fields In this paper, we propose SocraticLM, which achieves a Socratic ""Thought-Provoking"" teaching paradigm that fulfills the role of a real classroom teacher in actively engaging students in the thought process required for genuine problem-solving mastery."
__label__safety_in_machine_learning We have mediocre success in password-locking a model to mimic the answers a weaker model would give.
__label__deep_learning_architectures $\rm CALDERA$ obtains this decomposition by formulating it as an optimization problem $\min_{\mathbf{Q},\mathbf{L},\mathbf{R}}\lVert(\mathbf{Q} + \mathbf{L}\mathbf{R} - \mathbf{W})\mathbf{X}^\top\rVert_{\rm F}^2$, where $\mathbf{X}$ is the calibration data, and $\mathbf{Q}, \mathbf{L}, \mathbf{R}$ are constrained to be representable using low-precision formats.
__label__natural_language_processing Large language models (LLMs) have significantly advanced performance across a spectrum of natural language processing (NLP) tasks.
__label__neuroscience_and_cognitive_science Here, we develop SOFO, a second-order optimizer that efficiently navigates loss surfaces whilst _not_ requiring backpropagation.
__label__machine_vision Further, these TAs are larger than the student model and training them especially in large data settings can be computationally intensive.
__label__machine_vision To address this challenge, this paper presents a Denoising Fine-Tuning framework, called DeFT, for adapting vision-language models.
__label__graph_neural_networks We empirically validate the expressive and counting power of $r$-$\ell$MPNN on several synthetic datasets and demonstrate the scalability and strong performance on various real-world datasets, particularly on sparse graphs.
__label__evaluation Additionally, we explore time series encoders and find that patching and attention structures perform similarly to LLM-based forecasters.
__label__machine_learning_for_physical_sciences piecewise linear (PWL) systems.
__label__robotics Project page: https://pjlab-adg.github.io/LeapAD
"__label__optimization_for_deep_networks Unfortunately, we show sparse and
dense networks do not share the same optimal HPs."
__label__machine_vision One contributing factor is their direct comparison of a query image's features with those of few-shot normal images.
__label__machine_learning_for_other_sciences_and_fields Working as a plugin, GLAFF adaptively adjusts the combined weights for global and local information, enabling seamless collaboration with any time series forecasting backbone.
__label__interpretability_and_explainability Spatio-temporal (ST) prediction has garnered a De facto attention in earth sciences, such as meteorological prediction, human mobility perception.
__label__privacy At the protocol level, PrivCirNet customizes the HE encoding algorithm that is fully compatible with the block circulant transformation and reduces the computation latency in proportion to the block size.
__label__probabilistic_methods Experimental results demonstrate that FocalBO can efficiently leverage large amounts of offline and online data to achieve state-of-the-art performance on robot morphology design and to control a 585-dimensional musculoskeletal system.
__label__machine_learning_for_other_sciences_and_fields We also repurpose linker design methods as strong baselines for this task.
__label__deep_learning_architectures CherryQ outperforms existing quantization approaches in terms of perplexity and downstream task performance.
__label__active_learning Motivated by this, we design a novel active learning algorithm that takes three complementary aspects, namely learnability, diversity, and uncertainty, into account.
__label__diffusion_based_models However, alleviating the misalignment between the text prompts and images is still challenging.
__label__machine_vision Sharpness-aware minimization (SAM) is an effective technique for traditional uni-modal domain generalization (DG), however, with limited improvement in MMDG.
__label__diffusion_based_models Project page: neurips13025.github.io
__label__natural_language_processing These methods, while effective, often involve manually intensive prompt engineering.
__label__robotics To address this fundamental limitation, we propose a novel motion forecasting framework for continuous driving, named RealMotion.
__label__bandits The control is based on the implicit exploration scheme and adaptive skipping of observations with excessive delays.
__label__speech_and_audio REBORN alternates between (1) training a segmentation model that predicts the boundaries of the segmental structures in speech signals and (2) training the phoneme prediction model, whose input is a segmental structure segmented by the segmentation model, to predict a phoneme transcription.
__label__reinforcement_learning However, canonical techniques for learning SFs from pixel-level observations often lead to representation collapse, wherein representations degenerate and fail to capture meaningful variations in the data.
__label__optimization 2024), VaSSO (Li & Giannakis, 2023), RSAM (Liu et al., 2022), and to the unnormalized versions of SAM such as USAM (Andriushchenko & Flammarion, 2022).
__label__causal_inference Traditional causal effect estimation frameworks, e.g., relying on structural causal models and do-calculus, are typically limited to i.i.d.
__label__causal_inference To remedy this, we employ the InfoMax principle, maximizing a lower bound of mutual information between sequence data and its representation.
__label__reinforcement_learning \thanks{Code is available in \url{https://github.com/datake/SinkhornDistRL}.
__label__diffusion_based_models As text-to-image (T2I) synthesis models increase in size, they demand higher inference costs due to the need for more expensive GPUs with larger memory, which makes it challenging to reproduce these models in addition to the restricted access to training datasets.
__label__online_learning When such outliers occur at the end of the data stream, this can cause the final solution to have unexpectedly low accuracy.
__label__probabilistic_methods We introduce a method for eliciting the expert's belief density as a normalizing flow based solely on preferential questions such as comparing or ranking alternatives.
__label__natural_language_processing Specifically, IN2 training leverages a synthesized long-context question-answer dataset, where the answer requires (1) **fine-grained information awareness** on a short segment (~128 tokens) within a synthesized long context (4K-32K tokens), and (2) the **integration and reasoning** of information from two or more short segments.
__label__machine_learning_for_other_sciences_and_fields Multimodal Sentiment Analysis (MSA) is an important research area that aims to understand and recognize human sentiment through multiple modalities.
__label__other For the former, we use adaptations of prior work on gradient approximations of black-box components as a baseline, and show that ISED achieves comparable accuracy but in a more data- and sample-efficient manner.
__label__machine_learning_for_other_sciences_and_fields [2010], gRNAde obtains higher native sequence recovery rates (56% on average) compared to Rosetta (45% on average), taking under a second to produce designs compared to the reported hours for Rosetta.
__label__reinforcement_learning Furthermore, we adopt low-cost option embeddings instead of traditional, computationally expensive option tuples, enhancing scalability and expressiveness.
__label__other The results further demonstrate the great potential of our ZOPP for real-world scenarios.
__label__machine_learning_for_other_sciences_and_fields Unfortunately, with the commonly used binary type of loss and negative sampling, we have empirically found that learning with labeled and pseudo-labeled samples can result in the variance bias problem between the feature distributions of positive and negative samples for each label.
__label__learning_theory We design algorithms for this task and prove that they achieve optimal query complexity.
__label__natural_language_processing Large Language Models are known to capture real-world knowledge, allowing them to excel in many downstream tasks.
__label__natural_language_processing Through extensive experimentation and analysis of recent commercial or open-sourced large vision language models, we assess their capability to comprehend the complex interplay of the narrative humor inherent in these comics.
__label__natural_language_processing In this paper, we consider another dimension of scaling: the amount of data available at inference time.
__label__safety_in_machine_learning It builds upon the work of Podkopaev and Ramdas [2022], who address scenarios where labels are available for tracking model errors over time.
__label__online_learning Our results imply that such team can be $O(\log^2 n)$-competitive as soon as $k\geq n^2$.
__label__learning_theory Anchor-based strategies have been treated as effective ways to alleviate such efficiency problems by propagation on representative entities instead of the whole graph.
__label__reinforcement_learning This work proposes to further explore the in-context learning capabilities of pre-trained transformer models in competitive multi-agent games, i.e., in-context game-playing (ICGP).
__label__machine_vision It masks non-occluded object queries by balancing the masked and preserved query portions adaptively according to the depth information.
__label__diffusion_based_models In this work, we critically examine the limitations of the CLIP text encoder in understanding attributes and investigate how this affects diffusion models.
__label__graph_neural_networks Within each round, we run a layerwise GNN from scratch, based on the current state of the alignment.
__label__graph_neural_networks Mastering games is a hard task, as games can be extremely complex, and still fundamentally different in structure from one another.
__label__optimization_for_deep_networks To mitigate cross-domain feature representation variance,  we introduce FedPLVM, which establishes variance-aware dual-level prototypes clustering and employs a novel $\alpha$-sparsity prototype loss.
__label__evaluation Digging deeper, we show that the modeling framework of IRT, by explicitly modeling the difficulty levels of questions, allows us to quantitatively distinguish between LLMs that answer questions in “human-like” patterns versus LLMs that do not.
__label__learning_theory In this work, we consider the natural question of how the interplay of models and strategic interactions affects the relationship between performance at equilibrium and the expressivity of model classes.
__label__generative_models Using StyleGAN2 as an example, we present a roadmap of simplification and modernization that results in a new minimalist baseline---R3GAN.
__label__machine_vision We revamping the blind-spot technique by leveraging the transformer’s capability for long-range pixel interactions, which is crucial for effectively removing noise dependence in relating pixel–a requirement for achieving great performance for the blind-spot technique.
__label__causal_inference Causal discovery with time series data remains a challenging yet increasingly important task across many scientific domains.
__label__machine_vision We show that this filtering step is crucial to provide an online supervision that helps balancing the shortcoming of the respective input motions, thus being important for not only capturing accurate global motion trajectories but also producing physically plausible human poses.
__label__machine_vision Experiments conducted on various DGSS settings show the state-of-the-art performance of our FADA and its versatility to a variety of VFMs.
__label__machine_vision We leverage the concept of Continual Learning and develop an incremental training strategy atop pre-trained MLLMs, enabling their expansion to new modalities using uni-modal data, without executing joint-modal pretraining.
__label__machine_vision They suffer from long training time and slow inference speed.
__label__natural_language_processing ReST-MCTS* circumvents the per-step manual annotation typically used to train process rewards by tree-search-based reinforcement learning: Given oracle final correct answers, ReST-MCTS* is able to infer the correct process rewards by estimating the probability this step can help lead to the correct answer.
__label__optimization Unfortunately, using these models for IR is computationally expensive due to the increased complexity of multi-vector retrieval and scoring.
__label__machine_vision To address these issues, we propose Blind Image Restoration via fast Diffusion inversion (BIRD) a blind IR method that jointly optimizes for the degradation model parameters and the restored image.
__label__diffusion_based_models The resulting approach also applies to the original parameterization of the concrete score.
__label__optimization This gap is the difference between the highest and $k$-th highest weight in the sequence.
__label__machine_vision A learned incident light field accounts for shadowing.
__label__generative_models We also extend our analysis to Zebra puzzles (known as Einstein puzzles) and show that the model solves $92.04 \%$ of the puzzles fully correctly.
__label__generative_models Furthermore, after generating subnets that inherit specific weights from the original LLMs, we introduce a reformation algorithm that utilizes the omitted weights to rectify the inherited weights  with a small amount of calibration data.
__label__diffusion_based_models Previous work for stereotype mitigation mainly concentrated on mitigating stereotypes engendered with individual objects within images, which failed to address stereotypes engendered by the association of multiple objects, referred to as *Association-Engendered Stereotypes*.
__label__optimization However, existing ZO minimax algorithms have high complexity and rely on some strict restrictive conditions for ZO estimations.
__label__machine_vision However, training 3D models with labels directly derived from pseudo-LiDAR is inadequate due to imprecise boxes estimated from noisy point clouds and severely occluded objects.
__label__deep_learning_architectures While numerous OOD studies focusing on environment factors have achieved remarkable performance, they still fail to systematically solve the two issue of environment inference and utilization.
__label__machine_vision In this paper, we introduce SELMA: Skill-Specific Expert Learning and Merging with Auto-Generated Data, a novel paradigm to improve the faithfulness of T2I models by fine-tuning models on automatically generated, multi-skill image-text datasets, with skill-specific expert learning and merging.
__label__natural_language_processing We present AMOR, an agent framework based on open-source LLMs, which reasons with external knowledge bases and adapts to specific domains through human supervision to the reasoning process.
__label__privacy Alongside our method, we establish MMUBench, a new benchmark for MU in MLLMs and introduce a collection of metrics for its evaluation.
__label__machine_learning_for_healthcare However, due to architectural design and lack of supervision, PLMs model mutations implicitly with evolutionary plausibility, which is not satisfactory to serve as explainable and engineerable tools in real-world studies.
__label__probabilistic_methods Our research reveals that DNNs, during forward propagation, indeed perform approximations of PGM inference that are precise in this alternative PGM structure.
__label__machine_vision Further scaling up the number of visual tokens for pre-training leads to stronger performances, competitive to existing approaches in a series of benchmarks.
__label__machine_vision To address these issues, we first present a simple yet well-crafted framework named SocialGPT, which combines the perception capability of Vision Foundation Models (VFMs) and the reasoning capability of Large Language Models (LLMs) within a modular framework, providing a strong baseline for social relation recognition.
__label__online_learning This assumption, however, does not hold for many applications with indivisible bids.
__label__optimization_for_deep_networks To tackle the difficulty of choosing the best model, one effective solution is model fusion, which combines multiple models in a parameter space.
__label__machine_vision Idefics2 achieves state-of-the-art performance within its size category across various multimodal benchmarks, and is often on par with models four times its size.
__label__causal_inference Our conditions permit complex causal hierarchical structures beyond latent trees and multi-level directed acyclic graphs in prior work and can handle high-dimensional, continuous observed variables, which is well-suited for unstructured data modalities such as images.
__label__diffusion_based_models Diffusion models recently proved to be remarkable priors for Bayesian inverse problems.
__label__optimization Given multiple tasks drawn i.i.d.
__label__learning_theory We also extend this analysis to the multi-class setting with features distributed according to a Gaussian mixture model.
__label__causal_inference In this way, we provide a foundation for moving from causal representations to interpretable, concept-based representations by bringing together ideas from these two neighboring disciplines.
__label__machine_learning_for_physical_sciences Neural operators effectively solve PDE problems from data without knowing the explicit equations, which learn the map from the input sequences of observed samples to the predicted values.
__label__natural_language_processing To address the representation density problem, we introduce a simple but effective contrastive learning strategy, namely SignCL, which encourages gloss-free models to learn more discriminative feature representation in a self-supervised manner.
__label__optimization_for_deep_networks We show that quantizing the eigenvector matrix of the preconditioner in 4-bit Shampoo is remarkably better than quantizing the preconditioner itself both theoretically and experimentally.
__label__active_learning Overcoming the problem of optimizing an unknown time-varying reward subject to unknown time-varying safety constraints, we propose TVSAFEOPT, a new algorithm built on Bayesian optimization with a spatio-temporal kernel.
__label__reinforcement_learning We present a novel framework for expressing users' constraints and preferences about agent behavior in a partially observable setting using parameterized belief-state query (BSQ) constraints in the setting of goal-oriented partially observable Markov decision processes (gPOMDPs).
__label__learning_theory In practice, classification algorithms often operate by first assigning a continuous score (for instance, an estimated probability) to each possible label, then taking the maximizer---i.e., selecting the class that has the highest score.
__label__generative_models However, large 2D visual models exhibit spatial perception hallucinations, leading to multi-view inconsistency in 3D content generated through Score Distillation Sampling (SDS).
__label__probabilistic_methods The latter have been recently shown to be of interest in machine learning beyond coresets, but come with a limited theoretical toolbox, to the extension of which our result contributes.
__label__machine_vision Extensive experiments validate our effectiveness in general fusion tasks across diverse scenarios against the competing methods without additional training.
__label__deep_learning_architectures To address this, we formulate the image SR task as an imbalanced distribution transfer learning problem from a statistical probability perspective, proposing a plug-and-play Weight-Balancing framework (WBSR) to achieve balanced model learning without changing the original model structure and training data.
__label__natural_language_processing Our datasets, models and code are publicly available at https://github.com/hkust-nlp/dart-math.
__label__reinforcement_learning With the ability to learn from static datasets, Offline Reinforcement Learning (RL) emerges as a compelling avenue for real-world applications.
__label__other We derive an alternative parameterization of the low-rank problem based on the _latent coupling_ (LC) factorization previously introduced by [Lin et al.
__label__machine_vision This task is challenging because the point clouds may be non-overlapped, and they may have arbitrary initial positions.
__label__interpretability_and_explainability Looking forward, our results suggest that for complex high-dimensional settings, merely providing a pointwise prediction and explanation could be insufficient, as there is no way for the users to verify that the provided explanations are not completely made-up.
__label__probabilistic_methods In one special case, inferring intermediate representations will be equivalent to interpolating between the learned representations.
__label__diffusion_based_models We investigate a modified form of gradient guidance based on a forward prediction loss, which leverages the information in pre-trained score functions and provably preserves the latent structure.
__label__human-AI_interaction We attribute the success of IDA to preserving human autonomy while simultaneously offering assistance to prevent the human from entering universally bad states.
__label__machine_vision 3D Gaussian Splatting has shown fast and high-quality rendering results in static scenes by leveraging dense 3D prior and explicit representations.
__label__speech_and_audio Additionally, we propose a generalized speech-text pretraining scheme that enhances the capture of cross-modal semantics.
__label__natural_language_processing When applied to open-source LLMs including Llama and Mistral, MoICE surpasses prior methods across multiple tasks on long context understanding and generation, all while maintaining commendable inference efficiency.
__label__diffusion_based_models We introduce positive and negative binding vectors to enhance disentanglement, further with a neighbor strategy to increase accuracy.
__label__graph_neural_networks We extensively evaluate SGRL across various downstream tasks on benchmark datasets, demonstrating its efficacy and superiority over existing GCL methods.
__label__optimization In recent years, there has been a surge of interest in the use of machine-learned predictions to bypass worst-case lower bounds for classical problems in combinatorial optimization.
__label__natural_language_processing High-quality preference data leads to improvements of up to 8% in instruction following and truthfulness.
__label__optimization_for_deep_networks Despite its widespread usage and being extensively studied in the classical literature, its role remains poorly understood for deep learning.
__label__neuroscience_and_cognitive_science Current state-of-the-art synchrony-based models encode object bindings with complex-valued activations and compute with real-valued weights in feedforward architectures.
__label__safety_in_machine_learning Based on this finding, we propose two training schemes to further improve the detection ability of the reward model, namely (i) continual preference fine-tuning to make reward model prefer aligned LGTs even further and (ii) reward modeling of Human/LLM mixed texts (a rephrased texts from human-written texts using aligned LLMs), which serves as a median preference text corpus between LGTs and human-written texts to learn the decision boundary better.
__label__diffusion_based_models We break down the problem into two causes: concept ignorance and concept mismapping.
__label__machine_vision Using robust feature distillation, feature residual mining, and robust optimization, 3DGM simultaneously performs 2D segmentation and 3D mapping without human intervention.
__label__graph_neural_networks Message-passing graph neural networks (MPNNs) have emerged as a powerful paradigm for graph-based machine learning.
__label__optimization Districting is a complex combinatorial problem that consists in partitioning a geographical area into small districts.
__label__neuroscience_and_cognitive_science Extensive experiments demonstrate that our method yields significant performance improvements in event-based object tracking and recognition.
__label__graph_neural_networks We adopt the contrastive language image pretraining (CLIP) combined with the Transformer to improve understanding and generalization ability in semantic consistency across different data modalities.
__label__interpretability_and_explainability Experiments demonstrate our method's superior performance in disentanglement and reconstruction.
__label__deep_learning_architectures In fact, this is the root cause of the scalability issue w.r.t.
__label__graph_neural_networks Inspired by the success of foundation models in applications such as ChatGPT, as graph data has been ubiquitous, one can envision the far-reaching impacts that can be brought by Graph Foundation Models (GFMs) with broader applications in the areas such as scientific research, social network analysis, drug discovery, and e-commerce.
__label__machine_vision Additionally, we discuss the curation of high-quality visual instruction-tuning data from publicly available sources, emphasizing the importance of distribution balancing.
__label__machine_learning_for_other_sciences_and_fields The temporal encoder captures temporal dependencies within ETS data, taking into account exogenous variables.
__label__privacy Amplification by subsampling is one of the main primitives in machine learning with differential privacy (DP): Training a model on random batches instead of complete datasets results in stronger privacy.
__label__online_learning We study a data pricing problem, where a seller has access to $N$ homogeneous data points (e.g.
__label__machine_learning_for_other_sciences_and_fields We introduce gRNAde, a geometric RNA design pipeline operating on 3D RNA backbones to design sequences that explicitly account for structure and dynamics.
__label__deep_learning_architectures Multimodal Large Language Models (MLLMs) have demonstrated remarkable comprehension and reasoning capabilities with complex language and visual data.
__label__natural_language_processing As Archimedes famously said, ``Give me a lever long enough and a fulcrum on which to place it, and I shall move the world'', in this study, we propose to use a tiny Language Model (LM), \eg, a Transformer with 67M parameters, to lever much larger Vision-Language Models (LVLMs) with 9B parameters.
__label__machine_learning_for_other_sciences_and_fields Worldwide geolocalization aims to locate the precise location at the coordinate level of photos taken anywhere on the Earth.
__label__generative_models We provide the code, along with a blog post and video tutorial on the project page: https://s-sahoo.com/mdlm
__label__machine_learning_for_other_sciences_and_fields Both the experts and the router output spiking sequences, and their element-wise operation makes SEMM computation spike-driven and dynamic sparse-conditional.
__label__machine_vision Recently, there have been some works studying self-supervised adversarial training, a learning paradigm that learns robust features without labels.
__label__privacy To overcome the challenge, we propose an efficient method, Single Image Unlearning (SIU), to unlearn the visual recognition of a concept by fine-tuning a single associated image for few steps.
__label__natural_language_processing Personalization has emerged as a critical research area in modern intelligent systems, focusing on mining users' behavioral history and adapting to their preferences for delivering tailored experiences.
__label__diffusion_based_models In our evaluation on popular Stable Diffusion checkpoints, Stylus achieves greater CLIP/FID Pareto efficiency and is twice as preferred, with humans and multimodal models as evaluators, over the base model.
__label__learning_theory Interestingly, transformers have shown particularly strong in-context learning capability -- even without fine-tuning, they are still able to solve unseen tasks well purely based on task-specific prompts.
__label__reinforcement_learning Moreover, based on RCaI, we derive the risk-sensitive reinforcement learning (RL) methods: the policy gradient and the soft actor-critic.
__label__machine_vision The existing computing devices have not adapted the calculation of the attention mechanism well, which leads to a burden on computation quantity and inference latency.
__label__other To answer this question, we introduce a diagnostic setting - **recurring TTA** where environments not only change but also recur over time, creating an extensive data stream.
__label__machine_vision To this end, we dynamically align 3D Gaussians on the zero-level set of the neural SDF, and then render the aligned 3D Gaussians through the differentiable rasterization.
__label__safety_in_machine_learning Backdoor defenses are mainly based on backdoor inversion, which has been shown to be generic, model-agnostic, and applicable to practical threat scenarios.
__label__machine_learning_for_physical_sciences EScAIP also achieves state-of-the-art performance on a wide range of datasets including catalysts (OC20 and OC22), molecules (SPICE), and materials (MPTrj).
__label__evaluation Using a dataset of over 200 language model evaluations on Wikitext and Penn Treebank spanning 2012-2023, we find that the compute required to reach a set performance threshold has halved approximately every 8 months, with a 90\% confidence interval of around 2 to 22 months, substantially faster than hardware gains per Moore's Law.
__label__diffusion_based_models Our model achieves fast training with reduced data requirements, producing photo-realistic high-resolution images and demonstrating state-of-the-art performance in extensive experiments.
__label__machine_vision To address the above issues, we propose a novel and efficient dual-tier few-shot learning paradigm for WSI classification, named FAST.
__label__speech_and_audio Our approach is simple and zero-shot, and it leverages pre-trained text-to-image and text-to-spectrogram diffusion models that operate in a shared latent space.
__label__privacy We further design novel schemes to generate a surface trajectory that involves a series of fixed-length trajectories with dynamically adjusted step sizes.
"__label__reinforcement_learning Our ""temporal recall"" mechanism is inspired by the prefrontal cortex's role in sequence learning, where the agent uses an environmental model to replay memories at a finer temporal resolution than its processing speed while addressing the credit assignment problem caused by scalar rewards in sequence learning."
__label__learning_theory For the popular Denoising Diffusion Probabilistic Model (DDPM), we find that the dependency of the error incurred within each denoising step on the ambient dimension $d$ is in general unavoidable.
__label__machine_learning_for_physical_sciences The Pfaffian allows us to enforce the antisymmetry on arbitrary electronic systems without any constraint on electronic spin configurations or molecular structure.
__label__natural_language_processing Third, LFMs generalize to unseen environments, improving task-completion rate by 3.5-12.0% through one round of adaptation.
__label__machine_vision Despite this improvement, we further identify a common-action bias issue that the cross-modal baseline over-focus on common sub-actions due to a lack of ability to discriminate text-related visual parts.
__label__machine_vision In this paper, we adapt Mamba, a state-space language model, into VMamba, a vision backbone with linear time complexity.
__label__graph_neural_networks However, previous studies generally focused on the short-range dependencies within brain networks while neglecting the long-range dependencies, limiting an integrated understanding of brain-wide communication.
__label__natural_language_processing Recent works show that assembling multiple off-the-shelf large language models (LLMs) can harness their complementary abilities.
__label__machine_vision 3) Benchmark-level: we develop a comprehensive benchmark of 19 methods based on IEA40K.
__label__machine_learning_for_healthcare However, designing adequate and insightful models for such data is difficult because the response of a cell to perturbations essentially depends on contextual covariates (e.g., genetic background or type of the cell).
__label__reinforcement_learning We also propose a new Federated Q-learning algorithm, called Fed-DVR-Q, which is the first Federated Q-learning algorithm to simultaneously achieve order-optimal sample and communication complexities.
__label__machine_vision This pathway first bootstraps the semantics of individual objects and then modulates the model to prioritize features relevant to these semantics.
__label__other Long-term time series forecasting (LTSF) represents a critical frontier in time series analysis, characterized by extensive input sequences, as opposed to the shorter spans typical of traditional approaches.
__label__evaluation When the number of edits is slightly large, the intrinsic knowledge structure of the model is disrupted or even completely damaged.
__label__graph_neural_networks IntraMix is a theoretically grounded plug-in-play method that can be readily applied to all GNNs.
__label__machine_vision With the rise of powerful pre-trained models (PTMs), there is a growing interest of training incremental learning systems using these foundation models, rather than learning from scratch.
__label__machine_vision Code is available at: https://github.com/mxchen-mc/PCD.
__label__learning_theory Learning with reduced labeling standards, such as noisy label, partial label, and supplementary unlabeled data, which we generically refer to as imprecise label, is a commonplace challenge in machine learning tasks.
__label__natural_language_processing With the fine-grained key concepts detailed in every class session of the syllabus, we are able to generate diverse instructions with a broad coverage across the entire spectrum of human knowledge and skills.
__label__other Currently, KV cache quantization is performed per-channel or per-token independently.
__label__causal_inference Causal representation learning (CRL) is the process of using the observed data to recover the latent causal variables and the causal structure among them.
__label__reinforcement_learning Offline policy evaluation (OPE) allows us to evaluate and estimate a new sequential decision-making policy's performance by leveraging historical interaction data collected from other policies.
__label__generative_models Our project page is at: https://yulu.net.cn/freelong.
__label__natural_language_processing Decision boundaries are straightforward to visualize and provide important information about the qualitative behavior of the inductive biases of standard classifiers.
__label__deep_learning_architectures To this end, we introduce a multi-purpose framework to the representation learning community, which allows to: (i) compare different spaces in an interpretable way and measure their intrinsic similarity; (ii) find correspondences between them, both in unsupervised and weakly supervised settings, and (iii) to effectively transfer representations between distinct spaces.
__label__active_learning Our model leverages mixture components derived from unsupervised learning in the label space and improves prediction accuracy by predicting weight coefficients following the evidential learning paradigm.
__label__robotics Specifically, we propose effective strategies, including motion mask generation, adaptive Gaussian point management, and a hybrid camera tracking algorithm to improve the accuracy and robustness of pose estimation.
__label__natural_language_processing However, this setup fails to fully address the diversity of real-world user queries and assumes the existence of task-specific datasets.
__label__reinforcement_learning the epistemic uncertainty about the unknown dynamics.
__label__machine_vision Our code will be made open-source upon paper acceptance.
__label__causal_inference Identifying subgroups with differential responses to treatment is pivotal in randomized clinical trials, as tailoring treatments to specific subgroups can advance personalized medicine.
__label__machine_vision Defining a noise concept that comprehensively considers both intra-identity consistency and inter-identity discrimination, CION seeks the identity correlation from cross-video images by modeling it as a progressive multi-level denoising problem.
__label__privacy Our work highlights that privacy risk is significant even for extremely simple model classes when individuals can request deletion of their data from the model.
"__label__learning_theory By leveraging the approximation 
capabilities of neural networks, NCP efficiently handles a wide variety of complex probability distributions."
__label__machine_learning_for_other_sciences_and_fields The results demonstrate significant improvements in forecasting accuracy, suggesting a potential paradigm shift in time series forecasting through the effective utilization of unstructured news data.
__label__neuroscience_and_cognitive_science It is shown that a rich output feature representation, i.e., the feature vector before classifier) is beneficial to training an accurate model in ANNs for classification.
__label__natural_language_processing As Large Language Models (LLMs) become more capable of handling increasingly complex tasks, the evaluation set must keep pace with these advancements to ensure it remains sufficiently discriminative.
__label__deep_learning_architectures We can thus progressively distill the Transformer architecture by matching different degrees of granularity in the SSM: first matching the mixing matrices themselves, then the hidden units at each block, and finally the end-to-end predictions.
__label__neuroscience_and_cognitive_science Further, we illustrate our findings with numerical experiments.
__label__safety_in_machine_learning In total, by careful construction of a diverse ensemble, we can utilize agreement-on-the-line-based methods to predict the OOD performance of foundation models with high precision.
__label__generative_models Most advances in 3D Generative Adversarial Networks (3D GANs) largely depend on ray casting-based volume rendering, which incurs demanding rendering costs.
__label__machine_learning_for_other_sciences_and_fields Molecular property prediction (MPP) is integral to drug discovery and material science, but often faces the challenge of data scarcity in real-world scenarios.
__label__causal_inference In this paper, we develop a novel approach to combine IV and observational data to enable reliable CATE estimation in the presence of unobserved confounding in the observational data and low compliance in the IV data, including no compliance for some subgroups.
__label__online_learning In this research, we examine dynamic environments and choose \emph{dynamic regret} and \emph{adaptive regret} to measure the performance.
__label__diffusion_based_models Furthermore, we extensively evaluate our quantized model across various benchmark datasets and through human evaluation to demonstrate its superior generation quality.
__label__reinforcement_learning This is contrary to the full-information case (Zhao et al., 2024), where the regret can be independent of the number of states even for unstructured reward functions.
__label__optimization Stochastic gradient descent (SGD) is perhaps the most prevalent optimization method in modern machine learning.
__label__machine_vision The proposed interface has the following favorable attributes: (1) Generalizable.
__label__reinforcement_learning Instead of a binary rank condition, we propose principal angles as a more refined measure of similarity and dissimilarity between transition laws.
__label__machine_vision Our extensive evaluations show that OctreeOcc not only surpasses state-of-the-art methods in occupancy prediction, but also achieves a 15%-24% reduction in computational overhead compared to dense-grid-based methods.
__label__natural_language_processing We identify a key obstacle towards reliability: LLMs are trained to answer any question, even with incomplete context or insufficient knowledge.
__label__optimization_for_deep_networks Quantization via decoupleQ is linear and uniform, making it hardware-friendlier than non-uniform counterpart, and enabling the idea to be migrated to high-bit quantization to enhance its robustness.
"__label__other However, due
to the heterogeneity between the clients’ data distributions, the model obtained
through the use of FL algorithms may perform poorly on some client’s data."
__label__reinforcement_learning In contrast, a vast amount of human videos exist, capturing intricate tasks and interactions with the physical world.
__label__optimization_for_deep_networks The states forming the preconditioner and its inverse root restrict the maximum size of models trained by second-order optimizers.
__label__reinforcement_learning Thanks to the semantically meaningful behavior tokens, the resulting VLA model, OmniJARVIS, can reason (by producing chain-of-thoughts), plan, answer questions, and act (by producing behavior tokens for the imitation learning policy decoder).
__label__machine_vision Correspondences with the smallest errors are identified as inliers to generate a transformation hypothesis for each local set.
__label__online_learning To learn informative modulations in an efficient manner, we utilize amortization-based meta-learning, which substitutes an otherwise required optimization process with a single forward pass of the encoder.
__label__diffusion_based_models Experiments show that PuLID achieves superior performance in both ID fidelity and editability.
__label__natural_language_processing Our analysis reveals that within the self-attention and feed-forward networks, only the fine-tuned attention parameters are particularly beneficial when the training set's distribution does not fully align with the test set.
__label__machine_vision With the proposed feature-space occlusion and completion, MonoMAE learns enriched 3D representations that achieve superior monocular 3D detection performance qualitatively and quantitatively for both occluded and non-occluded objects.
__label__safety_in_machine_learning To improve the post-purification robustness, we propose a straightforward tuning defense, Path-Aware Minimization (PAM), which promotes deviation along backdoor-connected paths with extra model updates.
__label__reinforcement_learning For these settings, we introduce Contextual Bilevel Reinforcement Learning (CB-RL), a stochastic bilevel decision-making model, where the lower level consists of solving a contextual Markov Decision Process (CMDP).
__label__causal_inference We then develop a novel, orthogonal learner for the bounds on the CDTE, which we call AU-learner.
__label__reinforcement_learning This paper proposes a novel domain knowledge-based regularization technique and adaptively refines the initial domain knowledge to considerably boost performance in limited data with partially omitted states.
__label__causal_inference Accurately predicting conditional average treatment effects (CATEs) is crucial in personalized medicine and digital platform analytics.
__label__learning_theory We also present an additive risk decomposition for ensembles of weighted estimators and show that the risks are equivalent along the paths when the ensemble size goes to infinity.
__label__safety_in_machine_learning In recent years, there has been significant research focusing on addressing security concerns in single-modal person re-identification (ReID) systems that are based on RGB images.
__label__algorithmic_game_theory Team-Nash equilibrium (TNE) predicts the outcomes of such coordinated interactions.
__label__optimization In particular, we only require the width of the network to be greater than or equal to the rank of the output label matrix.
__label__machine_vision The proposed method not only simplifies NetVLAD but also enhances the generalizability across different domains.
__label__natural_language_processing Reasoning capabilities are crucial for Large Language Models~(LLMs), yet a notable gap exists between English and non-English languages.
__label__machine_learning_for_physical_sciences UPTs efficiently propagate dynamics in the latent space, emphasized by inverse encoding and decoding techniques.
__label__optimization Frequently, the burgeoning field of black-box optimization encounters challenges due to a limited understanding of the mechanisms of the objective function.
__label__deep_learning_architectures In addition, more experiment results indicate that linear attention, as long as endowed with these two properties, can outperform Softmax attention across various tasks while maintaining lower computation complexity.
__label__generative_models GDB leverages a probabilistic approach to evolve geometric state distributions, employing an equivariant diffusion bridge derived by a modified version of Doob's $h$-transform for connecting geometric states.
__label__causal_inference The task is motivated by high-stake scenarios such as healthcare and welfare where algorithmic action recommendations are made to a human expert, opening the option of deferring making a recommendation in cases where the human might act better on their own.
__label__fairness We evaluate SureMap on disaggregated evaluation tasks in multiple domains, observing significant accuracy improvements over several strong competitors.
__label__deep_learning_architectures Additionally, $\mathbf{L}$ and $\mathbf{R}$ are readily amenable to low-rank adaptation, consequently enhancing the zero-shot performance.
__label__machine_learning_for_healthcare To address this bottleneck, we introduce TurboHopp, an accelerated pocket-conditioned 3D scaffold hopping model that merges the strategic effectiveness of traditional scaffold hopping with rapid generation capabilities of consistency models.
__label__generative_models Alignment methods are designed to reduce such undesirable output, via techniques such as fine-tuning, prompt engineering, and representation engineering.
__label__machine_learning_for_other_sciences_and_fields We experimentally demonstrate the effectiveness of this approach on ff-EBMs using Deep Hopfield Networks (DHNs)  as energy-based blocks, and show that a standard DHN can be arbitrarily split into any uniform size while maintaining or improving performance with increases in simulation speed of up to four times.
__label__machine_learning_for_other_sciences_and_fields This is due to random bit-flips in static random access memory (SRAM), where model parameters are stored.
__label__optimization_for_deep_networks These features are then coarsely reconstructed during the backward pass to implement the update rules.
__label__machine_vision In this work, we introduce MeshFormer, a sparse-view reconstruction model that explicitly leverages 3D native structure, input guidance, and training supervision.
__label__neuroscience_and_cognitive_science Yet most prior research has focused on modeling implicit and explicit human behavior in isolation; and often limited to a specific type of visual content.
__label__interpretability_and_explainability Lastly, we establish that our methods are still effective under vision-language-model-based concept annotations, alleviating the need for a human-annotated validation set.
__label__learning_theory Consequently, if these algorithms are applied in scientific studies, they may lead to contradictory results that erode public trust in science.
__label__natural_language_processing We iteratively optimize the policy based on the feedback from a base EA model.
__label__learning_theory In this paper, we introduce a new convolution method based on  $\ell_p$-norm.
__label__reinforcement_learning EASI features simplicity, low cost, and high fidelity, enabling the construction of a more realistic simulator with minimal requirements for real-world data, thus aiding in transferring simulated-trained policies to the real world.
__label__learning_theory We study credit attribution by machine learning algorithms.
__label__machine_vision Code: https://github.com/Brandon3964/MultiModal-Task-Vector
__label__online_learning We show that the risk of the ensemble classifier is bounded with respect to the regret of the underlying online learning method.
__label__machine_vision Zero- and few-shot visual anomaly segmentation relies on powerful vision-language models that detect unseen anomalies using manually designed textual prompts.
__label__robotics To achieve this capability, our framework decomposes the complex learning process into two manageable progressive objectives and introduce two components to realize them.
__label__deep_learning_architectures Our experiments demonstrate state-of-the-art performance on the Long Range Arena, Sequential CIFAR, and Speech Commands tasks among convolution models and linear-time transformers.
"__label__deep_learning_architectures We propose a neural network weight encoding method for network property prediction that utilizes set-to-set and set-to-vector functions
to efficiently encode neural network parameters."
__label__reinforcement_learning However, various systems are inherently continuous in time, making discrete-time MDPs an inexact modeling choice.
__label__machine_learning_for_healthcare Extensive experiments show the proposed Samba outperforms the VMamba baseline by an average accuracy of 23.5\%, 5.6\% and 4.1\% on the cross-domain grading of fatigue fracture, breast cancer and diabetic retinopathy, respectively.
__label__diffusion_based_models In this work, we present Time-Aware Conditional Synthesis (TACS), a novel approach to conditional generation on diffusion models.
__label__algorithmic_game_theory This innovation empowers us to provide faster last-iterate convergence rates against the existing payoff perturbed algorithms, even in the presence of additive noise.
__label__machine_learning_for_other_sciences_and_fields To address challenges in this setting, we explore a simple yet effective solution, a Federated Dual-Personalizing Adapter (FedDPA) architecture.
__label__optimization Even though the problem is canonical, only a primal-dual-based learning-augmented algorithm was explicitly designed for it.
__label__optimization Methods developed to handle support constraints, such as those based on mirror maps, barriers, and penalties, are not suited for this task.
__label__diffusion_based_models To address this challenge, we propose AdaFlow, an imitation learning framework based on flow-based generative modeling.
__label__robotics In this paper, we propose the Centroid-Free Probing (CFP) stage, making novel use of second-order features for more effective use of descriptors from VFMs.
__label__reinforcement_learning We use a didatic toycase example to show how previous diffusion-based methods fail to generate optimal actions due to leveraging these errors and how Diffusion-DICE successfully avoid that.
__label__natural_language_processing On the one hand, we propose counterfactual rewards to assess the contribution of a single agent’s reflection within the system, alleviating the credit assignment problem.
__label__reinforcement_learning Recently, neural networks (NN) have made great strides in combinatorial optimization problems (COPs).
__label__deep_learning_architectures Plugging CondTSF into previous dataset condensation methods facilitates a reduction in the distance between the predictions of the model trained with the full dataset and the model trained with the synthetic dataset, thereby enhancing performance.
__label__probabilistic_methods There are fast UQ methods for graphical models with closed-form solutions and convergence guarantee but with uncertainty underestimation.
__label__other Our technique requires only an estimate of the label distribution of a downstream task.
__label__privacy In this work, we propose FOOGD, a method that estimates the probability density of each client and obtains reliable global distribution as guidance for the subsequent FL process.
__label__machine_vision In this work, we propose FleVRS, a single model that seamlessly integrates the above three aspects in standard and promptable visual relationship segmentation, and further possesses the capability for open-vocabulary segmentation to adapt to novel scenarios.
__label__generative_models Recent advancements in visualizing deep neural networks provide insights into their structures and mesh extraction from Continuous Piecewise Affine (CPWA) functions.
__label__optimization In this paper, we draw new connections between these two areas and present new sparse recovery algorithms inspired by the OBS framework that come with theoretical guarantees under reasonable assumptions and have strong practical performance.
__label__interpretability_and_explainability We find that these random transformers can perform a wide range of meaningful algorithmic tasks, including modular arithmetic, in-weights and in-context associative recall, decimal addition, parenthesis balancing, and even some aspects of natural language text generation.
__label__machine_learning_for_physical_sciences Recently, flow matching has been employed to train Boltzmann Generators for small molecular systems in Cartesian coordinates.
__label__machine_vision Code is available at https://github.com/liangchen527/LFME.
__label__safety_in_machine_learning Second, there exists a conflict between adversarial losses at different distances, which causes difficulties in optimization.
__label__generative_models We further introduce a new learning technique, Discriminator Weighted Supervised Learning (DWSL) for preferential IVM training that prioritizes high-quality data samples.
__label__probabilistic_methods We show that due to the non-differentiability of activation functions in the ReLU family, leapfrog HMC for networks with these activation functions has a large local error rate of $\Omega(\epsilon)$ rather than the classical error rate of $\mathcal{O}(\epsilon^3)$.
__label__reinforcement_learning Second, we find that a big barrier to improving offline RL performance is often imperfect policy generalization on test-time states out of the support of the training data, rather than policy learning on in-distribution states.
__label__fairness We apply our method to three real-world datasets and derive new insights on bias amplification in prediction and decision-making.
__label__learning_theory Finally, we show that $\mathsf{GD}-\beta$ estimators can be efficiently optimized with gradient flow, despite a non-convex training objective.
__label__natural_language_processing Our code can be found at https://github.com/thunlp/InfLLM.
__label__reinforcement_learning On the other hand, offline domain calibration utilizes only static data from the target domain to adjust the physics parameters of the source domain (e.g., a simulator) to align with the target dynamics, enabling the direct deployment of the trained policy without sacrificing performance, which emerges as the most promising for policy deployment.
__label__natural_language_processing Besides, we show the feasibility of distilling advanced LLMs’ language processing abilities to a smaller yet effective StruXGPT-7B to execute structurization, addressing the practicality of our approach.
__label__machine_vision To evaluate it, we conduct extensive experiments in two transfer learning scenarios in the RS domain: from natural to optical RS images, and from optical RS to multi-spectrum RS images.
__label__learning_theory Our results point to a distinct implicit bias of predicting in latent space that may shed light on its success in practice.
__label__probabilistic_methods This paper studies *amortized* sampling of the posterior over data, $\mathbf{x}\sim p^{\rm post}(\mathbf{x})\propto p(\mathbf{x})r(\mathbf{x})$, in a model that consists of a diffusion generative model prior $p(\mathbf{x})$ and a black-box constraint or likelihood function $r(\mathbf{x})$.
__label__other Besides, our DDN can serve as a plug-in-play module, and thus can be easily incorporated into other forecasting models.
__label__machine_learning_for_healthcare Whole slide image (WSI) analysis is gaining prominence within the medical imaging field.
__label__machine_learning_for_healthcare Notably, this performance is achieved while reducing the number of parameters and computational complexity.
__label__machine_vision The training time is reduced by scene decomposition, and the training convergence and stability are guaranteed through the consensus on the shared 3D Gaussians.
__label__evaluation In this paper, we first established comprehensive principles for honesty LLM and further created the HoneSet with 930 queries across six categories, which is designed to evaluate LLMs’ ability to maintain honesty.
__label__diffusion_based_models Moreover, although MBD does not require external data, it can be naturally integrated with data of diverse qualities to steer the diffusion process.
__label__diffusion_based_models In this work, we propose SceneDiffuser, a scene-level diffusion prior for traffic simulation.
__label__natural_language_processing refusals should not be judgmental) along with a LLM grader.
__label__machine_learning_for_other_sciences_and_fields The resulting FL ecosystem has two features: (i) self-interest, and (ii) competition among FL-PTs.
__label__interpretability_and_explainability This work presents a novel Denoising Diffusion Path (DDPath) to tackle this challenge by harnessing the power of diffusionmodels for denoising.
__label__probabilistic_methods The lower bounds require only mild model assumptions typical of Bayesian asymptotic analyses, while the upper bounds require the log-likelihood functions to satisfy a generalized subexponentiality criterion that is weaker than conditions used in earlier work.
__label__machine_vision In the dynamic visual world, it is crucial for AI systems to continuously detect new objects and establish their relationships with existing ones.
__label__causal_inference We present the corresponding identification algorithm and conduct simulated experiments to verify the effectiveness of our method.
"__label__learning_theory For this SSL setting, we analyze information theoretic lower bounds for accurate feature selection as well as computational lower bounds, 
assuming the low-degree likelihood hardness conjecture."
__label__machine_learning_for_other_sciences_and_fields However, classification methods tend to struggle with these small datasets, leading to poor predictive performance.
__label__diffusion_based_models Pretrained diffusion models (DMs) have recently been popularly used in solving inverse problems (IPs).
__label__other Furthermore, HFTT employs a clever textual data synthesis method, effectively emulating the integration of unknown visual data distribution into the training process at no extra cost.
__label__natural_language_processing Challenging this norm, we posit that ''Not all tokens in a corpus are equally important for language model training''.
__label__online_learning Next, we present a novel algorithm called FTPL-A that dynamically maintains a group of FTPL experts and combines them with an advanced meta-algorithm to obtain $O(\sqrt{\tau\log{T}})$ adaptive regret for any interval of length $\tau$.
"__label__reinforcement_learning Our key experiments included:
1."
__label__reinforcement_learning Previous approaches have not been able to scale to complex environments and are constrained to receiving feedback at the state level which can be expensive to collect.
__label__robotics To make use of this extra flexibility, QueST imparts causal inductive bias from the action sequence data into the latent space, leading to more semantically useful and transferable representations.
__label__causal_inference These general laws inspire us that the estimator designs is not merely about eliminating bias, reducing variance, or simply achieve a bias-variance trade-off.
__label__optimization Our analysis relies on a fine-grained treatment of the nested dependence between client participation and errors in the control variates, which results in tighter guarantees than previous work.
__label__machine_vision It consistently outperforms regression-based methods and averaging ensemble approaches on 14 benchmarks across 3 image restoration tasks, including super-resolution, deblurring and deraining.
__label__online_learning The standard approach to addressing this problem involves a reduction to bandit convex optimization with memory.
__label__learning_theory compression of a generic set of points.
__label__natural_language_processing Experiments with emergent communication games validate our theoretical results.
__label__deep_learning_architectures Extensive experiments on real-world datasets demonstrate that CCM can (1) boost the performance of CI and CD models by an average margin of 2.4% and 7.2% on long-term and short-term forecasting, respectively; (2) enable zero-shot forecasting with mainstream time series forecasting models; (3) uncover intrinsic time series patterns among channels and improve interpretability of complex time series models.
__label__fairness Large language models (LLMs) have been observed to exhibit bias towards certain cultures due to the predominance of training data obtained from English corpora.
__label__reinforcement_learning Subsequently, we introduce **B**ilin**E**ar **CAUS**al r**E**presentation (BECAUSE), an algorithm to capture causal representation for both states and actions to reduce the influence of the distribution shift, thus mitigating the objective mismatch problem.
__label__probabilistic_methods We develop a Mean-Field (MF) view of the learning dynamics of overparametrized Artificial Neural Networks (NN) under distributional symmetries of the data w.r.t.
__label__machine_vision When optimized for OOD accuracy, the ensemble model exhibits a noticeable decline in ID accuracy, and vice versa.
__label__neuroscience_and_cognitive_science Here, we report analyses of an fMRI dataset where we manipulate the complexity of large language models, testing 28 pretrained models from 8 different families, ranging from 124M to 14.2B parameters.
__label__safety_in_machine_learning KLE defines positive semidefinite unit trace kernels to encode the semantic similarities of LLM outputs and quantifies uncertainty using the von Neumann entropy.
__label__reinforcement_learning Comprehensive evaluations on public benchmarks and varying datasets showcase OASIS’s superiority in benefiting offline safe RL agents to achieve high-reward behavior while satisfying the safety constraints, out- performing established baselines.
__label__machine_vision The Adversarial Significance Identifier, is tasked with discerning token significance by integrating global contextual analysis, utilizing a structural salience index algorithm alongside an auxiliary supervisory mechanism.
__label__machine_learning_for_other_sciences_and_fields While recent work demonstrates the advantage of generative models in this realm, the exploration of different probability paths are still insufficient, and hallucinations during sampling are persistently occurring.
__label__machine_learning_for_other_sciences_and_fields Molecular learning is pivotal in many real-world applications, such as drug discovery.
__label__reinforcement_learning In this work, we establish a novel theoretical result that links the context length of a policy to the time needed to reliably evaluate its performance (i.e., its mixing time) in large scale partially observable reinforcement learning environments that exhibit latent sub-task structure.
__label__reinforcement_learning The proposed method, to the best of our knowledge, is the first to guarantee convergence to an optimum in the tabular setting.
__label__probabilistic_methods Second, the discrete nature of integers renders the construction of meaningful gradients challenging, which is problematic for learning.
__label__robotics For next-best-view planning, we aim to reduce the uncertainty of the NGF through a graspness inconsistency-guided policy, selecting views based on discrepancies between NGF outputs and a pre-trained graspness network.
__label__diffusion_based_models To generate high quality images, this paper explores the problem of matching the prompt to a Stylus of relevant adapters, built on recent work that highlight the performance gains of composing adapters.
__label__natural_language_processing In this spirit, we study the properties of \emph{affine} alignment of language encoders and its implications on extrinsic similarity.
__label__privacy This has led to a significant amount of research on evaluating the robustness of these networks against such perturbations.
__label__safety_in_machine_learning This paper proposes the Rank Calibrated Class-conditional CP (RC3P) algorithm to reduce the prediction set sizes to achieve class-conditional coverage, where the valid coverage holds for each class.
__label__interpretability_and_explainability In this work, we study the emergence of in-context learning and skill composition in a collection of modular arithmetic tasks.
__label__natural_language_processing Instead, our work aims to provide practical guidance for instruction tuning LMs, especially in low-resource scenarios.
__label__natural_language_processing Furthermore, our method, without safety data, rejects more harmful queries of safety benchmark AdvBench than both ChatGPT and GPT-4.
__label__algorithmic_game_theory We consider a dynamic mechanism design problem where an auctioneer sells an indivisible good to two groups of buyers in every round, for a total of $T$ rounds.
__label__other Dataset distillation aims to distill a large dataset into a small synthesized dataset such that models trained on it can achieve similar performance to those trained on the original dataset.
__label__reinforcement_learning Recent efforts like GLAM and TWOSOME manually constrain the action space to a restricted subset and employ reinforcement learning to align agents' knowledge with specific environments.
__label__evaluation I-Div transfers the sampling patterns from the test distribution to the training distribution by estimating density and likelihood ratios.
__label__learning_theory (2022).
__label__probabilistic_methods Theoretical guarantees are provided to demonstrate that our approach yields MI estimates for the original data.
__label__natural_language_processing While previous research has advocated for constraining policy optimization, our study introduces a novel approach to enhance the reward model's generalization ability against distribution shifts by regularizing the hidden states.
__label__deep_learning_architectures In recent years, the merging of vast datasets with powerful computational resources has led to the emergence of large pre-trained models in the field of deep learning.
__label__natural_language_processing To address these limitations, we propose a comprehensive framework, StrategyLLM, allowing LLMs to perform inductive reasoning, deriving general strategies from specific task instances, and deductive reasoning, applying these general strategies to particular task examples, for constructing generalizable and consistent few-shot prompts.
__label__fairness This trade-off raises concerns about the stability of FL,  as data-rich clients may opt to leave the current coalition and join another that is more closely aligned with its expected high performance.
__label__neuroscience_and_cognitive_science Our experimental results show that the SWS coding scheme outperforms the existing neural coding schemes in very deep SNNs, and significantly reduces operations and latency.
__label__optimization_for_deep_networks We find that $\mathcal{NC}$ properties that develop with scale (and regularization) are linked to generalization.
__label__generative_models In particular, PBP-GFN enhances the discovery of high-reward objects, maintains the diversity of the objects, and consistently outperforms existing methods.
"__label__causal_inference By a standard simulation 
result, namely $\mathsf{PSPACE} \subseteq \mathsf{EXP}$,
this algorithm has exponential running time which vastly improves 
the state-of-the-art double exponential time method using a Gröbner basis 
approach."
__label__machine_vision RDMs are smaller and easier to train than TAs, especially in large data regimes, since they operate on the teacher embeddings and do not need to relearn low level input feature extractors.
"__label__learning_theory A single-index model (SIM) is a function of the form $\sigma(\mathbf{w}^{\ast} \cdot \mathbf{x})$, where
$\sigma: \mathbb{R} \to \mathbb{R}$ is a known link function and $\mathbf{w}^{\ast}$ is a hidden unit vector."
__label__fairness We assess the fairness of an algorithm by comparing the GPI of different groups, and say that it achieves perfect *Perceptual Fairness* (PF) if the GPIs of all groups are identical.
__label__machine_vision Second, a dynamic camera positioning that extracts features for each proposal.
__label__reinforcement_learning To have a loss function for a given context, we employ the LLM-based code generation with iterative refinement, by which the code and controlled trajectory are validated to align with the context in a closed-loop manner.
__label__machine_vision To tackle these issues, we present **HumanSplat**, which predicts the 3D Gaussian Splatting properties of any human from a single input image in a generalizable manner.
__label__machine_learning_for_other_sciences_and_fields Specifically, we align two pockets in 3D space with protein-ligand binding priors and build two complex graphs with shared ligand nodes for SE(3)-equivariant composed message passing, based on which we derive a composed drift in both 3D and categorical probability space in the generative process.
__label__natural_language_processing This result thus demonstrates that code synthesis can be applied to a much broader class of problems than previously considered.
__label__other To address this issue, we introduce the Evolving Rate (EvoRate), which quantitatively approximates the intensity of evolving patterns in the data with Mutual Information.
__label__machine_vision Specifically, we refine the feature channels in the visual domain to ensure they contain domain-invariant and class-relevant features by using a lightweight adapter.
__label__deep_learning_architectures Codes and models are available at https://github.com/Dmmm1997/SimVG.
__label__machine_learning_for_social_sciences It communicates among agents within the system to collaboratively solve tasks, under the premise of shared information.
__label__reinforcement_learning Our method is essentially model-free and can be implemented in $\tilde{O}(A_{\text{tot}})$-space when given generative model access.
__label__neuroscience_and_cognitive_science Our approach demonstrates superior performance across these tasks, precisely identifying language-based concepts within brain signals, enhancing interpretability, and providing deeper insights into neural processes.
__label__neuroscience_and_cognitive_science Training our autoencoder to accurately pattern-complete and reconstruct sensory experiences with a constraint on total activity causes spatially localized firing fields, i.e., place cells, to emerge in the encoding layer.
__label__machine_vision Importantly, our ReFIR requires no training and is adaptable to various LRMs.
__label__other We analytically characterize the convergence behavior of MTGC under general non-convex settings, overcoming challenges associated with couplings between correction terms.
__label__diffusion_based_models However, existing models still have many difficulties when faced with multiple-object compositional generation.
__label__safety_in_machine_learning We show that this simple trick can improve the OOD detection performance of a variety of methods and additionally propose a distance-based method that leverages the properties of the augmented WeiPer space.
__label__learning_theory Recent works on the parallel complexity of Boosting have established strong lower bounds on the tradeoff between the number of training rounds $p$ and the total parallel work per round $t$.
__label__neuroscience_and_cognitive_science Modern contrastive learning and generative models improved the performance of visual decoding and reconstruction based on functional Magnetic Resonance Imaging (fMRI).
__label__natural_language_processing This helps us reasonably explain the surprising experimental findings.
__label__machine_vision Specifically, we create and evolve two sets of prototypes—textual and visual—to progressively capture more accurate multi-modal representations for target classes during test time.
__label__machine_learning_for_other_sciences_and_fields Extensive experiments on three public datasets validate our method achieves state-of-the-art performance in the presence of hidden confounding, regardless of RCT data availability.
__label__machine_vision Pre-trained vision-language models like CLIP have remarkably adapted to various downstream tasks.
__label__online_learning (2023).
__label__machine_learning_for_other_sciences_and_fields To address this challenge, we introduce **Neural P$^3$M**, a versatile enhancer of geometric GNNs to expand the scope of their capabilities by incorporating mesh points alongside atoms and reimaging traditional mathematical operations in a trainable manner.
__label__natural_language_processing Addressing the limitations of static $\beta$ values, we introduce a novel framework that dynamically calibrates $\beta$ at the batch level, informed by data quality considerations.
__label__natural_language_processing Chain-of-Thought (CoT) reasoning has emerged as a promising approach for enhancing the performance of large language models (LLMs) on complex reasoning tasks.
__label__machine_vision Furthermore, MDA exhibits diverse potential applications, with comprehensive experiments exploring model decision route analysis, model compression, knowledge distillation, and more.
__label__machine_vision The first stage of our approach ensures data consistency by focusing on accurately reconstructing the low-frequency content with a spatially varying deconvolution method that adjusts to changes in the Point Spread Function (PSF) across the camera's field of view.
__label__machine_vision To ensure accurate class representation, we employ conditioning mechanisms, specifically by conditioning on image edges and subject representation.
__label__diffusion_based_models We evaluate CatFlow on one abstract graph generation task and two molecular generation tasks.
__label__machine_learning_for_other_sciences_and_fields Nevertheless, they overlook the conformational changes (i.e., flexibility) within proteins upon binding, leading to poor generalization ability.
__label__machine_learning_for_other_sciences_and_fields However, these methods have only been tested using large training datasets that are expensive to collect, and focus on designing promoters for markedly different cell types, overlooking the complexities associated with designing promoters for closely related cell types that share similar regulatory features.
__label__machine_vision Video understanding has witnessed significant progress with recent video foundation models demonstrating strong performance owing to self-supervised pre-training objectives;  Masked Autoencoders (MAE) being the design of choice.
__label__bandits We study stochastic linear bandits where, in each round, the learner receives a set of actions (i.e., feature vectors), from which it chooses an element and obtains a stochastic reward.
"__label__safety_in_machine_learning We prove that MEG satisfies
several desiderata and demonstrate our algorithms with small-scale experiments."
__label__algorithmic_game_theory In this paper, we initiate the study of tractable $\Phi$-equilibria in non-concave games and examine several natural families of strategy modifications.
__label__diffusion_based_models However, effective editing requires inverting the source image into a latent space, a process often hindered by prediction errors inherent in DDIM inversion.
__label__optimization Numerical experiments validate our theoretical results and show that SFL outperforms FL and split learning (SL) when data is highly heterogeneous across a large number of clients.
__label__other Specifically, we maintain and store a set of device-shared _domain knowledge vectors_, which accumulates the knowledge learned from all devices during their lifelong adaptation process.
__label__reinforcement_learning Parameter $\alpha$ will be employed to adaptively regulate the variance of the added noise, which is applied to the action output by the diffusion model.
__label__learning_theory We study the problem of estimating the means of well-separated mixtures when an adversary may add arbitrary outliers.
__label__learning_theory We validate our theoretical findings through various empirical assessments.
__label__optimization_for_deep_networks The modular norm is defined recursively in tandem with the network architecture itself.
__label__diffusion_based_models Moreover, recognizing that existing metrics are insufficient for accurately evaluating association-engendered stereotypes, we propose a novel metric, *Stereotype-Distribution-Total-Variation*(*SDTV*), to evaluate stereotypes in T2I.
__label__interpretability_and_explainability To this end, we introduce Successive Concept Bottleneck Agents (SCoBots), that integrate consecutive concept bottleneck (CB) layers.
__label__privacy In this work, we define the fundamental PUT of private sampling in the minimax sense, using the $f$-divergence between original and sampling distributions as the utility measure.
__label__optimization_for_deep_networks Code is available at https://github.com/Chen-Junbao/FedCCFA.
__label__learning_theory Under the central model, the performance loss due to the privacy mechanism is further weakened, such that the additional sample complexity becomes negligible.
__label__reinforcement_learning This way, **Cinderella** is shown to achieve state-of-the-art regret bounds for all previously known (and some new) continuous MDPs for which RL is learnable and feasible.
__label__reinforcement_learning In this paper, we propose a novel Structural Information principles-based Effective Exploration framework, namely SI2E.
__label__machine_learning_for_social_sciences To facilitate our study, we collect a large dataset of questions from competitive forecasting platforms.
__label__deep_learning_architectures The key challenge lies in identifying important components useful to the model’s predictions.
__label__natural_language_processing Addressing these challenges, we propose *SparseLLM*, a novel framework that redefines the global pruning process into manageable, coordinated subproblems, allowing for resource-efficient optimization with global optimality.
__label__neuroscience_and_cognitive_science Simulations show that the goal-reducer can be integrated into RL frameworks like Deep Q-learning and Soft Actor-Critic.
__label__machine_vision This allows us to exploit self-supervision from repeated traversals to achieve environment-object decomposition.
__label__natural_language_processing In this work, we introduce **Uncertainty of Thoughts (UoT)**, an algorithm to augment large language models with the ability to actively seek information by asking effective questions.
__label__interpretability_and_explainability Our experimental results provide evidence of SCoBots' competitive performances, but also of their potential for domain experts to understand and regularize their behavior.
__label__reinforcement_learning Despite the success of Multi-Agent Reinforcement Learning (MARL) algorithms in cooperative tasks, previous works, unfortunately, face challenges in heterogeneous scenarios since they simply disable parameter sharing for agent specialization.
__label__reinforcement_learning As a prominent category of imitation learning methods, adversarial imitation learning (AIL) has garnered significant practical success powered by neural network approximation.
"__label__causal_inference We show that by making simple yet
often realistic independence assumptions, it is possible 
to uniquely estimate the probability of an interventional formula (including
the well-studied notions of probability of sufficiency and necessity)."
__label__machine_learning_for_healthcare Furthermore, we observe that employing centerline masks predicted by GraphMorph significantly reduces false positives in the segmentation task, which is achieved by a simple yet effective post-processing strategy.
__label__machine_vision Empirically, we find that directly finetuning on fashion data leads CLIP to frequently ignore minor yet important details such as logos and composition, which are critical in fashion tasks such as retrieval and captioning.
__label__other Autonomous driving system aims for safe and social-consistent driving through the behavioral integration among interactive agents.
__label__diffusion_based_models While existing literature has studied various unlearning techniques, these often suffer from either poor unlearning quality or degradation in text-image alignment after unlearning, due to the competitive nature of these objectives.
__label__generative_models While time series diffusion models have received considerable focus from many recent works, the performance of existing models remains highly unstable.
__label__generative_models Recent advances in 4D generation mainly focus on generating 4D content by distilling pre-trained text or single-view image conditioned models.
__label__speech_and_audio Moreover, the black-box nature of existing models limits their use in real-world scenarios, where explanations are required for model decisions.
__label__machine_learning_for_other_sciences_and_fields However, most existing DM-based methods rely on approximations in the generative process to be generic to different inverse problems, leading to inaccurate sample distributions that deviate from the target posterior defined within the Bayesian framework.
__label__deep_learning_architectures However, it exhibits the low expert activation issue, i.e., only a small subset of experts are activated for optimization, leading to suboptimal performance and limiting its effectiveness in learning a larger number of experts in complex tasks.
__label__machine_learning_for_other_sciences_and_fields In this work, we propose a method that clusters data belonging to a union of nonlinear manifolds.
__label__robotics Moreover, benefit from the scene graph representation, we further design a re-perception mechanism to empower the object navigation framework with the ability to correct perception error.
__label__generative_models Furthermore, unlike previous works, we allow the diffusion coefficient to be unbounded instead of a constant, which is closer to the SOTA models.
__label__other Drawing from the multifaceted information, we present a new efficient LLVM, Mamba-based traversal of rationales (Meteor), which leverages multifaceted rationale to enhance understanding and answering capabilities.
__label__generative_models We propose a new mixture-of-experts prior, softly guiding each modality's latent representation towards a shared aggregate posterior.
__label__optimization Given an optimization problem, the first stage of the methodology is to design an appropriate electric circuit whose continuous-time dynamics converge to the solution of the optimization problem at hand.
__label__optimization_for_deep_networks Code is available at https://github.com/RitianLuo/EigenSAM.
__label__natural_language_processing In certain scenarios, such as multi-tenant serving, deploying multiple LLMs becomes necessary to meet complex demands.
__label__deep_learning_architectures Specifically, we define key scenario attributes (e.g, camera-viewpoint, lighting condition) and train specialized PEFT modules for each attribute.
__label__machine_learning_for_other_sciences_and_fields These metrics enable the first automated clustering of mathematical formulas.
__label__infrastructure However, it has yet to take advantage of new capabilities present in recent hardware, with FlashAttention-2 achieving only 35% utilization on the H100 GPU.
__label__neuroscience_and_cognitive_science Overall, our works takes an important step towards decoding dynamic visual perception from EEG signals.
__label__machine_vision Second, by embedding the combination of the text and zero-shot location model into the diffusion fusion process, a text-controlled fusion re-modulation strategy is developed.
__label__machine_vision Extensive experimental results show that we achieve better or comparable performances on the LVU, COIN, and Breakfast datasets.
"__label__learning_theory The majority of the literature on active learning has 
  focused on obtaining uniform guarantees on the error rate which are
  only able to explain the upper envelope of the learning curves over families
  of different data-generating distributions."
__label__graph_neural_networks A small-scale numerical experiment is conducted to directly validate our theoretical findings.
__label__generative_models Our extensive experiments on LVMs and LLMs demonstrate that finetuning only a small fraction of the parameters in the base model significantly outperforms LoRA while enabling both rapid switching and multi-adapter fusion.
__label__diffusion_based_models We further reveal an interesting connection of our method with existing methods that minimize mode-seeking KL.
__label__robotics Unlike traditional planning methods that rely on domain-specific knowledge and handcrafted rules, LMs generalize from diverse data and adapt to various tasks with minimal tuning, acting as a compressed knowledge base.
__label__safety_in_machine_learning Existing works have shown the insufficiency of Principal Component Analysis (PCA) straightforwardly applied on the features of DNNs in detecting OoD data from In-Distribution (InD) data.
__label__natural_language_processing Through experiments across 21 diverse benchmarks, we show that, in many scenarios, IM can effectively improve the LM performance on both NLP tasks (*e.g.,* MMLU, TruthfulQA, and HumanEval) and open-ended generation benchmarks (*e.g.,* MT-Bench and AlpacaEval).
__label__diffusion_based_models Masked (or absorbing) diffusion is actively explored as an alternative to autoregressive models for generative modeling of discrete data.
__label__neuroscience_and_cognitive_science More interestingly, via psychophysics experiments, we found that MP recognizes biological movements in a way that aligns with human behaviors.
__label__neuroscience_and_cognitive_science We observe that the bifurcations from continuous attractors in theoretical neuroscience models display various structurally stable forms.
__label__safety_in_machine_learning Nevertheless, this guarantee is violated when the data is subjected to adversarial attacks, which often result in a significant loss of coverage.
__label__generative_models In this paper, we present ZipCache, an accurate and efficient KV cache quantization method for large language models (LLMs).
__label__probabilistic_methods We leveraged the fact that deep models, including both random forests and deep-nets, learn internal representations which are unions of polytopes with affine activation functions to conceptualize them both as partitioning rules of the feature space.
__label__learning_theory First, under a certain condition of data distribution, we prove that an autoregressively trained transformer learns $W$ by implementing one step of gradient descent to minimize an ordinary least squares (OLS) problem in-context.
__label__neuroscience_and_cognitive_science After discovering some pitfalls in the original derivation, i.e., unbiased estimation does not hold, we improve the algorithm by proposing an efficient gradient descent-based optimization that minimizes the cost-to-go while only imposing linearity of the control law.
__label__machine_learning_for_other_sciences_and_fields Specifically, we leverage LLMs to synthesize code that outsources delicate reasoning to external expert tools, such as using a parsing library to extract program values of interest and invoking an automated theorem prover to validate path feasibility.
__label__diffusion_based_models We further consider an iteratively fine-tuned version of gradient-guided diffusion where guidance and score network are both updated with newly generated samples.
__label__optimization_for_deep_networks We present several hypergradient estimation strategies on manifolds and analyze their estimation errors.
__label__privacy The standard practice is to select the noise scale to satisfy a given privacy budget ε.
__label__learning_theory Our result builds upon the recent short-flat decomposition framework of [KLLST23a, KLLST23b] and leverages fast algorithms for flow problems on graphs to solve adaptive reweighting subproblems efficiently.
__label__machine_learning_for_other_sciences_and_fields This paper extends the Agda ecosystem into machine learning territory, and, vice versa, makes Agda-related resources available to machine learning practitioners.
__label__machine_learning_for_physical_sciences We open-source our code and model weights trained at multiple scales for reproducibility.
__label__optimization_for_deep_networks In-context learning (ICL) refers to a remarkable capability of pretrained large language models, which can learn a new task given a few examples during inference.
__label__learning_theory In this work, we employ the framework of Permutation Equivariant and Relative Margin-based (PERM) losses [Wang and Scott, 2024] to introduce a multiclass extension of the exponential tail property.
__label__natural_language_processing In this paper, we propose CORY, extending the RL fine-tuning of LLMs to a sequential cooperative multi-agent reinforcement learning framework, to leverage the inherent coevolution and emergent capabilities of multi-agent systems.
__label__machine_learning_for_physical_sciences Machine learning methods are promising in significantly accelerating charge density prediction, yet existing approaches either lack accuracy or scalability.
__label__diffusion_based_models Our code is available at \url{https://github.com/tuananhbui89/Erasing-Adversarial-Preservation}.
__label__optimization Numerical experiments corroborate our theories.
__label__other To address this gap, we formally investigate why MP helps CF from multiple perspectives and show that many assumptions made by previous works are not entirely accurate.
__label__bandits While intervention on the parents of the reward node is optimal in the absence of latent confounders, this is not necessarily the case in general.
"__label__learning_theory Besides, we conduct exploratory analyses beyond the first data condition 
and prove that generally, the trained transformer will not perform vanilla gradient descent for the OLS problem."
__label__optimization_for_deep_networks We also suggest an initialization for the variance in Adam, which provides benefits similar to warmup.
__label__natural_language_processing To retain focus content, we design a memory unit that updates with task progress by decision agent.
__label__natural_language_processing A subsequent rotation further smooths the activation landscape, enhancing model performance.
__label__learning_theory In particular, we provide a polynomial-time algorithm that with high probability recovers $\beta^*$ up to error $O(\sqrt{\varepsilon})$  as long as  $n \ge \tilde{O}(k^2/\varepsilon)$, only assuming some bounds on the third and the fourth moments of $\mathcal{D}$.
__label__natural_language_processing To conduct this analysis, we develop a procedure to generate \textit{clones} of a given natural language data set, which rigorously capture the interactions between tokens up to a specified order.
__label__generative_models However, the current human-computer interaction based on keyboards may not meet the requirements of disabled people.
__label__machine_learning_for_other_sciences_and_fields The computational task is to reconstruct the 3D density of the particle, along with 3D pose of the particle in each 2D image, for which the posterior pose distribution is highly multi-modal.
__label__diffusion_based_models The resulting general scheme is shown to match the best-known sampling methods for Ising models, and is further validated on high-dimensional Gaussian mixture models.
__label__probabilistic_methods The model is regularized by appropriate priors on the unitary transformations, posterior summaries of which may then be suitably interpreted as optimal data-driven rotations of a fixed orthonormal basis for the Hilbert space.
__label__machine_vision the agent must effectively leverage its sequentially observed aerial views when searching for the goal.
__label__learning_theory As an application, we introduce a novel complexity measure, the \emph{Decision Dimension}, which facilitates the new lower bounds for interactive decision making that extend the DEC methodology by incorporating the complexity of estimation.
__label__online_learning However, there is a large gap between its delay-dependent part, i.e., $O((n\bar{d})^{1/3}T^{2/3})$, and an existing $\Omega(\sqrt{\bar{d}T})$ lower bound.
__label__machine_vision This enables semantic segmentation models to be trained simultaneously on multiple datasets, resulting in performance improvements.
__label__infrastructure Moreover, our model insight analysis reveals that pFedClub generates personalized models of reasonable size in a controllable manner, significantly reducing computational costs.
__label__active_learning Informativeness refers to the complexity of graph signals that are learnable from the responses of queried nodes, while representativeness refers to the capacity of queried nodes to control generalization errors given noisy node-level information.
__label__privacy For FSwoR we consider both add/remove and replace-one adjacency, where we improve on the best current computable bound by a factor of $4$.
__label__machine_learning_for_other_sciences_and_fields In this paper, we propose SARAD, an approach that leverages spatial information beyond data autoencoding errors to improve the detection and diagnosis of anomalies.
__label__learning_theory This is the first global convergence result for Gaussian mixtures with more than $2$ components.
__label__interpretability_and_explainability On the other hand, concept-based methods, such as TCAV (Testing with Concept Activation Vectors), provide global explainability, but cannot compute the attribution of a concept in a specific prediction nor show the locations where the network detects these concepts.
__label__machine_vision While studies develop various AV representation learning frameworks, the importance of AV data alignment is usually undermined for achieving high-quality representation.
__label__machine_vision Our numerical and visual comparisons show our superiority over the state-of-the-art results on the widely used benchmarks.
"__label__other We call such composites ""neural programs"" and focus on the problem of learning the DNN parameters when the training data consist of end-to-end input-output labels for the composite."
__label__natural_language_processing We first show that the tree-search policy in ReST-MCTS* achieves higher accuracy compared with prior LLM reasoning baselines such as Best-of-N and Tree-of-Thought, within the same search budget.
__label__interpretability_and_explainability Further, we observe moments of sudden turns in the direction of a model’s learning dynamics in concept space.
__label__probabilistic_methods The code is available at \url{https://github.com/Badr-MOUFAD/dcps}
__label__machine_learning_for_healthcare Generative models learn image distributions and can be used to reconstruct high-quality images from undersampled k-space data.
__label__safety_in_machine_learning Vision transformers have shown remarkable advancements in the computer vision domain, demonstrating state-of-the-art performance in diverse tasks (e.g., image classification, object detection).
__label__learning_theory Numerical results are provided to demonstrate the effectiveness of the proposed method.
__label__diffusion_based_models To address these issues, we propose a novel Self-supervised Hierarchical Makeup Transfer (SHMT) method via latent diffusion models.
__label__diffusion_based_models To save the usage of GPU memory and accelerate the editing process, we further introduce the temporal-dimensional token merging strategy, which can effectively reduce the redundancy.
__label__machine_vision On the other hand, we introduce supplemental constraints from the 3D space by using a 3D detector to guide a further merging process.
__label__machine_vision Motion modeling is critical in flow-based Video Frame Interpolation (VFI).
__label__safety_in_machine_learning FedGMark leverages the unique graph structure and client information in FedGL to learn customized and diverse watermarks.
"__label__fairness We find that our algorithm increases fairness
with only a minor decrease (and at times, even an increase) in efficiency."
__label__speech_and_audio Despite increased processing power in Microcontroller Units (MCUs), MCUs used in the smallest appliances are still not capable of running even moderately big, state-of-the-art artificial neural networks (ANNs) especially in time-sensitive scenarios.
__label__neuroscience_and_cognitive_science This ability arises from the collaboration between the auditory periphery, which encodes sound as precisely timed spikes, and the auditory cortex, which performs spike-based computations.
__label__natural_language_processing Our method is compatible with a range of text encoders.
__label__human-AI_interaction Moreover, as the agent's library of examples grows, it becomes more efficient, relying less on human feedback and requiring fewer environment interactions per demonstration.
__label__natural_language_processing While existing methods typically focus either on training KGE models solely based on graph structure or fine-tuning pre-trained language models with classification data in KG, KG-FIT leverages LLM-guided refinement to construct a semantically coherent hierarchical structure of entity clusters.
__label__interpretability_and_explainability Our experiments show that our model can generally achieve higher performance than the existing prototype based models.
__label__other Code is available at https://github.com/tom4649/lp-ft_ntk.
__label__other Furthermore, we demonstrate that CQ can preserve model quality reasonably with KV cache quantized down to 1 bit.
__label__reinforcement_learning In experiments conducted on sequential games with full data coverage, ARDT can generate a maximin (Nash Equilibrium) strategy, the solution with the largest adversarial robustness.
__label__learning_theory In this paper, we propose a method called Stratified Prediction-Powered Inference (StratPPI), in which we show that the basic PPI estimates can be considerably improved by employing simple data stratification strategies.
__label__diffusion_based_models While recent work has treated conditioning linear processes in infinite dimensions, conditioning non-linear processes in infinite dimensions has not been explored.
__label__natural_language_processing However, these parameters are often high-dimensional and hard to interpret.
__label__natural_language_processing Intuitively, we observe that a chain of explanations on the wrong code followed by code refinement helps LLMs better analyze the wrong code and do refinement.
__label__privacy We demonstrate that our attacks outperform existing methods in terms of both effectiveness and computational efficiency.
__label__other Our code and log data are open-sourced at [https://github.com/pm25/semi-supervised-regression](https://github.com/pm25/semi-supervised-regression).
__label__probabilistic_methods Evidential Deep Learning (EDL), grounded in Evidence Theory and Subjective Logic (SL), provides a robust framework to estimate uncertainty for out-of-distribution (OOD) detection alongside traditional classification probabilities.However, the EDL framework is constrained by its focus on evidence that supports only single categories, neglecting the other collective evidences that could corroborate multiple in-distribution categories.
__label__optimization_for_deep_networks In this study, we comprehensively analyse the bottleneck of traditional N:M sparse training and recognize three drawbacks with discontinuity: incorrect descending direction, inability to predict the amount of descent and sparse mask oscillation.
__label__generative_models The code is publicly available in \url{https://github.com/dbsxodud-11/GTG}.
__label__diffusion_based_models This paper considers the problem of sampling from non-logconcave distribution, based on queries of its unnormalized density.
__label__machine_vision Moreover, the concept synergy mechanism enables table perception-related and comprehension-related tasks to work in harmony, as they can effectively leverage the needed clues from the corresponding source perception embeddings.
__label__bandits Elimination-style algorithms using experimental design methods in combination with a novel finite-time confidence interval on an instrumental variable style estimator are presented with sample complexity upper bounds nearly matching a minimax lower bound.
__label__natural_language_processing This paper presents a novel approach for approximating MBR decoding using matrix completion techniques, focusing on a machine translation task.
__label__learning_theory In this paper, we derive *entrywise* error bounds for low-rank approximations of kernel matrices obtained using the truncated eigen-decomposition (or singular value decomposition).
__label__machine_vision Video Anomaly Detection (VAD) systems can autonomously monitor and identify disturbances, reducing the need for manual labor and associated costs.
__label__learning_theory In particular, we show that reducing the input size leads to saturation of the test loss decay at a characteristic training set size that can be predicted in our framework.
__label__natural_language_processing Therefore, we present a low-cost and fast structured pruning method for LLMs named SlimGPT based on the Optimal Brain Surgeon framework.
__label__machine_learning_for_other_sciences_and_fields Visual geo-localization demands in-depth knowledge and advanced reasoning skills to associate images with real-world geographic locations precisely.
__label__reinforcement_learning Recurrent Neural Networks (RNNs) are used to learn representations in partially observable environments.
__label__natural_language_processing Their limited context awareness can lead to overlooking critical information and subsequent task failures.
__label__reinforcement_learning However, designing or evaluating such a cost function can be prohibitively expensive.
__label__reinforcement_learning We propose a new framework for formulating optimal transport distances between Markov chains.
__label__neuroscience_and_cognitive_science Without batch processing, dimmer neurons and events are harder to identify and unrecognized neurons can create false positives when computing the activity of known neurons.
__label__generative_models Previous methods based on Score Distillation Sampling (SDS) can produce diversified 3D results by distilling 3D knowledge from large 2D diffusion models, but they usually suffer from long per-case optimization time with inconsistent issues.
__label__natural_language_processing Extensive studies have been proposed to extend the context window and achieve length extrapolation of LLMs, but there is still a lack of in-depth interpretation of these approaches.
__label__machine_vision In this work, we theoretically investigate the properties of this approach and unveil that a surprisingly strong TTA method lies dormant and hidden within it.
__label__natural_language_processing The experimental results demonstrate that Text2NKG achieves state-of-the-art performance in F1 scores on the fine-grained n-ary relation extraction benchmark.
__label__reinforcement_learning Our results mark the first efficient and near-optimal reduction from CMDPs to offline density estimation without imposing any structural assumptions on the model class.
__label__optimization_for_deep_networks Recent work suggests an additional asymmetry of the valley beyond the flat and sharp ones, yet without thoroughly examining its causes or implications.
__label__machine_learning_for_other_sciences_and_fields However, existing approaches do not take into account specific block structures—which are closely related to the problem formulations—in the constraint coefficient matrices (CCMs) of MILPs.
__label__learning_theory Many of the recent advances in computer vision and language models can be attributed to the success of transfer learning via the pre-training of large foundation models.
__label__diffusion_based_models In this work, we propose PFDiff, a novel training-free and orthogonal timestep-skipping strategy, which enables existing fast ODE solvers to operate with fewer NFE.
__label__natural_language_processing Based on these observations, we propose two strategies to improve the learning of factual associations in language models.
__label__learning_theory Our analysis provides results for various data batching schemes, including fully online and minibatch.
__label__generative_models }, image), resulting in suboptimal performance when the codebook is applied to multi-modal downstream tasks (\emph{e.g.
__label__machine_vision both NDS and mAP.
__label__reinforcement_learning Through extensive experiments, we show that CurrMask exhibits superior zero-shot performance on skill prompting tasks, goal-conditioned planning tasks, and competitive finetuning performance on offline RL tasks.
__label__learning_theory In this paper, we take a step in this direction by providing a tight theoretical analysis of the emergence of semantic attention in a solvable model of dot-product attention.
__label__active_learning In response to this challenge, we present Partially Observable Cost-Aware Active-Learning (POCA), a new learning approach aimed at improving model generalization in data-scarce and data-costly scenarios through label and/or feature acquisition.
__label__generative_models We further verify whether and how \Tina understands world knowledge by analyzing its capabilities under zero-shot/few-shot image prompts, different numbers of personalized classes, prompts of natural language descriptions, and predicting unseen entities.
__label__machine_vision Occupancy prediction has increasingly garnered attention in recent years for its fine-grained understanding of 3D scenes.
__label__diffusion_based_models However, a significant flaw in these models is evident: they struggle to locate a desired instance when other instances within the same class are presented.
__label__causal_inference In particular, we explore under what conditions on the significance of the variability of the transitions we can build a model to identify the distribution shifts.
__label__learning_theory However, a rigorous examination of the sigmoid gating function is lacking in current literature.
__label__machine_vision Inspired by previous works in supervised AT, we then incorporate a self-supervised double perturbation scheme to self-supervised learning (SSL), which promotes robustness transferable to downstream classification.
__label__natural_language_processing In this work, we propose a novel calibration method that can be used to combat hallucinations.
__label__reinforcement_learning Code is available at https://github.com/FanmingL/Recurrent-Offpolicy-RL.
__label__diffusion_based_models In the experiments using the CAPE, MultiHuman and Hi4D datasets, the proposed method outperforms various SOTA methods based on SMPL, implicit functions, point cloud diffusion, and their combined, under synthetic and real occlusions.
__label__speech_and_audio speech emotion classification, audio classification, text-to-speech generation, speech enhancement, etc.
__label__machine_vision We make the first attempt to tackle this fundamental and challenging problem and propose NeCGS, a neural compression paradigm, which can compress hundreds of detailed and diverse 3D mesh models ($\sim$684 MB) by about 900 times (0.76 MB) with high accuracy and preservation of detailed geometric details.
__label__deep_learning_architectures In this work, we introduce a new approach to model group actions in autoencoders.
__label__neuroscience_and_cognitive_science Using known firing field statistics, we investigate how changes to place cell firing properties affect the distances between representations of different environments within firing rate space.
__label__machine_learning_for_healthcare To tackle this problem, we design a novel retrieval-augmented framework for incorporating similar structure information in known protein structures.
__label__optimization In this paper, we propose a particular case of the UGW problem, termed Partial Gromov-Wasserstein (PGW).
__label__reinforcement_learning To address it, this paper proposes a novel entropy-regularized diffusion policy and takes into account the confidence of the Q-value prediction with Q-ensembles.
__label__machine_vision To further enhance the spatial proximity, we propose a Dual-scale SSM Block to establish a hierarchical structure, enabling a larger receptive field in the 1D serialization curve, as well as more complete local regions in 3D space.
__label__machine_learning_for_healthcare However, the heterogeneity and imprecise nature of the multimodal data still pose challenges in developing an effective method to model from two views.
__label__natural_language_processing Lastly, we show that uncertainty estimates inform human use of LLMs in human-AI collaborative settings through a user study.
__label__deep_learning_architectures As highlights, we introduce a novel unnormalised transformer block, the Outlier Protected block, and present a previously unknown benefit of non-diagonal preconditioning optimisers, finding both approaches to significantly reduce OFs and improve quantisation without compromising convergence speed, at scales of up to 7B parameters.
__label__algorithmic_game_theory Finally, we also study stronger notions of proportional representation, in which deviations do not only happen to single, but multiple candidate centers, and show that stronger proportionality notions of Brill and Peters imply approximations to these stronger guarantees.
__label__other We show that there is a large gap between the performance of current prompt compression methods and the optimal strategy, and propose Adaptive QuerySelect, a query-aware, variable-rate adaptation of a prior work to close the gap.
__label__generative_models This shortcut eliminates the need for cumbersome primal-dual policy iterations, greatly reducing the computational burden and improving training stability.
__label__reinforcement_learning In this robust RL paradigm, two main challenges emerge: managing distributional robustness while striking a balance between exploration and exploitation during data collection.
__label__machine_learning_for_physical_sciences However, most data-driven weather forecasting models are black-box systems that focus on learning data mapping rather than fine-grained physical evolution in the time dimension.
__label__machine_vision Based on this insight, we propose a lightweight conditional score-based model designed with 3D spatial awareness at its core.
__label__machine_learning_for_physical_sciences DiffPhyCon excels by simultaneously minimizing both the learned generative energy function and the predefined control objectives across the entire trajectory and control sequence.
__label__natural_language_processing All these tasks of the LLM are carried out as next token prediction on a serialized view of the dialogue in real-time.
__label__diffusion_based_models We populate this space by creating a dataset of over 60,000 models, each of which is a base model fine-tuned to insert a different person's visual identity.
__label__safety_in_machine_learning One key challenge in Out-of-Distribution (OOD) detection is the absence of ground-truth OOD samples during training.
__label__reinforcement_learning We tackle this challenge by extending state-of-the-art approaches to introduce DUPLEX, a method that explicitly defines a diversity objective with constraints and makes robust estimates of policies’ expected behavior through successor features.
__label__machine_learning_for_healthcare FCR learns multiple cellular representations that are disentangled, comprised of covariate-specific (Z_x), treatment-specific (Z_t) and interaction-specific (Z_tx) representations.
__label__other Extensive experiments demonstrate the effectiveness and efficiency of our proposed method, outperforming state-of-the-art approaches on five visual grounding benchmarks.
__label__machine_vision Code and benchmarks are available at \url{https://github.com/auniquesun/Point-PRC}.
__label__diffusion_based_models Inspired by the recent success of the Latent Diffusion Model (LDM) in image generation, we propose ReF-LDM—an adaptation of LDM designed to generate HQ face images conditioned on one LQ image and multiple HQ reference images.
__label__safety_in_machine_learning In addition, without introducing obvious cost, the combination achieves >30% absolute increase in attack success rates compared with GCG when generating both query-specific (38% ->68%) and universal adversarial prompts (26.68% -> 60.32%) for attacking the Llama-2-7B-Chat model on AdvBench.
__label__online_learning Our code is publicly available at https://github.com/Minhchuyentoancbn/MoE_PromptCL.
__label__probabilistic_methods These processes find wide application in the natural sciences and machine learning, but their inference is known to be far from trivial.
__label__robotics Robotic motor control necessitates the ability to predict the dynamics of environments and interaction objects.
__label__causal_inference To tackle the challenges induced by the presence of latent variables in a sub-population, we first extend the classical relevant graphical definitions, such as c-components and Hedges, initially defined for the so-called ID problem (Pearl, 1995; Tian & Pearl, 2002), to their new counterparts.
__label__reinforcement_learning We compare NeoRL to other baselines on several deep RL environments and empirically demonstrate that NeoRL achieves the optimal average cost while incurring the least regret.
__label__reinforcement_learning For this leveraged matrix estimation procedure, we establish entry-wise guarantees that remarkably, do not depend on the coherence of the matrix but only on its spikiness.
__label__neuroscience_and_cognitive_science A biological model should therefore integrate both learning and visual guidance.
__label__machine_vision Additionally, DDR serves as an effective unsupervised learning objective in image restoration tasks, yielding notable advancements in image deblurring and single-image super-resolution.
__label__optimization The key idea behind the developed method is the use of gradient clipping to control stochastic gradient differences in recursive variance reduction.
__label__interpretability_and_explainability Training models can be seen as random sampling from this distribution of optimized models.
__label__optimization Recent works along these lines learn task-dependent regularizers.
__label__machine_learning_for_other_sciences_and_fields Overall, we introduce _synthetic programming elicitation and compilation_ (SPEAC), an approach that enables LLMs to generate syntactically valid code even for VLPLs.
__label__privacy OSLO leverages transfer-based black-box adversarial attacks.
__label__machine_learning_for_other_sciences_and_fields Although multivariate time series data generally consist of the trend, seasonal and residual terms, existing works mainly focus on optimizing the modeling for the first two items.
__label__privacy We first describe a hypothesis class that is online learnable under approximate DP but not online learnable under pure DP under the adaptive adversarial setting.
__label__diffusion_based_models Extensive experiments on natural language generation and machine translation tasks demonstrate the superior performance of our method in terms of both generation speed and sample quality compared to existing methods for discrete diffusion models.
__label__deep_learning_architectures Extensive experiments demonstrate that our method significantly outperforms existing structured state space models on image classification, object detection and segmentation.
__label__online_learning Surprisingly,  the posterior updates for these experts are equivalent to a scaled and projected form of stochastic gradient descent (SGD) over the network weights.
__label__machine_learning_for_other_sciences_and_fields Automatic translation of programming languages has garnered renewed interest, driven by recent advancements in large language models (LLMs).
__label__algorithmic_game_theory We focus on the computational aspects of the problem, aiming to design algorithms that efficiently compute (almost) optimal strategies for the sender.
__label__learning_theory Specifically, we show that the improvement in performance achieved by strong models over their weaker counterparts is quantified by the *misfit error* incurred by the strong model on labels generated by the weaker model.
__label__learning_theory Our theoretical results show that the over-parameterization gradient flow can adapt to the underlying structure of the signal and significantly outperform the vanilla gradient flow method.
__label__probabilistic_methods In this work we introduce several modifications to improve the convolutional deep kernel machine’s generalisation, including stochastic kernel regularisation, which adds noise to the learned Gram matrices during training.
__label__neuroscience_and_cognitive_science We introduce question-answering embeddings (QA-Emb), embeddings where each feature represents an answer to a yes/no question asked to an LLM.
__label__causal_inference Yet, the aleatoric uncertainty of the treatment effect has received surprisingly little attention in the causal machine learning community.
__label__deep_learning_architectures Thirdly, we conduct the first study to explore NAS for ViT’s OoD robustness.
__label__reinforcement_learning Extensive policy evaluation, selection, and learning experiments highlight the versatility and favorable performance of LS.
__label__learning_theory Our work highlights the provable benefits of combining labeled and unlabeled data for classification and feature selection in high dimensions.
__label__learning_theory In particular, we consider a sequence of $T\ge2$ data distributions $P_1,\ldots,P_T$ undergoing a gradual shift, where each pair of consecutive measures $P_i,P_{i+1}$ are close to each other in Wasserstein distance.
__label__natural_language_processing We further enhance the performance of sub-tasks with publicly available data.
__label__reinforcement_learning To address this challenge we develop a method for focusing the capacity of the world model through a synergy of a pretrained segmentation model, a task-aware reconstruction loss, and adversarial learning.
__label__diffusion_based_models Subsequently, we delve deeper into optimizing the infusion of information from the support mask and simultaneously re-evaluating how to provide reasonable supervision from the query mask.
__label__optimization_for_deep_networks Our approach is suitable for both pretraining and fine-tuning models.
__label__privacy At the core of this computation lies a subsampling method that uses a privacy amplification lemma to enhance the privacy guarantees provided by the additive noise.
__label__machine_vision Meanwhile, with rare benchmarks designed for evaluating retrieval systems, we leverage large multi-modality model (LMM) to evaluate the aesthetic performance with their strong abilities.
__label__interpretability_and_explainability Our code is available at https://github.com/Yinan-Xia/TTD.
__label__machine_vision The code will be released.
__label__probabilistic_methods To address such discontinuities, Aldous introduced the extended weak convergence, which can fully characterise all essential properties, including the filtration, of stochastic processes; however, it was considered to be hard to find efficient numerical implementations.
__label__online_learning The ratio between the strong convexity modulus and the diameter of the regularizer is a key parameter in the analysis of FOMs.
__label__machine_vision The adapted prototypes in semantic and spatial domains are then simultaneously considered to accomplish classification decisions.
__label__natural_language_processing However, the underlying mechanism of those findings still remains an open question.
__label__machine_vision By translating all multilingual image-text pairs from a raw web crawl to English and re-filtering them, we increase the prevalence of (translated) multilingual data in the resulting training set.
__label__generative_models We analyze empirically what causes these models to succeed and fail, and take steps toward understanding how to achieve better out-of-distribution generalization.
__label__algorithmic_game_theory Despite the intractability of computing the best response of an agent in the general case, we provide oracle-efficient algorithms for scenarios where the learner’s hypothesis class consists of low-dimensional linear classifiers or when the agents’ cost function satisfies a sub-modularity condition.
__label__interpretability_and_explainability To this end, we introduce a general framework which can identify the roles of various components in ViTs beyond CLIP.
__label__machine_vision However, current Multimodal Large Language Models (MLLMs) primarily focus on question answering, often neglecting the full potential of questioning and assessment skills.
__label__safety_in_machine_learning Vision-Language Models (VLMs) excel in generating textual responses from visual inputs, but their versatility raises security concerns.
__label__optimization Our approach incorporates novel techniques to accelerate and theoretically guarantee convergence while leveraging vectorization and GPU parallelism for efficiency.
__label__learning_theory Understanding the training dynamics of transformers is important to explain the impressive capabilities behind large language models.
__label__robotics Due to the challenges in controlling a humanoid with dexterous hands, prior methods often use a disembodied hand and only consider vertical lifts or short trajectories.
__label__interpretability_and_explainability To use artificial intelligence and machine learning models wisely we must understand how they interact with the world, including how they depend causally on data inputs.
__label__deep_learning_architectures As a result, the cost and dynamics functions of trajectory optimization can be learned end-to-end.
__label__graph_neural_networks Firstly, we revisit over-smoothing from the perspective of overlapping neighborhood subgraphs, and based on this, we explain how residual methods can alleviate over-smoothing by integrating multiple orders neighborhood subgraphs to avoid the indistinguishability of the single high-order neighborhood subgraphs.
__label__machine_vision Furthermore, we demonstrate the superior stability of PsHD compared to sample representation and pair-wise similarity distillation methods theoretically and experimentally.
__label__machine_vision This architecture not only leverages global and local visual contexts effectively, but also facilitates the flexible extension of visual tokens through a compound token scaling strategy, allowing up to a 16x increase in the token count post pre-training.
__label__safety_in_machine_learning Typically, these purified models exhibit low Attack Success Rates (ASR), rendering them resistant to backdoored inputs.
__label__natural_language_processing As language models continue to scale, Large Language Models (LLMs) have exhibited emerging capabilities in In-Context Learning (ICL), enabling them to solve language tasks by prefixing a few in-context demonstrations (ICDs) as context.
__label__machine_vision Human motion capture from monocular videos has made significant progress in recent years.
__label__natural_language_processing Although recent advancements in large language models (LLMs) have significantly improved their performance on various tasks, they still face challenges with complex and symbolic multi-step reasoning, particularly in mathematical reasoning.
__label__reinforcement_learning We also observe MOMBO to converge faster than these approaches in a large set of benchmark tasks.
__label__diffusion_based_models Based on a dedicated general-form ODE solver, we propose two paradigms: consistency bridge distillation and consistency bridge training, which is flexible to apply on DDBMs with broad design choices.
__label__reinforcement_learning In this work, we show that the conditions developed under full access to the expert's policy cannot guarantee transferability in the more practical scenario where we have access only to demonstrations of the expert.
__label__diffusion_based_models To address this problem, we present ZoDiac, which uses a pre-trained stable diffusion model to inject a watermark into the trainable latent space, resulting in watermarks that can be reliably detected in the latent vector even when attacked.
__label__neuroscience_and_cognitive_science Our code is available at https://github.com/ncclab-sustech/EEG_Image_decode.
__label__neuroscience_and_cognitive_science This brain area also contains place cells, whose location-selective firing fields implement maps supporting spatial memory.
__label__machine_vision Extensive experiments demonstrate that the PIIP achieves superior performance in tasks such as object detection, segmentation, and image classification, compared to traditional image pyramid methods and single-branch networks, while reducing computational cost.
__label__optimization_for_deep_networks In this paper, we solve the $k$-sparse parity problem with sign stochastic gradient descent, a variant of stochastic gradient descent (SGD) on two-layer fully-connected neural networks.
__label__interpretability_and_explainability Transparent models, which are machine learning models that produce inherently interpretable predictions, are receiving significant attention in high-stakes domains.
__label__machine_vision However, the convergence of the enhancement procedure cannot be guaranteed, leading to sensitivity to the number of iterations and limited performance.
__label__natural_language_processing Motivated by these promising results, we conduct extensive experiments to delve deeper into $G_{\text{stack}}$ to address $\textit{O}$2 and $\textit{O}$3.
__label__machine_vision To tackle the above challenges, we propose a label-**F**ree p**ro**mpt distribution **l**earning and b**i**as **c**orrection framework, dubbed as **Frolic**, which boosts zero-shot performance without the need for labeled data.
__label__natural_language_processing We open source our code at https://github.com/IBM/limitations-lm-algorithmic-compositional-learning.
__label__other Topological Data Analysis (TDA) allows us to extract powerful topological, and higher-order information on the global shape of a data set or point cloud.
__label__machine_learning_for_healthcare Both approaches suffer from suboptimal performance due to the loss of key information.
__label__natural_language_processing The resulting LLM-based program synthesis algorithm is broadly applicable: Across loop invariant synthesis, visual reasoning puzzles, and competition programming problems, we find that our new method can solve more problems using fewer language model calls.
__label__active_learning The evidential learning approach provides an uncertainty-aware connection between input features and the predicted coefficients and components.
__label__diffusion_based_models In this work, we propose to identify and preserving concepts most affected by parameter changes, termed as *adversarial concepts*.
__label__learning_theory Our analysis encompasses scenarios where the source and the target domain are related via i) a bounded density ratio, and ii) a measure-preserving transformation.
__label__optimization This implies that optimizing our surrogate loss yields a best-in-class policy asymptotically, even in misspecified settings.
__label__machine_vision This is done by utilizing gradients transferred between the clients and the global server during training or by knowing the model architecture at the client end.
__label__safety_in_machine_learning However, safe RL often suffers from sample inefficiency, requiring extensive interactions with the environment to learn a safe policy.
__label__machine_vision In this paper, we start from the classic convolutional neural network (CNN) and explore a paradigm that does not require training to obtain new models.
__label__other Our codes are available at https://github.com/HFTT-anonymous/HFTT.
__label__diffusion_based_models In this work, we present Discrete Flow Matching, a novel discrete flow paradigm designed specifically for generating discrete data.
__label__optimization Motivated by large-scale applications and the fact that dynamic data often exhibits patterns, we ask the following question: can predictions be used to accelerate the update time of dynamic submodular maximization algorithms?
__label__probabilistic_methods Nonparametric Bayesian models naturally capture this phenomenon, but have significant practical barriers to widespread adoption, namely implementation complexity and computational inefficiency.
__label__privacy We compare OSLO against state-of-the-art label-only attacks and demonstrate that, despite requiring only one query, our method significantly outperforms previous attacks in terms of precision and true positive rate (TPR) under the same false positive rates (FPR).
__label__natural_language_processing This approach opens up the possibilities of studying how interactions of different orders in the data affect learning, in natural language processing and beyond.
__label__other We also provide evidence that the difference in regularization performance between gradient penalties and weight noise can be explained by the NME.
__label__safety_in_machine_learning We adapt a discrete optimization method to measure the tails of reward models, finding that they are consistent with light-tailed error.
__label__machine_learning_for_physical_sciences The sequential nature of our latent representation can be interpreted spatially and permits the use of a conditional transformer for modeling the temporal dynamics of PDEs.
__label__evaluation Such applications have all operated under the assumption, that LLMs can implicitly model the numerical parameter constraints in DS library APIs and produce valid code.
__label__generative_models Unconditional generation -- the problem of modeling data distribution without relying on human-annotated labels -- is a long-standing and fundamental challenge in generative models, creating a potential of learning from large-scale unlabeled data.
__label__evaluation Our benchmarks’ advantages lie in (1) a 0.96 model ranking correlation with Chatbot Arena arising from the highly impartial query distribution and grading mechanism, (2) fast, cheap, and reproducible execution (6% of the time and cost of MMLU), and (3) dynamic evaluation enabled by the rapid and stable data update pipeline.
__label__optimization To estimate the changes in residual error after swaps, we propose a matched swap pair construction method to bound the approximation loss, ensuring a constant probability of loss reduction in each local search step.
__label__machine_vision Does the intermediate generator provide additional information over directly training on relevant parts of the upstream data?
__label__deep_learning_architectures By leveraging MGDL, we offer insights into overcoming spectral bias limitation of DNNs, thereby enhancing the performance and applicability of deep learning models in tasks requiring the representation of high-frequency information.
"__label__learning_theory Taken together, our results characterize the complexity of learning 
general halfspaces under Gaussian marginals in these models."
__label__privacy Federated learning (FL) is inherently susceptible to privacy breaches and poisoning attacks.
__label__machine_learning_for_healthcare The key idea is that a neural network trained for motion-free reconstruction has a small loss if there is no motion, thus optimizing over motion parameters passed through the reconstruction network enables accurate estimation of motion.
__label__machine_learning_for_other_sciences_and_fields We also provide an extensive theoretical analysis of our methods.
__label__diffusion_based_models However, training these models typically requires access to large amounts of clean data, which could prove difficult in some settings.
__label__machine_vision Unlike existing methods, our approach facilitates seamless training without the need for additional manual reannotation or taxonomy reconciliation.
__label__machine_vision Despite this, we find CLIP pre-trained thereupon exhibits notable robustness to the data imbalance compared to supervised learning, and demonstrates significant effectiveness in learning generalizable representations.
__label__machine_vision They are particularly relevant for self-labeled clustering methods, where latent pseudo-labels $y$ are jointly estimated with the model parameters and uncertainty is prevalent.
__label__machine_vision While prior works have addressed unsupervised semantic segmentation, they significantly lag behind supervised models.
__label__privacy Our work improves over the previous best result (which required roughly $k^2 d^4$ samples) and is provably optimal when $d$ is much larger than $k^2$.
__label__deep_learning_architectures This is achieved by eliminating the corresponding weights from the network, without the need for retraining.
__label__other To implement MagR, we address the $\ell_\infty$-regularization by employing an efficient proximal gradient descent algorithm.
__label__optimization The exact iEF and EF methods are experimentally evaluated using practical deep learning setups, including widely-used setups for parameter-efficient fine-tuning of pre-trained models (T5-base with LoRA and Prompt-Tuning on GLUE tasks, and ViT with LoRA for CIFAR100).
__label__machine_learning_for_healthcare In this paper, we introduce Medformer, a multi-granularity patching transformer tailored specifically for MedTS classification.
"__label__machine_vision However, generating high-quality novel views under challenging settings,
such as sparse input views, remains difficult due to insufficient information in
under-sampled areas, often resulting in noticeable artifacts."
__label__machine_vision M$^3$GPT operates on three fundamental principles.
"__label__machine_learning_for_physical_sciences In this work, we introduce two approaches that achieve a constant sample complexity, independent
of system size $n$, for learning ground state properties."
__label__other Through this work, we aim to unleash the power of a restricted set of parameters by capitalizing on domain characteristics—a timely reminder that in the realm of LTSF, bigger is not invariably better.
__label__privacy To tackle these challenges, researchers have separately devised secure aggregation mechanisms to protect data privacy and robust aggregation methods that withstand poisoning attacks.
__label__deep_learning_architectures We first show that the dictionary is an instance of the recently proposed Universal Hopfield Network framework.
__label__interpretability_and_explainability We introduce a novel method for using transcoders to perform weights-based circuit analysis through MLP sublayers.
__label__graph_neural_networks To tackle these issues, We introduce Mecoin to efficient construct and Preserve memory.
__label__learning_theory Previous works have analyzed the dynamical equations describing learning in the relatively simplified context of the perceptron under assumptions of a student-teacher framework or a linearized output.
__label__optimization Recent studies try to extend BO to a transfer learning setup to speed up the optimization, where search space transfer is one of the most promising approaches and has shown impressive performance on many tasks.
__label__reinforcement_learning These results, we believe, can be valuable for a wide range of interactive learning problems beyond the LMDP class, and especially, for partially observed environments.
__label__safety_in_machine_learning Specifically, for each model, we find a single direction such that erasing this direction from the model's residual stream activations prevents it from refusing harmful instructions, while adding this direction elicits refusal on even harmless instructions.
__label__machine_learning_for_other_sciences_and_fields We empirically evaluate the performance of SPEAC in a case study for the UCLID5 formal verification language and find that, compared to existing retrieval and fine-tuning baselines, SPEAC produces syntactically correct programs more frequently and without sacrificing semantic correctness.
__label__interpretability_and_explainability (1) If the loss function and the dimension of the model are not fixed, \textsc{Debuggable} is NP-complete regardless of the training order in which all the training samples are processed during SGD.
__label__machine_vision Object tracking is a fundamental task in computer vision, requiring the localization of objects of interest across video frames.
__label__privacy To the best of our knowledge, these are the first results towards understanding trajectory-wise privacy protection in multi-agent RL.
__label__machine_vision Our codes are available at https://github.com/Zoilsen/Attn_Temp_CDFSL.
__label__causal_inference In this work, we show how to sample from any identifiable interventional distribution given an arbitrary causal graph through a sequence of push-forward computations of conditional generative models, such as diffusion models.
__label__speech_and_audio The separated sequences are then reconstructed by the weight-shared decoder, which also performs cross-speaker processing.
__label__machine_learning_for_physical_sciences Namely, we propose to augment CNNs with advection by designing a novel semi-Lagrangian push operator.
__label__bandits In this work, we provide improved Bayes regret bounds for hierarchical Bayesian bandit algorithms in the multi-task linear bandit and semi-bandit settings.
__label__natural_language_processing However, these methods naively inherit the inefficiencies of standard training procedures, indiscriminately applying uniform weight across all tokens, which can lead to unnecessary parameter updates and increased forgetting.
__label__machine_vision Building on the success of powerful image-level vision-language (ViL) foundation models like CLIP, recent efforts have sought to harness their capabilities by either training a contrastive model from scratch with an extensive collection of region-label pairs or aligning the outputs of a detection model with image-level representations of region proposals.
__label__neuroscience_and_cognitive_science This goal reduction mechanism also models human problem-solving.
__label__online_learning Without experience replay, our model achieves SOTA performance in continual learning tasks.
__label__machine_vision In this work, we propose a framework, Scalable Ensemble Diversification (SED), for scaling up existing diversification methods to large-scale datasets and tasks (e.g.
"__label__machine_vision In image classification, query-based pixel
attacks often rely on patches, which heavily depend on randomness and neglect
the fact that scattered pixels are more suitable for adversarial attacks."
__label__safety_in_machine_learning We engage the dual critic system in both code generation stage as well as code execution stage, providing preemptive and post-hoc guidance respectively to LLMs.
__label__machine_vision In this paper, we present OpenDlign, a novel open-world 3D model using depth-aligned images generated from a diffusion model for robust multimodal alignment.
__label__learning_theory In this paper, following the philosophy behind that work, we investigate two questions, namely, for every concept class: (1) What are the minimal assumptions on the data process admitting online learnability?
__label__deep_learning_architectures The basic example is provided by feedforward layered networks of ReLU units trained with $L_2$ regularizers, which exhibit balance after proper training.
__label__safety_in_machine_learning We improve on prior work with a _query-based_ attack that leverages API access to a remote language model to construct adversarial examples that cause the model to emit harmful strings with (much) higher probability than with transfer-only attacks.
__label__probabilistic_methods Probabilistic integral circuits (PICs) have been recently introduced as probabilistic models enjoying the key ingredient behind expressive generative models: continuous latent variables (LVs).
__label__learning_theory In particular, we justify that, to achieve better classification, the features from the minor classes should align with more directions.
__label__optimization Dynamic and adaptive regret guarantees are obtained for DR-submodular maximization, marking the first algorithms to achieve such guarantees in these settings.
__label__reinforcement_learning Evaluated in the DeepMind Control suite, our framework termed Meta-Controller demonstrates superior few-shot generalization to unseen embodiments and tasks over modular policy learning and few-shot IL approaches.
__label__causal_inference This can be challenging due to covariate shift---differences in the distribution of $X$, $Z$, and surrogate variables, which can affect the conditional distribution of $Y \mid X, Z$---rendering traditional CRT approaches invalid.
__label__interpretability_and_explainability Code is available at https://github.com/fiveai/understanding_safety_finetuning.
__label__optimization Under the additional assumption of Lipschitz continuous gradients, we further design a parameter-free version by tracking the Hessian Lipschitz constant locally and ensuring the iterates remain bounded.
__label__evaluation We hope our work provides insights for the community and opens up new avenues for research in multimodal video models.
__label__natural_language_processing Specifically, we integrate a value model with the LLM, automatically generating both process supervision and step-level evaluation signals in MCTS.
__label__graph_neural_networks By stacking these layers, we obtain a deep GNN model called \emph{deep homomorphism network (DHN)}.
__label__optimization_for_deep_networks Moreover, to address data heterogeneity, we study the feature alignment under distributed concept drift, and find two factors that are crucial for feature alignment: the conditional distribution $P(\mathcal{Y}|\mathcal{X})$ and the degree of data heterogeneity.
__label__reinforcement_learning In large-scale sequential games and continuous adversarial RL environments with partial data coverage, ARDT demonstrates significantly superior robustness to powerful test-time adversaries and attains higher worst-case returns compared to contemporary DT methods.
__label__machine_vision Nevertheless, existing driving world models still have limitations in generalization to unseen environments, prediction fidelity of critical details, and action controllability for flexible application.
__label__safety_in_machine_learning This paper investigates this phenomenon through the lens of information theory, revealing a fundamental tradeoff between uncertainty and perception.
__label__graph_neural_networks Based on this theoretical insight, we propose HEGNN, a high-degree version of EGNN to increase the expressivity by incorporating high-degree steerable vectors while maintaining EGNN's efficiency through the scalarization trick.
__label__optimization Algorithms with predictions is a recent framework for decision-making under uncertainty that leverages the power of machine-learned predictions without making any assumption about their quality.
__label__speech_and_audio Experiments show that AVR surpasses current leading methods by a substantial margin.
__label__machine_learning_for_other_sciences_and_fields We demonstrate the utility of $\texttt{DESP}$ in improving solve rates and reducing the number of search expansions by biasing synthesis planning towards expert goals on multiple new benchmarks.
__label__machine_learning_for_healthcare We present empirical results in a synthetic data study validating the usage of causal effect estimation for gaming detection and show in a case study of diagnosis coding behavior in the U.S. that our approach highlights features associated with gaming.
__label__machine_learning_for_healthcare In this paper, we first analyze how the low-rank nature of the long-sequence attention matrix constrains the representation ability of WSI modelling.
__label__reinforcement_learning The ability to approach the same problem from different angles is a cornerstone of human intelligence that leads to robust solutions and effective adaptation to problem variations.
__label__machine_learning_for_other_sciences_and_fields Two main approaches have emerged: the forward approach, which learns a mapping from input to its value, thereby acting as a proxy to guide optimization, and the inverse approach, which learns a mapping from value to input for conditional generation.
__label__machine_vision Experiments show that our method exhibits superior performance compared to previous methods in terms of reconstruction quality and expression accuracy.
__label__causal_inference Despite recent advances for determining the identifiability of the effectiveness of policies in a target domain, there are still challenges for the accurate estimation of effects from finite samples.
__label__reinforcement_learning In this study, we aim to first obtain a clear understanding of the generalization capability of world models by examining the impact of _latent representation error_, and then devise new methods to enhance its generalization.
__label__privacy It helps prevent effective unlearning from interfering with the retained performance.
__label__graph_neural_networks In contrast, global attention-based models such as graph transformers demonstrate strong performance in practice, but comparing their expressive power with the $k$-WL hierarchy remains challenging, particularly since these architectures rely on positional or structural encodings for their expressivity and predictive performance.
__label__machine_learning_for_healthcare In this paper, we make an explicit correspondence between the mutual information of the distribution of per-pixel intensity and labels, and the performance of classical registration methods.
__label__machine_learning_for_healthcare Noisy labels are a critical issue in medical datasets and can significantly degrade model performance.
__label__speech_and_audio We have verified the inclusion of prosody in speech tokens that predominantly contain semantic information and have used this foundation to construct a prosody-infused speech-text model.
__label__interpretability_and_explainability In time-series analysis, many recent works seek to provide a unified view and representation for time-series across multiple domains, leading to the development of foundation models for time-series data.
__label__natural_language_processing We draw parallels between the behavior of LLMs and the evolution of human culture, as the latter has been extensively studied by cognitive scientists for decades.
__label__privacy DDFed simultaneously boosts privacy protection and mitigates poisoning attacks, without introducing new participant roles or disrupting the existing FL topology.
__label__other This paper introduces novel auction mechanisms for ad allocation and pricing within the textual outputs of LLMs, leveraging retrieval-augmented generation (RAG).
__label__machine_learning_for_healthcare However, research exploring scaling law in molecular pretraining model remains unexplored.
__label__machine_learning_for_other_sciences_and_fields We establish the optimality guarantee of EduQate and demonstrate its efficacy compared to baseline policies, using students modeled from both synthetic and real-world data.
__label__machine_learning_for_healthcare i.
__label__probabilistic_methods We develop an algorithm to easily marginalize random effects in LMMs.
__label__interpretability_and_explainability In contrast to current CB models, SCoBots do not just represent concepts as properties of individual objects, but also as relations between objects which is crucial for many RL tasks.
__label__machine_vision Taken together, our work provides theoretically grounded recommendations that can be used to improve SSL convergence and efficiency.
__label__machine_learning_for_other_sciences_and_fields In this paper, we propose GFNSeqEditor, a novel biological-sequence editing algorithm which builds on the recently proposed area of generative flow networks (GFlowNets).
__label__safety_in_machine_learning Moreover, when the fine-tuning dataset contains a mixture of both benign and malicious data,  $\textsf{Safe LoRA}$ mitigates the negative effect made by malicious data while preserving performance on downstream tasks.
"__label__diffusion_based_models The Diffusion Model has not only garnered noteworthy achievements in the realm of image generation 
but has also demonstrated its potential as an effective pretraining method utilizing unlabeled data."
__label__diffusion_based_models Additionally, we present an AID variant called \textbf{Prompt-guided Attention Interpolation via Diffusion (PAID)}, which \textbf{3)} treats interpolation as a condition-dependent generative process.
__label__generative_models Finally, we demonstrate the potential of applying learned semantics to improve Code LLMs' debugging and self-refining capabilities.
__label__interpretability_and_explainability Using this, we investigate three well-known safety fine-tuning methods—supervised safety fine-tuning, direct preference optimization, and unlearning—and provide significant evidence demonstrating that these methods minimally transform MLP weights to specifically align unsafe inputs into its weights’ null space.
__label__reinforcement_learning In learning, posterior sampling of the latent variable naturally integrates sub-trajectories to form a consistent abstrac- tion despite the finite context.
__label__machine_vision In this work, we introduce the Target-Guided Adversarial Point Cloud Transformer, termed APCT, a novel architecture designed to augment global structure capture through an adversarial feature erasing mechanism predicated on patterns discerned at each step during training.
__label__safety_in_machine_learning Using a set of textual action descriptions, we collect value judgments from humans, as well as similarity judgments from both humans and multiple language models, and demonstrate that representational alignment enables both safe exploration and improved generalization when learning human values.
__label__safety_in_machine_learning (2024) reported that even benign fine-tuning on seemingly safe datasets can give rise to unsafe behaviors in the models.
__label__machine_vision Our approach employs a sequence of visual resamplers that capture visual details at various spacial scales.
__label__deep_learning_architectures In this paper, we propose PointMamba, transferring the success of Mamba, a recent representative state space model (SSM), from NLP to point cloud analysis tasks.
__label__graph_neural_networks While existing local solvers approximate diffusion vectors through heuristic local updates, they often operate sequentially and are typically designed for specific diffusion types, limiting their applicability.
__label__optimization However, the existing work is limited to investigations of parameter-free methods for the stepsize, and parameter-free methods for other hyperparameters have not been explored.
__label__natural_language_processing While recent popular open-source models have demonstrated substantial improvements in performance from the RL step, in this paper we question whether the complexity of this RL step is truly warranted for AI feedback.
__label__other We propose Large Pre-trained Time-series Models (LPTM) that introduces a novel method of adaptive segmentation that automatically identifies optimal dataset-specific segmentation strategy during pre-training.
__label__natural_language_processing To date, most studies have concentrated on isolated components within these models, such as the Multilayer Perceptrons and attention head.
__label__machine_vision We conduct experiments on the public RetargetMe benchmark and demonstrate through objective experimental results and subjective user studies that our method outperforms previous approaches in terms of preserving semantics and aesthetics, as well as better generalization across diverse aspect ratios.
__label__machine_learning_for_other_sciences_and_fields Diffusion models have recently advanced Combinatorial Optimization (CO) as a powerful backbone for neural solvers.
__label__probabilistic_methods In this way, we establish the first polynomial-time rehearsal-based approach for addressing the AUF problem.
__label__natural_language_processing However, the potential of optimized prompts on domain generalization has been under-explored.
__label__online_learning To address the challenge of non-convexity, we utilize randomized sampling in the process of tracking experts.
__label__safety_in_machine_learning We find that current safety purification methods are vulnerable to the rapid re-learning of backdoor behavior, even when further fine-tuning of purified models is performed using a very small number of poisoned samples.
__label__reinforcement_learning Our result is the first that is minimax optimal (up to log factors) in all parameters $S,A,\mathsf{H}$, and $\varepsilon$, improving on existing work that either assumes uniformly bounded mixing times for all policies or has suboptimal dependence on the parameters.
__label__machine_vision Compared to the baseline, our approach, StreamFlow, achieves performance enhancements of 15.45% and 11.37% on the Sintel clean and final test sets respectively, with gains of 15.53% and 10.77% on occluded regions and only a 1.11% rise in latency.
__label__machine_learning_for_other_sciences_and_fields To reduce annotation costs, it is common in crowdsourcing to collect only a few noisy labels from different crowd workers for each instance.
__label__safety_in_machine_learning Our experiments demonstrate performance improvements by deploying the same Aligner model across 11 different LLMs, evaluated on the 3H dimensions (helpfulness, harmlessness, and honesty).
__label__machine_vision We then learn an interactive behavior generator by querying paired data of agents' perception and actions from the 4D reconstruction.
__label__deep_learning_architectures Thus, for the already converged dimensions, the computations can be skipped.
__label__diffusion_based_models To ensure tractable inference and learning, we employ a recently popularized Markov approximation of fBM (MA-fBM) and derive its reverse-time model, resulting in *generative fractional diffusion models* (GFDM).
__label__generative_models Sufficient scene-level experiments on both object-centric and forward-facing datasets verify the effectiveness of MVInpainter, including diverse tasks, such as multi-view object removal, synthesis, insertion, and replacement.
__label__deep_learning_architectures Motivated by these concerns, we propose a novel and effective Learngene approach termed LeTs (Learnable Transformation), where we transform the learngene module along both width and depth dimension with a set of learnable matrices for flexible variablesized model initialization.
__label__reinforcement_learning Our experimental results demonstrate that the proposed method successfully generates an instructive curriculum of environments, outperforming UED baselines in zero-shot generalization across novel, out-of-distribution environments.
__label__learning_theory For a point set $S$, the map $\varphi_\ell:\mathbb{R}^d \to N^{-1/2}\{-1,1\}^N$ has the property that storing  $\varphi_\ell(S)$ (a sketch of $S$) allows one to report squared distances between points up to some  multiplicative  $(1\pm \epsilon)$ error with high probability.
__label__diffusion_based_models To address the challenge, consistency models have been proposed to facilitate fast inference, albeit at the cost of sample quality.
__label__machine_vision Extensive experiments validate the effectiveness of our method, which ensures that the post-edit MLLM simultaneously maintains excellent reliability, generality, and locality.
__label__optimization The most computationally expensive procedure in IPMs is to solve systems of linear equations via matrix factorization.
__label__online_learning Moreover, to deal with the unknown function type in practical problems, we propose a multi-level \textit{universal} algorithm that is able to achieve the desirable bounds for three types of time-varying functions simultaneously.
__label__natural_language_processing To achieve this, we create three new datasets: DaTikZv2, the largest TikZ dataset to date, containing over 360k human-created TikZ graphics; SketchFig, a dataset that pairs hand-drawn sketches with their corresponding scientific figures; and MetaFig, a collection of diverse scientific figures and associated metadata.
__label__safety_in_machine_learning BLM constructs an iteratively-updated probabilistic label mapping matrix, with each element quantifying a pairwise relationship between pretrained and downstream labels.
__label__evaluation Finally, some next steps are outlined for studying self-awareness in machines.
__label__generative_models DRE-based models can directly output the likelihood for any given input, a highly desired property that is lacking in most generative techniques.
__label__natural_language_processing Moreover, we introduce C-AdvIPO, an adversarial variant of IPO that does not require utility data for adversarially robust alignment.
__label__generative_models With the rapidly increasing number of satellites in space and their enhanced capabilities, the amount of earth observation images collected by satellites is exceeding the transmission limits of satellite-to-ground links.
__label__natural_language_processing Audio-Visual Question Answering (AVQA) is a complex multi-modal reasoning task, demanding intelligent systems to accurately respond to natural language queries based on audio-video input pairs.
__label__neuroscience_and_cognitive_science This approach compresses the spikes by weighting the significance of the spike in each step of neural computation, achieving high performance and low energy consumption.
__label__optimization In this work, we address this gap by providing the first high-probability complexity guarantees for nonconvex/PL minimax problems corresponding to a smooth function that satisfies the PL-condition in the dual variable.
__label__reinforcement_learning However, the coordination problem still exists since agents cannot communicate actual actions with each other at the same time due to the circular dependencies.
__label__machine_vision While previous works generally relied on hand-crafted procedures to extract task graphs from videos, in this paper, we propose an approach based on direct maximum likelihood optimization of edges' weights, which allows gradient-based learning of task graphs and can be naturally plugged into neural network architectures.
__label__machine_learning_for_healthcare Scaffold hopping is an efficient strategy that facilitates the identification of similar active compounds by strategically modifying the core structure of molecules, effectively narrowing the wide chemical space and enhancing the discovery of drug-like products.
__label__probabilistic_methods Unlike existing methods that employ a user-defined function that hand-tunes the trust level adjustment, our approach enables data-driven adjustments.
__label__optimization This allows us to bound the potential harm caused by Byzantine workers, even during iterations when all sampled clients are Byzantine.
__label__generative_models We derive *BonBon Alignment* as a method for achieving this.
__label__graph_neural_networks Specifically, we generate substitute unknowns to mimic the distribution of real open-set samples firstly, based on the information of graph structures.
__label__natural_language_processing Humans draw to facilitate reasoning: we draw auxiliary lines when solving geometry problems; we mark and circle when reasoning on maps; we use sketches to amplify our ideas and relieve our limited-capacity working memory.
__label__optimization These models produce a single embedding $x \in \mathbb{R}^d$ per data-point, allowing for fast retrieval via highly optimized maximum inner product search (MIPS) algorithms.
__label__learning_theory Meanwhile, we characterize the asymptotic overlap distribution for $p$-step QAOA, discovering an intriguing sine-Gaussian law verified through simulations.
__label__optimization The convergence rates are $O(1/T)$ and $O(1/\sqrt[3]{T})$, respectively, where $T$ denotes the total number of rounds for SFL training.
__label__generative_models Experimental results demonstrate the effectiveness of our approach in diversifying foundation model responses while maintaining high quality, showcased through the HumanEval and MBPP tasks in the code generation domain and several tasks in the natural language understanding domain, highlighting its potential to enrich user experience across various applications.
__label__graph_neural_networks We theoretically prove that MRQSampler maximizes the accumulated spectral energy of subgraphs (i.e., the Rayleigh quotient) to preserve the most significant anomaly information.
__label__machine_vision Our code will be available at https://github.com/mathXin112/PEIF.git.
__label__learning_theory We analyze two anisotropic scenarios: homogeneous, with identical covariance matrices, and heterogeneous, with distinct matrices per cluster.
__label__neuroscience_and_cognitive_science MP also demonstrates superior performance in these cases.
__label__natural_language_processing The results demonstrate that SLED consistently improves factual accuracy by up to 20\% compared to existing decoding methods while maintaining natural language fluency and negligible latency overhead.
__label__deep_learning_architectures Extensive experiments demonstrate the efficiency and effectiveness of the proposed QT-ViTs, showcasing the state-of-the-art results.
__label__learning_theory This represents the first theoretical demonstration that the DDPM sampler can adapt to unknown low-dimensional structures in the target distribution, highlighting the critical importance of coefficient design.
__label__machine_vision Thanks to the decomposed formulation of DKL loss, we have identified two areas for improvement.
__label__deep_learning_architectures While recent radar-camera fusion methods have made significant progress by fusing information in the bird's-eye view (BEV) representation, they often struggle to effectively capture the motion of dynamic objects, leading to limited performance in real-world scenarios.
__label__optimization_for_deep_networks Forward gradients, solely based on directional derivatives computed from two forward calls, have been recently used for model training, with substantial savings in computation and memory.
__label__privacy In the $L_{\infty}$ threat model, ARS enables flexible adaptation through high-dimensional input-dependent masking.
__label__learning_theory We characterize conditions on the signal to noise ratio (SNR) of the model parameters giving rise to benign versus non-benign, or harmful, overfitting: in particular, if the SNR is high then benign overfitting occurs, conversely if the SNR is low then harmful overfitting occurs.
__label__probabilistic_methods and ``how did we get here?''
__label__natural_language_processing Furthermore, applying data and reasoning processes from our benchmark to other lateral thinking-related benchmarks, e.g., RiddleSense and BrainTeaser, leads to performance enhancements.
__label__machine_vision Our extensive experiments demonstrate KptLLM's superiority in various keypoint detection benchmarks and its unique semantic capabilities in interpreting keypoints.
__label__machine_learning_for_healthcare However, in a temporal setting, multi-modal data are often inherently asynchronous.
__label__online_learning We close this gap by introducing a pair of primal-dual treeplex norms, which we contend form the natural analytic viewpoint for studying the strong convexity of DilEnt.
__label__online_learning We investigate the problem of universal online learning with gradient-variation regret.
__label__natural_language_processing However, the underlying rules for the effectiveness of MM-ICL remain under-explored.
__label__reinforcement_learning To achieve this, we perform planning using the empirical distribution of the reward and transition observations, in contrast to vanilla approaches that only rely on estimated expectations.
__label__generative_models We further show that TRLM scoring outperforms conventional forward scoring of response given query, resulting in significant gains in applications such as citation generation and passage retrieval.
__label__other Our analysis extends existing theories from bounding generalized prediction loss (on unseen data) with loss sharpness to bounding the worst-case generalized surrogate sharpness with its empirical estimate on training data, providing a new perspective on sharpness regularization.
__label__machine_vision The code is available [here](https://github.com/littlespray/StreamFlow).
__label__diffusion_based_models In this paper, we study a particular failure mode in diffusion models, which we term ***mode interpolation***.
__label__machine_vision However, foundational approaches like 3D Gaussian Splatting impose substantial storage overhead, as Structure-from-Motion (SfM) points can grow to millions, often requiring gigabyte-level disk space for a single unbounded scene.
__label__optimization_for_deep_networks Although recent works show that neural network can operate in a neural tangent kernel (NTK) regime that does not allow feature learning, many works also demonstrate the potential for neural networks to go beyond NTK regime and perform feature learning.
__label__natural_language_processing However, current quantization methods suffer severe performance degradation when the bit-width is extremely reduced, and thus focus on utilizing 4-bit or 8-bit values to quantize models.
__label__optimization We show that this limitation is essential in the sense that most models written in terms of elementary functions cannot achieve the learnability demonstrated in this theorem.
__label__bandits We examine multi-armed bandit problems featuring strategic arms under debt-free reporting.
__label__optimization_for_deep_networks In this work, we propose a novel framework for improving the optimization of such models by relaxing the hard equivariance constraint during training: We relax the equivariance constraint of the network's intermediate layers by introducing an additional non-equivariant term that we progressively constrain until we arrive at an equivariant solution.
__label__optimization_for_deep_networks Spry makes feasible previously impossible FL deployments on commodity mobile and edge devices.
__label__machine_learning_for_other_sciences_and_fields Using the theory of random walk, we formally show that the mean curvature is lower near boundaries than at other points.
__label__machine_learning_for_physical_sciences These insights motivate us to develop an NNIP architecture designed for scalability: the Efficiently Scaled Attention Interatomic Potential (EScAIP).
__label__safety_in_machine_learning In this paper, we introduce a novel robust attention mechanism designed to enhance the resilience of transformer-based architectures.
__label__privacy An extension GReM-LNN (Gaussian Residuals-to-Marginals with Local Non-negativity) reconstructs marginals under Gaussian noise satisfying consistency and non-negativity, which often reduces error on reconstructed answers.
__label__machine_learning_for_healthcare Specifically, we harness an expressive structure encoder to propose a discrete, informative prior derived from structures, and establish a Markov bridge to connect this prior with native sequences.
__label__diffusion_based_models Despite significant advancements in video generation and editing using diffusion models, achieving accurate and localized video editing remains a substantial challenge.
__label__natural_language_processing However, a pivotal aspect of alignment for honesty involves discerning an LLM's knowledge boundaries, which demands comprehensive solutions in terms of metric development, benchmark creation, and training methodologies.
__label__infrastructure Distributed training, particularly through Sharded Data Parallelism (ShardedDP) which partitions optimizer states among workers, has emerged as a crucial technique to mitigate training time and memory usage.
__label__learning_theory In the asymptotic limit of high-dimensional data and a comparably large number of training samples we provide a tight closed-form characterization of the global minimum of the non-convex empirical loss landscape.
__label__graph_neural_networks In this work,  we reveal that different node pairs within the same dataset necessitate varied pairwise information for accurate prediction and models that only apply the same pairwise information uniformly could achieve suboptimal performance.
__label__deep_learning_architectures Shift-and-add reparameterization offers a promising solution by replacing costly multiplications with hardware-friendly primitives in both the attention and multi-layer perceptron (MLP) layers of an LLM.
__label__natural_language_processing This paper introduces a novel autonomous RL approach, called DigiRL, for training in-the-wild device control agents through fine-tuning a pre-trained VLM in two stages: offline and offline-to-online RL.
__label__graph_neural_networks We further demonstrate strong finetuning scaling behavior on 38 highly competitive downstream tasks, outclassing previous large models.
__label__machine_vision Furthermore, our model, trained solely on images, showcases remarkable zero-shot capabilities in video understanding as well.
__label__learning_theory The two algorithms represent different levels of trade-offs between efficiency and accuracy.
__label__interpretability_and_explainability Furthermore, we also propose semi-parametric distributions based on the Saw distribution to model the convergence trend, satisfying all the counter-intuitive observations.
__label__speech_and_audio To make the visual scene model audio adaptive, we propose a point densification and pruning strategy to optimally distribute the Gaussian points, with the per-point contribution in sound propagation (e.g., more points needed for texture-less wall surfaces as they affect sound path diversion).
__label__diffusion_based_models We present a singular framework that unifies two key stages of simulation: scene initialization and scene rollout.
__label__deep_learning_architectures It extracts multi-frequency domains and focuses on minimizing their individual uncertainties.
__label__safety_in_machine_learning This framework incorporates two components: the Attention Refinement module and the Attention-based Model Constraint module.
__label__learning_theory As a consequence, it is shown that there exist data distributions such that, to be generalizable for them, the memorization network must have an exponential number of parameters in the data dimension.
__label__deep_learning_architectures Transformers, renowned for their self-attention mechanism, have achieved state-of-the-art performance across various tasks in natural language processing, computer vision, time-series modeling, etc.
__label__diffusion_based_models Extensive experiments demonstrate that UniEdit covers video motion editing and various appearance editing scenarios, and surpasses the state-of-the-art methods.
"__label__optimization This setting covers many interesting examples including Hölder smooth problems
and various inexact computations of the stochastic gradient."
__label__bandits We show that the proposed algorithm is optimal as $\delta\to 0$.
"__label__learning_theory In addition, our algorithm is simple and practical, 
relying on online SGD on a carefully selected sequence of convex losses."
__label__machine_vision TransCLIP is applicable as a plug-and-play module on top of popular inductive zero- and few-shot models, consistently improving their performances.
__label__robotics Skill chaining offers a feasible solution for these tasks by pre-training the skills for each sub-task and linking them sequentially.
__label__machine_vision FlexCap is trained to produce length-conditioned captions for input boxes, enabling control over information density, with descriptions ranging from concise object labels to detailed captions.
__label__reinforcement_learning In particular, we do not assume the existence of a reward function and an oracle preference signal drawn from the Bradley-Terry model as most of the prior works do.
__label__optimization_for_deep_networks Recent studies on deep ensembles have identified the sharpness of the local minima of individual learners and the diversity of the ensemble members as key factors in improving test-time performance.
__label__robotics These methods struggle with 3D point-level tasks due to weak feature expressiveness and inaccurate 2D-3D feature associations.
__label__optimization As model sizes in deep learning continue to expand, memory-efficient optimizers are increasingly critical to manage the substantial memory demands of popular algorithms like Adam and AdamW.
__label__safety_in_machine_learning Extensive experiments demonstrate that PAM significantly improves post-purification robustness while maintaining a good clean accuracy and low ASR.
__label__other We accelerate the iterative hard thresholding (IHT) method, which finds \(k\) important elements from a parameter vector in a linear regression model.
__label__reinforcement_learning Furthermore, for convex utility on a widely used polyhedral ambiguity set, we design an algorithm and obtain its convergence rate to a global optimal solution.
"__label__causal_inference In this paper, we give a new sound and complete algorithm for generic 
identification which runs in polynomial space."
__label__learning_theory This models applications where learners crowdsource information from non-expert human workers or conduct noisy experiments to determine group structure.
__label__learning_theory We partially close this gap by focusing on the case where datasets are sampled on an $m$-dimensional submanifold of $\mathbb{R}^d$.
__label__natural_language_processing In the face of uncertainty, the ability to *seek information* is of fundamental importance.
__label__deep_learning_architectures Specifically, we devise a dual-attention interactive structure that embraces a dual-stream self-attention and a layer-aware dual-stream cross-attention mechanism to simultaneously capture intra-layer and inter-layer feature correlations.
__label__optimization_for_deep_networks The dual-level prototypes clustering strategy creates local clustered prototypes based on private data features, then performs global prototypes clustering to reduce communication complexity and preserve local data privacy.
__label__interpretability_and_explainability To this end, we introduce Knowledge-enhanced Bottlenecks (KnoBo), a class of concept bottleneck models that incorporates knowledge priors that constrain it to reason with clinically relevant factors found in medical textbooks or PubMed.
__label__natural_language_processing Large language models (LLMs) exhibit complementary strengths in various tasks, motivating the research of LLM ensembling.
__label__deep_learning_architectures Existing models generally address either multi-party VFL or fuzzy VFL between two parties.
__label__deep_learning_architectures The effectiveness of CODA is verified by benchmarking against a set of SOTA models in terms of worst-group accuracy and maximum group accuracy gap based on two famous datasets, ColoredMNIST and CelebA.
__label__machine_vision Experimental comparisons lead to a new benchmark,  demonstrating PCoTTA's superiority in boosting the model's transferability towards the continually changing target domain.
__label__probabilistic_methods In addition, several trust region-based LBO methods select the anchor, the center of the trust region, based solely on the objective function value without considering the trust region's potential to enhance the optimization process.
__label__bandits The above top 2 algorithm for any $\beta\in(0, 1)$ has sample complexity within a constant of the lower bound.
__label__machine_learning_for_other_sciences_and_fields The resulting Drift-Resilient TabPFN can be applied to unseen data, runs in seconds on small to moderately sized datasets and needs no hyperparameter tuning.
__label__diffusion_based_models First, we find that a straightforward implementation of AT compromises DMs’ image generation quality post-unlearning.
__label__machine_learning_for_social_sciences Leveraging recent technologies for LLM-based agents, we aim to explore AI's potential to create a human-like agent capable of executing comprehensive multi-agent missions by integrating three fundamental capabilities: 1) strategic planning with memory and reflection; 2) goal-oriented negotiation with social reasoning; and 3) augmenting memory through self-play games for self-evolution without human in the loop.
__label__optimization We compare the performance of different modeling choices numerically using a stochastic shortest path example and provide practical insights from the empirical results.
__label__generative_models Notably, our method directly supervises the decoded geometry using a semi-continuous surface sampling strategy, diverging from previous methods relying on rendered images as supervision signals.
__label__privacy Our results demonstrate significant improvements over standard and clustering-based baselines, and in particular, they show that it is possible to improve over direct personalization of a single global model.
__label__other Finally, we demonstrate the performance of the learned kernel across different modalities.
__label__machine_vision TALoS utilizes these observations to obtain self-supervision about occupancy and emptiness, guiding the model to adapt to the scene in test time.
__label__neuroscience_and_cognitive_science Our code is available at Github ( https://github.com/sangwoohwang/SpikedAttention ).
__label__generative_models Recent progress in generative models has stimulated significant innovations in many fields, such as image generation and chatbots.
__label__interpretability_and_explainability Periodic extension techniques are employed to accelerate the decompression.
__label__reinforcement_learning This formulation goes beyond the conventional rectangularity paradigm, offering new perspectives and expanding the analytical framework for robust RL.
__label__machine_learning_for_healthcare FuncMol performs all-atom generation of 3D molecules without assumptions on the molecular structure and scales well with the size of molecules, unlike most existing approaches.
__label__generative_models Data, code, and models are open-released.
__label__causal_inference Nevertheless, the estimation of the CQTE is challenging and often depends upon the smoothness of the individual quantiles as a function of the covariates rather than smoothness of the CQTE itself.
__label__deep_learning_architectures Our analysis uncovers several key insights.
__label__deep_learning_architectures By exploiting the ability of contrastive learning to intervene in the learning of category label fitting, we propose a novel multimodal learning approach that dynamically integrates unsupervised contrastive learning and supervised multimodal learning to address the modality imbalance problem.
__label__machine_vision Extensive experiments demonstrate that our method achieves state-of-the-art results on multiple corruption benchmarks.
__label__machine_vision Experimental results demonstrate that MaVEn significantly enhances MLLMs' understanding in complex multi-image scenarios, while also improving performance in single-image contexts.
__label__natural_language_processing GIF-MCTS surpasses all baselines on the CWMB and two other benchmarks, and we show that the Code World Models synthesized with it can be successfully used for planning, resulting in model-based RL agents with greatly improved sample efficiency and inference speed.
__label__evaluation They naturally arise in a variety of contexts—human labeling, noisy labeling, and weak labeling (i.e., image classification), for example.
__label__machine_learning_for_physical_sciences Altogether, our work illustrates how generative modeling can unlock value from MD data towards diverse downstream tasks that are not straightforward to address with existing methods or even MD itself.
__label__algorithmic_game_theory Selling a single item to $n$ self-interested bidders is a fundamental problem in economics, where the two  objectives  typically considered are welfare maximization and revenue maximization.
__label__causal_inference DeepITE can concurrently learn from both unlabeled and labeled data with different intervention targets and causal graphs, harnessing correlated information in a self or semi-supervised manner.
__label__robotics The source code is available at our project page https://3d-aigc.github.io/OpenGaussian.
__label__deep_learning_architectures This paper is dedicated to simulating human learning to address the shortcomings of instance training, focusing on instruction learning to enhance cross-task generalization.
__label__interpretability_and_explainability However, most of these models are currently evaluated primarily on prediction accuracy, overlooking the validity of the rationales behind their accurate predictions.
__label__bandits When applied to the linear case, our results not only are the first ones with no dependence on a certain problem-dependent constant that can be exponentially large, but also enjoy other advantages such as computational efficiency, dimension-free regret bounds, or the ability to handle completely adversarial contexts and rewards.
__label__reinforcement_learning While existing approaches attempt to solve this problem, we highlight its continuing impact on leading MBRL methods ---including DreamerV3 and DreamerPro--- with a novel environment where background distractions are intricate, predictable, and useless for planning future actions.
__label__optimization_for_deep_networks Motivated by this, we propose **Direction-Aware SHrinking (DASH)**, a method aiming to mitigate plasticity loss by selectively forgetting memorized noise while preserving learned features.
__label__probabilistic_methods These findings are accompanied by empirical results which demonstrate the strengths and weaknesses of the proposed approach.
"__label__safety_in_machine_learning By integrating prefixed safety examples into the fine-tuning dataset, the subsequent fine-tuning process effectively acts as the ""backdoor attack"", establishing a strong correlation between the secret prompt and safety generations."
__label__bandits However, this result may be far from the lower bound $\Omega(\max\{N\log T/\Delta^2, K\log T/\Delta\})$ since the number $K$ of arms (workers, publisher slots) may be much larger than that $N$ of players (employers in labor markets, advertisers in online advertising, respectively).
__label__graph_neural_networks Computing such neural set divergence requires aligning nodes and edges of the two graphs.
__label__diffusion_based_models Drag-based image editing using generative models provides precise control over image contents, enabling users to manipulate anything in an image with a few clicks.
__label__optimization Sometimes, the output of the forward functions in certain layers is determined by the solutions to mathematical optimization problems, leading to the emergence of differentiable optimization layers that permit gradient back-propagation.
__label__reinforcement_learning We formulate this problem as an online bi-level optimization problem where the upper level dynamically adjusts the learned reward according to the newly observed state-action pairs with the help of a meta-regularization term, and the lower level learns the corresponding policy.
__label__machine_vision This paper introduces VeXKD, an effective and Versatile framework that integrates Cross-Modal Fusion with Knowledge Distillation.
__label__other Extensive experiments on three benchmark datasets demonstrate the effectiveness of iLoRA, highlighting its superior performance compared to existing methods in mitigating negative transfer and improving recommendation accuracy.
__label__deep_learning_architectures Face feature fusion is indispensable for robust face recognition, particularly in scenarios involving long-range, low-resolution media (unconstrained environments) where not all frames or features are equally informative.
__label__algorithmic_game_theory Additionally, we address the learner’s optimization problem, offering both positive and negative results on determining the optimal information release to maximize expected accuracy, particularly in settings where an agent’s qualification can be represented by a real-valued number.
__label__natural_language_processing But do LLMs actually recognize their own outputs when they give those texts higher scores, or is it just a coincidence?
__label__reinforcement_learning In recent years, significant attention has been directed towards learning average-reward Markov Decision Processes (MDPs).
__label__machine_vision LocCa significantly outperforms standard captioners on downstream localization tasks, achieving state-of-the-art results on RefCOCO/+/g, while maintaining comparable performance on holistic tasks.
__label__infrastructure In this work, we present novel gradient coding protocols that judiciously leverage the work performed by partial stragglers.
__label__reinforcement_learning This paper demonstrates how agents can communicate about spatial relationships within their observations.
__label__machine_learning_for_healthcare Magnetic resonance imaging (MRI) is a widely used non-invasive imaging modality.
__label__deep_learning_architectures We also show that MatrixNet respects group relations allowing generalization to group elements of greater word length than in the training set.
__label__algorithmic_game_theory Within the class of strictly convex games, we present an algorithm named \texttt{Common-Points-Picking} that returns a point in the expected core given a polynomial number of samples, with high probability.
__label__machine_learning_for_other_sciences_and_fields However, we find that the residual term is more crucial for getting accurate fillings, since it is more related to the diverse changes of data and the biggest component of imputation errors.
__label__neuroscience_and_cognitive_science This investigation may offer valuable insights into the fundamental principles of neural computation.
__label__machine_learning_for_other_sciences_and_fields Code is publicly available at [https://github.com/Computational-Imaging-RU/SCI-BDVP](https://github.com/Computational-Imaging-RU/SCI-BDVP).
__label__reinforcement_learning With approximate forward models, we prove that the resulting gradient bias diminishes with the approximation error while retaining the same variance reduction.
__label__machine_vision We tackle the efficiency problem of learning local feature matching.Recent advancements have given rise to purely CNN-based and transformer-based approaches, each augmented with deep learning techniques.
__label__probabilistic_methods Real-world applications empirically demonstrate that our method not only outperforms existing baselines, but also maintains robustness despite varying labelling accuracy, in tasks of battery design with human experts.
"__label__diffusion_based_models Extensive experiments on sparse view and limited angle CT reconstruction
show that our DiffusionBlend method significantly outperforms previous methods
and achieves state-of-the-art performance on real-world CT reconstruction problems with high-dimensional 3D image (i.e., $256 \times 256 \times 500$)."
__label__deep_learning_architectures In this paper, we present Depth-wise Separable Convolution Pruning (DEPrune), a novel pruning method applied to both point-wise and depth-wise convolutions.
__label__learning_theory How to solve high-dimensional linear programs (LPs) efficiently is a fundamental question.
__label__graph_neural_networks Recognizing this, we explore another GNN structure called the second-order folklore GNN (2-FGNN) that overcomes this limitation, and the aforementioned universal approximation theorem can be extended to the entire MILP space using 2-FGNN, regardless of the MP-tractability.
__label__online_learning $\bullet$ Statistical complexity.
__label__fairness Trained on enormous amounts of data, these models have been shown to contain harmful biases which can hurt their performance when adapted for a downstream classification task.
__label__natural_language_processing We believe the knowledge circuit holds potential for advancing our understanding of Transformers and guiding the improved design of knowledge editing.
__label__machine_learning_for_other_sciences_and_fields Extensive experiments on four benchmark datasets and three downstream tasks demonstrate that our approach significantly outperforms existing models, underscoring the effectiveness of Mobility-LLM in advancing our understanding of human mobility data within LBS contexts.
__label__machine_vision In this way, we can significantly reduce feature variations.
__label__interpretability_and_explainability This approach facilitates a gradual reduction of noise along the path.
__label__safety_in_machine_learning In this paper, we present DeSparsify, an attack targeting the availability of vision transformers that use token sparsification mechanisms.
"__label__interpretability_and_explainability This paper introduces a general method for the exploration of equivalence classes in the input space of
Transformer models."
__label__natural_language_processing To achieve robust speculative decoding, Sequoia uses a novel sampling and verification method that outperforms prior work across different decoding temperatures.
__label__optimization In this paper, we extend the hypothesis class of IO objective functions to a reproducing kernel Hilbert space (RKHS), thereby enhancing feature representation to an infinite-dimensional space.
__label__machine_vision However, their out-of-the-box abilities to accept diverse inputs and perform diverse tasks are limited by the (usually small) number of modalities and tasks they are trained on.
__label__machine_vision Nevertheless, the majority of prior works that leverage MAE pre-training have focused on relatively short video representations (16 / 32 frames in length) largely due to hardware memory and compute limitations that scale poorly with video length due to the dense memory-intensive self-attention decoding.
__label__machine_vision Our findings reveal that MAE reconstructs coherent images from visible patches not through interactions between patches in the decoder but by learning a global representation within the encoder.
__label__machine_learning_for_other_sciences_and_fields This paper considers the problem of learning from multiple sets of inaccurate labels, which can be easily obtained from low-cost annotators, such as rule-based annotators.
__label__optimization For linear inequality constraints, we attain $(\delta,\epsilon)$-Goldstein stationarity in $\widetilde{O}(d{\delta^{-1} \epsilon^{-3}})$ gradient oracle calls, where $d$ is the upper-level dimension.
__label__safety_in_machine_learning Contrary to computer vision, there are no effective attacks to properly evaluate the adversarial robustness of deep tabular models due to intrinsic properties of tabular data, such as categorical features, immutability, and feature relationship constraints.
__label__optimization Among them is the approximated gradient class of methods, which implements a strategy similar to gradient descent.
__label__neuroscience_and_cognitive_science For example, switching and decomposed models describe complex systems using latent variables that evolve according to simple locally linear dynamics.
__label__safety_in_machine_learning With more 3D point cloud data containing sensitivity information, unauthorized usage of this new type data has also become a serious concern.
__label__graph_neural_networks To support our regularization method, we provide a theoretical analysis that proves its effect in promoting low trace norm of the predicted tensor to reduce overfitting.
__label__optimization Gradient descent and its variants are de facto standard algorithms for training machine learning models.
__label__natural_language_processing MetaAligner models multi-objective alignment into three stages: (1) dynamic objectives reformulation algorithm reorganizes traditional alignment datasets to supervise the model on performing flexible alignment across different objectives; (2) conditional weak-to-strong correction paradigm aligns the weak outputs of fixed policy models to approach strong outputs with higher preferences in the corresponding alignment objectives, enabling plug-and-play inferences on any policy models, which significantly reduces training costs and facilitates alignment on close-source policy models; (3) generalizable inference method flexibly adjusts target objectives by updating their text descriptions in the prompts, facilitating generalizable alignment to unseen objectives.
__label__reinforcement_learning Recently, an algorithm with a sample complexity of order $SAH/\varepsilon^2$ has been proposed, but it requires the knowledge of $H$.
__label__generative_models In this work, we propose to train discrete EBMs with Energy Discrepancy, a loss function which only requires the evaluation of the energy function at data points and their perturbed counterparts, thus eliminating the need for Markov chain Monte Carlo.
__label__generative_models To achieve more accurate and nuanced multimodal instruction following, we introduce Instruction-guided Visual Masking (IVM), a new versatile visual grounding model that is compatible with diverse multimodal models, such as LMM and robot model.
__label__probabilistic_methods On a variety of recent high dimensional benchmark tasks in control and molecular design, our approach significantly outperforms standard SVGPs and is capable of achieving comparable rewards with up to $10\times$ fewer function evaluations.
__label__human-AI_interaction We then design an automatic data annotation pipeline utilizing GPT-4 to generate the VOILA-COCO dataset.
__label__machine_vision Experiments on various segmentation tasks show the effectiveness of the proposed method.
__label__safety_in_machine_learning Theoretically, we show that CT-SSF surpasses previous methods defined in the output space.
__label__active_learning We show the benefits of our method when compared to related approaches across synthetic and real-data problems.
__label__fairness CultureLLM employs the World Value Survey (WVS) as seed data and generates semantically equivalent training data through the proposed semantic data augmentation.
__label__natural_language_processing This setup makes it possible for a detector to later identify the source of the text if the user publishes it.
__label__optimization_for_deep_networks Through this, we also shed light on how LoRA can adapt to a new distribution by capturing the shift between task covariances.
__label__generative_models We add temporal self-attention layers to the base LGM to help it learn consistency across time, and utilize a per-timestep multiview rendering loss to train the model.
__label__machine_learning_for_healthcare Specifically, we segregate the modeling of biological data from phenotype data in a graph-based learning framework.
__label__optimization Here, we obtain a bound of $\tilde{O}\big(\frac{1}{\sqrt{n}} + \frac{\sqrt{d}}{n\epsilon}\big)$ on the strong SP-gap, where $n$ is the number of samples and $d$ is the dimension.
__label__active_learning Seemingly unrelated, learning a scaling from oracles offering graded pairwise preferences (GPC) is underexplored, despite a rich history in psychometrics.
__label__diffusion_based_models We introduce a novel controllable motion generation method, InterControl, to encourage the synthesized motions maintaining the desired distance between joint pairs.
__label__human-AI_interaction Our work underscores the value of using diverse data to create more inclusive multimodal systems and lays the groundwork for developing VLMs that better represent global perspectives.
__label__safety_in_machine_learning Building on these stronger linear trends, we demonstrate that combining TTA and AGL-based methods can predict the OOD performance with high precision for a broader set of distribution shifts.
__label__algorithmic_game_theory Finally, we illustrate the practical ramifications of our results on simple two-player zero-sum games.
__label__machine_vision In this work, we introduce a novel approach that utilizes pseudo LiDAR point clouds generated from low-cost miniatures or real-world videos, which is called Pseudo Ground Truth augmentation (PGT-Aug).
__label__graph_neural_networks In contrast, we develop a flexible question-answering framework targeting real-world textual graphs, applicable to multiple applications including scene graph understanding, common sense reasoning, and knowledge graph reasoning.
__label__learning_theory Our bound follows from a new generalization bound that depends on both the dimension as well as the learning rate and number of iterations.
__label__reinforcement_learning In offline reinforcement learning, a policy is learned using a static dataset in the absence of costly feedback from the environment.
"__label__probabilistic_methods Moreover, we provide a theoretical analysis of the behaviour of the gradient flow
of a related free energy functional: establishing the existence and uniqueness of
solutions as well as propagation of chaos results."
__label__machine_vision In this paper, a novel multimodal Probabilistic Conformal Distillation (PCD) method is proposed, which considers the inherent indeterminacy in this alignment.
__label__diffusion_based_models The three modules of IMAGPose work together to unify the task of person image generation under various user scenarios.
__label__reinforcement_learning Meanwhile, recent advances in both of these areas have increasingly relied on conditioning policies on large context lengths.
__label__diffusion_based_models On the visual spatial understanding dataset VSD, our system outperforms the mainstream T2I and I2T methods significantly.
__label__other Kernel methods have emerged as a natural approach for learning to classify these distributions.
__label__natural_language_processing We pursue this hypothesis by developing a family of **Representation Finetuning (ReFT)** methods.
__label__machine_vision VeXKD applies knowledge distillation exclusively to the Bird's Eye View (BEV) feature maps, enabling the transfer of cross-modal insights to single-modal students without additional inference time overhead.
__label__learning_theory This so called jailbreaking of LLMs is typically achieved by adversarially modifying the input prompt to the LLM.
__label__privacy Many high-stakes applications require machine learning models that protect user privacy and provide well-calibrated, accurate predictions.
__label__reinforcement_learning We present a methodology for identifying sub-networks within a larger network in reinforcement learning (RL).
__label__other Further, we quantify the superiority of  _semantic_ RSMs over _spatio-semantic_ RSMs through image retrieval and by comparing the similarity between representations to the similarity between predicted class probabilities.
__label__optimization Through fine-grained analysis in the lens of primal-dual cyclic coordinate methods and the introduction of novel smoothness parameters, we present several results for shuffled SGD on smooth and non-smooth convex losses, where our novel analysis framework provides tighter convergence bounds over all popular shuffling schemes (IG, SO, and RR).
__label__natural_language_processing Nonetheless, data selection and labeling are still a bottleneck for these systems, particularly at large scale.
__label__reinforcement_learning However, in PbRL scenarios demanding heightened risk awareness, such as in AI systems, healthcare, and agriculture, risk-aware measures are requisite.
__label__reinforcement_learning Before online fine-tuning, we re-evaluate the pessimistic critic trained on the offline dataset in an optimistic way and then calibrate the misaligned critic with the reliable offline actor to avoid erroneous update.
__label__privacy Without privacy constraints, the standard estimators for this task are U-statistics, which commonly arise in a wide range of problems, including nonparametric signed rank tests, symmetry testing, uniformity testing, and subgraph counts in random networks, and are the unique minimum variance unbiased estimators under mild conditions.
__label__safety_in_machine_learning Our code is available at https://github.com/BililiCode/WaveAttack.
__label__diffusion_based_models Our analysis is based on a generalized version of Girsanov's theorem and is compatible with both the SDE and probability flow ODE implementations.
__label__machine_vision Our first insight is to exploit per-scene optimized Neural Radiance Fields (NeRFs) by generating dense depth and virtual camera targets from them, which helps our model to learn enhanced 3D geometry from sparse non-overlapping image inputs.
__label__machine_learning_for_other_sciences_and_fields Foundation models in computer vision have demonstrated exceptional performance in zero-shot and few-shot tasks by extracting multi-purpose features from large-scale datasets through self-supervised pre-training methods.
__label__optimization Our results improve upon the $R = \Omega(\log n \cdot \log d)$ requirement  in a recent work of Price.
__label__machine_vision With the increasing inference cost of machine learning models, there is a growing interest in models with fast and efficient inference.
__label__machine_learning_for_other_sciences_and_fields Such cell-type-specific promoters are difficult to discover using existing methods, requiring either manual curation or access to large datasets of promoter-driven expression from both targeted and untargeted cells.
__label__machine_learning_for_other_sciences_and_fields This integration allows our model to infuse geometric information into node features while preserving the spatial semantics of coordinates, leading to greater expressive power than standard FA models.
__label__machine_vision However, CLIP struggles with dense prediction tasks due to the poor grasp of the fine-grained details.
__label__interpretability_and_explainability In addition, we propose a new metric called Number of Effective Concepts (NEC) to control the information leakage and provide better interpretability.
__label__diffusion_based_models We promote the generation of AID samples during the training of a generative model by utilizing a feature extractor to guide the process and filter out detrimental samples during generation.
__label__natural_language_processing We demonstrate the effectiveness of DigiRL using the Android-in-the-Wild (AitW) dataset, where our 1.5B VLM trained with RL achieves a 49.5\% absolute improvement -- from 17.7 to 67.2\% success rate -- over supervised fine-tuning with static human demonstration data.
__label__neuroscience_and_cognitive_science Autonomous driving demands an integrated approach that encompasses perception, prediction, and planning, all while operating under strict energy constraints to enhance scalability and environmental sustainability.
__label__online_learning Moreover, we show that saliency-based modulation successfully encourages the learning of features that are more robust to the presence of spurious features and to adversarial attacks than baseline methods.
__label__optimization We also provide a lower bound showing that any algorithm using $O(hd/R)$ bits of space can obtain at most $1 - \Omega(1/R^2)$ correlation with the top eigenvector.
__label__human-AI_interaction We show finetuning our retrieval-augmented in-context agent yields additional improvements.
__label__learning_theory Simulation results suggest that the approximation is good for different prominent models and data sets.
__label__probabilistic_methods Through this framework, it is possible to leverage tools in OT to build unbalanced losses to handle noisy views and customize the representation space by changing the constraints on alignment.
__label__machine_learning_for_other_sciences_and_fields The number of edits can be regulated through specific hyperparameters.
__label__neuroscience_and_cognitive_science These methodologies enhance model performance and advance our understanding of the neural circuits underlying cognition.
__label__machine_vision Extensive experiments and ablation studies demonstrate the significant advantages of our NeCGS over state-of-the-art methods both quantitatively and qualitatively.
"__label__online_learning *A priori*, the seller is unaware of the
distribution of buyers, but can repeat the market for $T$ rounds so as to learn the revenue-optimal pricing curve $p:[N] \rightarrow [0, 1]$."
__label__algorithmic_game_theory Specifically, we examine scenarios where daycares have similar priorities over children, a common characteristic in practical markets.
__label__generative_models We demonstrate that FouRA enhances the generalization of fine-tuned models thanks to its adaptive rank selection.
__label__algorithmic_game_theory We show that when $\Phi$ is finite, there exists an efficient uncoupled learning algorithm that approximates the corresponding $\Phi$-equilibria.
__label__neuroscience_and_cognitive_science Our method extends from traditional scalar-valued measures by learning eigenvalues, eigenfunctions, and projection spaces of density ratios from realizations of the signal, addressing the interpretability, scalability, and local temporal dependence of cortico-muscular connectivity.
__label__optimization Experiments on various types of NLPs, including Quadratic Programs and Quadratically Constrained Quadratic Programs, show that our approach can significantly accelerate NLP solving, reducing iterations by up to 60% and solution time by up to 70% compared to the default solver.
__label__generative_models Our method consists of three components: a multivariate noise schedule, adaptive input-conditional diffusion, and auxiliary variables; these components ensure that the ELBO is no longer invariant to the choice of the noise schedule as in previous works.
"__label__optimization_for_deep_networks In the mini
batch regime, we observe that stochasticity introduces confounding effects that
explain the previous success of some learning rate tuners at appropriate batch
sizes."
__label__speech_and_audio Experimental results show that STAR achieves an average of 13.5% relative reduction in word error rate across 14 target domains, and it sometimes even approaches the upper-bound performance of supervised adaptation.
__label__reinforcement_learning Building on this, we propose a novel algorithm that combines the benefits of both methods.
__label__reinforcement_learning The codes of SCR are available in https://github.com/jianda-chen/SCR.
__label__safety_in_machine_learning However, traditional fine-tuning methods are computationally intensive and require updating billions of model parameters.
__label__infrastructure Neighborhood attention, and more generally sliding window attention patterns, have long been bounded by infrastructure, particularly in higher-rank spaces (2-D and 3-D), calling for the development of custom kernels, which have been limited in either functionality, or performance, if not both.
__label__optimization Solving districting problems using traditional methods is intractable even for small geographical areas and existing heuristics often provide sub-optimal results.
__label__fairness We establish a new model-agnostic optimization framework for out-of-distribution generalization via multicalibration, a criterion that ensures a predictor is calibrated across a family of overlapping groups.
__label__generative_models Furthermore, a simple but efficient attention layer, named row-wise attention, is used to enforce epipolar priors in the multiview diffusion, facilitating efficient cross-view information fusion.
"__label__diffusion_based_models Inspired by the success of diffusion models in tabular data modeling, we introduce 
 \textbf{C}luster \textbf{La}tent \textbf{Va}riable guided \textbf{D}enoising \textbf{D}iffusion \textbf{P}robabilistic \textbf{M}odels (ClavaDDPM)."
__label__machine_vision Through extensive experimentation, we demonstrate that using these diffusion features in a graph based segmentation algorithm, significantly outperforms previous state-of-the-art methods on zero-shot segmentation.
__label__graph_neural_networks We further analyze how N2C-Attn combines bi-level feature maps of queries and keys, demonstrating its capability to merge dual-granularity information.
__label__graph_neural_networks We experimentally evaluate our approach in 13 temporal graphs from biological and social systems and show that it considerably improves the prediction of betweenness and closeness centrality compared to (i) a static Graph Convolutional Neural Network, (ii) an efficient sampling-based approximation technique for temporal betweenness, and (iii) two state-of-the-art time-aware graph learning techniques for dynamic graphs.
__label__bandits This paper addresses this problem and establishes that in a graph with $N$ nodes, maximum in-degree $d$ and maximum causal path length $L$, after $T$ interaction rounds the regret upper bound scales as $\tilde{\mathcal{O}}((cd)^{L-\frac{1}{2}}\sqrt{T} + d + RN)$ where $c>1$ is a constant and  $R$ is a measure of intervention power.
__label__natural_language_processing However, existing works often overlooks the geo-diversity of cultural and legal standards across the world.
__label__safety_in_machine_learning We reveal that widely used quantization methods can be exploited to produce a harmful quantized LLM, even though the full-precision counterpart appears benign, potentially tricking users into deploying the malicious quantized model.
__label__machine_vision In the reverse process, starting from randomly sampled Gaussian noise, the scene flow vector field prediction is recovered by conditioning on a source and a target point cloud.
__label__generative_models By aligning the DMI field during retargeting, MeshRet not only preserves motion semantics but also prevents self-interpenetration and ensures contact preservation.
__label__machine_vision The experiments on different cross-view adaptation benchmarks have shown the effectiveness of our approach in cross-view modeling, demonstrating that we achieve State-of-the-Art (SOTA) performance compared to prior unsupervised domain adaptation and open-vocabulary semantic segmentation methods.
__label__machine_learning_for_other_sciences_and_fields Extensive experiments on four different circuit benchmarks demonstrate that our method can precisely generate circuits with up to 1200 nodes.
__label__deep_learning_architectures Compared to conducting matrix multiplication, retrieving data blocks from memory is a much cheaper operation which requires little computations.
__label__reinforcement_learning Finally, we propose OTaCoS, an efficient model-based algorithm for our setting.
__label__machine_vision Previous methods that uniformly mitigate these degradations have proven inadequate in simultaneously restoring daytime domain information and preserving underlying semantics.
__label__neuroscience_and_cognitive_science During inference, we evaluate the generalization ability of all AI models and humans on 62,656 video stimuli spanning 24 BMP conditions using point-light displays in neuroscience.
__label__optimization_for_deep_networks To address such issues, we propose PACE, marrying generalization of PArameter-efficient fine-tuning with Consistency rEgularization.
__label__diffusion_based_models The prompt embedding is optimized to minimize artifacts in the generated image, while the watermark is optimized to achieve maximum strength.
__label__machine_vision Through the novel disentanglement of 3DGS, we represent the discrete and unstructured 3DGS with continuous Gaussian Splatting functions, where we then train a latent diffusion model with the target of generating these Gaussian Splatting functions both unconditionally and conditionally.
__label__generative_models Our insight is that satellite's earth observation photos are not just images but indeed multi-modal data with a nature of Text-to-Image pairing since they are collected with rich sensor data (e.g.
__label__machine_vision Specifically, we regularize the latent space such that maps between encodings preserve a learned inner product and commute with a learned functional operator, in the same manner as rigid-body transformations commute with the Laplacian.
__label__graph_neural_networks While significant progress has been made in SGNNs research, two issues (i.e., graph sparsity and unbalanced triangles) persist in the current SGNN models.
__label__optimization We develop a new learning-augmented algorithm, named PFSUM, that incorporates both history and short-term future to improve online decision making.
__label__reinforcement_learning However, the reward corresponding to an optimal policy is not unique, making it unclear if an IRL-learned reward is transferable to new transition laws in the sense that its optimal policy aligns with the optimal policy corresponding to the expert's true reward.
__label__neuroscience_and_cognitive_science The product of the method, called goal-reducer, distills high-quality subgoals from a replay buffer, all without the need for prior global environmental knowledge.
__label__machine_vision For example, on the VTAB-1K benchmark, CVPT outperforms VPT over 4\% in average accuracy, rivaling the advanced adapter methods in performance and efficiency.
__label__machine_learning_for_physical_sciences To address these issues, we propose *Codomain Attention Neural Operator* (CoDA-NO), which tokenizes functions along the codomain or channel space, enabling self-supervised learning or pretraining of multiple PDE systems.
__label__machine_vision The domain adaptation method effectively mitigates the negative impact of domain gaps on the performance of super-resolution (SR) networks through the guidance of numerous target domain low-resolution (LR) images.
__label__natural_language_processing Besides, we cannot guarantee the selected ratio is optimal for the specific domain.
__label__evaluation The results show that LLMs are great at generating simple DS programs, particularly those that follow common patterns seen in training data.
__label__machine_vision The classical framework of analysis by synthesis casts this inference as a joint optimization seeking to explain the observed pixels, and recent instantiations learn expressive 3D representations (e.g., Neural Fields) with gradient-descent-based pose refinement of initial pose estimates.
__label__machine_vision Extensive experiments in both static and dynamic scenes validate the effectiveness of our approach.
__label__causal_inference For computational efficiency, the proposed two-step algorithm relies on local information scores limited to the close surrounding variables of each node (step 1)  and edge (step 2).
__label__machine_learning_for_other_sciences_and_fields Existing graph learning-based cognitive diagnosis (CD) methods have made relatively good results, but their student, exercise, and concept representations are learned and exchanged in an implicit unified graph, which makes the interaction-agnostic exercise and concept representations be learned poorly, failing to provide high robustness against noise in students' interactions.
__label__generative_models Moreover, we propose a mean-field finite-size scaling hypothesis, confirming that the initial phase transition, reminiscent of the paramagnetic-to-ferromagnetic phase transition in mean-field ferromagnetism models, is governed by mean-field critical exponents.
__label__learning_theory (2020)  to include the disconnected case.
__label__causal_inference However, we show that there still remains a gap in that there exist causal directions that are identifiable while the algorithm fails to identify them.
__label__learning_theory Interestingly, we show a stark contrast in learnability where non-trivial concept classes are unlearnable.
__label__safety_in_machine_learning Our work is the first to formalize and investigate secret collusion among frontier foundation models, identifying it as a critical area in AI Safety and outlining a comprehensive research agenda to mitigate future risks of collusion between generative AI systems.
__label__reinforcement_learning Our learning strategy effectively incorporates future behavioral information into the representation space without introducing a significant number of additional parameters for modeling dynamics.
__label__probabilistic_methods Specifically, due to the fundamental stochasticity from the offline data-collection policies and the environment dynamics, highly non-trivial conditional/constrained generation is required to elicit rewarding actions.
__label__robotics Therefore, we present GarmentLab, a content-rich benchmark and realistic simulation designed for deformable object and garment manipulation.
__label__machine_vision Our numerical and visual comparisons with the stat-of-the-art methods show our superiority over these methods in surface reconstruction and point cloud denoising on widely used shape and scene benchmarks.
__label__machine_learning_for_healthcare *Accurate* and *interpretable* modeling of these systems is essential for understanding complex temporal processes, optimizing interventions, and minimizing adverse effects.
__label__graph_neural_networks At the same time, GNNs with CNA require substantially fewer learnable parameters than competing architectures.
__label__causal_inference Inspired by the two-way fixed effects regression model widely used in the panel data literature, we propose a two-way unmeasured confounding assumption to model the system dynamics in causal reinforcement learning and develop a two-way deconfounder algorithm that devises a neural tensor network to simultaneously learn both the unmeasured confounders and the system dynamics, based on which a model-based estimator can be constructed for consistent policy value estimation.
__label__machine_vision To address this, MADM designs diffusion-based pseudo-label generation which adds latent noise to stabilize pseudo-labels and enhance label accuracy.
__label__reinforcement_learning Our PO algorithm achieves rate-optimal regret with improved dependence on the other parameters of the problem (horizon and function approximation dimension) in two fundamental settings: adversarial losses with full-information feedback and stochastic losses with bandit feedback.
__label__generative_models In contrast, in the field of artificial intelligence, controllable generation methods like ControlNet heavily rely on annotated training datasets such as depth maps, semantic segmentation maps, and poses, which limits the method’s scalability.
__label__machine_vision To conduct a comprehensive evaluation, we further collect data from Amazon Reviews to build a new dataset for cross-modal retrieval in the fashion domain.
__label__reinforcement_learning To address the dormant neuron issue, we propose ReBorn, a simple but effective method that transfers the weights from over-active neurons to dormant neurons.
__label__diffusion_based_models This approach enables zero-shot conditional generation for universal control formats, which appears to offer a free lunch in diffusion guidance.
__label__online_learning In this work, we introduce Localized Adaptive Risk Control (L-ARC), an online calibration scheme that targets statistical localized risk guarantees ranging from conditional risk to marginal risk, while preserving the worst-case performance of ARC.
__label__machine_vision However, current methods depend on transformation labels and thus struggle with interdependency and complex transformations.
__label__generative_models Unlike existing vectorization focusing on the meaning of Chinese characters, CGE represents the shape and stroke features, providing glyph guidance for GAN to generate writing signals.
__label__safety_in_machine_learning To reduce the sensitivity and improve the corruption robustness, we propose Frequency Adversarial Training (FAT) that adopts frequency-domain adversarial examples as data augmentation to train robust point cloud recognition models against corruptions.
__label__diffusion_based_models Our approach ensures that the sampled graphs remain within the domain of graphs that satisfy the specified property throughout the entire trajectory in both the forward and reverse processes.
__label__learning_theory Yet, computationally efficient policy optimization methods that use (only) occupancy functions are virtually non-existent.
__label__diffusion_based_models Video editing is an emerging task, in which most current methods adopt the pre-trained text-to-image (T2I) diffusion model to edit the source video in a zero-shot manner.
__label__machine_vision This paper explores the problem of class-generalizable anomaly detection, where the objective is to train one unified AD model that can generalize to detect anomalies in diverse classes from different domains without any retraining or fine-tuning on the target data.
__label__natural_language_processing We finetune the pretrained SLT models on 5 downstream open-domain SLT benchmarks covering 5 sign languages.
__label__learning_theory Taken together, these results rigorously show the intrinsic dimension of data to be a crucial quantity affecting transformer scaling laws in both theory and practice.
__label__natural_language_processing Experiments on benchmark datasets and open source LLMs display the method effectiveness.
__label__diffusion_based_models We present CAT3D, a method for creating anything in 3D by simulating this real-world capture process with a multi-view diffusion model.
__label__natural_language_processing SpaceByte consists of a byte-level Transformer model, but with extra larger transformer blocks inserted in the middle of the layers.
__label__diffusion_based_models We provide convergence analyses by first constructing a general framework, i.e.
__label__generative_models While in this paper, we investigate the capability of GenAI for text-to-model generation, to see whether GenAI can comprehend hyper-level knowledge embedded within AI itself parameters.
__label__privacy The code is publicly available at \href{https://github.com/UCF-Lou-Lab-PET/Private-Data-Prune}.
__label__deep_learning_architectures These results show that transformers excel at many graph reasoning tasks, even outperforming specialized graph neural networks.
__label__causal_inference Our setting is more general than that of prior research—we allow all variables, including both observed and latent ones, to be flexibly related, and we consider the coefficients of all edges, whereas most existing works focus only on the edges between observed variables.
__label__machine_learning_for_physical_sciences This study not only advances the field of computational quantum chemistry but also highlights the important role of discretized evolution in variational quantum algorithms, offering a scalable and robust framework for future quantum research.
"__label__learning_theory Recent work [Cui et al., 2023]
established that linear regression provides Bayes-optimal test error to learn such
a function when the number of available samples is only linear in the dimension."
__label__machine_vision Existing single-view 3D reconstruction methods often overlook this underlying composition, presuming rigidity or neglecting external forces.
__label__deep_learning_architectures We argue existing Fourier-based methods do not involve basis functions thus fail to interpret frequency coefficients precisely and consider the time-frequency relationship sufficiently, leading to inconsistent starting cycles and inconsistent series length issues.
__label__machine_vision Although learnable approaches like Q-Former and Perceiver Resampler have been developed to reduce the vision token burden, they overlook the context causally modeled by LLMs (i.e., key-value cache), potentially leading to missed visual cues when addressing user queries.
__label__other We evaluate this framework by applying it to Personalized Pagerank computation for ranking tasks.
__label__infrastructure Utilizing the designed CMSR (Controllable Model Searching and Reproduction) algorithm, pFedClub generates a range of personalized candidate models for each client.
__label__generative_models To bridge the gap between assumption and reality, Era3D first proposes a diffusion-based camera prediction module to estimate the focal length and elevation of the input image, which allows our method to generate images without shape distortions.
__label__machine_vision Stereo matching algorithms that leverage end-to-end convolutional neural networks have recently demonstrated notable advancements in performance.
__label__neuroscience_and_cognitive_science Although prior works have employed various surrogate gradient training methods that use an alternative function to replace the firing process during back-propagation, these approaches ignore an intrinsic problem: gradient vanishing.
__label__graph_neural_networks Experimental results demonstrate that each component contributes to overall performances that are superior to baselines, even when baselines additionally exploit 30\% of dangling entities labeled for training.
__label__other We demonstrate the effectiveness of RLGSSL through extensive experiments on several benchmark datasets and show that our approach achieves consistent superior performance compared to state-of-the-art SSL methods.
__label__machine_vision In this work, we make significant strides towards closing this gap, by introducing a new learning technique, dubbed UDON (Universal Dynamic Online distillatioN).
__label__deep_learning_architectures Code and models are available at https://github.com/THU-MIG/yolov10.
__label__reinforcement_learning First, we propose the direction-aware attention model (DaAM) to in corporate directionality into the embedding process, facilitating more effective one-stage decision-making.
__label__machine_learning_for_other_sciences_and_fields Agda is a dependently-typed programming language and a proof assistant, pivotal in proof formalization and programming language theory.
__label__machine_vision In this work, we introduce a novel approach to model 3D facial textures under such unnatural illumination.
__label__machine_learning_for_social_sciences Both experimental results and theoretical analyses demonstrate the superiority of ELCRec from six perspectives.
__label__learning_theory Finally, we empirically demonstrate our algorithm's use on real-world and simulated graph transfer problems.
__label__optimization Unfortunately, existing efforts to expedite conformalization often carry strong assumptions and are developed specifically for certain models, or they only offer approximate solution sets.
__label__deep_learning_architectures We theoretically show that $\mathcal{NC}$ exists in DEQ under balanced conditions.
__label__generative_models We show that by using LLMs to produce the code for the search components we can solve the entire datasets with 100% accuracy with only a few calls to the LLM.
__label__other Our project page is available at [https://hthieu166.github.io/petta](https://hthieu166.github.io/petta).
__label__diffusion_based_models Moreover, given the same computational resources, a ReNO-optimized one-step model outperforms widely-used open-source models such as SDXL and PixArt-alpha, highlighting the efficiency and effectiveness of ReNO in enhancing T2I model performance at inference time.
__label__machine_vision This method leverages the increasing availability of large datasets.
__label__reinforcement_learning To address this, we propose CurrMask, a curriculum masking pretraining paradigm for sequential decision making.
__label__machine_learning_for_other_sciences_and_fields However, leveraging these languages requires developers to rewrite existing code using the specific DSL's API.
__label__machine_learning_for_social_sciences This allows simultaneous optimization of recommendation and clustering via mini-batch data.
__label__graph_neural_networks For example, malicious adversaries could perform data extraction attack to recover private training data, thereby threatening commercial secrets and collaborative trust.
__label__natural_language_processing \textsc{AutoPSV} begins by training a verification model on the correctness of final answers, enabling it to generate automatic process annotations.
__label__machine_learning_for_healthcare GraphMorph comprises two main components: a Graph Decoder and a Morph Module.
__label__natural_language_processing Experiments show AssistRAG significantly outperforms benchmarks, especially benefiting less advanced LLMs, by providing superior reasoning capabilities and accurate responses.
__label__bandits The main contribution of this paper is the introduction of a new modeling of the bid space.
__label__optimization This is completely different from previous analyses of Oja's algorithm and matrix products, which have been done when the $r_{\mathsf{eff}}$ is bounded.
__label__diffusion_based_models Code is available at: https://github.com/YangLing0818/RealCompo
__label__optimization As a by-product of our analysis, we derive finite-time bounds on higher moment $\mathbb{E}[||\theta_k-\theta^\ast||^{2p}]$ and present non-asymptotic geometric convergence rates for the iterates, along with a Central Limit Theorem.
__label__deep_learning_architectures The past neural network design has largely focused on feature \textit{representation space} dimension and its capacity scaling (e.g., width, depth), but overlooked the feature \textit{interaction space} scaling.
__label__learning_theory This limits the possibility and benefit of sequential prior updates, because the final bounds depend only on the size of the final batch.
__label__generative_models We further present a multi-part editing pipeline that enables us to synthesize different textures across various regions.
__label__optimization Extensive experiments conducted on various large scale benchmark datasets validate the effectiveness of our method.
__label__machine_learning_for_other_sciences_and_fields We propose to design dual-target drugs with diffusion models that are trained on single-target protein-ligand complex pairs.
__label__online_learning In class-incremental learning (class-IL), models must classify all previously seen classes at test time without task-IDs, leading to task confusion.
__label__machine_vision This design is based on the observation that local attention is essentially the same as cost-volume, both of which are computed by dot-production between a query and its surrounding features.
__label__optimization Our analysis reveals that the structure of the solution path is inherently piecewise smooth, and indicates that utilizing second-order information of difference equations suffices to approximate the entire solution spectrum arbitrarily.
__label__deep_learning_architectures Local-Global SIRENs are based on combining local and global feature extraction for signal encoding.
__label__optimization However, as the number of objectives increases, the cost of learning and surrogation, as well as the difficulty of maintaining solution diversity, increases rapidly.
__label__probabilistic_methods However, real-world data often exhibit significant error autocorrelation and cross-lag correlation due to factors such as missing covariates.
__label__optimization A serious concern with algorithms that use predictions is that  these predictions can be biased and, as a result, cause the algorithm to make decisions that are deemed unfair.
__label__deep_learning_architectures Yet, studies on time series forecasting have cast doubt on scaling behaviors of deep learning methods for time series forecasting: while more training data improves performance, more capable models do not always outperform less capable models, and longer input horizon may hurt performance for some models.
__label__learning_theory Previously established Universal Approximation Theorems for PQCs are either nonconstructive or assisted with parameterized classical data processing, making it hard to justify whether the expressive power comes from the classical or quantum parts.
__label__privacy Leveraging weak duality theory, we show that the optimality gap is upper bounded by $\mathcal{O}(\frac{\sqrt{m\ln(1/\delta)}}{\varepsilon})$, and constraint violation is no more than $\mathcal{O}(\frac{\sqrt{m\ln(1/\delta)}}{\varepsilon})$ per constraint.
__label__generative_models While our focus here is on chemical search, we also obtain excellent results on an AI supervised task for LLM alignment, showing that the method is scalable and general.
__label__generative_models However, training models for long video generation tasks require significant computational and data resources, posing a challenge to developing long video diffusion models.
__label__machine_vision Compared with previous methods, our FAM directly operates on discrete surface points without utilizing connectivity information, thus significantly reducing the strict requirements for mesh quality and even applicable to unstructured point cloud data.
__label__safety_in_machine_learning Extensive experiments demonstrate that PamaCF effectively defends against various types of poisoning attacks while significantly enhancing recommendation performance.
__label__fairness Building on insights from our analysis, we propose two ideas: (i) *Prompt Queuing* and (ii) *Attention Amplification* to address the quality issue.
__label__natural_language_processing Utilizing the emotion appraisal theory from psychology, we propose to evaluate the empathy ability of LLMs, i.e., how their feelings change when presented with specific situations.
__label__machine_vision Codes will be released.
__label__machine_vision In addition, we propose an online multi-domain distillation learning strategy to improve the adaption process.
__label__machine_vision Extensive experiments across 6 IR tasks confirm the proposed SemanIR's state-of-the-art performance, quantitatively and qualitatively showcasing advancements.
__label__learning_theory Remarkably, the solutions learned for each individual task resemble those obtained by solving a kernel method, revealing a novel connection between neural networks and kernel methods.
__label__machine_learning_for_other_sciences_and_fields In recent years, a large number of algorithms for label integration and noise correction have been proposed to infer the unknown true labels of instances in crowdsourcing.
__label__machine_learning_for_social_sciences Previous AI agents have demonstrated their ability to handle multi-step games and large action spaces in multi-agent tasks.
__label__probabilistic_methods In this paper, we investigate this issue in the context of the increasingly popular linearized Laplace approximation.
__label__neuroscience_and_cognitive_science Internal representations from LLMs achieve state-of-the-art brain scores, leading to speculation that they share computational principles with human language processing.
__label__causal_inference Methodologically, we propose a novel likelihood-based parameter estimation method that addresses the variance indeterminacy of latent variables in a specific way and can asymptotically recover the underlying parameters up to trivial indeterminacy.
__label__machine_vision However, the sparse and asynchronous nature of event data makes frame-based visual processing methods inappropriate.
__label__machine_learning_for_healthcare It is common to assess a model's merit for scientific discovery, and thus novel insights, by how well it aligns with already available domain knowledge - a dimension that is currently largely disregarded in the comparison of neural network models.
__label__reinforcement_learning We present Multi-Agent Divergence Policy Optimization (MADPO), equipped with mutual policy divergence maximization framework.
__label__diffusion_based_models Diffusion models have demonstrated remarkable success in various domains, including molecular generation.
__label__learning_theory This rate reveals the structural properties of the Transformer and suggests the types of sequential relationships it is best suited for approximating.
__label__natural_language_processing In controlled-sentiment generation and summarization, we use tuned and untuned $\texttt{gpt2}$s to improve the alignment of large models without additional training.
__label__generative_models While Augmented and Virtual Reality technologies (AR/VR) have introduced tools for producing drawings with hand motions (air drawing), they typically require costly hardware and additional digital markers, thereby limiting their accessibility and portability.
__label__infrastructure We develop three main techniques to speed up attention on Hopper GPUs: exploiting asynchrony of the Tensor Cores and TMA to (1) overlap overall computation and data movement via warp-specialization and (2) interleave block-wise matmul and softmax operations, and (3) block quantization and incoherent processing that leverages hardware support for FP8 low-precision.
__label__machine_learning_for_social_sciences In these models, a central object of interest is the discrete origin-destination matrix which captures spatial interactions and agent trip counts between locations.
__label__generative_models Utilizing a diverse set of language skills---including rhetorical, literary, reasoning, theory of mind, and common sense---GPT was used to generate text samples that exhibit random subsets of $k$ skills.
__label__natural_language_processing However, this approach compromises speed and security, and fine-tuning risks the language model losing prior capabilities.
__label__other Theoretical and numerical results demonstrate the effectiveness of the proposed method in controlling both individual and interactive constraints.
__label__natural_language_processing Specifically, we identify three unique patterns in long-context attention matrices-the A-shape, Vertical-Slash, and Block-Sparse-that can be leveraged for efficient sparse computation on GPUs.
__label__machine_vision NeRFs can adapt to such conditions easily through per-image embedding vectors, but 3DGS struggles due to its explicit representation and lack of shared parameters.
__label__machine_vision To remedy this limitation, we propose a novel task called Compositional Incremental Learning (composition-IL), which enables the model to recognize a variety of state-object compositions in an incremental learning fashion.
__label__learning_theory for any pair of elements $u$ and $v$ in the set.
__label__learning_theory We also give a simple data structure for our lower bound instance with asymptotically matching upper bounds.
__label__graph_neural_networks However, such architectures often fail to deliver solid predictive performance on real-world tasks, limiting their practical impact.
__label__learning_theory Specifically, for inadequately constrained distributions, the error can exponentially escalate as we progress through the gradual shifts.
__label__natural_language_processing While large language models (LLMs) process input contexts through a causal and sequential perspective, this approach can potentially limit their ability to handle intricate and complex inputs effectively.
__label__optimization_for_deep_networks Weight decay is a broadly used technique for training state-of-the-art deep networks from image classification to large language models.
__label__other It consists of the query adaption module that can be seamlessly integrated into CLIP and generate the referential query to provide the prior context for decoder, along with a task-specific decoder.
__label__other Concretely, at the group identification stage, we first estimate the adaptive density of each user point, where areas with higher densities are more likely to be recognized as group centers.
__label__infrastructure However, although these techniques have been widely adopted in single-task scenarios, research is scarce in multi-task scenarios.
__label__machine_vision The code and models will be available on our github page.
__label__machine_learning_for_other_sciences_and_fields Additionally, we introduce an instance-wise dynamic weighting mechanism that dynamically integrates the global and local adapters for each test instance during inference, facilitating effective test-time personalization.
__label__bandits This is achieved through a novel procedure that we design for checking whether the best arm is eliminated, which is of independent interest.
__label__safety_in_machine_learning Recent jailbreaking methods generate adversarial prompts capable of bypassing safety filters and producing unsafe content, exposing vulnerabilities in influential commercial models.
__label__machine_learning_for_other_sciences_and_fields The code is available at https://github.com/CIAM-Group/NCO_code/tree/main/single_objective/UDC-Large-scale-CO-master
__label__natural_language_processing Specifically, we design a novel active learning policy to significantly reduce the annotation space by prioritizing the most valuable entities based on the entire inter-KG and intra-KG structure.
__label__optimization_for_deep_networks * We analyze different metrics for the update size including the $\ell_2$-norm, resulting directional change, and impact on the representations of the network, providing a new perspective on warmup.
__label__natural_language_processing In this framework, the final inference speed is decided by the decoding speed of the draft model and the acceptance rate of the draft provided by the draft model.
__label__evaluation However, when faced with prompts in different sizes of solution spaces, LVLMs fail to always give consistent answers regarding the same knowledge point.
"__label__machine_vision The enhanced views are then
used to fine-tune the initial 3DGS model, significantly improving its rendering
performance."
__label__deep_learning_architectures Despite satisfactory results on ``easy'' cases of single image reflection separation, prior dual-stream methods still suffer from considerable performance degradation when facing complex ones, i.e, the transmission layer is densely entangled with the reflection having a wide distribution of spatial intensity.
__label__natural_language_processing While VoT works surprisingly well on LLMs, the ability to generate mental images to facilitate spatial reasoning resembles the mind's eye process, suggesting its potential viability in MLLMs.
__label__generative_models Furthermore, it can output the likelihood of any input in a single forward pass, achieving state-of-the-art negative log likelihood (NLL) among methods with this property.
__label__neuroscience_and_cognitive_science Our first result generalizes the analytical Gaussian PID result to the much larger class of stable distributions.
__label__optimization In this paper, we present a comprehensive study of the performance of \avglink \, in metric spaces, regarding several natural criteria that capture separability and cohesion, and are more interpretable than Dasgupta's cost function and its variants.
__label__machine_vision Thanks to the decoupling from spatial resolution, NerPE can output the predicted heatmaps at arbitrary resolution during inference without retraining, which easily achieves sub-pixel localization precision.
__label__interpretability_and_explainability To address this we explore **transcoders**, which seek to faithfully approximate a densely activating MLP layer with a wider, sparsely-activating MLP layer.
__label__probabilistic_methods VISA extends importance-weighted forward-KL variational inference by employing a sequence of sample-average approximations, which are considered valid inside a trust region.
__label__deep_learning_architectures In this paper, we reveal that the powerful Mamba model shares surprising similarities with linear attention Transformer, which typically underperform conventional Transformer in practice.
__label__diffusion_based_models It can be used to augment pre-trained diffusion-based text-to-image models in a zero-shot manner.
__label__reinforcement_learning Three sampling techniques are presented for comparative investigations.
__label__diffusion_based_models Moreover, unlike SDXL, our KOALA models can generate 1024px high-resolution images on consumer-grade GPUs with 8GB of VRAMs (3060Ti).
__label__generative_models Addressing this challenge, our study unveils GuardT2I a novel moderation framework that adopts a generative approach to enhance Text-to-Image models’ robustness against adversarial prompts.
__label__deep_learning_architectures The aforementioned two fundamental differences significantly contribute to the disparities between these two attention paradigms, which is demonstrated by our substantial empirical validation in the paper.
__label__machine_vision Procedural activities are sequences of key-steps aimed at achieving specific goals.
__label__reinforcement_learning Training offline RL models using visual inputs poses two significant challenges, *i.e.
__label__deep_learning_architectures They can also model bounded hierarchical structure with optimal memory even without simulating a stack.
__label__natural_language_processing Specifically, LLM-DA harnesses the capabilities of LLMs to analyze historical data and extract temporal logical rules.
__label__algorithmic_game_theory The problem exhibits computational intractability for CC and PAV, and polynomial solvability for AV and SAV.
__label__diffusion_based_models We investigate the training methodologies and the decoder architecture of LiteVAE and propose several enhancements that improve the training dynamics and reconstruction quality.
__label__machine_learning_for_healthcare The results show that the AID model can robustly generate sequentially coherent image sequences.
__label__natural_language_processing Recent work has found that the choice of LLM is consequential, and different LLMs may be good for different input samples.
__label__online_learning We believe our approach for managing non-stationarity with experts can be of interest to the RL community.
__label__deep_learning_architectures Point-based methods have made significant progress, but improving their scalability in large-scale 3D scenes is still a challenging problem.
__label__graph_neural_networks Temporal link prediction, aiming at predicting future interactions among entities based on historical interactions, is crucial for a series of real-world applications.
__label__optimization In this paper, we study Adam in non-convex smooth scenarios with potential unbounded gradients and affine variance noise.
__label__optimization To solve this problem, convergent algorithms are developed with both single-loop and stochastic variants.
__label__privacy This approach first employs a generative adversarial network to learn a fixed distributional prior, which is then used to guide the inversion process during the attack.
__label__diffusion_based_models As the training object is symmetric, samples belonging to the two distributions, images and semantic masks, can be effortlessly transferred reversibly.
__label__machine_vision To handle geometrical variations between prompt and query images, we propose a soft feature alignment module that bridges paired-image change perception and single-image semantic segmentation.
__label__diffusion_based_models Recent advancements in diffusion models and diffusion bridges primarily focus on finite-dimensional spaces, yet many real-world problems necessitate operations in infinite-dimensional function spaces for more natural and interpretable formulations.
__label__reinforcement_learning Here, $d$ is the feature dimension and $\epsilon > 0$ is a given  error.
__label__safety_in_machine_learning However, we find empirically that the outlier samples often present a distribution shift compared to the true OOD samples, especially in Long-Tailed Recognition (LTR) scenarios, where ID classes are heavily imbalanced, $\textit{i.e.
__label__neuroscience_and_cognitive_science Finally, we show that ORGaNICs can be trained by backpropagation through time without gradient clipping/scaling, thanks to its intrinsic stability property and adaptive time constants, which address the problems of exploding, vanishing, and oscillating gradients.
__label__deep_learning_architectures In this paper we leverage recent advances in representation learning and differentiable information estimators to put forward a data-driven method to find emergent variables.
__label__safety_in_machine_learning Ideally, the obtained unlearnability can prevent algorithms from training usable models.
__label__reinforcement_learning It is especially crucial yet challenging to Deep RL, from which the remedies to notorious issues like sample inefficiency and learning instability could be obtained.
__label__natural_language_processing Our work marks a step forward in effectively and efficiently aligning models to diverse and intricate human preferences in a controllable and Pareto-optimal manner.
"__label__neuroscience_and_cognitive_science This involves constructing a self-supervised task based on 
  EEG representations that possess high SNR and rich semantic information, 
  rather than on raw signals."
__label__evaluation We provide theoretical support for our methods and validate their effectiveness through extensive simulated, semi-simulated, and real experiments involving images, gene expressions, protein expressions, and medical sensors, demonstrating their ability to identify the unique information in the foreground group.
__label__bandits Then, we leverage this convex reformulation of the lower bound to design the Track and Stop with Preferences (TSwP) algorithm that identifies the most preferred policy.
__label__safety_in_machine_learning However, fine-tuning all parameters of LLMs requires significant hardware resources, which can be impractical for typical users.
__label__generative_models This aims to eliminate discrepancies arising from multiple diffusion models, allowing for generating spatial-temporally consistent 4D content.
__label__machine_vision Experiments on two benchmark MHIF datasets verify the state-of-the-art (SOTA) performance of the proposed method, both visually and quantitatively.
__label__generative_models Code and pre-trained models will be released at https://github.com/LeapLabTHU/ENAT.
__label__safety_in_machine_learning TAP utilizes an attacker LLM to iteratively refine candidate (attack) prompts until one of the refined prompts jailbreaks the target.
__label__optimization Optimal Transport (OT, also known as the Wasserstein distance) is a popular metric for comparing probability distributions and has been successfully used in many machine-learning applications.
__label__optimization_for_deep_networks This paper rigorously analyzes the convergence properties of gradient flow in training Transformers with weight decay regularization.
__label__machine_vision To address this issue, we propose a novel framework enabling Gaussian Splatting to perform discontinuity-aware image rendering.
__label__safety_in_machine_learning The core idea of Vaccine is to produce invariant hidden embeddings by progressively adding crafted perturbation to them in the alignment phase.
__label__online_learning We explore different ways to learn such compositions.
__label__interpretability_and_explainability Furthermore, our theoretical analysis of gradient-based learning dynamics reveals that LLMs can learn both the adjacency  and a limited form of the reachability matrices.
__label__other This innovative learning paradigm offers dual benefits: (1) reduced forgetting of old knowledge by mitigating drastic changes in model predictions under small parameter updates; and (2) enhanced new task performance by preventing overfitting to new tasks.
__label__machine_vision This paper presents \textit{Wild-GS}, an innovative adaptation of 3DGS optimized for unconstrained photo collections while preserving its efficiency benefits.
__label__safety_in_machine_learning Adversarial robustness and privacy of deep learning (DL) models are two widely studied topics in AI security.
__label__machine_vision Query Mask Predictor module and Query-Sentence Alignment module are introduced for coarse-grained alignment between masks and query.
__label__optimization_for_deep_networks Compared to existing structured matrices, the BLAST matrix offers substantial flexibility, as it can represent various types of structures that are either learned from data or computed from pre-existing weight matrices.
__label__safety_in_machine_learning Current research in adversarial robustness of LLMs focuses on \textit{discrete} input manipulations in the natural language space, which can be directly transferred to \textit{closed-source} models.
__label__causal_inference Meanwhile, COAT also adopts CDs to find causal relations among the identified variables as well as to provide feedback to LLMs to iteratively refine the proposed factors.
__label__neuroscience_and_cognitive_science This inference is only valid if the subset of neural activity predicted by LLMs reflects core elements of language processing.
__label__machine_vision The encoder predicts keypoints and the decoder utilizes the generated keypoints to reconstruct the objects.
__label__machine_learning_for_physical_sciences We evaluate our approach on the ISPD 2005 and ICCAD 2015 benchmark, comparing the global half-perimeter wirelength and regularity of our proposed method against several competitive approaches.
__label__robotics The previous study typically assumes that each demand instruction requires only one object to be fulfilled and does not consider individual preferences.
"__label__other Experimentally, our DeformableTST achieves
the consistent state-of-the-art performance in a broader range of time series tasks,
especially achieving promising performance in forecasting tasks unsuitable for
patching, therefore successfully reducing the reliance on patching and broadening
the applicability of Transformer-based models."
__label__fairness In this paper, we take an important step to address this issue in numerous statistical and causal notions of fairness.
__label__reinforcement_learning We study the sample complexity of learning an $\varepsilon$-optimal policy in an average-reward Markov decision process (MDP) under a generative model.
__label__generative_models Additionally, we review the literature on the Generalized KS test and discuss the connections between KSGAN and existing adversarial generative models.
__label__diffusion_based_models Our proposed method demonstrates significant performance in conditional 3D molecular generation and offers a promising approach towards inverse molecular design, potentially facilitating advancements in drug discovery, materials science, and other related fields.
__label__machine_learning_for_healthcare Then, we present our implementation of FCR, and empirically demonstrate that FCR outperforms state-of-the-art baselines in various tasks across four single-cell datasets.
__label__machine_vision To achieve this, existing methods use the agent model to extract information from the target dataset and embed it into the distilled dataset.
__label__safety_in_machine_learning Leveraging this insight, we propose a novel white-box jailbreak method that surgically disables a model's ability to refuse, with minimal effect on other capabilities.
__label__evaluation It bridges (1) comprehensive and well-distributed real-world user queries and (2) efficient and fairly-graded ground-truth-based benchmarks, by matching queries mined from the web with similar queries from existing benchmarks.
__label__reinforcement_learning We propose a stochastic Hyper Policy Gradient Descent (HPGD) algorithm to solve CB-RL, and demonstrate its convergence.
__label__probabilistic_methods To address this limitation, we propose *generative adversarial model-based optimization* using **adaptive source critic regularization (aSCR)**—a task- and optimizer- agnostic framework for constraining the optimization trajectory to regions of the design space where the surrogate function is reliable.
__label__fairness Our findings represent a dual contribution: a significant technical advancement in understanding the relationship between AUROC and AUPRC and a stark warning about unchecked assumptions in the ML community.
__label__natural_language_processing n-gram analysis) struggle to produce.
__label__machine_vision In addition, a novel counting loss is proposed, that directly optimizes the detection task and avoids the issues of the standard surrogate loss.
__label__diffusion_based_models However, the potential of adopting real-world datasets, which can produce significantly more realistic 3D scenes, remains largely unexplored.
__label__natural_language_processing The methodology presented is domain-agnostic,  even though this article applies it to math problems.
__label__generative_models The poor performance of transformers on arithmetic tasks seems to stem in large part from their inability to keep track of the exact position of each digit inside of a large span of digits.
__label__machine_vision However, plane-based methods rely on the inappropriate low-rank assumption and excessively decompose the space-time 4D encoding, resulting in overmuch feature overlap and unsatisfactory rendering quality.
__label__online_learning Despite recent solutions to classical CURL, none address non-stationary MDPs.
__label__neuroscience_and_cognitive_science While previous studies typically curtail variability to allow for high task performance in neural networks, our approach takes the reversed perspective.
__label__diffusion_based_models Comprehensive experiments demonstrate that our framework effectively mitigates association-engendered stereotypes.
__label__generative_models Extensive experiments demonstrate that our method can generate high-fidelity videos from text with flexible control over each element.
__label__machine_learning_for_healthcare End-to-end feature learning also facilitates interpretability of features, and out-of-the-box promptability using additional label-fidelity terms at inference.
__label__online_learning We call these classifiers the *neural tangent experts* and show they output valid probability distributions over the labels.
__label__other These programs can be stored and applied locally, re-used and extended, and cost orders of magnitude less.
__label__learning_theory The evidence lower bound is formulated as the cross-entropy between the student and teacher models, along with the Kullback-Leibler (KL) divergence of the prior mixture.
__label__reinforcement_learning Additionally, methods that utilize the bisimulation metric for evaluating state discrepancies face a theory-practice gap due to improper approximations in metric learning, particularly struggling with *hard exploration* tasks.
__label__generative_models Direct alignment from preferences (DAP) methods, such as DPO, have recently emerged as efficient alternatives to reinforcement learning from human feedback (RLHF), that do not require a separate reward model.
__label__optimization_for_deep_networks Modern optimizers such as AdamW, equipped with momentum and adaptive learning rate, are designed to escape local minima and explore the vast parameter space.
__label__causal_inference In the theoretical investigations, we demonstrate that our design has a higher probability of correctly identifying the best set of subgroups compared to conventional designs.
__label__optimization_for_deep_networks Indeed, we show a lower bound on the sharpness of minimizers, which grows linearly with depth.
__label__natural_language_processing Notably, we achieve substantial improvements across five datasets in mathematics and commonsense reasoning.
__label__safety_in_machine_learning Despite only having negative samples, our ablation study shows that unlearning can still achieve better alignment performance than RLHF with just 2% of its computational time.
__label__optimization_for_deep_networks Our findings reveal that the dynamics of standard SAM effectively reduce to applying SAM solely in the last layer in wide neural networks, even with optimal hyperparameters.
__label__machine_learning_for_other_sciences_and_fields While inorganic retrosynthesis planning is essential in the field of chemical science, the application of machine learning in this area has been notably less explored compared to organic retrosynthesis planning.
__label__machine_vision Current GFSS methods rely on several techniques such as using combinations of customized modules, carefully designed loss functions, meta-learning, and transductive learning.
__label__neuroscience_and_cognitive_science We further demonstrate that our proposed method systematically increases models' ability to predict responses in macaque inferior temporal cortex.
__label__interpretability_and_explainability Many of these interactions between features represented by singular vectors are interpretable and semantic, such as attention between relevant objects, between parts of an object, or between the foreground and background.
__label__neuroscience_and_cognitive_science We also found that the effective dimensionality of weights decreases in a network pretrained with random noise.
__label__reinforcement_learning However, it suffers from overestimating Q-value functions on out-of-distribution (OOD) data points due to the offline dataset limitation.
__label__algorithmic_game_theory Specifically, the players can enforce  the equitable utility profile  of $(1/2, 1/2)$  in the limit  on every trajectory of play, by keeping the other player's utility to approximately $1/2$ on average while guaranteeing they themselves get at least approximately $1/2$ on average.
__label__causal_inference We formulate conditions to facilitate the identification of the proposed causal model, which reveals when learning such concepts from unsupervised data is possible.
__label__reinforcement_learning For each of these empirical MDPs, it learns an estimated Q-function denoted as $\widehat{Q}$.
__label__generative_models We demonstrate both quantitatively and qualitatively that our approach is capable of generating new and interesting games, including in regions of the potential rules space not covered by existing games in the Ludii dataset.
__label__bandits In terms of the leading dependence on $\varepsilon$, this improves upon existing bounds for the problem, that are of the form $O(K/\varepsilon^2)$.
__label__machine_learning_for_other_sciences_and_fields To this end, we present a formulation of synthesis planning with starting material constraints.
__label__other We also give results for the more general scenario when $t$ negatives are allowed.
__label__probabilistic_methods Unlike the classical Kullback-Leibler (KL) divergence that involves density ratios, the KKL compares probability distributions through covariance operators (embeddings) in a reproducible kernel Hilbert space (RKHS), and compute the Kullback-Leibler quantum divergence.
__label__neuroscience_and_cognitive_science Thus, our results suggest that feedback control may guide credit assignment in biological recurrent neural networks, enabling both rapid and efficient learning in the brain.
__label__machine_vision Additionally, a gradient modulation is employed to adopt appropriate clues for capturing interaction regions across various egocentric scenarios.
__label__natural_language_processing We evaluate multiple LLMs on the created causal queries and find that causal reasoning is challenging even for activities trivial to humans.
__label__machine_vision Finally, the positive and overshooting gating, benefiting from graph-based granularity alignment, aggregates high-confident masks and filters the false-positive masks for final prediction, reducing the usage of additional hyperparameters and redundant mask generation.
__label__graph_neural_networks In this paper, we revisit these frameworks and reveal a common mechanism—representation scattering—that significantly enhances their performance.
__label__safety_in_machine_learning $\texttt{SGen}^{\texttt{Sup}}$, a direct modification of the selective prediction, is a supervised learning algorithm which exploits entailment-labeled data, annotated by humans.
__label__natural_language_processing The inner workings of how these models store knowledge have long been a subject of intense interest and investigation among researchers.
__label__human-AI_interaction Despite this, we show in a ubiquitous experimental domain, Overcooked-AI, that state-of-the-art techniques for human-machine teaming (HMT), which rely on imitation or reinforcement learning, are brittle and result in a machine agent that aims to decouple the machine and human’s actions to act independently rather than in a synergistic fashion.
__label__diffusion_based_models The training of score-based diffusion models (SDMs) is based on score matching.
__label__privacy We compare DPConvCNP with a DP Gaussian Process (GP) baseline with carefully tuned hyperparameters.
__label__other Thus this should be reflected in the definition of similarity between image responses for computer vision systems.
__label__probabilistic_methods Which one do researchers mean when they talk about ``planning as inference''?
__label__speech_and_audio Experimental results show that LLMs equipped with the LLM-Codec, named as UniAudio 1.5, prompted by only a few examples, can perform effectively in simple scenarios, validating our cross-modal in-context learning approach.
"__label__interpretability_and_explainability In particular, LLMs are  capable of ""lying"", knowingly outputting false statements."
__label__natural_language_processing The model also effectively aligns unseen objectives, marking the first step towards generalizable multi-objective preference alignment.
__label__privacy As a result, such approaches often struggle with hyper-parameter tuning and convergence.
__label__infrastructure By adopting a novel column-wise sparse representation of attention masks, FlashMask achieves a linear memory complexity of $\mathcal{O}(N)$ and computational complexity of $\mathcal{O}(N)\sim\mathcal{O}(N^2)$.
"__label__learning_theory Moreover, we identify combinatorial complexity
  measures that give rise to each case of our tetrachotomic characterization."
__label__learning_theory Our main result is a watermarking scheme which achieves both (a) and (b) when the alphabet size for the language model is allowed to grow as a polynomial in the security parameter.
__label__safety_in_machine_learning To identify major components from objects, REMA introduces a selective slot-based reconstruction module to dynamically map dense pixels into a sparse and discrete set of slot vectors in an unsupervised manner.
__label__learning_theory In this work, we focus on \emph{policy regret} -- a counterfactual notion that aims to compete with the return that would have been attained if the learner had followed the best fixed sequence of policy, in hindsight.
__label__optimization This provides a much-needed routine to create synthetic problems where the ground-truth OT map is known, by analogy to the Brenier theorem, which states that the gradient of any convex potential is always a valid Monge map for the $\ell_2^2$ cost; *(ii)* We propose a loss to *learn* the parameter $\theta$ of a parameterized regularizer $\tau_\theta$, and apply it in the case where $\tau_{A}({\bf z}):=\|A^\perp {\bf z}\|^2_2$.
__label__generative_models Atlas3D ensures the generation of self-supporting 3D models that adhere to physical laws of stability under gravity, contact, and friction.
__label__machine_vision Scene flow estimation is an essential ingredient for a variety of real-world applications, especially for autonomous agents, such as self-driving cars and robots.
__label__machine_vision Furthermore, it exceeds GroundingDINO-L by 11.0 AP for rare categories on the LVIS minival set.
__label__graph_neural_networks To address this, we propose TACO, a topology-aware graph coarsening and continual learning framework that stores information from previous tasks as a reduced graph.
__label__safety_in_machine_learning Our work provides a new perspective on understanding the effectiveness of backdoor safety tuning and highlights the importance of faithfully assessing the model's safety.
__label__graph_neural_networks In this paper, we propose a spectral GNN with triple filter ensemble (TFE-GNN), which extracts homophily and heterophily from graphs with different levels of homophily adaptively while utilizing the initial features.
__label__causal_inference Extensive experimental results on both large-scale synthetic and real-world problems verify the scalability and efficacy of $S^2GCSL$.
__label__machine_vision To our surprise, we find that the fine-tuned model neither forgets the relationship among the other classes nor degrades the features to recognize these classes.
__label__safety_in_machine_learning Theoretically, we provide a guarantee of FAT on its out-of-distribution generalization performance.
__label__other This process is guided by carefully constructed loss functions that measure the deviation of prediction intervals from the targeted properties.
__label__fairness To address this issue, in this paper, we propose a novel Fair Kernel K-Means (FKKM) framework.
__label__machine_learning_for_physical_sciences Furthermore, we introduce a lead time-aware training framework to promote the generalization of the model at different lead times.
__label__machine_learning_for_other_sciences_and_fields Domain-specific languages (DSLs) have become integral to various software workflows.
__label__machine_learning_for_other_sciences_and_fields An effective strategy for reducing such consumption is supply-voltage reduction, but if done too aggressively, it can lead to accuracy degradation.
__label__diffusion_based_models We are the first to show that it is possible to scale the output of a pre-trained diffusion model by a factor of 1000, opening the road to gigapixel image generation at no extra cost.
__label__graph_neural_networks Through our analyses, we further argue that, while the reconstruction errors for a given graph are effective features for GLAD, leveraging the multifaceted summaries of the reconstruction errors, beyond just mean, can further strengthen the features.
__label__algorithmic_game_theory More precisely, we demonstrate that when data quality indices are private, the coalition may undergo a phenomenon known as unravelling, wherein it shrinks up to the point that it becomes empty or solely comprised of the worst agent.
__label__probabilistic_methods By exploiting a previously established connection between the stochastic and probability flow ordinary differential equations (pfODEs) underlying DBMs, we derive a class of models, \emph{inflationary flows,} that uniquely and deterministically map high-dimensional data to a lower-dimensional Gaussian distribution via ODE integration.
__label__deep_learning_architectures APM possesses a unique property: it can learn using just this single representation and starts predicting semantically-aware features.
__label__generative_models Since the rapid development of Large Language Models (LLMs) has achieved remarkable success, understanding and rectifying their internal complex mechanisms has become an urgent issue.
__label__probabilistic_methods What is more, we show that our model performs on par with state-of-the-art models which are trained on the target datasets.
__label__machine_learning_for_other_sciences_and_fields Thus, we propose LLaMo: Large Language Model-based Molecular graph assistant, which is an end-to- end trained large molecular graph-language model.
__label__deep_learning_architectures However, DNNs still lag behind Gradient Boosting Decision Trees (GBDT) on tabular data, a format extensively utilized across various domains.
__label__machine_vision However, given a sparse set of observed views, the observations may not provide sufficient direct evidence to obtain complete and accurate 3D.
__label__machine_vision To handle this challenging light condition, we design a transformer-based framework for enhancing the perception of global context features.
__label__interpretability_and_explainability While there has been a large body of research attempting to circumvent tokenization for language modeling (Clark et al.
__label__machine_learning_for_healthcare Our implementation is available at https://github.com/IMOP-lab/U-Shaped-Connection.
__label__active_learning As one headline result, CoLoR-Filter data selected using a pair of 150m parameter auxiliary models can train a 1.2b parameter target model to match a 1.2b parameter model trained on 25b randomly selected tokens with 25x less data for Books and 11x less data for the downstream tasks.
__label__learning_theory We tackle it through a novel tensor decomposition perspective, proposing the Functional t-Singular Value Decomposition (Ft-SVD) theorem which extends the classical tensor SVD to infinite and continuous feature domains, providing a natural tool for representing and analyzing multi-output functions.
__label__reinforcement_learning We consider the challenge of mitigating the generation of negative or toxic content by the Large Language Models (LLMs) in response to certain prompts.
__label__optimization_for_deep_networks We then propose a denoising-style approximation that avoids the Jacobian computations altogether.
__label__natural_language_processing We evaluate SWE-agent on SWE-bench and HumanEvalFix, achieving state-of-the-art performance on both with a pass@1 rate of 12.5% and 87.7%, respectively, far exceeding the previous state-of-the-art achieved with non-interactive language models.
__label__machine_learning_for_other_sciences_and_fields (b) Yet, the trained proxy is susceptible to out-of-distribution issues.
__label__machine_learning_for_physical_sciences Garment animation is ubiquitous in various applications, such as virtual reality, gaming, and film producing.
__label__diffusion_based_models These capabilities are validated on applications featuring both convex and challenging, non-convex, constraints as well as ordinary differential equations, in domains spanning from synthesizing new materials with precise morphometric properties, generating physics-informed motion, optimizing paths in planning scenarios, and human motion synthesis.
__label__safety_in_machine_learning In addition, introducing attentional truncation can mitigate the overfitting over complex interactions between tokens in deep ViT layers to further improve the transferability.
__label__machine_vision This combination of strong face identity embeddings and our neural representation enables accurate reconstruction of not only facial features but also accessories and hair, and can be meshed to provide render-ready assets for gaming and telepresence.
__label__optimization_for_deep_networks Recently, in-context learning has been studied from a mathematical perspective with simplified linear self-attention without softmax unit.
__label__machine_vision We will release our code and hope our work will stimulate more research on fine-grained 4D understanding from videos.
__label__safety_in_machine_learning ALI-Agent operates through two principal stages: Emulation and Refinement.
__label__diffusion_based_models Mass transport problems arise in many areas of machine learning whereby one wants to compute a map transporting one distribution to another.
__label__machine_vision We propose a simple image and text-based multimodal solution TokenCLIPose that addresses this limitation.
__label__reinforcement_learning In this paper, we analyze this assumption and investigate how popular algorithms perform as the learned dynamics model is improved.
__label__probabilistic_methods Our method also includes an adaptive tempering mechanism that allows the discovery of multiple modes in the target distribution.
__label__natural_language_processing Despite recent advances in Large Language Models (LLMs) for Chinese medicine, their performance is sub-optimal in pediatric applications due to inadequate instruction data and vulnerable training procedures.
__label__natural_language_processing To mitigate such knowledge conflicts, we propose a novel framework, IRCAN (Identifying and Reweighting Context-Aware Neurons) to capitalize on neurons that are crucial in processing contextual cues.
__label__machine_learning_for_physical_sciences To address these issues, we propose a novel optimization framework, *Dual Cone Gradient Descent* (DCGD), which adjusts the direction of the updated gradient to ensure it falls within a dual cone region.
__label__bandits However, with a slightly different definition of the total variance and with the assumption that the reward follows a Gaussian distribution, one can achieve a regret of $\tilde{O}(\sqrt{A\Lambda} + d_{\text{elu}})$.
__label__machine_learning_for_physical_sciences This paper mitigates this concern by incorporating the Helmholtz-Hodge decomposition into a Gaussian process model, leading to a versatile framework that simultaneously learns the curl-free and divergence-free components of a dynamical system.
__label__deep_learning_architectures Multimodal contrastive learning (MCL) has recently demonstrated significant success across various tasks.
"__label__optimization A well-known theoretical tool in
maximal monotone operator theory, the Fitzpatrick function naturally leads to a
refined Fenchel-Young inequality, making Fitzpatrick losses tighter than
Fenchel-Young losses, while maintaining the same link
function for prediction."
__label__natural_language_processing We design a dynamic decision module for each transformer layer that decides whether a network unit should be executed or skipped.
__label__generative_models Diffusion models have emerged as a powerful tool for generating high-quality images from textual descriptions.
__label__learning_theory This modification translates into an interacting particle system that cannot be interpreted as a mean-field gradient flow.
__label__neuroscience_and_cognitive_science We first find that when using shuffled train-test splits, as done in previous studies with these datasets, a trivial feature that encodes temporal autocorrelation not only outperforms LLMs but also accounts for the majority of neural variance that LLMs explain.
__label__safety_in_machine_learning Unfortunately, inevitable misspecification of the distribution map can lead to a poor approximation of the true PO.
__label__deep_learning_architectures We demonstrate the generality and effectiveness of our approach with convolutional neural networks and transformers.
__label__natural_language_processing SparseLLM's approach, which conceptualizes LLMs as a chain of modular functions and leverages auxiliary variables for problem decomposition, not only facilitates a pragmatic application on LLMs but also demonstrates significant performance improvements, particularly in high-sparsity regimes where it surpasses current state-of-the-art methods.
__label__causal_inference Extensive empirical evaluation on both synthetic and real datasets validates our method's superiority in effectiveness and efficiency, particularly in handling high-dimensional data and dealing with large-scale scenarios.
__label__machine_vision Although promising, SDF-based methods often fail to capture detailed geometric structures, resulting in visible defects.
__label__machine_vision We decompose the decoding mechanism for masked reconstruction into self-attention between mask tokens and cross-attention between masked and visible tokens.
__label__deep_learning_architectures In this paper, we explore parameter-efficient transfer learning for audio-visual learning and propose the Audio-Visual Mixture of Experts (\ourmethodname) to inject adapters into pre-trained models flexibly.
__label__natural_language_processing This technique leverages semantic features to control the representation of LLM's intermediate hidden states, enabling the model to meet specific requirements such as increased honesty or heightened safety awareness.
__label__deep_learning_architectures These $\(\mathbf{\Delta W}\)$ matrices are structured for efficient parameterization, often using techniques like low-rank approximations or scaling vectors.
__label__reinforcement_learning First, we improve the $poly(d, A, H)T^{5/6}$ regret bound of Zhao et al.
__label__machine_vision It is trained on a curated set of 3D shapes and works on novel shape instances during testing.
__label__generative_models Spatial and temporal consistency in 4D geometry has been extensively explored respectively in 3D-aware diffusion models and traditional monocular video diffusion models.
__label__deep_learning_architectures Our method, called MOHAWK, is able to distill a Mamba-2 variant based on the Phi-1.5 architecture (Phi-Mamba) using only 3B tokens.
__label__optimization In this paper, we propose a novel Proactive Infeasibility Prevention (PIP) framework to advance the capabilities of neural methods towards more complex VRPs.
__label__optimization_for_deep_networks Our theoretical results imply that when training self-attention-only Transformers for fundamental regression tasks, the models learned by gradient-descent and Transformers show great similarity.
__label__safety_in_machine_learning Deep neural networks face persistent challenges in defending against backdoor attacks, leading to an ongoing battle between attacks and defenses.
__label__causal_inference Covariate-adjusted response-adaptive randomization (CARA) designs are gaining increasing attention.
__label__machine_vision We consider the task of active geo-localization (AGL) in which an agent uses a sequence of visual cues observed during aerial navigation to find a target specified through multiple possible modalities.
__label__reinforcement_learning We characterize this problem as a POMDP and propose a suite of RL algorithms that exploit task structure under uncertain interpretation of the domain-specific vocabulary.
__label__machine_learning_for_healthcare Experiments on multiple real-world clinical datasets demonstrate that our method outperforms state-of-the-art deep survival models in both discrimination and calibration.
__label__natural_language_processing RTD constructs a reference datastore from the provided training examples and optimizes the LLM's final vocabulary distribution by flexibly selecting suitable references based on the input, resulting in more trustable responses and enabling the model to adapt to downstream tasks at a low cost.
__label__machine_vision Towards this end, we introduce a novel module dedicated to the extraction of illumination-invariant features from low-light images, which can be easily integrated into existing object detection frameworks.
__label__diffusion_based_models To ensure temporal consistency, we introduce a simple post-hoc test-time guidance towards (self)-equivariant solutions.
__label__deep_learning_architectures We conduct downstream tasks of image classification and image-text retrieval on four medical datasets, where EGMA achieved state-of-the-art performance and stronger generalization across different datasets.
__label__machine_learning_for_other_sciences_and_fields We use the same model to represent a policy and value function for guiding proof search.
__label__machine_vision The code is available at github.com/TedLentsch/UNION.
__label__machine_learning_for_other_sciences_and_fields In this paper, we provide a tight characterization of inductive inference by establishing a novel link to online learning theory.
__label__reinforcement_learning These interactions, influenced by individual-specific factors ranging from personal preferences to physiological differences, can causally affect state transitions, such as the health conditions in healthcare or learning progress in education.
__label__natural_language_processing The results demonstrate that HiTs consistently outperform all baselines in these tasks, underscoring the effectiveness and transferability of our re-trained hierarchy encoders.
__label__reinforcement_learning To solve these problems, we propose the Hierarchical Programmatic Option framework (HIPO), which aims to solve long and repetitive RL problems with human-readable programs as options (low-level policies).
__label__infrastructure Despite these efforts, the reported training throughput only achieves 36.54% of model FLOPs utilization (MFUs), still not comparable to full on-device training.
__label__graph_neural_networks By guiding the walker towards the next hop with higher correlation value, our strategy simulates the real-world brain-wide communication.
__label__reinforcement_learning In this paper, based on that empathic responses are modulated by learned social relationships between agents, we propose LASE (**L**earning to balance **A**ltruism and **S**elf-interest based on **E**mpathy), a distributed multi-agent reinforcement learning algorithm that fosters altruistic cooperation through gifting while avoiding exploitation by other agents in mixed-motive games.
__label__machine_learning_for_physical_sciences To handle such data heterogeneity challenges, we exploit the specialty of molecular tasks that there are physical laws connecting them, and design consistency training approaches that allow different tasks to exchange information directly so as to improve one another.
__label__reinforcement_learning Based on this, we propose a novel approach, Diffusion-DICE, that directly performs this transformation using diffusion models.
__label__optimization In contrast, it is well-known in practice and recently confirmed in theory that stochastic methods based on without-replacement sampling, e.g., Random Reshuffling (RR) method, perform better than ones that sample the gradients with-replacement.
__label__online_learning Moreover, we demonstrate that FTPL-A also attains an $\tilde{O}(T^\frac{2}{3}(V_T+1)^\frac{1}{3})$ dynamic regret bound.
__label__machine_learning_for_healthcare Our approach consistently outperforms previous explanation attempts with particularly improved faithfulness scores on challenging biomarker prediction tasks.
__label__machine_vision We propose VFM-6D, a novel framework that explores harnessing existing vision and language models, to elaborate object pose estimation into two stages: category-level object viewpoint estimation and object coordinate map estimation.
__label__optimization The simulations verify the correctness of the analyses.
__label__other In this work, we observe that model trained on vast general images via masking strategy, has been naturally embedded with their distribution knowledge, thus spontaneously attains the underlying potential for strong image denoising.
__label__optimization_for_deep_networks After empirically establishing the prevalence of (NRC1)-(NRC3) for a variety of datasets and network architectures, we provide an explanation of these phenomena by modeling the regression task in the context of the  Unconstrained Feature Model (UFM), in which the last layer feature vectors are treated as free variables when minimizing the loss function.
__label__deep_learning_architectures We introduce ``FuseMoE'', a mixture-of-experts framework incorporated with an innovative gating function.
__label__reinforcement_learning We show that in both of our extensions, the optimal policies have the same structure even though the derivations are very different.
__label__probabilistic_methods We investigate the practical details of prompting to elicit coherent predictive distributions, and demonstrate their effectiveness at regression.
__label__probabilistic_methods Pearson's $r$ is essentially a scaled covariance, rooted in the renowned Cauchy-Schwarz Inequality.
__label__machine_vision Extensive experiments demonstrate that Normal-GS achieves near state-of-the-art visual quality while obtaining accurate surface normals and preserving real-time rendering performance.
__label__reinforcement_learning In recent years, the application of reinforcement learning (RL) involving interactions with individuals has seen significant growth.
__label__machine_learning_for_other_sciences_and_fields We argue for a learning-based approach to semantic routing as a more scalable and general alternative.
__label__neuroscience_and_cognitive_science Finally, we train recurrent neural networks on analog memory tasks to support the appearance of these systems as solutions and their generalization capabilities.
__label__machine_vision In contrast to these approaches, this paper proposes the Vision Mamba Mender, a systematic approach for understanding the workings of Mamba, identifying flaws within, and subsequently optimizing model performance.
__label__learning_theory The existing literature on rank collapse, however, has mostly overlooked other critical components in transformers that may alleviate the rank collapse issue.
__label__reinforcement_learning Empirical results indicate that DUPLEX improves over previous methods and successfully learns competitive driving styles in a hyper-realistic simulator (i.e., GranTurismo ™ 7) as well as diverse and effective policies in several multi-context robotics MuJoCo simulations with OOD gravity forces and height limits.
__label__machine_learning_for_physical_sciences Specifically, we present a quantum encoding scheme designed for 3-D molecules with qubits complexity $\mathcal{O}(C\log n)$ ($n$ is the number of atoms) and adopt a von Mises-Fisher (vMF) distributed latent space to meet the inherent coherence of the quantum system.
"__label__safety_in_machine_learning Previous attacks have succeeded in reverse-engineering model parameters
up to a precision of float64 for models trained on random data with at most three
hidden layers using cryptanalytical techniques."
__label__evaluation We present sDBSCAN, a scalable density-based clustering algorithm in high dimensions with cosine distance.
__label__interpretability_and_explainability Under a Huber characterization of the data heterogeneity across vendors, we propose a maximum mean discrepancy (MMD)-based valuation method which enables theoretically principled and actionable policies for comparing data distributions from samples.
__label__privacy This metric characterizes the model output’s rate of change or sensitivity to perturbations in the input feature.
__label__machine_vision In greater detail, PGRT is the generalization of Partial Radon Transform (PRT), which transforms a subset of function arguments non-linearly while HHRT is the composition of PRT and multiple domain-specific PGRT on marginal domain arguments.
__label__privacy We consider two such auditing measures: one additive, and on multiplicative.
__label__machine_learning_for_other_sciences_and_fields Most existing FL methods are based on the homogeneity assumption, namely, different clients have the same architectural models with the same tasks, which are unable to handle complex and multivariate data and tasks.
__label__generative_models Despite their successes, these models often exhibit limited diversity in the sampled images, particularly when sampling with a high classifier-free guidance weight.
__label__machine_vision Quantitative evaluations on benchmark datasets demonstrate that our EZ-HOI achieves state-of-the-art performance across various zero-shot settings with only 10.35\% to 33.95\% of the trainable parameters compared to existing methods.
__label__reinforcement_learning The main tool that makes this result possible is due to Weisz et al.
__label__natural_language_processing In particular, we introduce scores to evaluate the quality of the extraction and provide an extensive discussion on how to interpret them.
__label__optimization This paper establishes a structural characterization of the GVF that enables it to be modeled as a particular neural network architecture, which we then use to learn the GVF in a way that benefits from three notable properties.
__label__robotics We achieve this with a framework called MeMo which learns (Me)aningful, (Mo)dular controllers.
__label__natural_language_processing Experiments on a web-scale corpus demonstrate that our approach significantly enhances performance on standard language evaluations and long-context benchmarks, reaching target accuracy with up to 6x faster training compared to the baseline.
__label__machine_vision Grid heatmap is a novel concept that represents the latent variables for grid points sampled uniformly in the 3D cubic space, where these variables are the shortest distance between the grid points and the “skeleton” connected by keypoint pairs.
__label__interpretability_and_explainability In contrast, Knowledge Graphs (KGs) can provide explicit and editable knowledge for LLMs to alleviate these issues.
__label__diffusion_based_models This innovation achieves one-step latent semantic optimization and hence significantly promotes editing speeds.
__label__optimization To the best of our knowledge, this is the first time that global convergence is established for federated multi-task RL using policy optimization.
__label__graph_neural_networks We generalize this approach to graphs of arbitrary size and dimension by approaching sub-kernel assignment as a learnable multinomial assignment problem.
__label__deep_learning_architectures We propose a new paradigm to formulate the visual object tracking problem as a denoising learning process.
__label__natural_language_processing InfoNCE loss is commonly used to train dense retriever in information retrieval tasks.
__label__deep_learning_architectures Key-value (KV) caching plays an essential role in accelerating decoding for transformer-based autoregressive large language models (LLMs).
__label__neuroscience_and_cognitive_science The other is feature-based learning, which is slow and enables the brain to improve feature representations to match the statistical change of the environment.
__label__diffusion_based_models TwinAct addresses the limitations of existing methods that struggle to decouple actions from other semantics (e.g., the actor's appearance) due to the lack of an effective inductive bias with few exemplar images.
__label__algorithmic_game_theory We study the proportional clustering problem of Chen et al.
__label__bandits Furthermore, the algorithms achieve sublinear regret bounds of $\tilde{O}(\min\\{\sqrt{T}Q_{\max},T^{3/4}\\})$, where $Q_{\max}$ represents the maximum queue length over agents and times.
__label__natural_language_processing Conventional methods in TKGR typically depend on deep learning algorithms or temporal logical rules.
__label__machine_vision Without bells and whistles, our positional alignment method surpasses existing hallucination mitigation strategies by large margins on multiple object hallucination benchmarks.
__label__safety_in_machine_learning We introduce the Governance of the Commons Simulation (GovSim), a generative simulation platform designed to study strategic interactions and cooperative decision-making in LLMs.
__label__machine_vision The code and dataset are available in \href{https://github.com/mRobotit/Pixel-level-No-reference-Image-Exposure-Assessment}{\textcolor{red} {here}}.
__label__reinforcement_learning However, NLs descriptions are not always readily available and are expensive to collect.
__label__machine_vision Cooperating with the diverse range of existing prompting methods, CLAP can surpass the predominant deterministic finetuning approaches for CL with CLIP.
__label__machine_vision To this end, we propose a novel method to selectively incorporate the physics models with the kinematics observations in an online setting, inspired by a neural Kalman-filtering approach.
__label__interpretability_and_explainability In this work, we investigate the mechanisms of how transformers behave on unseen compositional tasks.
__label__machine_vision Experimental comparisons with state-of-the-art global synchronization methods on real datasets demonstrate the potential of this algorithm for significantly improving location estimation accuracy.
__label__machine_vision Moreover, our unified optimized projection space exhibits encouraging robustness performance for unseen data (degraded and depth images).
__label__natural_language_processing Our method does not degrade the general capability of LLMs and retains the ability to train the model on harmless tasks.
__label__causal_inference This randomness is referred to as aleatoric uncertainty and is necessary for understanding the probability of benefit from treatment or quantiles of the treatment effect.
__label__machine_learning_for_social_sciences We validate our framework with a case study in the criminal justice system, a domain characterized by limited and ethically challenging data collection.
__label__machine_vision Extensive experiments demonstrate that ReFIR can achieve not only high-fidelity but also realistic restoration results.
__label__generative_models Experimental results show that GDPO achieves state-of-the-art performance in various graph generation tasks with complex and diverse objectives.
__label__natural_language_processing Our experimental results demonstrate that the introduced regularization technique markedly improves the accuracy of learned reward models across a variety of out-of-distribution (OOD) tasks and effectively alleviates the over-optimization issue in RLHF, offering a more reliable and robust preference learning paradigm.
__label__learning_theory those covered by the low-degree conjecture.
__label__diffusion_based_models Additionally, we introduce several techniques including a pseudo corrector and sample-aware compilation to further reduce inference time.
__label__machine_vision Inspired by the recent advances of state space models (SSMs), we present a Voxel SSM, termed as Voxel Mamba, which employs a group-free strategy to serialize the whole space of voxels into a single sequence.
__label__learning_theory In this study, we theoretically explain the counterintuitive success of perturbation learning.
__label__probabilistic_methods In this paper, we obtain the Berry–Esseen bound for multivariate normal approximation for the Polyak-Ruppert averaged iterates of the linear stochastic approximation (LSA) algorithm with decreasing step size.
__label__interpretability_and_explainability We delve into the training process of this  dual model from a representation learning standpoint and further derive a generalization error bound related to the quantity of demonstration tokens.
__label__causal_inference To this end, we introduce a new causal model, where individual events of the cause trigger events of the effect with dynamic delays.
__label__reinforcement_learning We propose representing temporal goals using compositions of deterministic finite automata (cDFAs) and use cDFAs to guide RL agents.
__label__learning_theory In this work, we study the experts problem in the distributed setting where an expert's cost needs to be aggregated across multiple servers.
__label__machine_learning_for_social_sciences Our approach outperforms the prior art in terms of reconstruction error and ground truth matrix coverage, at a fraction of the computational cost.
__label__diffusion_based_models data distributions.
__label__probabilistic_methods BMRS is based on two recent methods: Bayesian structured pruning with multiplicative noise, and Bayesian model reduction (BMR), a method which allows efficient comparison of Bayesian models under a change in prior.
__label__machine_vision Another set of dynamic scene reconstruction methods, are entity-specific, mostly focusing on humans, and relies on template models.
__label__deep_learning_architectures Furthermore, we design a dynamic weight-balance distillation method in the multi-branch synchronous learning process to enhance the representation capability of the simpler branch.
__label__safety_in_machine_learning For some algorithms like Tree-Ring watermarks, the extracted pattern can also forge convincing watermarks on clean images.
__label__neuroscience_and_cognitive_science Accumulating evidence suggests stochastic cortical circuits can perform sampling-based Bayesian inference to compute the latent stimulus posterior.
"__label__graph_neural_networks We argue that 
the graph heterogeneity reflected on node types, node attributes, and  neighborhood structures can 
impose significant challenges for generalizing 
LDL onto graphs."
"__label__machine_learning_for_healthcare However,
single-cell sequencing technologies are inherently destructive and can only measure a limited array of data modalities simultaneously."
__label__generative_models Code is available at https://github.com/sail-sg/GDPO.
__label__causal_inference Our algorithm also enables us to conduct a causal analysis to evaluate spurious correlations among input features of generative models pre-trained on the CelebA dataset.
__label__probabilistic_methods Compared to Deep Ensemble baselines, CreDEs demonstrate higher test accuracy, lower expected calibration error, and significantly improved epistemic uncertainty estimation.
__label__reinforcement_learning However, existing techniques for world modelling do not guarantee that the effect of actions are represented in such systematic ways.
__label__deep_learning_architectures Our contributions are threefold: we provide a novel tropical geometric approach to selecting sampling domains among linear regions; an algebraic result allowing for a guided restriction of the sampling domain for network architectures with symmetries; and an open source library to analyze neural networks as tropical Puiseux rational maps.
__label__machine_vision However, due to the visual diversity and blurry edges of the non-grid smoke, noisy labels are almost inevitable in large-scale pixel-level smoke datasets.
__label__learning_theory We call a kernel function that offers these attributes an *inverse M-kernel*; it is reminiscent of the inverse M-matrix.
__label__interpretability_and_explainability Large Language Models (LLMs) have shown remarkable reasoning capabilities on complex tasks, but they still suffer from out-of-date knowledge, hallucinations, and opaque decision-making.
__label__optimization This paper provides a comprehensive non-asymptotic analysis of SGD with biased gradients and adaptive steps for non-convex smooth functions.
__label__natural_language_processing Beyond improving reward model performance, we show this way of training RM representations enables improved steerability because it allows us to evaluate the likelihood of an action achieving a particular goal-state (e.g.
__label__safety_in_machine_learning Our work provides useful insights into how to leverage foundation models in a data-efficient and computationally affordable manner to protect images against image segmentation models.
__label__active_learning This result is a generalization of the well-known $k$-means++ guarantee to a broad problem class which is also of independent interest.
__label__optimization This rate is nearly optimal for any $p,q\in[1,2]$.
__label__machine_vision Further, StreamDSGN applies three strategies to enhance the perception accuracy: (1) A feature-flow-based fusion method, which generates a pseudo-next feature at the current moment to address the misalignment issue between feature and ground truth.
__label__learning_theory We characterize the effects of the learning rule (supervised or reinforcement learning, SL/RL) and input-data distribution on the perceptron's learning curve and the forgetting curve as subsequent tasks are learned.
__label__bandits We study the interplay between local differential privacy (LDP) and robustness to Huber corruption and possibly heavy-tailed rewards in the context of multi-armed bandits (MABs).
__label__machine_vision Moreover, we introduce an alignment mechanism that utilizes reinforcement learning techniques to enhance the reverse generation trajectory of the diffusion model, ensuring improved performance and accuracy.
__label__natural_language_processing Consequently, various approaches to finetuning on specialized data would necessarily provide mixed results on downstream tasks, unless the model has already seen the right sequence of tokens.
__label__fairness To alleviate the identified bias factors, we design a novel fairness regularizer that offers a versatile use.
__label__machine_learning_for_social_sciences In light of this issue, we propose the $\underline{\bf{Lo}}$ng-$\underline{\bf{T}}$ail Adjusted $\underline{\bf{Next}}$ POI Prediction (LoTNext) framework for mobility prediction, combining a Long-Tailed Graph Adjustment module to reduce the impact of the long-tailed nodes in the user-POI interaction graph and a novel Long-Tailed Loss Adjustment module to adjust loss by logit score and sample weight adjustment strategy.
__label__diffusion_based_models We show that, through the adversarial training, the multi-steps video diffusion model, i.e., Stable Video Diffusion (SVD), can be trained to perform single forward pass to synthesize high-quality videos, capturing both temporal and spatial dependencies in the video data.
__label__learning_theory The decomposition rewrites the loss of the posterior as an excess loss relative to a downscaled loss of the prior plus the downscaled loss of the prior, which is bounded recursively.
__label__algorithmic_game_theory Addressing this challenge, we introduce a tractable theoretical model of algorithmic monoculture in a two-sided matching market with many participants.
__label__machine_vision While success on certain occasions are observed, enhancing the baseline across most scenarios remains challenging.
__label__other For KRR with injective feature mappings of deep neural networks, we show that while one data point per class is not sufficient in general, $k+1$ data points can be sufficient for deep linear neural networks, where $k$ is the number of classes.
__label__graph_neural_networks The field of graph learning has been substantially advanced by the development of deep learning models, in particular graph neural networks.
__label__deep_learning_architectures To address this issue, we propose the MambaTree network, which first dynamically generates a tree topology based on spatial relationships and input features.
__label__optimization For constrained, not necessarily monotone submodular maximization, all known approximation algorithms with ratio greater than $1/e$ require continuous ideas, such as queries to the multilinear extension of a submodular function and its gradient, which are typically expensive to simulate with the original set function.
__label__optimization Neur2BiLO serves as a heuristic that produces high-quality solutions extremely fast for four applications with linear and non-linear objectives and pure and mixed-integer variables.
__label__deep_learning_architectures VB-LoRA achieves extreme parameter efficiency while maintaining comparable or better performance compared to state-of-the-art PEFT methods.
__label__causal_inference To validate our theoretical results, we perform a series of simulations, which support and substantiate our findings.
"__label__robotics It comprises two integral streams both at the scene level:
(1) The scene context stream progressively accumulates historical scene information until the present moment, capturing temporal interactive relationships among scene elements."
__label__algorithmic_game_theory Next, we consider the problem of revenue maximization in this environment.
__label__natural_language_processing Large language models (LLMs) have developed impressive performance and strong explainability across various reasoning scenarios, marking a significant stride towards mimicking human-like intelligence.
__label__machine_learning_for_other_sciences_and_fields Therefore, we propose to predict label distribution from ternary labels, allowing experts to annotate labels in a three-way annotation scheme.
__label__optimization Mirror Descent is a popular algorithm, that extends Gradients Descent (GD) beyond the Euclidean geometry.
__label__online_learning We investigate the problem of sequentially adapting a model to non-stationary environments, where the data distribution is continuously shifting and only a small amount of unlabeled data are available each time.
__label__deep_learning_architectures Our empirical findings reveal that TA-BN enables the stacking of more layers within Neural ODEs, enhancing their performance.
__label__machine_vision Extensive evaluations conducted on various benchmarks (e.g., Human3.6M, 3DPW, and 3DPW-Occ) have demonstrated its effectiveness.
__label__optimization However, our current understanding of bias formation mostly focuses on the initial and final stages of learning, leaving a gap in knowledge regarding the transient dynamics.
__label__learning_theory The goal is to recover the class memberships of all tokens from a finite number of samples of $f$.
__label__machine_vision In this paper, we theorize that enhancing performance requires expanding the semantic pool, while increasing the expected probability of selected OOD labels being activated by OOD samples, and ensuring low mutual dependence among the activations of these OOD labels.
__label__machine_vision However, we observe that these methods are often not robust in real scenarios where non-reflective as well as reflective components are present.
__label__machine_vision This paper presents a new architecture *DeepStack* for LMMs.
__label__machine_vision Further, we show the significant improvement in distilling CLIP like models over a huge 12M image-text dataset.
__label__safety_in_machine_learning We investigate a family of simple long-context attacks on large language models: prompting with hundreds of demonstrations of undesirable behavior.
__label__machine_vision Furthermore, we employ the Dempster-Shafer evidence theory to evaluate the uncertainty of each prediction generated by diverse semantics.
__label__active_learning In this work, we propose to use different k-hop neighborhoods of vertices as pairwise descriptors for shape matching.
__label__evaluation Quantitative evaluations show that Prism, when configured with a vanilla 2B LLaVA and freely accessible GPT-3.5, delivers performance on par with VLMs $10 \times$ larger on the rigorous multimodal benchmark MMStar.
__label__infrastructure Our theoretical analysis confirms Distributed Lion's convergence properties.
__label__natural_language_processing To evaluate Large Language Models (LLMs) for question answering (QA), traditional methods typically focus on directly assessing the immediate responses generated by the models based on the given question and context.
__label__bandits In this paper, we make inroads into this inquiry by establishing a regret lower bound $\Omega(\sqrt{\beta_M(G) T})$, where $M$ is the number of contexts, $G$ is the feedback graph, and $\beta_M(G)$ is our proposed graph-theoretic quantity that characterizes the fundamental learning limit for this class of problems.
__label__generative_models Our method only requires generating prediction questions and responses from the CGM and evaluating its response log probability.
__label__machine_vision Furthermore, built on trajectory matching, PAD achieves remarkable improvements on various benchmarks, achieving state-of-the-art performance.
__label__probabilistic_methods We investigate the impact of the permutation function used to obtain the marginal training samples and present a novel architectural solution based on derangements.
__label__probabilistic_methods While the choice of prior strongly affects the resulting posterior distribution, computational tractability and lack of interpretability of the weight space typically limit the Laplace approximation to isotropic Gaussian priors, which are known to cause pathological behavior as depth increases.
__label__machine_vision We hope UniSeg3D can serve as a solid unified baseline and inspire future work.
__label__learning_theory However, a comprehensive theoretical understanding of these methods remains elusive.
__label__machine_vision Besides, an adaptive control strategy is applied to avoid the emergence of redundant superpoints.
__label__machine_vision Objects are localized by matching them to prototypes, which are constructed by unsupervised image-wide object appearance aggregation.
__label__generative_models After initial training on existing molecules and their properties, we adopt an online learning algorithm to progressively shift the model distribution towards regions that support desired target properties.
__label__machine_vision When using the same amount of parameters with prior works, CoFie reduces the shape error by 48% and 56% on novel instances of both training and unseen shape categories.
__label__deep_learning_architectures While these trackers exhibit promising performance, their deployment on resource-constrained devices remains challenging due to inefficiencies.
__label__optimization_for_deep_networks However, this regularizer is intractable for high-dimensional problems, as it requires computing a large Jacobian matrix and taking its SVD.
__label__reinforcement_learning VMOC naturally integrates maximum entropy intrinsic rewards to promote the exploration of diverse and effective options.
__label__safety_in_machine_learning Moreover, we indicate the biased message-passing schemes for graph structures and propose the personalized preference module.
__label__bandits In this work, we obtain the first sparse regret bounds that hold when $S$ is unknown and the action sets are adversarially generated.
__label__machine_vision In this paper, we recognize the different degradation patterns in nighttime images and propose N2D3 (Night to Day via Degradation Disentanglement).
__label__graph_neural_networks Extensive experiments on multiple benchmark datasets from various domains demonstrate the superior anomaly detection performance, efficiency, and generalizability of ARC.
__label__natural_language_processing We identify the differences between two forms of knowledge representation in language models: knowledge in the form of co-occurrence statistics is encoded in the middle layers of the transformer model and does not generalize well to reasoning scenarios beyond simple question answering, while true factual associations are encoded in the lower layers and can be freely utilized in various reasoning tasks.
__label__optimization_for_deep_networks Our analysis further reveals the importance of the element-wise recurrence design pattern combined with careful parametrizations in mitigating this effect.
__label__interpretability_and_explainability This yields a clustering of inputs based on whether the model deems them safe or not.
"__label__diffusion_based_models A primary source for this issue is that the ID embeddings reside in the \emph{image token space} (``image prompts""), which is not fully composable with the text prompt encoded by the CLIP text encoder."
__label__diffusion_based_models We thus introduce a computationally efficient technique for training with improved robustness that does not require any additional data, and effectively complements existing augmentation approaches.
__label__natural_language_processing Moreover, we propose assigning personas to LEAs to better simulate groups of real human evaluators.
__label__robotics In addition, we will make our code and models publicly available.
__label__reinforcement_learning However, existing works mainly focus on applying diffusion policies in offline RL, while their incorporation into online RL has been less investigated.
__label__machine_learning_for_other_sciences_and_fields Meanwhile, to effectively capture the spatial-temporal dependencies among intersections, we design a Spatial-Temporal transFormer (STFormer) architecture.
__label__neuroscience_and_cognitive_science We demonstrate that LDMs with redundancy reduction and prototype-based regularizations produce near-human-like drawings (regarding both samples' recognizability and originality) -- better mimicking human perception (as evaluated psychophysically).
__label__reinforcement_learning (2023) provided an affirmative answer to this question in the tabular PAC RL case, the question remains unsettled for both the regret-minimizing RL case and the non-tabular case.
__label__reinforcement_learning Finally, we conduct deep RL experiments to validate our theoretical findings.
__label__reinforcement_learning This framework can be applied to other safe RL algorithms.
__label__optimization_for_deep_networks To maintain orthogonality and minimize forgetting, we further involve the gradient projection technique that keeps the low-rank subspaces of each new task orthogonal to those of previous tasks.
__label__machine_vision Correspondences in point cloud registration are prone to outliers, significantly reducing registration accuracy and highlighting the need for precise inlier identification.
__label__robotics This paper introduces OpenGaussian, a method based on 3D Gaussian Splatting (3DGS) that possesses the capability for 3D point-level open vocabulary understanding.
__label__machine_learning_for_other_sciences_and_fields The key to our methodology is introducing metrics based on the convergence dynamics of the formulas, rather than on the numerical value of the formula.
__label__evaluation As exemplars of our approach, we create \textit{Lifelong-CIFAR10} and \textit{Lifelong-ImageNet}, containing (for now) 1.69M and 1.98M test samples, respectively.
__label__natural_language_processing For the former, we use question-answering samples to obtain the covariance matrices, and use the decomposed components with the smallest $r$ singular values to initialize a learnable adapter, with the others frozen such that the world knowledge is better preserved.
__label__speech_and_audio And natural images, when played as spectrograms, make unnatural sounds.
__label__neuroscience_and_cognitive_science To facilitate the algorithm's widespread adoption among experimental neuroscientists, we created a user-friendly software and conducted a first-of-its-kind benchmarking study involving about 160,000 annotations.
__label__machine_vision We believe VisionLLM v2 will offer a new perspective on the generalization of MLLMs.
__label__learning_theory Drawing inspiration from insights on early learning regularization, we develop a principled method to improve self-training under distribution shifts based on temporal consistency.
__label__machine_learning_for_healthcare In this work, we establish a representation theorem for the graph of a time series and derive its consequences on the LDDMM framework.
__label__evaluation (3) Compared to open-source models, closed-source models exhibit a pronounced bias advantage in terms of Consistency.
__label__machine_learning_for_healthcare We conduct multimodal survival analysis on Whole Slide Images and Multi-omic data on four cancer datasets from The Cancer Genome Atlas (TCGA).
__label__optimization Data-driven algorithm design is a paradigm that uses statistical and machine learning techniques to select from a class of algorithms for a computational problem an algorithm that has the best expected performance with respect to some (unknown) distribution on the instances of the problem.
__label__learning_theory Moreover, how to further enhance view cooperation for better fusion still needs to be explored.
__label__diffusion_based_models However, the inevitable discretization errors of the ODE solvers are significantly magnified when the number of function evaluations (NFE) is fewer.
__label__machine_vision 3D Gaussian Splatting introduced high-quality novel view synthesis at real-time speeds.
__label__active_learning We consider settings where upon deploying a set of services, users choose the one minimizing their personal losses and the learner iteratively learns by interacting with diverse users.
__label__other Due to the absence of the text encoding as contrastive target, SuperClass does not require a text encoder and does not need to maintain a large batch size as CLIP does.
__label__machine_learning_for_healthcare Its advantages include high prediction accuracy, interpretability across multiple dimensions, and automation through an end-to-end concept labeling process that reduces the need for extensive human training effort when working with new datasets.
__label__safety_in_machine_learning Backdoor attacks on deep learning represent a recent threat that has gained significant attention in the research community.
__label__diffusion_based_models We locate the cause of content shift as one inherent characteristic of diffusion models, which suggests the broad existence of this phenomenon in diffusion feature.
__label__machine_vision We theoretically justify the rationality behind our method and empirically verify its effectiveness on both the out-of-distribution and the cross-domain datasets, showcasing its applicability in  real-world situations.
__label__fairness Prior work applies this framework to centroid-based clustering, where points are partitioned into clusters, and the cost to each data point is measured by its distance to a centroid assigned to its cluster.
__label__natural_language_processing The code will be available at https: //github.com/SamsungLabs/aespa.
__label__safety_in_machine_learning Based on this generator, together with an efficient AE production procedure, we design a new **Dual Adversarial Training (DAT)** strategy.
__label__safety_in_machine_learning Specifically, ALI-Agent incorporates a memory module to guide test scenario generation, a tool-using module to reduce human labor in tasks such as evaluating feedback from target LLMs, and an action module to refine tests.
__label__interpretability_and_explainability 2022), the current consensus is that it is a necessary initial step for designing state-of-the-art performant language models.
__label__diffusion_based_models We prove that the optimal diffusion model for the target domain integrates pre-trained diffusion models on the source domain with additional guidance from a domain classifier.
__label__machine_learning_for_social_sciences By simulating the process of job application creation, we examine the language patterns and biases that emerge when the model is prompted with diverse job postings.
__label__machine_vision Crucially, it delivers high-quality results in 4 minutes, which is 12$\times$ faster than NeRF-based methods and on par with traditional algorithms.
__label__deep_learning_architectures Similar to the development of most deep learning models, the construction of these attention mechanisms relies on heuristics and experience.
__label__speech_and_audio To train ELSA: (a) we spatially augment  the audio and captions of three open-source audio datasets totaling 4,738 hours and 890,038 samples of audio comprised from 8,972 simulated spatial configurations, and (b) we design an encoder to capture the semantics of non-spatial audio, and the semantics and spatial attributes of spatial audio using contrastive learning.
__label__speech_and_audio Extensive experiments demonstrate the effectiveness of SongCreator by achieving state-of-the-art or competitive performances on all eight tasks.
__label__safety_in_machine_learning Prior works on physical adversarial camouflage against vehicle detectors mainly focus on the effectiveness and robustness of the attack.
__label__optimization The nadir objective vector plays a key role in solving multi-objective optimization problems (MOPs), where it is often used to normalize the objective space and guide the search.
__label__graph_neural_networks Existing methods exploit the feature space of trained network and attempt at estimating the uncertainty in the predictions.
__label__privacy We study private stochastic convex optimization (SCO) under user-level differential privacy (DP) constraints.
__label__reinforcement_learning Despite its encouraging results, GAIL training is often brittle and unstable.
__label__generative_models Hence, we are curious about whether LLMs' reasoning ability can be further enhanced by Self-Playing this Adversarial language Game (SPAG).
__label__machine_vision Existing works in a coarse-to-fine manner either suffer from severe noisy correspondences caused by unreliable coarse matching or struggle to form outlier-free coarse-level correspondence sets.
__label__other Recent studies empirically indicate that language models (LMs) encode rich world knowledge beyond mere semantics, attracting significant attention across various fields.
__label__robotics In robotic tasks, the ability to predict future images and generate actions is highly correlated since they share the same underlying dynamics of the physical world.
__label__probabilistic_methods This allows us to propose a new functional gradient ParVI method for constrained sampling, called *constrained functional gradient flow* (CFG), with provable continuous-time convergence in total variation (TV).
__label__reinforcement_learning Based on the sub-gradient, a robust policy mirror descent approach is further proposed.
__label__fairness Specifically, we delve into Influence Function, one of the standard methods for mislabeled sample detection, for identifying bias-conflicting samples and propose a simple yet effective remedy for biased models by leveraging them.
__label__safety_in_machine_learning Its primary goal is to enhance the adaptability and resilience of machine learning models when faced with new, unseen, and potentially adversarial data that significantly diverges from their original training datasets.
__label__machine_vision Moreover, by combining with other generative methods, our model enables many downstream 2D tasks, such as text-based relighting and object insertion.
__label__probabilistic_methods Current BAX methods use expected information gain to guide this selection.
__label__diffusion_based_models Our approach, termed Diffusion model for Long-Tail recognition (DiffuLT), represents a pioneer application of generative models in long-tail recognition.
__label__generative_models Existing transformer-based diffusion methods suffer from quadratic computation cost and limited resolution extrapolation capabilities, making them less effective for this task.
__label__privacy Machine unlearning aims to efficiently remove the effect of certain data points on the trained model parameters so that it can be approximately the same as if one retrains the model from scratch.
__label__privacy In this setting, there are $n$ users (e.g., cell phones), each possessing $m$ data items (e.g., text messages), and we need to protect the privacy of each user's entire collection of data items.
__label__reinforcement_learning In recent years, significant progress has been made in multi-objective reinforcement learning (RL) research, which aims to balance multiple objectives by incorporating preferences for each objective.
__label__diffusion_based_models Experimental results highlight the capability of our framework to generate interactions with multiple human characters and its potential to work with off-the-shelf physics-based character simulators.
__label__other We develop a novel framework, namely *corssing sparse proximity graph (CSPG)*, based on random partitioning of the dataset.
__label__reinforcement_learning Specifically, we focus on ergodic Markov chains.
__label__natural_language_processing It is based on a large language model (LLM) carefully aligned to be aware of a perception module, a motor function module, and the concept of a simple finite state machine (called neural FSM) with two states.
__label__reinforcement_learning (2024) to $poly(d, A, H)T^{2/3}$ for the full-information unknown transition setting, where $d$ is the rank of the transitions, $A$ is the number of actions, $H$ is the horizon length, and $T$ is the number of episodes.
__label__infrastructure FACT is the first federated mechanism that: (1) eliminates federated free riding by using a penalty system, (2) ensures agents provide truthful information by creating a competitive environment, and (3) encourages agent participation by offering better performance than training alone.
__label__other We propose M$^3$-Impute, which aims to leverage the missingness information and such correlations with novel masking schemes.
__label__fairness We revisit the recently developed framework of proportionally fair clustering, where the goal is to provide group fairness guarantees that become stronger for groups of data points that are large and cohesive.
__label__privacy Due to statistical lower bounds on the learnability of many function classes under privacy constraints, there has been recent interest in leveraging public data to improve the performance of private learning algorithms.
__label__generative_models Yet diffusion models have garnered increasing attention, as they offer comparable performance while being permutation-invariant.
__label__other They should in-principle yield the same performance and share the same optimal learning rate.
__label__evaluation However, evaluating these models poses a significant challenge.
__label__reinforcement_learning Project homepage: https://kalmneurips2024.github.io.
__label__machine_vision In this work, we leverage the potential of Diffusion models to address the text-video modality gap by progressively aligning text and video embeddings in a unified space.
__label__interpretability_and_explainability On classification tasks, we show that the representations of VQShape can be utilized to build interpretable classifiers, achieving comparable performance to specialist models.
__label__learning_theory It applies whenever the algorithm generates a distribution, which is absolutely continuous distribution relative to some a-priori measure, and the logarithm of its density is exponentially concentrated about its mean.
__label__reinforcement_learning Our algorithm enjoys an $\widetilde{\mathcal{O}}(d \sqrt{H^3 K} + \sqrt{HK(H + \bar{P}_K)})$ dynamic regret, where $d$ is the feature mapping dimension, $H$ is the episode length, $K$ is the number of episodes, $\bar{P}_K$ is the non-stationarity measure.
__label__learning_theory (2020) sets a threshold for the average surrogate loss at training time; above the threshold, gradient descent is run as usual, but below the threshold, a switch to gradient *ascent* is made.
__label__diffusion_based_models Next, we propose a high-frequency guidance sampling method to explicitly control the denoising process and preserve high-frequency details in unprocessed areas.
__label__learning_theory Transfer learning is an attractive framework for problems where there is a paucity of data, or where data collection is costly.
__label__deep_learning_architectures In this paper, we introduce a novel method, RoAd, which employs a straightforward 2D rotation to adapt LLMs and addresses all the above challenges: (1) RoAd is remarkably parameter-efficient, delivering optimal performance on GLUE, eight commonsense reasoning tasks and four arithmetic reasoning tasks with <0.1% trainable parameters; (2) RoAd facilitates the efficient serving of requests requiring different adapters within a batch, with an overhead comparable to element-wise multiplication instead of batch matrix multiplication; (3) RoAd enhances LLM's interpretability through integration within a framework of distributed interchange intervention, demonstrated via composition experiments.
__label__other the class label set of test data is identical to those used in training phase.
__label__optimization We also provide a probabilistic convergence result for Adam under a generalized smooth condition which allows unbounded smoothness parameters and has been illustrated empirically to capture the smooth property of many practical objective functions more accurately.
__label__natural_language_processing Extensive experiments demonstrate that our method enhances the ability of various text encoders in detecting AI-generated text across multiple benchmarks and achieves state-of-the-art results.
__label__optimization Results demonstrate substantial improvements in generalization capabilities, particularly under severely imbalanced conditions.
__label__natural_language_processing Our trained LINE module excels in capturing critical information from clinical notes, leveraging highly de-identified data.
__label__graph_neural_networks We apply the smoothing method for seven baseline models to show its effectiveness.
__label__natural_language_processing The robustness of SelectIT has also been corroborated in various foundation models and domain-specific tasks.
__label__machine_vision Furthermore, the theory tells us that approximating the reformulated loss should be improved by increasing the number of augmentations, and as such using multiple augmentations should lead to improved convergence.
__label__machine_vision However, the cluster center primarily focuses on commonality, overlooking divergence and variety.
__label__reinforcement_learning This paper proposes the utilization of dynamic sparse training (DST), a technique proven effective in deep supervised learning tasks, to alleviate the computational burdens in MARL training.
__label__interpretability_and_explainability Extensive experiments across various data modalities and model architectures show that R2ET achieves superior stability against stealthy attacks, and generalizes effectively across different explanation methods.
__label__privacy We study the problems of differentially private federated online prediction from experts against both *stochastic adversaries* and *oblivious adversaries*.
__label__robotics To this end, we aim to learn sequence planning for ObjectNav.
__label__machine_learning_for_healthcare The efficacy of our method in the centerline extraction and segmentation tasks has been substantiated through experimental evaluations across various datasets.
__label__machine_learning_for_physical_sciences In this work, we propose a variational formulation of Doob's $h$-transform as an optimization problem over trajectories between a given initial point and the desired ending point.
"__label__deep_learning_architectures Specifically, we show that wide and structured networks
can utilize training FLOPs more efficiently, with fewer parameters and lower
loss than dense models at their optimal trade-off."
__label__diffusion_based_models However, previous blind image restoration methods still need to assume the type of degradation model while leaving the parameters to be optimized, limiting their real-world applications.
__label__reinforcement_learning This involves constructing a new graph kernel for graphs with both continuous and categorical attributes, as well as new optimisation methods for learning heuristic functions for numeric planning.
__label__probabilistic_methods We present two realizations of BMRS derived from different priors which yield different structured pruning characteristics:  1) BMRS_N with the truncated log-normal prior, which offers reliable compression rates and accuracy without the need for tuning any thresholds and 2) BMRS_U with the truncated log-uniform prior that can achieve more aggressive compression based on the boundaries of truncation.
__label__deep_learning_architectures The code of DOFEN is available at: https://github.com/Sinopac-Digital-Technology-Division/DOFEN
__label__machine_learning_for_other_sciences_and_fields This success paves the way towards a generative model that creates formulas fulfilling specified mathematical properties, accelerating the rate of discovery of useful formulas.
__label__learning_theory Therefore, it is imperative to explore the generalization performance of the ASGD algorithm.
__label__neuroscience_and_cognitive_science Large language models (LLMs) have rapidly improved text embeddings for a growing array of natural-language processing tasks.
__label__generative_models Extending generative retrieval to accommodate multi-graded relevance poses challenges, including the need to reconcile likelihood probabilities for docid pairs and the possibility of multiple relevant documents sharing the same identifier.
__label__machine_learning_for_physical_sciences We explore the convergence guarantees of such methods in both linear and nonlinear cases, addressing challenges such as spectral bias and slow convergence.
__label__interpretability_and_explainability Intuitively, RD dynamically highlights the dominant regions of each source and can be naturally converted to the corresponding fusion weight, achieving robust results.
__label__machine_vision Experiments over multiple widely adopted detection benchmarks show that KGD outperforms the state-of-the-art consistently by large margins.
__label__optimization It achieves the best test performance and the lowest training loss for the majority of the tasks, even when compared to well-tuned AdamW/Adafactor baselines.
"__label__optimization We demonstrate the effectiveness of Fitzpatrick losses for
label proportion estimation."
__label__reinforcement_learning In such scenarios, the cost function can be learned from feedback collected offline in between training rounds.
__label__reinforcement_learning To optimally analyze this reduction, we develop improved bounds for $\gamma$-discounted MDPs, showing that $\widetilde{O}\left(SA\frac{\mathsf{H}}{(1-\gamma)^2\varepsilon^2} \right)$ and $\widetilde{O}\left(SA\frac{\mathsf{B} + \mathsf{H}}{(1-\gamma)^2\varepsilon^2} \right)$ samples suffice to learn $\varepsilon$-optimal policies in weakly communicating and in general MDPs, respectively.
__label__reinforcement_learning The source code is available in the supplementary.
__label__natural_language_processing In this paper, we investigate if self-recognition capability contributes to self-preference.
__label__fairness We ascertain that such a fairness requirement with no prior demographic information essential promotes training losses to exhibit a Dirac delta distribution.
"__label__learning_theory Our novel analysis reveals that CutMix training makes the network learn all features and noise vectors ""evenly"" regardless of the rarity and strength, which provides an interesting insight into understanding patch-level augmentation."
__label__natural_language_processing We propose PV-Tuning - a representation-agnostic framework that generalizes and improves upon existing fine-tuning strategies, and provides convergence guarantees in restricted cases.
__label__interpretability_and_explainability In this work, we connect the implicit-differentiation-based and unrolling-based approaches and combine their benefits by introducing Source, an approximate unrolling-based TDA method that is computed using an influence-function-like formula.
__label__optimization Compared to existing works, the proposed method offers a theoretical foundation for the submodel extraction and eliminates the need for additional information beyond the model parameters themselves to determine parameter importance, significantly reducing the overhead on clients.
__label__interpretability_and_explainability We provide theoretical guarantees and demonstrate the power of our methods in applications like training data attribution, Hessian spectrum analysis, and intrinsic dimension computation for pre-trained language models.
__label__optimization In effect, our method obviates the need for reformulating HMVC as QUBO.
__label__other This high within-class similarity can be attributed to the fact that previous methods use samples from different classes to construct a single batch for batch normalization (BN) matching.
__label__fairness We show a linear structure of the grouping function class spanned by density ratios, resulting in a unifying framework for robust learning by designing specific grouping functions.
__label__probabilistic_methods Furthermore, disentangling the learned representations remains a significant challenge and has not been sufficiently explored in GRL research.
__label__safety_in_machine_learning \textcolor{red}{Trigger Warning: This paper contains model behavior that can be offensive in nature.}
__label__bandits In this work, we close the fundamental gap of theory and practice by providing an improved regret bound for linear ensemble sampling.
__label__diffusion_based_models With the advent of generative models, such as stable diffusion, that can create fake but realistic images, watermarking has become particularly important to make human-created images reliably identifiable.
__label__deep_learning_architectures This is made possible by utilizing an alternative method for feature transformation to replace the linear projection of fully-connected layers.
__label__bandits To start with, we present the first tight characterization of the mean estimation error in high probability under both LTC and CTL settings.
__label__optimization We develop a framework for analyzing the training and learning rate dynamics on a large class of high-dimensional optimization problems, which we call the high line, trained using one-pass stochastic gradient descent (SGD) with adaptive learning rates.
__label__online_learning To address this gap, we propose a new framework based on the paradigm of online control.
__label__learning_theory Specifically, there is a regime where it is possible to construct in polynomial time an accurate SSL classifier.
"__label__learning_theory But how efficient are neural
networks at extracting features from higher-order cumulants?"
__label__interpretability_and_explainability These findings provide new insights into how the internal mechanisms of autoregressive learning facilitate intelligent planning and deepen our understanding of how future LLMs might achieve more advanced and general planning-and-reasoning capabilities across diverse applications.
__label__generative_models LoRA also exhibits concept-loss when multiple adapters are used concurrently.
__label__deep_learning_architectures We explore its convergence, conduct extensively experimental benchmarking, and provide consistent complexity evaluation by considering chip architecture, memory hierarchy, dataflow, and arithmetic precision.
__label__diffusion_based_models This unifies the frameworks of feature extraction and denoising, where the former progressively embeds features from low-level to high-level, and the latter recursively denoises features step-by-step.
__label__deep_learning_architectures We apply MGDL to synthetic, manifold, colored images, and MNIST datasets, all characterized by presence of high-frequency features.
__label__optimization_for_deep_networks The softmax unit, which is a key part of the attention mechanism, normalizes the attention scores.
__label__interpretability_and_explainability Prior efforts to interpret ViTs tend to focus on characterizing relevant low-level visual features.
__label__neuroscience_and_cognitive_science Thirdly, we propose a novel baseline EEG2Video for video reconstruction from EEG signals that better aligns dynamic movements with high temporal resolution brain signals by Seq2Seq architecture.
__label__generative_models Motion generation from discrete quantization offers many advantages over continuous regression, but at the cost of inevitable approximation errors.
__label__neuroscience_and_cognitive_science Spiking neural networks (SNNs) represent a promising approach to developing artificial neural networks that are both energy-efficient and biologically plausible.
__label__reinforcement_learning We present the first formal analysis of such constraints and prove that while the expected cost of a BSQ constraint is not a convex function w.r.t its parameters, it is piecewise constant and yields an implicit discrete parameter search space that is finite for finite horizons.
__label__machine_vision Please see our project page at [illuminerf.github.io](illuminerf.github.io).
__label__optimization The algorithm relies on novel generic stochastic gradient collection strategies with theoretical guarantees that can be of interest on their own, and may be used in the design of future optimization methods.
__label__machine_learning_for_other_sciences_and_fields Experiments show better performance and robustness of DisenGCD than state-of-the-art CD methods and demonstrate the effectiveness of the disentangled learning framework and meta multigraph module.The source code is available at https://github.com/BIMK/Intelligent-Education/tree/main/DisenGCD.
__label__graph_neural_networks While methods like self-supervised learning and graph prompt learning have been extensively explored, they often rely on fine-tuning with task-specific labels, limiting their effectiveness in zero-shot scenarios.
__label__optimization_for_deep_networks In this work, we introduce a sharper ambient dimension-independent convergence analysis for sketch-DL using the second-order geometry specified by the loss Hessian.
__label__machine_vision Compared to previous work we achieve comparable or better results at a fraction of optimization and rendering time while enabling detailed control over material attributes.
__label__machine_vision In this paper, we propose a novel FedMVC framework, which concurrently addresses two challenges associated with heterogeneous hybrid views, i.e., client gap and view gap.
__label__machine_learning_for_physical_sciences On the TinyMol dataset, we outperform the `gold-standard' CCSD(T) CBS reference energies by 1.9m$E_h$ and reduce energy errors compared to previous generalized neural wave functions by up to an order of magnitude.
__label__graph_neural_networks Despite their successful and diverse applications, oversmoothing prohibits deep architectures due to node features converging to a single fixed point.
__label__other Experimentally, we first motivate IMM using logistic regression as a toy example.
__label__optimization In 2022, Ben-Eliezer, Eden, and Onak showed a dense-sparse trade-off technique that elegantly combined sparse recovery with known techniques using differential privacy and sketch switching to achieve adversarially robust algorithms for $L_p$ estimation and other algorithms on turnstile streams.
__label__neuroscience_and_cognitive_science Our code is available at [https://github.com/ridgerchu/SAD](https://github.com/ridgerchu/SAD).
__label__deep_learning_architectures We show that competitive architectures can be obtained by searching from scratch, and we consistently find large improvements when initialising the search with strong baselines.
__label__natural_language_processing We demonstrate the theoretical advantages of Cal-DPO over existing approaches.
__label__machine_vision Second, to better integrate object text and image, we design a balanced loss function with a noise parameter, ensuring both optimal editability and fidelity of the object image.
__label__machine_learning_for_social_sciences Meanwhile, it guides the network to learn intents from behaviors by forcing behavior embeddings close to cluster centers.
__label__fairness Through comprehensive analysis and experiments on diverse datasets, we demonstrate that our new perspective can boost the precision of detection and rectify biased models effectively.
__label__natural_language_processing Experiments demonstrate the state-of-the-art performance of DiscoPOP and its successful transfer to held-out tasks.
__label__neuroscience_and_cognitive_science We introduce *Brain-JEPA*, a brain dynamics foundation model with the Joint-Embedding Predictive Architecture (JEPA).
"__label__other With unlearning research still being at its infancy, many fundamental open questions exist: 
Are there interpretable characteristics of forget sets that substantially affect the difficulty of the problem?"
__label__human-AI_interaction Rather than providing single-label predictions, these systems provide sets of label predictions constructed using conformal prediction, namely prediction sets, and ask human experts to predict label values from these sets.
__label__natural_language_processing We evaluate HLM-Cite on a dataset across 19 scientific fields, demonstrating a 17.6\% performance improvement comparing SOTA methods.
__label__safety_in_machine_learning Our approach aims to achieve safety in the sense of finite-horizon reachability proofs, and is comprised of three key parts.
__label__graph_neural_networks Our extensive experiments demonstrate that HEGNN not only aligns with our theoretical analyses on toy datasets consisting of symmetric structures, but also shows substantial improvements on more complicated datasets such as $N$-body and MD17.
__label__reinforcement_learning The future thinking agent collects observation-action trajectories of the target agents and leverages the pre-trained multi-character policy to infer their characters.
__label__online_learning This approach simultaneously achieves high accuracy and low resource consumption.
__label__machine_learning_for_other_sciences_and_fields Intuitively, it is beneficial to establish a correlation between the interaction time interval and the model uncertainty to provide effective recommendations.
__label__machine_vision Our project page is in \url{https://tau-yihouxiang.github.io/projects/X-Ray/X-Ray.html}.
__label__machine_vision Subsequently, SCC introduces a simple connectivity classification task, which enables us to locate and correct connectivity noise with the guidance of loss distribution.
__label__diffusion_based_models Recent advancements in 3D generation have leveraged synthetic datasets with ground truth 3D assets and predefined camera trajectories.
__label__machine_vision While stronger language models can enhance multimodal capabilities, the design choices for vision components are often insufficiently explored and disconnected from visual representation learning research.
__label__machine_learning_for_social_sciences We further integrate NMM with our non-Euclidean graph variational autoencoder (VAE) framework, NMM-GNN.
__label__deep_learning_architectures With this perspective, we give several positive results.
"__label__causal_inference Learning the unknown causal parameters of a linear structural causal 
model is a fundamental task in causal analysis."
"__label__other We evaluate our
 system, named BISCOPE, on texts generated by five latest commercial LLMs
 across five heterogeneous datasets, including both natural language and code."
__label__other Experiments confirm our findings on both synthetic and real-world sequence classification tasks.
__label__natural_language_processing Moreover, due to the scarcity of high-quality evaluation data, LLMs exhibit deficiencies in their evaluation capabilities.
__label__machine_learning_for_other_sciences_and_fields PhyloGen views phylogenetic inference as a conditionally constrained tree structure generation problem, jointly optimizing tree topology and branch lengths through three core modules: (i) Feature Extraction, (ii) PhyloTree Construction, and (iii) PhyloTree Structure Modeling.
__label__neuroscience_and_cognitive_science Experimental results show that our method consistently outperforms the current state-of-the-art algorithms on both popular non-spiking static and neuromorphic datasets.
__label__diffusion_based_models Code is available at https://github.com/zhenzhiwang/intercontrol.
__label__natural_language_processing To address this issue we introduce Direct Preference Heads (DPH), a fine-tuning framework that enables LMs to learn human preference signals through an auxiliary reward head without directly affecting the output distribution of the language modeling head.
__label__deep_learning_architectures Test-time adaptation (TTA) is the most realistic methodology for adapting deep learning models to the real world using only unlabeled data from the target domain.
__label__machine_learning_for_healthcare The study of cells and their responses to genetic or chemical perturbations promises to accelerate the discovery of therapeutics targets.
__label__reinforcement_learning This makes the world model softly state-invariant.
__label__safety_in_machine_learning Our extensive experiments show that WAGLE boosts unlearning performance across a range of LLM unlearning methods such as gradient difference and (negative) preference optimization, applications such as fictitious unlearning (TOFU benchmark), malicious use prevention (WMDP benchmark), and copyrighted information removal, and models including Zephyr-7b-beta and Llama2-7b.
__label__natural_language_processing In this work, we introduce an automated evaluation framework IQA-EVAL to Interactive Question Answering Evaluations, more specifically, we introduce LLM-based Evaluation Agent (LEA) that can: (1) simulate human behaviors to generate interactions with IQA models; (2) automatically evaluate the generated interactions.
__label__generative_models In this paper, we propose Metric Flow Matching (MFM), a novel simulation-free framework for conditional flow matching where interpolants are approximate geodesics learned by minimizing the kinetic energy of a data-induced Riemannian metric.
__label__generative_models [2023] utilizes the depth-first search-based decision tree (DFSDT) mechanism for multi-step reasoning with $16000+$ real-world APIs, effectively enhancing the performance of tool-augmented LLMs compared to traditional chain reasoning mechanisms.
__label__generative_models We extensively evaluate PBP-GFN across eight benchmarks, including hyper-grid environment, bag generation, structured set generation, molecular generation, and four RNA sequence generation tasks.
__label__machine_learning_for_healthcare We propose a new method to jointly reconstruct multiple cortical surfaces using weak supervision from brain MRI ribbon segmentation results.
__label__reinforcement_learning Code and pre-trained models are available at https://thuml.github.io/iVideoGPT.
__label__reinforcement_learning In this work, we present **DIVA**, an evolutionary approach for generating diverse training tasks in such complex, open-ended simulators.
__label__other To bridge the gap, we propose a novel dual-modality reasoning framework called Vision-Augmented Prompting (VAP).
__label__optimization_for_deep_networks We discover a trade-off between sharpness and diversity: minimizing the sharpness in the loss landscape tends to diminish the diversity of individual members within the ensemble, adversely affecting the ensemble's improvement.
__label__optimization_for_deep_networks Additionally, we study the generalization result of $S^{2} - SAM$ and provide theoretical proof for convergence.
__label__optimization_for_deep_networks We investigate this phenomenon in two-layer networks that satisfy a near-homogeneity condition.
__label__natural_language_processing In the final layers, LLMs generate responses aligned with the original language of the query.
__label__machine_learning_for_other_sciences_and_fields With language as a medium, our method adaptively integrates social events into forecasting models, aligning news content with time series fluctuations to provide richer insights.
__label__deep_learning_architectures The overall architecture can be used to implement models that can access short-term eidetic memory 'in-context,' permanent structural memory 'in-weights,' fading memory 'in-state,' and long-term eidetic memory 'in-storage' by natively incorporating retrieval from an asynchronously updated memory.
__label__deep_learning_architectures By allowing recurrence in depth, UTs have advantages over standard Transformers in learning compositional generalizations, but layer-sharing comes with a practical limitation of parameter-compute ratio: it drastically reduces the parameter count compared to the non-shared model with the same dimensionality.
__label__generative_models Our approach builds upon reward-free direct preference optimization methods, but unlike previous approaches, it seeks a robust policy which maximizes the worst-case group performance.
__label__graph_neural_networks Specifically, we introduce a prompt graph centered with a query-related example fact as context to understand the query relation.
__label__active_learning We validate the effectiveness of our pipeline on benchmark datasets such as SCAPE, TOSCA, TOPKIDS, and others.
__label__natural_language_processing Furthermore, SELF-DISCOVER outperforms inference-intensive methods such as CoT-Self-Consistency by more than 20%, while requiring 10-40x fewer inference compute.
__label__machine_vision Addressing this, we introduce Visual Context Compressor, which reduces the number of visual tokens to enhance training and inference efficiency without sacrificing performance.
__label__machine_learning_for_social_sciences More broadly, we envision our work to serve as a blueprint for how the recent definition of performative power can help integrate quantitative insights from online experiments with future investigations into the economic power of digital platforms.
__label__machine_vision Besides, excessive interactions can not meet real-time prediction requirements within the constrained drone-based communication bandwidth.
__label__algorithmic_game_theory In this model, the algorithm is enhanced with imperfect (machine-learned) information concerning the input, usually referred to as prediction.
__label__reinforcement_learning A teacher-student framework is utilized to develop HumanVLA.
__label__machine_vision Notably, our method is not only training-free but also circumvents the necessity for hyper-parameter tuning.
__label__learning_theory Predictions are made using observed *noisy* labels and noiseless features, while the performance is  measured via minimax risk when comparing against *true* labels.
__label__interpretability_and_explainability Finally, to corroborate our empirical findings, we prove that Transformers can implement $k$ iterations of Newton's method with $k + \mathcal O(1)$ layers.
__label__probabilistic_methods Our proposed method utilizes ideas from classical dictionary-based CS and, in particular, sparse Bayesian learning (SBL), to integrate a strong regularization towards sparse solutions.
__label__natural_language_processing Using PV-Tuning, we achieve the first Pareto-optimal quantization for Llama-2 family models at  2 bits per parameter.
__label__machine_vision By leveraging underutilized processors and data memory for channel extension, DEX facilitates parallel execution without increasing inference latency.
__label__graph_neural_networks Our code is available at https://github.com/JHL-HUST/LMSPS.
__label__diffusion_based_models Through empirical and theoretical analysis, we demonstrate the capability of our approach to outperform the best designs in offline data, leveraging the extrapolation capabilities of reward models while avoiding the generation of invalid designs through pre-trained diffusion models.
__label__machine_vision Experiments on three benchmarks, including ScanNet20, ScanRefer, and ScanNet200, demonstrate that the UniSeg3D consistently outperforms current SOTA methods, even those specialized for individual tasks.
__label__probabilistic_methods By imposing constraints on the structure of such circuits, certain inference queries become tractable, such as model counting and most probable configuration.
__label__machine_vision Improvements that have been proposed require either an empirical pre-set pruning ratio or importance score threshold to prune the point cloud.
__label__machine_vision To minimize information loss caused by the compression on visual tokens while maintaining training efficiency, we develop LLaVolta as a light and staged training scheme that incorporates stage-wise visual context compression to progressively compress the visual tokens from heavily to lightly compression during training, yielding no loss of information when testing.
__label__optimization This method can also be seen as an optimization-based derandomization approach, and is an idea and method that we believe can be applied to many other problems.
__label__online_learning We study a dynamic pricing problem for third-party platform service fees under strategic, far-sighted customers.
__label__machine_learning_for_physical_sciences We show that a single MPP-pretrained transformer is able to match or outperform task-specific baselines on all pretraining sub-tasks without the need for finetuning.
__label__machine_vision Consequently, the reconstructed objects fail to withstand real-world physical forces, resulting in instability or undesirable deformation -- diverging from their intended designs as depicted in the image.
__label__bandits To address this, we leverage human response times, which inversely correlate with preference strength, as complementary information.
__label__bandits In this work, we apply BO to functions that exhibit invariance to a known group of transformations.
__label__machine_vision To this end, we propose Multi-Path Aggregation (MPA) integrated into existing coding models for joint human-machine vision, unifying the feature representation with an all-in-one architecture.
__label__machine_learning_for_physical_sciences Macroscopic observables of a system are of keen interest in real applications such as the design of novel materials.
__label__evaluation In summary, while LLMs exhibit the ability to memorize common patterns of popular DS API usage through massive training, they overall lack genuine comprehension of the underlying numerical constraints.
__label__infrastructure With the remarkable achievements of large language models (LLMs), the demand for fine-tuning and deploying LLMs in various downstream tasks has garnered widespread interest.
__label__natural_language_processing Recently, to address these challenges, some NLP studies introduce non-learnable In-Context Vectors (ICVs) which extract useful task information from ICDs into a single vector and then insert it into the LLM to help solve the corresponding task.
__label__diffusion_based_models Based on these findings, we build two types of efficient text-to-image models, called KOALA-Turbo & -Lightning, with two compact U-Nets (1B & 700M), reducing the model size up to 54% and 69% of the SDXL U-Net.
__label__neuroscience_and_cognitive_science This dataset fills the gap in the lack of EEG-video pairs.
__label__generative_models Surprisingly, it turns out that the existing solvers are either based on heuristic principles or heavy-weighted with complex optimization objectives involving several neural networks.
__label__machine_learning_for_other_sciences_and_fields Tabular data – structured, heterogeneous, spreadsheet-style data with rows and columns – is widely used in practice across many domains.
__label__machine_vision **GenIR**, our pioneering contribution, is a dual-prompt learning pipeline that overcomes the limitations of existing datasets, which typically comprise only a few thousand images and thus offer limited generalizability for larger models.
__label__reinforcement_learning We propose integrating risk-averse principles into LLM fine-tuning to minimize the occurrence of harmful outputs, particularly rare but significant events.
__label__deep_learning_architectures Empirical experimental results and analyses on four public time series datasets demonstrate the effectiveness of our proposed method over other state-of-the-art benchmarks.
__label__machine_vision However, due to not being specifically trained on 3D data, their application to multi-view data often exacerbates inconsistency, hence impacting the overall quality of the 3D output.
__label__causal_inference Our theory reveals the intricate interplay between the underlying manifold's smoothness and the shift properties.
__label__optimization_for_deep_networks How to balance the learning ’sensitivity-stability’ upon new task training and memory preserving is critical in CL to resolve catastrophic forgetting.
__label__reinforcement_learning We further demonstrate how Diff-SR facilitates efficient policy optimization and practical algorithms while explicitly bypassing the difficulty and inference cost of sampling from the diffusion model.
__label__optimization_for_deep_networks This approach enables the generation of adaptation matrices with varying ranks across different layers, providing greater flexibility in adapting pre-trained models.
__label__probabilistic_methods We consider the design of practically-implementable schemes for the task of channel simulation.
__label__machine_learning_for_social_sciences However, the uneven distribution of visitations over time and space, namely the long-tail problem in spatial distribution, makes it difficult for AI models to predict those POIs that are less visited by humans.
__label__optimization_for_deep_networks To evaluate the generality of our method across various tasks, we conduct experiments on natural language understanding (NLU), natural language generation (NLG), and large model instruction tuning tasks.
__label__natural_language_processing Initiating with a broad initial vocabulary, we refine our tokenizer by monitoring changes in the model’s perplexity during training, allowing for the selection of a tokenizer that is closely aligned with the model’s evolving dynamics.
__label__deep_learning_architectures Notably, our combination of OP block and non-diagonal preconditioner (SOAP) achieves 14.87 weight-and-activation int8 perplexity (from 14.71 in standard precision), compared to 63.4 int8 perplexity (from 16.00) with a default OF-prone combination of Pre-Norm model and Adam, when quantising OPT-125m models post-training.
__label__privacy Private data analysis faces a significant challenge known as the curse of dimensionality, leading to increased costs.
__label__machine_vision Then, we generate pairs of high-resolution image and accurate segmentation mask using a multi-conditional control generation method.
__label__machine_vision This limits the broader application of pre-trained ViT models, especially when the model is computationally extensive.
__label__neuroscience_and_cognitive_science Localized receptive fields—neurons that are selective for certain contiguous spatiotemporal features of their input—populate early sensory regions of the mammalian brain.
__label__human-AI_interaction The recent rapid advancement of machine learning has been driven by increasingly powerful models with the growing availability of training data and computational resources.
__label__optimization We support our theory with strong empirical performance of using non-linear scalarizations that outperforms both their linear counterparts and other standard multiobjective algorithms in a variety of natural settings.
__label__speech_and_audio Furthermore, STAR exhibits high data efficiency that only requires less than one-hour unlabeled data, and seamless generality to alternative large speech models and speech translation tasks.
__label__machine_vision Diverse human motion prediction (HMP) is a fundamental application in computer vision that has recently attracted considerable interest.
__label__machine_learning_for_other_sciences_and_fields Model checking answers the question of whether every execution of a given system satisfies a desired temporal logic specification.
__label__learning_theory Leave-one-out CV can have a smaller bias as compared to plug-in; however, this bias improvement is negligible compared to the variability of the evaluation, and in some important cases leave-one-out again does not outperform plug-in once this variability is taken into account.
__label__machine_learning_for_healthcare However, existing molecular representation learning methods often introduce potential false positive and false negative pairs through conventional graph augmentations like node masking and subgraph removal.
__label__natural_language_processing Our experiments demonstrate that the proposed SignCL can significantly reduce the representation density and improve performance across various translation frameworks.
__label__reinforcement_learning However, current research for decision-making, like reinforcement learning (RL), has primarily required numerous real interactions with the target environment to learn a skill, while failing to utilize the existing knowledge already summarized in the text.
__label__diffusion_based_models Visual text generation has significantly advanced through diffusion models aimed at producing images with readable and realistic text.
__label__machine_vision With minimal fine-tuning on all downstream tasks, our model significantly outperforms existing methods on benchmarks including ScanRefer, Multi3DRefer, Scan2Cap, ScanQA, and SQA3D.
__label__generative_models This paper provides the first theoretical probe into the identifiability of hybrid-DGMs, and present meta-learning as a novel solution to construct identifiable hybrid-DGMs.
__label__online_learning This bound is tight and provides an answer to an open question previously posed and studied by Daniely and Helbertal ['13] and by Long ['17, '20], who focused on deterministic learners.
__label__online_learning Experiments show that BGE can generate better discriminative representation in CCSSL, especially for inter-task data, and improve classification results with various external data compositions.
__label__machine_vision Recent progress in self-supervised (SSL) visual representation learning has led to the development of several different proposed frameworks that rely on augmentations of images but use different loss functions.
__label__machine_vision Additionally, results on roaming datasets demonstrate that ODGS effectively restores fine details, even when reconstructing large 3D scenes.
"__label__learning_theory Prior algorithmic work in this setting had focused 
on learning in the realizable case or in the presence 
of semi-random noise."
__label__reinforcement_learning Inspired by the recent dominance of diffusion models in generative modeling, we propose Diffusion-Reward Adversarial Imitation Learning (DRAIL), which integrates a diffusion model into GAIL, aiming to yield more robust and smoother rewards for policy learning.
__label__reinforcement_learning In this paper, we model Intermittent control problem as an Intermittent Control Markov Decision Process, i.e agents are expected to generate action sequences corresponding to the unavailable states and transmit them before disabling interactions to ensure the smooth and effective motion of executors.
__label__probabilistic_methods Particle-based Bayesian deep learning often requires a similarity metric to compare two networks.
__label__fairness Our findings can be summarized as follows: (1) models which are calibrated out of the box tend to be relatively multicalibrated without any additional post-processing; (2) multicalibration can help inherently uncalibrated models and also large vision and language models; and (3) traditional calibration measures may sometimes provide multicalibration implicitly.
__label__machine_vision Alternatively, there exists a wide range of expert models that contain diversified vision and/or language knowledge pre-trained on different modalities, tasks, networks, and datasets.
__label__diffusion_based_models The existing methods mostly interleave iterative steps in the reverse diffusion process and iterative steps to bring the iterates closer to satisfying the measurement constraint.
__label__generative_models Our pipeline is further extended to a hierarchical and iterative inpainting process to continuously generate the placement of large furniture and small objects to enrich the scene.
__label__probabilistic_methods We first formally define the MAP and Marginal MAP tasks for LCNs and subsequently show how to solve these tasks exactly using search-based approaches.
__label__probabilistic_methods Learning the unique directed acyclic graph corresponding to an unknown causal model is a challenging task.
__label__graph_neural_networks These datasets are inadequate for meaningful training of deep learning methods.
__label__interpretability_and_explainability On the other hand, reasoning on indicator variables that represent missingness introduces a potentially large number of additional terms, sacrificing sparsity.
__label__deep_learning_architectures Integrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures.
__label__other Our approach leverages the inherent structure of class relationships, enabling models to reduce the specificity of their predictions when faced with uncertainty.
__label__machine_vision LightGaussian achieves an average 15 times compression rate while boosting FPS from 144 to 237 within the 3D-GS framework, enabling efficient complex scene representation on the Mip-NeRF 360 and Tank & Temple datasets.
__label__optimization The Gromov-Wasserstein (GW) distance has gained increasing interest in the machine learning community in recent years, as it allows for the comparison of measures in different metric spaces.
__label__infrastructure Experiment results show that ArkVale performs well on various long context tasks with negligible accuracy loss under 2k$\sim$4k cache budget and can improve decoding latency to $2.2\times$ and batching throughput to $4.6\times$ because it applies attention on only a small subset of pages and reduce per-sample memory usage of KV cache.
__label__learning_theory In practical settings, the subset is often of small size, as in the ``ten blue links'' of web search.
__label__diffusion_based_models Building upon this finding, we utilize MLLMs to perform fine-grained video preference annotations across two dimensions, resulting in the creation of VideoPrefer, which includes 135,000 preference annotations.
__label__neuroscience_and_cognitive_science Here, we examine the relationship between interacting attention heads and human episodic memory.
__label__graph_neural_networks However, current methods are mostly limited to discrete graph filtering operations.
__label__machine_vision However, training MoE models from scratch requires extensive data and computational resources, a challenge that limits their widespread adoption.
__label__reinforcement_learning In this paper, we propose a general framework for addressing CRL problems via *gradient-based primal-dual* algorithms, relying on an alternate ascent/descent scheme with dual-variable regularization.
__label__speech_and_audio Additionally, we develop an acoustic simulation platform, AcoustiX, which provides more accurate and realistic IR simulations than existing simulators.
__label__reinforcement_learning Moreover, our study shows that speculative execution can enhance the NN cache hit rate by 26\% during midgame.
__label__reinforcement_learning These findings highlight the benefits of leveraging inner states and simulations to predict future agent actions and events, thereby improving interaction and safety in real-world deployments.
__label__machine_vision This allows us to apply our method to few-shot dataset distillation problems and alleviate the black-box characteristics of deep learning models.
__label__deep_learning_architectures Numerous industrial sectors necessitate models capable of providing robust forecasts across various horizons.
__label__machine_learning_for_healthcare Concurrently, we employ multiple cascading style fusion modules that utilize point-wise instance normalization to progressively recombine content and style features, which enhances cross-modal alignment and structural consistency.
__label__graph_neural_networks Despite the success of WTGIA, we discover that defenders can easily enhance their defenses with customized text embedding methods or large language model (LLM)--based predictors.
__label__causal_inference We develop an adaptive strategy to design indirect experiments that optimally inform a targeted query about the ground truth mechanism in terms of sequentially narrowing the gap between an upper and lower bound on the query.
__label__optimization_for_deep_networks We first (a) elect a unified model from all the model weights and then (b) generate extremely lightweight task-specific modulators, including masks and rescalers, to align the direction and magnitude between the unified model and each specific model, respectively.
__label__generative_models To achieve this, GRPO adaptively and sequentially weights the importance of different groups, prioritizing groups with worse cumulative loss.
__label__bandits In particular, we show that, under standard assumptions, for a $d$-dimensional stochastic linear bandit with an interaction horizon $T$, ensemble sampling with an ensemble of size of order $\smash{d \log T}$ incurs regret at most of the order $\smash{(d \log T)^{5/2} \sqrt{T}}$.
__label__natural_language_processing Our approach achieves lossless (mostly improved) LoRA transfer between models within and across different base model families, and even between different PEFT methods, on a wide variety of tasks.
__label__probabilistic_methods Additionally, similar to Dirichlet prior networks, our model parameterizes a conjugate prior enabling its application for uncertainty quantification.
"__label__learning_theory Neural networks excel at discovering statistical patterns in
high-dimensional data sets."
__label__other Empirical experiments on both synthetic and real-world datasets further consolidate our theoretical findings.
__label__neuroscience_and_cognitive_science Our model achieves state-of-the-art performance on the 61-word classification task, surpassing all baselines.
__label__learning_theory Despite the prevalence of this approach, the inductive biases that arise from learning multiple tasks are poorly characterized.
__label__fairness We then present the DRO dual formulation as an efficient tool to convert the main problem into a more tractable and computationally efficient form.
__label__machine_vision These non-strict data alignment limits representation quality and downgrade application performance.
__label__robotics Current methods predominantly rely on LiDAR or camera inputs for 3D occupancy prediction.
__label__machine_vision TAPTRv2 improves TAPTR by addressing a critical issue regarding its reliance on cost-volume, which contaminates the point query’s content feature and negatively impacts both visibility prediction and cost-volume computation.
__label__machine_vision Consequently, it is intuitive to leverage the wealth of annotations in 2D images to alleviate the inherent data scarcity in OV-3Det.
__label__machine_vision It is shown to be less restrictive compared to previous measures and achieves state-of-the-art results when used as a loss function in training networks for partial shape matching.
__label__machine_vision Then, we introduce a novel method to flatten the high-loss region between minima from different modalities by interpolating mixed multi-modal representations.
__label__safety_in_machine_learning We support our findings with theoretical motivations and empirical observations, and run extensive ablations to provide insights into why WeiPer works.
__label__privacy However, the clipping operation induces bias, which is serious if the sample distribution is heavy-tailed.
__label__evaluation We formulate the problem as a Markov Decision Process over states defined by posterior beliefs on model performance.
__label__machine_vision If acheived, it can enhance user experience by enabling novel applications, e.g., 3D video conferencing and live volumetric video broadcast, among others.
__label__learning_theory In addition, prior to this work, even in the special case of Gaussian design $\mathcal{D} = N(0,\Sigma)$ and noise $\eta \sim N(0,1)$, no polynomial time algorithm was known to achieve error $o(\sqrt{\varepsilon})$ in the sparse setting $n < d^2$.
__label__reinforcement_learning In this paper, we propose an offline IL approach that leverages the larger set of sub-optimal demonstrations while effectively mimicking expert trajectories.
__label__diffusion_based_models To address these limitations, we propose Hyper-SD, a novel framework that synergistically amalgamates the advantages of ODE Trajectory Preservation and Reformulation, while maintaining near-lossless performance during step compression.
__label__graph_neural_networks This theoretical insight led us to integrate GNNs with LLMs to enhance overall performance.
__label__graph_neural_networks Our approach requires no information on the ground-truth number of steps of the algorithm, both during train and test time.
__label__machine_learning_for_other_sciences_and_fields Furthermore, for a given input data sample $y$ belonging to the $l$th manifold $\mathcal{M}_l$, we provide geometric conditions that guarantee a manifold-preserving representation of $y$ can be recovered from the solution to the proposed model.
__label__generative_models By pairing historical and future trajectories and applying contrastive learning on the encoded feature space, we enforce same-space consistency constraints.
__label__diffusion_based_models The ability to drive our diffusion model via MA-fBM offers flexibility and control.
__label__machine_vision Concretely, PTSA uses persistent homology to align the topological structures of the input and latent spaces, effectively preserving the structure information and improving the generalization performance of FR model.
__label__fairness However, we also present a new polynomial-time algorithm for computing the entire Pareto front when the cluster centers are fixed, and for perhaps the most natural fairness objective: minimizing the sum, over all clusters, of the imbalance between the two groups in each cluster.
__label__natural_language_processing Our approach consists of two key steps.
__label__reinforcement_learning Code is available at https://github.com/Improbable-AI/hepo.
__label__infrastructure To address this, we introduce a family of memory efficient building blocks with controllable activation memory, which can reduce the peak activation memory to 1/2 of 1F1B without sacrificing efficiency, and even to 1/3 with comparable throughput.
__label__machine_vision Since 3D scans are generally accompanied by multi-view images, leveraging 2D convolutional neural networks allows these images to be exploited as a rich source for extracting extrusion cylinder information.
__label__reinforcement_learning Project page: https://nturobotlearninglab.github.io/DRAIL/
__label__machine_learning_for_other_sciences_and_fields To address this, we propose ReMAP, a novel model repurposing strategy that leverages deep learning's reprogramming property, incorporating network inversion principles and retrieval-augmented mapping.
__label__other Recent advances have examined the effectiveness of tail task risk minimization in fast adaptation robustness improvement \citep{wang2023simple}.
__label__learning_theory Although both low nuclear norm and low rank regularization have been studied for these models, a unified understanding of when, how, and why they achieve different implicit regularization effects remains elusive.
__label__reinforcement_learning To achieve the distinguishability among trajectory representations of different agents, we introduce contrastive learning to maximize the mutual information between the trajectory representations and learnable identity representations of different agents.
__label__online_learning By designing its regularizer and learning rate to be adaptive to past observations, FTRL is known to work adaptively to various properties of an underlying environment.
__label__probabilistic_methods We propose novel active learning acquisition functions that directly target key quantities of derivative-based global sensitivity measures (DGSMs) under Gaussian process surrogate models.
__label__bandits To date, the best regret bound in the literature scales as $k n^{1/3} T^{2/3}$.
__label__deep_learning_architectures In this work we propose an alternative formulation of this class of models using random features, commonly used in kernel methods.
__label__diffusion_based_models This paper presents Diffusion Forcing, a new training paradigm where a diffusion model is trained to denoise a set of tokens with independent per-token noise levels.
__label__bandits For example, in online marketplaces, the revenue of a good depends on discounts applied to competing goods.
__label__machine_vision Considering this, we present OccFusion, an approach that utilizes efficient 3D Gaussian splatting supervised by pretrained 2D diffusion models for efficient and high-fidelity human rendering.
__label__generative_models In particular, it can generate consistent interaction sequences and explainable reward functions at interaction states, opening the path for training generative models of the future.
__label__natural_language_processing At the same time, multi-agent LLM systems, involving automated interactions among agents, are also increasing in prominence.
__label__deep_learning_architectures Our visualization results also demonstrate that the soft clustering module produces a meaningful semantic grouping effect with only IN1k classification supervision, which may induce better interpretability and inspire new weakly-supervised semantic segmentation approaches.
__label__deep_learning_architectures With this comprehensive benchmark at hand, we analyze the factors that contribute to the OoD generalization of ViT architecture.
__label__graph_neural_networks Associating pseudo nodes to input graphs with their measured relations, graph nodes can communicate with each other intermediately through pseudo nodes under linear complexity.
__label__machine_vision in form of jittery motion and struggle to achieve smooth and physically plausible motions.
__label__deep_learning_architectures The main reasons come from the lack of concern on the feature correlation during interaction, and the limited receptive field.
__label__safety_in_machine_learning Past analyses of reinforcement learning from human feedback (RLHF) assume that the human evaluators fully observe the environment.
__label__interpretability_and_explainability Meanwhile, we provide a comprehensive analysis of the optimization and generalization efficiency for the A-FedPD method on smooth non-convex objectives, which confirms its high efficiency and practicality.
__label__machine_vision Experiments on several benchmarks and in-the-wild tests demonstrate that Hamba significantly outperforms existing SOTAs, achieving the PA-MPVPE of 5.3mm and F@15mm of 0.992 on FreiHAND.
__label__natural_language_processing In this paper, we delve into the computation graph of the language model to uncover the knowledge circuits that are instrumental in articulating specific knowledge.
__label__machine_vision It accepts intuitive user input, such as point clicks, coarse scribbles, or text.
__label__deep_learning_architectures **Our OoD-NAS-ViT benchmark and code are available at [https://hosytuyen.github.io/projects/OoD-ViT-NAS](https://hosytuyen.github.io/projects/OoD-ViT-NAS)**
__label__learning_theory We first introduce the adaptive lasso estimator for rare-events data and establish its oracle properties, thereby validating the use of subsampling.
__label__machine_vision Furthermore, to capture the complicated interplay between subjects and objects, we propose a new lightweight module called mutual visual adapter.
__label__causal_inference Researchers have alleviated this issue by simulating causal relations with neural models.
__label__diffusion_based_models Advances in 3D reconstruction have enabled high-quality 3D capture, but require a user to collect hundreds to thousands of images to create a 3D scene.
"__label__evaluation We develop a family of magnitude-based measures of the intrinsic
diversity of latent representations, formalising a novel notion of
dissimilarity between magnitude functions of finite metric spaces."
__label__diffusion_based_models Recently, a series of diffusion-aware distillation algorithms have emerged to alleviate the computational overhead associated with the multi-step inference process of Diffusion Models (DMs).
__label__safety_in_machine_learning harmful) examples, which are much easier and cheaper to collect (e.g.
__label__learning_theory This algorithm can be implemented by a few tweaks to the most popular induction scheme for decision tree induction (*i.e.
__label__safety_in_machine_learning We provide an extensive evaluation by considering six text domains across twelve aligned LLMs, where our method demonstrates state-of-the-art results.
__label__other Our source code and datasets are available at https://github.com/andylolu2/ollm.
__label__machine_vision Comprehensive empirical analyses demonstrate that our method yields substantial performance enhancements across both known and unknown classes in comparison to previous studies.
__label__diffusion_based_models We present Piecewise Rectified Flow (PeRFlow), a flow-based method for accelerating diffusion models.
__label__other Taking a locate-then-edit approach, we first address the ``where-to-edit`` challenge by meta-learning a hypernetwork on CutMix-augmented data generated for editing reliability.
__label__machine_vision Prevailing image reconstruction methods, generating intermediate frames from these spike streams, often rely on complex step-by-step network architectures that overlook the intrinsic collaboration of spatio-temporal complementary information.
__label__privacy In this paper, we found out that the popular diffusion models have introduced a new vulnerability to FL, which brings serious privacy threats.
"__label__diffusion_based_models The main contributions of our work are the following: 
we present systematic experimental study of these points, 
we propose a novel conditioning mechanism that disentangles semantic and low-level conditioning, 
we obtain state-of-the-art performance  on CC12M for text-to-image at 512 resolution."
__label__optimization By maintaining the canonical form through each iteration of ALS, we can efficiently compute (and sample from) the leverage scores, thus achieving significant speed-up in solving each sketched least-square problem.
__label__natural_language_processing Yet, in the context of LLMs, current methods for adversarial training are hindered by the high computational costs required to perform discrete adversarial attacks at each training iteration.
__label__optimization In particular, the set of learnable targets is not dense in $\mathbb R^d$, and any subset of $\mathbb R^d$ homeomorphic to the $W$-dimensional sphere contains non-learnable targets.
__label__machine_vision In this paper, we propose a new framework, High Dynamic Range Gaussian Splatting (HDR-GS), which can efficiently render novel HDR views and reconstruct LDR images with a user input exposure time.
__label__machine_vision In our testing, EDC achieves state-of-the-art accuracy, reaching 48.6% on ImageNet-1k with a ResNet-18 model at an IPC of 10, which corresponds to a compression ratio of 0.78%.
__label__generative_models Our data, code, and models are available at: https://github.com/ARiSE-Lab/SemCoder.
__label__learning_theory We then provide evidence that this mechanism holds for neural networks more generally.
__label__causal_inference In general, our results elegantly extend the identification boundary for causal discovery with discrete latent variables and expand the application scope of causal discovery with latent variables.
"__label__safety_in_machine_learning Machine learning models can often fail on subgroups that are underrepresented
during training."
__label__generative_models Unfortunately, most existing flow straightening methods are based on non-trivial iterative FM procedures which accumulate the error during training or exploit heuristics based on minibatch OT.
__label__machine_vision Spatial Tunability is constructed to the representation of distinct instances driven by prompts in the query space.
"__label__machine_vision PAnoramic Semantic Segmentation (PASS) is an important task in computer vision,
as it enables semantic understanding of a 360° environment."
__label__safety_in_machine_learning This restoration process is a practical issue overlooked in most existing unlearnable literature, i.e., even authorized users struggle to gain knowledge from 3D unlearnable data.
__label__machine_learning_for_healthcare Leveraging this, we introduce a physics-informed SSL algorithm that initially pretrains a neural network on the pretext task of learning a differentiable simulator of a physiological process.
__label__deep_learning_architectures Neural Episodic Control is a powerful reinforcement learning framework that employs a differentiable dictionary to store non-parametric memories.
__label__interpretability_and_explainability This integration seamlessly transforms the rule learning task into neural network training using backpropagation and stochastic gradient descent.
__label__privacy Thus, these reasons lead to high false-positive rates of MIAs in practical scenarios.
__label__machine_vision To this end, we introduce a new Diffusion-Inspired Truncated Sampler (DITS) that jointly performs progressive alignment and modality gap modeling in the joint embedding space.
__label__optimization We focus here on lifting two explicit algorithms: mirror descent and preconditioned gradient descent.
__label__privacy To efficiently leverage the benefits of Hessian modulation, we propose a fast-slow parameter update strategy to implicitly approximate the up-to-date salient unlearning direction.
__label__natural_language_processing In comprehensive in-domain and out-of-domain evaluation on 6 mathematical benchmarks, `DART-Math` outperforms vanilla rejection tuning significantly, being superior or comparable to previous arts, despite using much smaller datasets and no proprietary models.
__label__machine_vision Video understanding relies on accurate action detection for temporal analysis.
__label__learning_theory However, this simplified linear setting arguably does not demonstrate the statistical efficiency of ICL, since the pretrained transformer does not outperform directly solving linear regression on the test prompt.
__label__learning_theory bandwidth or privacy constraints that each server needs to satisfy.
__label__interpretability_and_explainability 2022, Xue et al.
__label__diffusion_based_models Empirical evaluations show that MBD outperforms state-of-the-art reinforcement learning and sampling-based TO methods in challenging contact-rich tasks.
__label__other Moreover, LeHaCE incorporates the curve slope as an innovative hallucination evaluation metric, reflecting the extent to which the object hallucination degree is affected by the image description length, achieving a more comprehensive evaluation.
__label__other A synergistic learning framework (BeTopNet) supervised by BeTop facilitates the consistency of behavior prediction and planning within the predicted topology priors.
__label__fairness However, such models frequently pick up inherent biases in the data during the training process, leading to unreliable predictions.
__label__interpretability_and_explainability Introducing pre-trained token embeddings to a randomly initialized model rescues its performance.
__label__deep_learning_architectures Many algorithms and observed phenomena in deep learning appear to be affected by parameter symmetries --- transformations of neural network parameters that do not change the underlying neural network function.
__label__neuroscience_and_cognitive_science Evaluated on the nuScenes dataset, SAD achieves competitive performance in perception, prediction, and planning tasks, while drawing upon the energy efficiency of SNNs.
__label__safety_in_machine_learning For example, our method achieves >80% (mostly >95%) ASRs on Llama-2-7B and Llama-3-8B without multiple restarts, even if the models are enhanced by strong defenses such as perplexity detection and/or SmoothLLM, which is challenging for suffix-based jailbreaking.
__label__diffusion_based_models However, the lack of diverse instruction-following data hampers the development of models that effectively recognize and execute user-customized instructions, particularly in low-level tasks.
__label__natural_language_processing Temporal Knowledge Graph Reasoning (TKGR) is the process of utilizing temporal information to capture complex relations within a Temporal Knowledge Graph (TKG) to infer new knowledge.
__label__generative_models Beyond its novelty, our method enjoys several advantageous properties: (i) we establish quality bounds for the recovered solution; (ii) this approach seamlessly interconnects with the Energy-Based Models (EBMs) learning procedure enabling the use of well-tuned algorithms for the problem of interest; (iii) it provides an intuitive optimization scheme avoiding min-max, reinforce and other intricate technical tricks.
__label__machine_vision However, these researches are mostly confined to pre-training at the instance-level or single-video tracklet-level.
__label__optimization Experiments show that our approach outperforms existing methods as it can significantly reduce costs on real-world cities.
__label__natural_language_processing With the two-stage pipeline, we can scale the candidate sets to 100K papers, vastly exceeding the size handled by existing methods.
__label__machine_learning_for_other_sciences_and_fields Anomaly detection methods typically require fully observed data for model training and inference and cannot handle incomplete data, while the missing data problem is pervasive in science and engineering, leading to challenges in many important applications such as abnormal user detection in recommendation systems and novel or anomalous cell detection in bioinformatics, where the missing rates can be higher than 30\% or even 80\%.
__label__deep_learning_architectures Motivated by this observation, we propose CherryQ, a novel quantization method that unifies the optimization of mixed-precision parameters.
__label__safety_in_machine_learning Our key finding is that as these aligned LLMs are trained to maximize the human preferences, they generate texts with higher estimated preferences even than human-written texts; thus, such texts are easily detected by using the reward model (i.e., an LLM trained to model human preference distribution).
__label__natural_language_processing By adopting the latter approach, we can naturally emphasize informative low-frequency words in terms of their vector norm, which becomes evident from the information-geometric perspective, and in terms of the loss functions for imbalanced classification.
__label__natural_language_processing As a result, our guidelines facilitate the provision of relevant knowledge for the agent's current decision-making process, overcoming the limitations of the conventional demonstration-based learning paradigm.
__label__generative_models Generative models based on flow matching have attracted significant attention for their simplicity and superior performance in high-resolution image synthesis.
__label__causal_inference The DPPL method is capable of obtaining optimal policies even when multiple rewards are interrelated.
__label__generative_models Large Reconstruction Models have made significant strides in the realm of automated 3D content generation from single or multiple input images.
__label__machine_vision We study a novel problem to automatically generate video background that tailors to foreground subject motion.
__label__human-AI_interaction Accordingly, we propose $\textbf{C}$ollaborative $\textbf{B}$ayesian $\textbf{P}$olicy $\textbf{R}$euse ($\textbf{CBPR}$), a novel Bayesian-based framework that $\textbf{adaptively selects optimal collaborative policies matching the current meta-task from multiple policy networks}$ instead of just selecting actions relying on a single policy network.
__label__diffusion_based_models Rigorous theoretical analysis reveals that our algorithm achieves $\widetilde{\mathcal{O}}(\mathrm{poly} \log d)$ overall time complexity, marking \emph{the first implementation with provable sub-linear complexity w.r.t.
__label__probabilistic_methods We further show that this performance carries over to classical Bayesian optimization without explicit evaluation costs.
__label__machine_learning_for_healthcare Accurately restoring topology is both challenging and crucial in tubular structure extraction tasks, such as blood vessel segmentation and road network extraction.
__label__bandits Unlike traditional MAB problems, the reward of each unit depends on the treatments assigned to other units, i.e., there is *interference* across the underlying network of units.
__label__evaluation We propose two such distance metrics, KL-Divergence and flips, and show that they are well correlated.
__label__deep_learning_architectures A challenging problem in many modern machine learning tasks is to process weight-space features, i.e., to transform or extract information from the weights and gradients of a neural network.
__label__causal_inference The conditional quantile treatment effect (CQTE) can provide insight into the effect of a treatment beyond the conditional average treatment effect (CATE).
__label__diffusion_based_models Beyond simulation, we directly deploy policies generated by **Make-An-Agent** onto real-world robots on locomotion tasks.
__label__evaluation Recent work on studying memorization in self-supervised learning (SSL) suggests that even though SSL encoders are trained on millions of images, they still memorize individual data points.
__label__safety_in_machine_learning To address it, we re-investigate the characteristics of the backdoored models after defense (denoted as defense models).
__label__online_learning We first establish an upper bound of $1-\kappa$ on the competitive ratio for any deterministic online algorithm.
__label__machine_vision To this end, we introduce EVA, a drivable human model that can recover fine details based on 3D Gaussians and an expressive parametric human model, SMPL-X.
__label__machine_learning_for_other_sciences_and_fields Unsurprisingly, however, models struggle to compose syntactically valid programs in programming languages unrepresented in pre-training, referred to as very low-resource Programming Languages (VLPLs).
__label__machine_vision Compared with the prior art, Scala achieves an average improvement of 1.6% on ImageNet-1K with fewer parameters.
__label__interpretability_and_explainability It provably reduces the upper bound of generalization error.
__label__machine_vision Firstly, a well-designed graph structure is employed to model event data, which not only preserves the original temporal data but also captures spatial details.
__label__infrastructure The code is publicly available at https://github.com/sgl-project/sglang.
__label__reinforcement_learning However, pure modified rewards only ensure the behavior of the learned policy in the source domain resembles trajectories produced by the target optimal policies, which does not guarantee optimal performance when the learned policy is actually deployed to the target domain.
__label__safety_in_machine_learning In this paper, we circumvent this problem by leveraging the concept of textual entailment to evaluate the correctness of the generated sequence, and propose two selective generation algorithms which control the false discovery rate with respect to the textual entailment relation (FDR-E) with a theoretical guarantee: $\texttt{SGen}^{\texttt{Sup}}$ and $\texttt{SGen}^{\texttt{Semi}}$.
__label__machine_vision Real-world Image Dehazing (RID) aims to alleviate haze-induced degradation in real-world settings.
__label__natural_language_processing COGEX works by (1) training LMs to generate pseudo-programs and (2) teaching them to emulate their generated program’s execution, including those leaf functions, allowing the LM’s knowledge to fill in the execution gaps; and (3) using them to search over many programs to find an optimal one.
__label__generative_models Importantly, our method incurs minimal additional computational overhead, rendering it highly efficient compared to alternative data scaling strategies.
__label__interpretability_and_explainability Finally, we present a detailed case study where entropy neurons actively manage confidence: the setting of induction, i.e.
__label__natural_language_processing Public benchmarks play an essential role in the evaluation of large language models.
__label__machine_vision Notably, for several highly dynamic scenes, it reduces the model size to just 0.7 MB per frame while training in under 5 sec and rendering at ~350 FPS.
__label__interpretability_and_explainability To further address this problem, we propose a novel Aligned Federated Primal Dual (A-FedPD) method, which constructs virtual dual updates to align global consensus and local dual variables for those protracted unparticipated local clients.
__label__graph_neural_networks To mitigate this drawback, we introduce a novel algorithm, Fast Graph Sharpness-Aware Minimization (FGSAM), that integrates the rapid training of Multi-Layer Perceptrons (MLPs) with the superior performance of GNNs.
__label__deep_learning_architectures Moreover, we optimize the visual encoder to disentangle the cross-layer dependency for fine-grained decomposition of search space, so that the search cost is further reduced without harming the quantization accuracy.
__label__privacy Code is available at [Unified-Unlearning-w-Remain-Geometry](https://github.com/K1nght/Unified-Unlearning-w-Remain-Geometry).
__label__learning_theory In this paper, we introduce an end-to-end quantum algorithm for linear-quadratic control with provable speedups.
__label__interpretability_and_explainability To the best of our knowledge, this is the first result of its kind highlighting the role of initialization.
__label__machine_vision The code and models will be open-sourced.
__label__safety_in_machine_learning These modules engage in an adversarial game, enhancing cross-camera stability.
__label__diffusion_based_models Text-conditioned motion synthesis has made remarkable progress with the emergence of diffusion models.
__label__neuroscience_and_cognitive_science Predictive coding (PC) is an energy-based learning algorithm that performs iterative inference over network activities before updating weights.
__label__graph_neural_networks Inspired by the Braess phenomenon, we argue that deleting edges can address over-squashing and over-smoothing simultaneously.
__label__neuroscience_and_cognitive_science To this end, we train path-integrating recurrent neural networks (piRNNs) on a spatial navigation task, whose goal is to predict the agent's position with a special focus on rewarded locations.
__label__reinforcement_learning However, existing methods often struggle with ineffective exploration and unstable updates when learning action and option policies simultaneously.
__label__optimization_for_deep_networks Theoretically, we show that our training strategy achieves a better sharpness-diversity trade-off.
__label__reinforcement_learning In most existing studies, specific preferences must be provided during deployment to indicate the desired policies explicitly.
__label__machine_vision Moreover, we demonstrate the efficacy of DDR across a spectrum of applications.
__label__interpretability_and_explainability We further observe correspondence between the low performance when masking out entire layers, and the task recognition layers.
__label__deep_learning_architectures Through extensive experiments on several datasets and state-of-the-art baselines, we show that incorporating S3 results in significant improvements for the tasks of time-series classification, forecasting, and anomaly detection, improving performance on certain datasets by up to 68\%.
__label__deep_learning_architectures Despite its widespread use in neural networks, error backpropagation has faced criticism for its lack of biological plausibility, suffering from issues such as the backward locking problem and the weight transport problem.
__label__machine_vision Moreover, after training, the learned ISP pipelines are mostly fixed at the inference time, whose performance degrades in dynamic scenes.
__label__machine_learning_for_healthcare To address this issue, we introduce the U-shaped Connection (uC), utilizing simplified 2D U-Net in place of standard skip connections to augment the extraction of the axial-slice plane features while concurrently preserving the volumetric context afforded by 3D convolutions.
__label__machine_vision The 3D-to-2D transitional prior and task-shared knowledge is captured from the prompt space, and then (ii) Learning deep query.
__label__machine_vision Previous method relies on NeRF for geometry reasoning.
__label__causal_inference We propose a perturbation model that allows changes in the transition kernel densities up to a given multiplicative factor or its reciprocal, extending the classic marginal sensitivity model (MSM) for single time-step decision-making to infinite-horizon RL.
__label__reinforcement_learning Yet, it poses challenges due to substantial parameter size and limited scalability, which is particularly critical in sequential decision-making scenarios where resources are constrained such as in robots and drones with limited computational power.
__label__machine_learning_for_other_sciences_and_fields To this end, we develop an unsupervised learning approach based on the predicted pairwise contact map between a protein and a nucleic acid and demonstrate its effectiveness in protein-aptamer binding prediction.
__label__infrastructure Communication of quantum states that potentially limit the amount of information that can be extracted from them about the data and model parameters may also lead to improved privacy guarantees for distributed computation.
__label__machine_learning_for_physical_sciences Experimental results on real-world and synthetic data showcase the efficacy of FIDE over baseline methods, highlighting its potential in advancing Generative AI for time series analysis, specifically in accurately modeling extreme events.
__label__safety_in_machine_learning In this paper, we draw attention to a common feature of recent powerful LLMs, namely the alignment training, i.e., training LLMs to generate human-preferable texts.
__label__deep_learning_architectures To address these limitations, this paper proposes a novel LLM-based long-tailed data augmentation framework called LLM-AutoDA, which leverages large-scale pretrained models to automatically search for the optimal augmentation strategies suitable for long-tailed data distributions.
__label__safety_in_machine_learning Our research shows that EOT-based attacks face gradient dilemmas due to global gradient averaging, resulting in ineffective evaluations.
__label__privacy To the best of our knowledge, this is the first study on designing truthful (and privacy-preserving) mechanisms for high dimensional sparse linear regression.
__label__deep_learning_architectures On the other hand, we identify a design choice in current SSMs that limits their expressive power.
__label__graph_neural_networks A key innovation is the use of an adjacency matrix as a trainable parameter, optimized through a new adaptive learning rate technique called AdaRelation, which adjusts based on the historical sensitivity of the decoder to changes in the interaction graph.
__label__machine_vision Contemporary transformer-based point cloud recognition models, albeit advanced, tend to overfit to specific patterns, consequently undermining their robustness against corruption.
__label__deep_learning_architectures The implementation of our model is available at: https://github.com/dongbeank/CATS.
__label__machine_vision This setting assumes test samples are already divided into either base or novel classes, limiting its application to realistic scenarios.
__label__machine_vision To tackle these issues, we present a dual strategy: GenIR, an innovative data curation pipeline, and DreamClear, a cutting-edge Diffusion Transformer (DiT)-based image restoration model.
__label__interpretability_and_explainability Surprisingly, these points precisely correspond to the emergence of hidden capabilities, i.e., where latent interventions show the model possesses the capability to manipulate a concept, but these capabilities cannot yet be elicited via naive input prompting.
__label__diffusion_based_models Facial stylization is predominantly challenged by two significant hurdles.
__label__learning_theory A recent successful approach that falls under the JEPA framework is self-distillation, where an online encoder is trained to predict the output of the target encoder, sometimes with a lightweight predictor network.
__label__machine_learning_for_physical_sciences Experimentally, RoPINN consistently boosts the performance of diverse PINNs on a wide range of PDEs without extra backpropagation or gradient calculation.
__label__machine_vision However, traditional distillation strategies often fail in unlabeled data with inaccurate or noisy information, limiting their efficiency in feature spaces undergoing substantial changes during continual learning.
__label__natural_language_processing Our data and code are available at https://github.com/usail-hkust/UrbanKGent.
__label__robotics Moreover, by incorporating multiple simulation methods such as FEM and PBD, along with our proposed sim-to-real algorithms and real-world benchmark, we aim to significantly narrow the sim-to-real gap.
__label__learning_theory The reduction in sample complexity depends directly on the predictor’s quality, measured by its total variation distance from $p$.
__label__machine_vision Focused on enhancing expressiveness, our work makes three key contributions.
__label__machine_learning_for_social_sciences A collection (papers, codes, datasets) of deep group recommendation/intent learning methods is available on GitHub\footnote{https://github.com/yueliu1999/Awesome-Deep-Group-Recommendation}.
__label__online_learning Conversely, the opposite scenario is relatively benign.
__label__optimization_for_deep_networks We first demonstrate that input perturbations can be mimicked by multiplicative perturbations in the weight space.
__label__interpretability_and_explainability The concept of probabilistic values, such as Beta Shapley values and weighted Banzhaf values, has gained recent attention in applications like feature attribution and data valuation.
__label__machine_learning_for_other_sciences_and_fields To justify the prospect of our approach, we show that i) injecting spatial structural information of IMUs/joints learned from data improves accuracy, while ii) injecting temporal structural information based on smooth priors reduces jitter (i.e., improves steadiness), in a spatial-temporal transformer solution for inertial pose estimation.
__label__interpretability_and_explainability Or do they rely purely on collections of simple heuristics?
__label__machine_vision Then the task-specific decoding is carried out by flexibly routing the shared representation to lightweight task decoders with negligible training efforts.
__label__other In this work, we introduce a feature learning theory framework that provides a theoretical foundation for understanding the differences between multi-modal and single-modal contrastive learning.
__label__natural_language_processing Furthermore, we find that CoT reasoning is a crucial component for performance improvement, as removing the CoT reasoning results in a significant decrease in the overall performance of our method.
__label__interpretability_and_explainability Here, we show that these methods are not equivalent -- parametric methods retain global structure but lose significant local details.
__label__algorithmic_game_theory We complement our positive results, by exploring the limitations of known classes of strategyproof mechanisms that can be devised using output recommendation.
__label__deep_learning_architectures End-to-end transformer-based trackers have achieved remarkable performance on most human-related datasets.
__label__machine_learning_for_healthcare We show that the propensity score encapsulates all shared information between a latent state and treatment, and can be used to define a distance between samples.
"__label__learning_theory Specifically, we show that
any active learner requires label complexity of 
$\tilde{\Omega}(d/(\log(m)\epsilon))$, where $m$ is the number of unlabeled examples."
__label__natural_language_processing We discuss how the method we propose could be used to help researchers be more specific in the claims they make about large language models.
__label__evaluation We conduct an extensive evaluation of Elo behavior across simulated and real-world scenarios, demonstrating that individual Elo computations can exhibit significant volatility.
__label__machine_vision We introduce the Biomechanical Pose Generator (BPG), which leverages biomechanical principles, specifically the normal range of motion, to autonomously generate a wide array of plausible 3D poses without relying on a source dataset, thus overcoming the restrictions of popularity bias.
__label__safety_in_machine_learning Extensive experiments and analyses have been conducted to understand and verify the effectiveness of CoVer.
__label__machine_learning_for_other_sciences_and_fields This structure allows for synchronized cross-functional agent collaboration towards unified goals through natural language interactions and equips each agent with greater memory capacity than humans.
__label__deep_learning_architectures Through qualitative experiments, we demonstrate that ProxyFusion learns discriminative information for importance weighting of face features without relying on intermediate features.
__label__optimization The natural question is: must we randomize?
__label__machine_learning_for_other_sciences_and_fields In this paper, we present a novel method to interpret the decision-making processes of these models, which are essential for detecting malicious activities without labeled attack data.
__label__machine_vision Long-tailed semi-supervised learning poses a significant challenge in training models with limited labeled data exhibiting a long-tailed label distribution.
__label__neuroscience_and_cognitive_science Taking into account the temporal dependence of representations under movie stimuli, we present Time-Series Representational Similarity Analysis (TSRSA) to measure the similarity between model representations and visual cortical representations of mice.
__label__generative_models Extensive qualitative and quantitative experiments illustrate the superior performance of Ctrl-X on various condition inputs and model checkpoints.
__label__machine_learning_for_social_sciences To address these challenges, we introduce a computationally efficient framework that scales linearly with the number of origin-destination pairs, operates directly on the discrete combinatorial space, and learns the agents' trip intensity through a neural differential equation that embeds spatial interactions.
__label__fairness We discuss utilitarian and egalitarian welfare objectives, and we explore how to optimize for them under stochastic and robust paradigms.
__label__deep_learning_architectures D3 is a straightforward drop-in layer that can be seamlessly integrated into any TPR-based model without modifications.
__label__machine_learning_for_healthcare Our method incorporates three novel mechanisms to leverage the unique characteristics of MedTS: cross-channel patching to leverage inter-channel correlations, multi-granularity embedding for capturing features at different scales, and two-stage (intra- and inter-granularity) multi-granularity self-attention for learning features and correlations within and among granularities.
__label__diffusion_based_models Image diffusion distillation achieves high-fidelity generation with very few sampling steps.
__label__evaluation We demonstrate the applicability of Diff-eRank in both single-modal (e.g., language) and multi-modal settings.
__label__neuroscience_and_cognitive_science It then employs state-of-the-art generative diffusion models to synthesize behavior videos that interpret the neural dynamics of each latent factor.
__label__learning_theory Here, $\eta>0$ is arbitrarily small, $\omega\lesssim 2.372$ is the matrix multiplication exponent, $\kappa(S)=\lVert S\rVert_2\lVert S^{-1}\rVert_2$, and $\mathrm{gap}_k$ is the gap between eigenvalues $k$ and $k+1$.
__label__natural_language_processing Despite numerous proposals for effective methods, a substantial memory overhead remains for gradient computations during updates.
__label__diffusion_based_models Diffusion models have become a leading method for generative modeling of both image and scientific data.
__label__causal_inference Causal discovery is a fundamental problem with applications spanning various areas in science and engineering.
__label__reinforcement_learning Such an issue describes the scenario when the agent encounters states out of the offline dataset during the test phase, leading to uncontrolled behavior and performance degradation.
__label__machine_vision This allows for collectively guiding the network's meta-learning process with the aim of learning generalizable image feature embeddings, while not introducing any extra computational cost in the inference phase.
__label__deep_learning_architectures However, their prominent adaptation performances heavily rely on complex model architectures, posing an unprecedented challenge in deploying them on resource-limited devices for real-time monitoring.
__label__machine_vision The object abstractor, consisting of a prompt encoder and transformer blocks, introduces spatially-diverse open-world object queries to discover never before seen objects in videos.
__label__reinforcement_learning Inspired by the concept of Human-on-the-Loop and the daily human hierarchical control, we propose a novel knowledge-guided multi-agent reinforcement learning framework (hhk-MARL), which combines human abstract knowledge with hierarchical reinforcement learning to address the learning difficulties among a large number of agents.
__label__other To overcome this, researchers often replace recurrent models with Neural ODE-based architectures to account for irregularly sampled data and use  Transformer-based architectures to account for long-range dependencies.
"__label__probabilistic_methods The key insight is that,
  in the online setting,
  we do not need to add the KL term to regularize to the prior (which comes from the posterior at the previous timestep);
  instead we can optimize just the expected log-likelihood,
  performing a single step of natural gradient descent
  starting at the prior predictive."
__label__machine_learning_for_healthcare However, current algorithms for training Neural SDEs require backpropagation through the SDE dynamics, greatly limiting their scalability and stability.
__label__graph_neural_networks To apply graph transformers with HDSE to large-scale graphs, we further propose a high-level HDSE that effectively biases the linear transformers towards graph hierarchies.
__label__natural_language_processing Global human evaluators also note a nearly 20% higher winning rate in helpfulness and harmfulness evaluation.
__label__machine_learning_for_physical_sciences This work demonstrates significant advancements in molecular simulations by optimizing energy minimization and convergence speeds, offering a new, efficient framework for simulating complex molecular systems.
__label__safety_in_machine_learning Our VRCP method is the first to support perturbations bounded by arbitrary norms including $\ell_1$, $\ell_2$, and $\ell_\infty$, as well as regression tasks.
__label__machine_vision CODE utilizes the comprehensive descriptions from model itself as visual counterpart to correct and improve response alignment with actual visual content.
__label__natural_language_processing Unlike traditional methods that require careful curation of a mixture of datasets to achieve comprehensive improvement, we can quickly experiment with preference weightings using MOD to find the best combination of models.
__label__reinforcement_learning As demonstrations, we propose a supervised and a self-supervised implementation of $I(Z; M)$, and empirically show that the corresponding optimization algorithms exhibit remarkable generalization across a broad spectrum of RL benchmarks, context shift scenarios, data qualities and deep learning architectures.
__label__online_learning Most existing procedures suffer from high variance due to the use of importance sampling over sequences of actions.
__label__generative_models By leveraging signal provided by data attribution methods such as influence functions, SPA partitions data into subsets, each targeting unique aspects of the data, and trains multiple model adaptations optimized for these subsets.
__label__learning_theory Self-Distillation is a special type of knowledge distillation where the student model has the same architecture as the teacher model.
__label__optimization The Bahncard problem is a generalization of the ski-rental problem, where a traveler needs to irrevocably and repeatedly decide between a cheap short-term solution and an expensive long-term one with an unknown future.
__label__deep_learning_architectures To this end, we present Local-Global SIRENs - a novel INR architecture that supports cropping by design.
__label__deep_learning_architectures This is substantiated by few-shot learning experiments.
__label__learning_theory Furthermore, we show that the trained transformer presents non-trivial prediction ability with dataset shift, which sheds light on the remarkable generalization performance of transformers.
__label__machine_vision Our findings highlight the potential of leveraging deep representations to efficiently assess adversarial vulnerability in deployment scenarios.
__label__fairness Importantly, prior work has demonstrated that many methods are susceptible to noisy labels.
__label__other We then utilize our approach to: (a) identify non-conjugate training dynamics between shallow and wide fully connected neural networks; (b) characterize the early phase of training dynamics in convolutional neural networks; (c) uncover non-conjugate training dynamics in Transformers that do and do not undergo grokking.
__label__diffusion_based_models Previous works have endeavored to integrate diffusion priors into the maximum a posteriori estimation (MAP) framework and design optimization methods to solve the inverse problem.
__label__natural_language_processing Empirically, we demonstrate that our proposed framework enhances the decision-making capabilities of VLM agents across various tasks, enabling 7b models to outperform commercial models such as GPT4-V or Gemini.
__label__fairness In this paper, we propose a general min-max optimization framework that can achieve interventional fairness with promising prediction accuracy and can be extended to maximally oriented PDAGs (MPDAGs) with added background knowledge.
__label__graph_neural_networks To tackle the challenge, we propose the integration of Sharpness-Aware Minimization (SAM)--a technique designed to enhance model generalization by finding a flat minimum of the loss landscape--into GNN training.
__label__probabilistic_methods In this paper, we make use of conformal prediction for this purpose.
__label__optimization Additionally, CRA effectively eliminates artificial rounding and accelerates the learning process.
__label__graph_neural_networks Spatial Message Passing Graph Neural Networks (MPGNNs) are widely used for learning on graph-structured data.
__label__diffusion_based_models Rendering algorithms decompose and simulate the imaging process of a camera, while are limited by the accuracy of modeled variables and the efficiency of computation.
__label__reinforcement_learning We approach this problem by first establishing a policy difference lemma for the episodic setting, which provides the theoretical foundation for the algorithm.
__label__optimization_for_deep_networks However, existing theoretical results are restricted to either linear models, the last two layers or binary classification.
__label__reinforcement_learning We present Preference Flow Matching (PFM), a new framework for preference alignment that streamlines the integration of preferences into an arbitrary class of pre-trained models.
__label__other To mitigate this, existing offline optimizers have proposed numerous conditioning techniques to prevent the learned surrogate from being too erratic.
__label__machine_vision Guided by such insight, we propose a novel training paradigm named MC-DiT for fully learning contextual information via diffusion denoising at different noise variances with clean-to-clean mask-reconstruction.
__label__privacy Nevertheless, adhering to Euclidean space may result in sub-optimal iterative trajectories due to the overlooked geometric structure of the output probability space.
__label__machine_learning_for_healthcare It surpasses both structure-based and ligand-based virtual screening methods for AUROC, BEDROC and EF.
__label__bandits While these findings can be interpreted positively, they suggest that external summarization—which may not be possible in more complex settings—is essential for desirable LLM behavior.
__label__natural_language_processing Current studies perceive LLMs' knowledge boundary on questions with concrete answers (close-ended questions) while paying limited attention to semi-open-ended questions that correspond to many potential answers.
__label__machine_learning_for_physical_sciences We present the Latent Neural Operator (LNO) solving PDEs in the latent space.
__label__natural_language_processing Recent attention implementations mask cross-document attention, reducing the effective length of a chunk of tokens.
__label__natural_language_processing Large language model (LLM) agents have demonstrated impressive capabilities in utilizing external tools and knowledge to boost accuracy and reduce hallucinations.
__label__natural_language_processing Large language models (LLMs) typically utilize the top-k contexts from a retriever in retrieval-augmented generation (RAG).
__label__optimization More broadly, this work highlights how the tools of star geometry can aid in understanding the geometry of unsupervised regularizer learning.
__label__online_learning We also strengthen our results under natural assumptions on the loss functions, including $\Theta(\log T)$ U-calibration error for Lipschitz proper losses, $\mathcal{O}(\log T)$ U-calibration error for a certain class of decomposable proper losses, U-calibration error bounds for proper losses with a low covering number, and others.
__label__safety_in_machine_learning Therefore, with WildTeaming we create WildJailbreak, a large-scale open-source synthetic safety dataset with 262K vanilla (direct request) and adversarial (complex jailbreak) prompt-response pairs.
__label__probabilistic_methods This makes it possible to reuse model evaluations across multiple gradient steps, thereby reducing computational cost.
__label__optimization_for_deep_networks Specifically, CoMERA is $2\times$ faster per training epoch and $9\times$ more memory-efficient than GaLore on a tested six-encoder transformer with single-batch training.
__label__algorithmic_game_theory Experimental results show that MCCFVFP achieved convergence speeds approximately 20\%$\sim$50\% faster than the most advanced MCCFR variants in games like poker and other test games.
__label__privacy However, existing methods for evaluating this robustness metric are either computationally expensive or not very accurate.
__label__online_learning We demonstrate consistent results in a permuted MNIST task with latent variables.
__label__evaluation Thus, we claim that psychometric modeling should play a larger role in the evaluation of LLM capabilities on exams designed for humans.
__label__generative_models However, MGMs still underperform in image synthesis compared to recent well-developed continuous diffusion models with similar size in terms of quality and diversity of generated samples.
__label__optimization The latter, concentrating on the locality of NMS, achieves an optimization at a constant level without an mAP loss penalty.
__label__probabilistic_methods Extensive experiments on 11 public datasets for two different graph learning tasks demonstrate that DiGGR consistently outperforms many previous self-supervised methods, verifying the effectiveness of the proposed approach.
__label__graph_neural_networks Most notably, we show that $r$-$\ell$WL can count homomorphisms of cactus graphs.
__label__neuroscience_and_cognitive_science However, applying SNNs to sequential tasks, such as text classification and time-series forecasting, has been hindered by the challenge of creating an effective and hardware-friendly spike-form positional encoding (PE) strategy.
__label__machine_vision To our knowledge, this is the first work that explicitly emphasizes assessing complex image exposure problems at a pixel level, providing a significant boost to the IEA and exposure-related community.
__label__diffusion_based_models Ensuring robust 3D object detection and localization is crucial for many applications in robotics and autonomous driving.
__label__diffusion_based_models SRDS is not only guaranteed to accurately solve the ODE and converge to the serial solution but also benefits from parallelization across the diffusion trajectory, enabling batched inference and pipelining.
__label__graph_neural_networks Recent research has underscored the efficacy of Graph Neural Networks (GNNs) in modeling diverse geometric structures within graph data.
__label__machine_vision The code will be released at https://github.com/HenryYu23/DAS.
__label__speech_and_audio In this study, we introduce a novel model framework TransVIP that leverages diverse datasets in a cascade fashion yet facilitates end-to-end inference through joint probability.
__label__robotics Long-horizon robotic manipulation tasks typically involve a series of interrelated sub-tasks spanning multiple execution stages.
__label__online_learning We explore the training dynamics of neural networks in a structured non-IID setting where documents are presented cyclically in a fixed, repeated sequence.
__label__optimization We further analyze the impacts of system stability on the existence of the PSC solution.
__label__active_learning By merging contrastive learning with inherent data groupings in medical imaging, we learn a metric that emphasizes the relevant differences in samples for training 3D medical segmentation models.
__label__reinforcement_learning Deep Multi-agent Reinforcement Learning (MARL) relies on neural networks with numerous parameters in multi-agent scenarios, often incurring substantial computational overhead.
__label__bandits Our methodology provides online services with an adaptive experimental design approach for learning the best-performing treatment for such encouragement designs.
__label__diffusion_based_models Experimentally, Our BIR-D has demonstrated superior practicality and versatility than off-the-shelf unsupervised methods across various tasks both on real-world and synthetic datasets, qualitatively and quantitatively.
__label__graph_neural_networks Additionally, our analysis of generated graphs allows us to better understand the properties of graph distances: depending on which diversity measure is used for optimization, the obtained graphs may possess very different structural properties which gives a better understanding of the graph distance underlying the diversity measure.
__label__algorithmic_game_theory Our main results include a characterization of scenarios where a single signal suffices and a computationally efficient algorithm to compute optimal signaling schemes.
__label__machine_vision Recent approaches have modeled each step of the DDIM inversion process as finding a fixed-point problem of an implicit function.
__label__natural_language_processing Remarkably, without in-context examples or Chain of Thought, the LLM can verbalize that the unknown city is Paris and use this fact to answer downstream questions.
__label__bandits We demonstrate our method's improved performance on a range of synthetic invariant and quasi-invariant functions.
__label__online_learning We then give an efficient gradient-based controller for these systems, with near-optimal regret bounds with respect to a broad class of linear policies.
__label__causal_inference In real-world scenarios, soft interventions are often more realistic than hard interventions, as the latter require fully controlled environments.
__label__reinforcement_learning A key challenge in lifelong reinforcement learning (RL) is the loss of plasticity, where previous learning progress hinders an agent's adaptation to new tasks.
__label__deep_learning_architectures Consequently, TS-forecasting has a more rigorous evaluation methodology compared to classification.
__label__machine_learning_for_other_sciences_and_fields We put forward the Segmentation Clustering Decision Tree (SCD-Tree), designed to dissect and understand the structure of normal data distributions.
__label__learning_theory Previously known algorithms either had worse time complexity, a larger factor  $\alpha$, or extra assumptions about the problem setting.
__label__machine_vision Recently, denoising diffusion probabilistic models have been gradually applied to visual tasks, enhancing controllable image generation through low-rank adaptation (LoRA).
__label__machine_vision Our Global-then-Local State Space Block (GLSSB) integrates LESSM and IRSK with layer normalization (LN) as its core.
__label__probabilistic_methods Moreover, we prove the non-asymptotic validity of the confidence intervals for parameter estimation with LSA based on multiplier bootstrap.
__label__probabilistic_methods We derive a tighter inequality than Cauchy-Schwarz Inequality, leverage it to refine Pearson's $r$, and propose a new correlation coefficient, i.e., rearrangement correlation.
__label__bandits We answer this question positively for contextual bandits, but in the negative for tabular RL, showing a separation between contextual bandits and RL.
__label__natural_language_processing We add a special [IDK] (“I Don't Know”) token to the model's vocabulary and introduce an objective function that shifts probability mass to the [IDK] token for incorrect predictions.
__label__bandits The expected reward is a fixed but unknown linear function of the chosen action.
__label__interpretability_and_explainability We then present a series of experiments showing settings in which this and other unsupervised methods result in classifiers that do not predict knowledge, but instead predict a different prominent feature.
__label__safety_in_machine_learning In this paper, we provide an affirmative answer to this question by thoroughly investigating the \textit{Post-Purification Robustness} of current backdoor purification methods.
__label__privacy Our numerical experiments demonstrate that models trained using BSR perform on par with the best existing methods, while completely avoiding their computational overhead.
__label__deep_learning_architectures In these scenarios, parties are often linked using fuzzy identifiers, leading to a common practice termed as _multi-party fuzzy VFL_.
__label__graph_neural_networks We also introduce novel node marking strategies and provide a theoretical analysis of their expressive power and other key aspects of our approach.
__label__graph_neural_networks Message Passing Graph Neural Networks (MPGNNs) have emerged as the preferred method for modeling complex interactions across diverse graph entities.
__label__neuroscience_and_cognitive_science Computational models of confidence, however, are limited to specific scenarios such as between choices with the same value.
__label__optimization In this paper, we propose a novel method called ordered momentum (OrMo) for ASGD.
__label__other We provide theoretical results on FRLC, and demonstrate superior performance on diverse applications -- including graph clustering and spatial transcriptomics --  while demonstrating its interpretability.
__label__fairness This compromise can be explained by a Pareto frontier where given certain resources (e.g., data), reducing the fairness violations often comes at the cost of lowering the model accuracy.
__label__interpretability_and_explainability Recently, interpretable machine learning has re-explored concept bottleneck models (CBM).
__label__reinforcement_learning However, such tools were largely developed for supervised learning rather than nonstationary RL, leading practitioners to adopt target networks, clipped policy updates, and other RL-specific implementation tricks to combat this mismatch, rather than directly adapting this toolchain for use in RL.
__label__machine_learning_for_other_sciences_and_fields Supervised learning requires heavy human annotation, which is particularly challenging for molecular data, e.g., the commonly used density functional theory (DFT) is highly computationally expensive.
__label__optimization We consider the model for dynamic algorithms with predictions where predictions regarding the insertion and deletion times of elements can be used for preprocessing.
__label__machine_vision To address this issue, we propose to embed $SE(3)$ equivariance into the Perceiver IO architecture.
__label__machine_learning_for_other_sciences_and_fields Under the hood, gRNAde is a multi-state Graph Neural Network that generates candidate RNA sequences conditioned on one or more 3D backbone structures where the identities of the bases are unknown.
__label__optimization These solvers offer several advantages over traditional methods and other learning-based methods, particularly for large-scale CO problems.
"__label__generative_models S${^2}$FT accomplishes this by ""selecting sparsely and computing densely""."
__label__machine_vision Starting with a pretrained neural radiance field (NeRF), existing methods typically learn a novel appearance that matches the given style.
__label__machine_vision Using the same context length, our DeepStack 7B and 13B parameters surpass their counterparts by 2.7 and 2.9 on average across 9 benchmarks, respectively.
"__label__diffusion_based_models Rather than merely mode-seeking, our method achieves sampling by ""pulling back"" the dynamics of the reverse-time process—from the image space to the diffrep parameter space—and updating the parameters according to this pulled-back process."
__label__interpretability_and_explainability Experiments on six widely used datasets show that our MRD criterion improves rationale quality (measured by the overlap with human-annotated rationales) by up to $10.4\%$ as compared to several recent competitive MMI variants.
__label__deep_learning_architectures Increased training parameters have enabled large pre-trained models to excel in various downstream tasks.
__label__machine_vision Masked Autoencoder (MAE) is a self-supervised approach for representation learning, widely applicable to a variety of downstream tasks in computer vision.
__label__optimization Additionally, by deriving a re-parameterized gradient estimator, this transformation also provides efficient stochastic optimization over the latent permutations.
__label__probabilistic_methods While various approximate GP models have been employed to scale Bayesian optimization to larger sample sizes, most suffer from overly-smooth estimation and focus primarily on problems that allow for large online samples.
__label__safety_in_machine_learning However, the tension between the complexity of current alignment methods and the need for rapid iteration in deployment scenarios necessitates the development of a model-agnostic alignment approach that can operate under these constraints.
__label__machine_learning_for_other_sciences_and_fields Interacting systems are prevalent in nature.
__label__machine_learning_for_physical_sciences We also consider a test case for atmospheric large-eddy simulation of a two-dimensional dry cold bubble, where we infer both continuous time-series and information about the system's conditions.
__label__optimization Recently, machine learning techniques have been utilized to solve ILPs.
__label__diffusion_based_models We demonstrate the effectiveness of our method on both regression and classification tasks, where our method outperforms existing NODEs with a significantly lower number of function evaluations.
__label__online_learning To effectively utilize these resources, we present an approach to actively select pre-trained models while minimizing labeling costs.
__label__safety_in_machine_learning They are, however, limited in that they only support $\ell_2$-bounded perturbations and classification tasks.
__label__natural_language_processing Our algorithm uses prior sample outputs to soundly overapproximate the future grammaticality of different output prefixes.
__label__machine_learning_for_physical_sciences The unresolved issue of rollout error accumulation results in unreliable estimations as the network predicts further into the future, with each step's error compounding and leading to an increase in inaccuracy.
__label__natural_language_processing Drawing on this insight, SnapKV automatically compresses KV caches by selecting clustered important KV positions for each attention head.
__label__machine_vision To maintain the effectiveness of sparse tuning with low-rank matrices, we extend the low-rank decomposition by applying nonlinear kernel functions to the whole-matrix merging.
__label__deep_learning_architectures Their common formulations typically require storing each pattern in a separate set of synaptic weights, which leads to the increase of the number of synaptic weights when new patterns are introduced.
"__label__probabilistic_methods Our algorithm requires only scalar feedback from 
the top-k recommendations and does not impose restrictive assumptions on the reward structure."
__label__reinforcement_learning This paper presents Deep Diffusion Policy Gradient (DDiffPG), a novel actor-critic algorithm that learns from scratch multimodal policies parameterized as diffusion models while discovering and maintaining versatile behaviors.
__label__generative_models NATs perform generation in a progressive manner, where the latent tokens of a resulting image are incrementally revealed step-by-step.
__label__diffusion_based_models In addition to robustness, AdvUnlearn also achieves a balanced tradeoff with model utility.
__label__safety_in_machine_learning These mechanisms employ an input-dependent strategy, in which uninformative tokens are discarded from the computation pipeline, improving the model’s efficiency.
__label__generative_models In this paper, we advocate for distinct contributions for each text token based on its visual correlation.
__label__interpretability_and_explainability Contrastive Language-Image Pretraining (CLIP) performs zero-shot image classification by mapping images and textual class representation into a shared embedding space, then retrieving the class closest to the image.
__label__reinforcement_learning Sequential updating scheme was thus proposed, naturally diversifies agents by encouraging agents to learn from preceding ones.
__label__probabilistic_methods We consider the use of approximately equivariant architectures in neural processes (NPs), a popular family of meta-learning models.
__label__generative_models This results in a highly sparse adapter which can be switched directly in the fused mode.
__label__machine_learning_for_healthcare In particular, we incorporate manual prompting and design a MACross module to deal with the multi-attribute characteristics of textual descriptions.
__label__privacy We introduce four targets, based on which we construct fine-tuning data for the concepts to be forgotten; (ii)  Joint training loss.
__label__diffusion_based_models Furthermore, we propose a spatiotemporal adaptive fusion module to integrate content and motion control across various sampling steps and spatial locations.
__label__generative_models To streamline our argument, we introduce a programming language called LSRL that compiles to these fully recurrent architectures.
__label__machine_vision We find that they still help mitigate the spurious features, providing a promising path for future developments.
__label__bandits Our first contribution is a tight, instance-dependent lower bound on the cost complexity.
__label__natural_language_processing In this work, we propose a novel method called RankRAG, which instruction-tunes a single LLM for both context ranking and answer generation in RAG.
__label__interpretability_and_explainability This provides a simple yet effective method to increase model flexibility and preserve interpretability.
__label__reinforcement_learning RCaI is shown to be equivalent to log-probability regularized risk-sensitive control, which is an extension of the maximum entropy (MaxEnt) control.
__label__machine_vision Most previous approaches addressed these tasks separately, either by designing highly specialized social gaze models that do not generalize to other social gaze tasks or by considering social gaze inference as an ad-hoc post-processing of the gaze following task.
__label__natural_language_processing Using SpecExec, we demonstrate inference of 50B+ parameter LLMs on consumer GPUs with RAM offloading at 4--6 tokens per second with 4-bit quantization or 2--3 tokens per second with 16-bit weights.
__label__diffusion_based_models To this end, we introduce AsCAN---a hybrid architecture, combining both convolutional and transformer blocks.
__label__diffusion_based_models The recent progress in text-to-image models pretrained on large-scale datasets has enabled us to generate various images as long as we provide a text prompt describing what we want.
__label__optimization_for_deep_networks ADOPT addresses the non-convergence issue of Adam by removing the current gradient from the second moment estimate and changing the order of the momentum update and the normalization by the second moment estimate.
__label__neuroscience_and_cognitive_science In particular, approaches to reinforcement learning typically strive for improvements over only a single lifetime.
__label__generative_models We show that our D-IMF procedure can provide the same quality of unpaired domain translation as the IMF, using only several generation steps instead of hundreds.
__label__machine_vision We believe that DiffGS provides a new direction for flexibly modeling and generating Gaussian Splatting.
__label__machine_vision Our results demonstrate consistent performance gains, underscoring the critical role of these additional tasks in fostering comprehensive intelligence in MLLMs.
__label__natural_language_processing Evaluating 43 different language models using de-facto standard prompting methodologies, we establish two dominant patterns.
__label__generative_models However, when extended to multimodal setting, it often suffers from misalignment between specific textual instruction and targeted local region of an image.
__label__graph_neural_networks Notably,  Link-Mo achieves a relative improvement of 18.71% on the MRR metric for the Pubmed dataset and 9.59% on the Hits@100 metric for the ogbl-ppa dataset, compared to the best baselines.
__label__deep_learning_architectures By leveraging the more informative signals from the bottom layer of the feedback network to guide the updates of the top layer of the feedforward network and vice versa, CCL enables the simultaneous transformation of source inputs to target outputs and the dynamic mutual influence of these transformations.
__label__interpretability_and_explainability In sensitive contexts, providers of machine learning algorithms are increasingly required to give explanations for their algorithms' decisions.
__label__graph_neural_networks Through extensive experiments on several datasets, along with a variety of edit cost settings, we show that $\texttt{GraphEdX}$ consistently outperforms state-of-the-art methods and heuristics in terms of prediction error.
__label__machine_vision However, processing images with complex degradations in the night using unpaired data still remains a challenge in this field.
__label__deep_learning_architectures Recent studies have highlighted the advantages of channel independence to resist distribution drift but neglect channel correlations, limiting further enhancements.
__label__reinforcement_learning At the core of our diffusion policy is a mean-reverting stochastic differential equation (SDE) that transfers the action distribution into a standard Gaussian form and then samples actions conditioned on the environment state with a corresponding reverse-time process.
__label__deep_learning_architectures When fine-tuning the Llama2-13B model, VB-LoRA only uses 0.4% of LoRA's stored parameters, yet achieves superior results.
__label__evaluation When users solve tasks with the system, models that fully match the task feature space are often rare or even unavailable.
__label__algorithmic_game_theory Algorithms with predictions are gaining traction across various domains, as a way to surpass traditional worst-case bounds through (machine-learned) advice.
"__label__generative_models The complementary learners, like ""wings"" on either side, are connected in parallel to each layer's attention block."
"__label__reinforcement_learning Often, small changes in
a hyperparameter can lead to drastic changes in performance, and different environments require very different hyperparameter settings to achieve state-of-the-art
performance reported in the literature."
__label__graph_neural_networks However, graph sizes often become unwieldy, leading to storage, computation, and analysis challenges.
__label__diffusion_based_models For image synthesis, we propose a finite perturbation approach to enhance the diversity of generated results without changing the semantic categories.
__label__safety_in_machine_learning Our code is available at https://github.com/sail-sg/I-FSJ.
__label__learning_theory We show here experimentally and theoretically under additional assumptions that distributions with high globality cannot be learned efficiently.
__label__generative_models Our experimental results demonstrate that Wings outperforms equally-scaled MLLMs in both text-only and visual question-answering tasks.
__label__machine_vision To observe the scene from a global perspective, we introduce a novel Dual Mamba module that models the point cloud in terms of spatial distribution and continuity.
__label__privacy Benefited from the intrinsic progressively adversarial level, the trajectory is capable of tolerating greater degree of alteration in decision boundaries.
__label__machine_vision Finally, inspired by the mixture-of-experts (MoE) mechanism, we introduce an enhanced adapter to further boost the adaptation performance.
__label__machine_vision Previous methods explore 3D Gaussian Splatting with neural priors (e.g.
"__label__safety_in_machine_learning Our method to plant backdoors ensures that even if the weights and architecture of the obfuscated model are accessible, the existence of
the backdoor is still undetectable."
__label__evaluation Guided by the axioms, our findings offer concrete guidelines for enhancing the reliability of LLM evaluation methods, suggesting a need for reassessment of existing comparative approaches.
__label__machine_vision Instead of relying on the multimodal LLM to directly annotate data, which we found to be suboptimal, we prompt it to reason about potential candidate entity labels by accessing additional contextually relevant information (such as Wikipedia), resulting in more accurate annotations.
__label__reinforcement_learning Assuming that the loss has a linear structure, we propose both model-based and model-free algorithms achieving $poly(d, A, H)T^{2/3}$ regret, though they are computationally inefficient.
__label__machine_vision Extensive experiments on both outdoor LiDAR point cloud datasets and indoor RGBD point cloud datasets demonstrate that our method achieves state-of-the-art accuracy, efficiency, and robustness.
__label__learning_theory In particular, natural learning processes rely on mechanisms where data representation and learning are intertwined in such a way to respect spatiotemporal locality.
__label__deep_learning_architectures Finally, we validate our theoretical analyses through experiments in both balanced and imbalanced scenarios.
__label__machine_vision Occupancy prediction, aiming at predicting the occupancy status within voxelized 3D environment, is quickly gaining momentum within the autonomous driving community.
__label__machine_learning_for_other_sciences_and_fields We attribute this issue to the imbalance between the abundance of tunable parameters and the scarcity of labeled molecules, and the lack of contextual perceptiveness in the encoders.
__label__natural_language_processing A conventional LLM unlearning task typically involves two goals: (1) The target LLM should forget the knowledge in the specified forget documents; and (2) it should retain the other knowledge that the LLM possesses, for which we assume access to a small number of retain documents.
__label__machine_vision To achieve this, we adjust the noise level (equivalently, number of diffusion iterations) to ensure the generated image retains low-level and background features from the source image while representing the target category, resulting in a hard negative sample for the source category.
__label__evaluation Model editing has become an increasingly popular alternative for efficiently updating knowledge within language models.
__label__other To mitigate this sub-optimality, we propose Coupled Quantization (CQ), which couples multiple key/value channels together for quantization to exploit their interdependence and encode the activations in a more information-efficient manner.
__label__causal_inference We demonstrate the expressiveness and consistency of this procedure and further propose a gradient-based optimization scheme for making scalable inferences in practice.
__label__online_learning A switching regret is defined relative to any segmentation of the trial sequence, and is equal to the sum of the static regrets of each segment.
__label__reinforcement_learning LASE allocates a portion of its rewards to co-players as gifts, with this allocation adapting dynamically based on the social relationship --- a metric evaluating the friendliness of co-players estimated by counterfactual reasoning.
__label__diffusion_based_models We conduct extensive experiments to validate LI-DIT across model size and data size.
__label__generative_models Extensive experiments show that COSMIC outperforms state-of-the-art baselines on both perceptual and distortion metrics.
__label__causal_inference Our extensive testing confirms that DeepITE not only surpasses 13 baseline methods in the Recall@k metric but also demonstrates expeditious inference times, particularly on large graphs.
__label__natural_language_processing However, LLM hallucination may result in robots confidently executing plans that are misaligned with user goals or even unsafe in critical scenarios.
__label__graph_neural_networks It projects graph nodes and learnable pseudo nodes into a common space with measurable spatial relations between them.
__label__safety_in_machine_learning However, this results in conspicuous and attention-grabbing patterns in the generated camouflage, which humans can easily identify.
__label__machine_vision Collaborative trajectory prediction can comprehensively forecast the future motion of objects through multi-view complementary information.
__label__diffusion_based_models We further investigate and visualize the impact of Meta-Diffu$B$'s noise scheduling on the generation of sentences with varying difficulties.
__label__online_learning We present a novel mathematical framework for class-IL and prove the Infeasibility Theorem, showing optimal class-IL is impossible with discriminative modeling due to task confusion.
__label__machine_vision However, it encounters two main challenges in multi-drone collaboration settings.
__label__learning_theory Matrix sketching is a powerful tool for reducing the size of large data matrices.
__label__reinforcement_learning Then, we show how these can be used to derive near-optimal guarantees of an optimistic exploration algorithm.
__label__probabilistic_methods Probabilistic circuits (PCs) have emerged as a powerful framework compactly representing probability distributions for efficient and exact probabilistic inference.
__label__deep_learning_architectures In particular, it suggests cross-channel interactions could play a vital role in future improvements.
__label__neuroscience_and_cognitive_science Using word embeddings at first, then large language models, researchers have created encoding models to analyze the brain signals.
__label__generative_models In this work, we propose a novel framework, Synthesize-Partition-Adapt (SPA), that leverages the abundant synthetic data available in many domains to elicit diverse responses from foundation models.
__label__learning_theory These results---which are proven using a complexity measure known as the \emph{Decision-Estimation Coefficient} (DEC)---capture difficulties unique to interactive learning, yet do not recover the tightest known lower bounds for passive estimation.
__label__other Specifically, before computing the gradients, we prune unnecessary elements in the parameter vector for the candidate set by utilizing upper bounds on absolute values of the parameters.
__label__privacy data from some distribution and $h$ is a permutation-invariant function.
__label__learning_theory Without making any assumptions on the underlying automatic labeling system or data distribution, we derive an algorithm for computing provably valid confidence intervals for parameters of any dimensionality that is based on stratified sampling.
__label__machine_vision The source code is available at https://github.com/undercutspiky/SFL/
__label__machine_learning_for_physical_sciences Many problems in physical sciences are characterized by the prediction of space-time sequences.
__label__learning_theory Despite several attempts to devise effective transferable FL approaches, several important issues remain unsolved.
__label__fairness Calibration is a well-studied property of predictors which guarantees meaningful uncertainty estimates.
__label__generative_models Finally, we introduce the temporal attention layers into our egocentric video diffusion pipeline to improve the temporal consistency cross egocentric frames.
__label__learning_theory In this paper, we introduce a novel theoretical framework for multi-task regression, applying random matrix theory to provide precise performance estimations, under high-dimensional, non-Gaussian data distributions.
__label__evaluation We illustrate their utility in three domains: game playing, logic puzzles, and navigation.
__label__generative_models The efficacy of task-specific finetuning largely depends on the selection of appropriate training data.
__label__reinforcement_learning Moreover, the quantity $\kappa$ can be exponentially small in the worst case, leading to a significant gap for the regret compared to linear function approximation.
__label__deep_learning_architectures Despite extensive investigations into the scaling behaviors of language and vision transformers, the scalability of CRATE remains an open question which this paper aims to address.
__label__probabilistic_methods It is a variant of shuffle coding that is many orders of magnitude faster than the original and enables 'one-shot' compression of single unordered objects.
__label__safety_in_machine_learning Recent studies show that Large Language Models (LLMs) with safety alignment can be jail-broken by fine-tuning on a dataset mixed with harmful data.
__label__machine_vision This is achieved by minimizing the inter-domain variance while maximizing the inter-class variance.
__label__infrastructure Previous methods employed low-rank adaptation (LoRA) for efficient federated fine-tuning but utilized traditional FL aggregation strategies on LoRA adapters.
__label__infrastructure We integrate this scheduler with the state-of-the-art LLM serving system and show significant performance improvement in several important applications: 2.8x lower latency in chatbot serving and 6.5x higher throughput in synthetic data generation.
"__label__other There has been significant recent interest in graph-based nearest neighbor search methods, many of which are centered on the construction of (approximately) ""navigable"" graphs over high-dimensional point sets."
__label__natural_language_processing In our study, we propose an innovative framework, AlphaMath, that bypasses the need for process annotations (from humans or GPTs) by leveraging Monte Carlo Tree Search (MCTS).
__label__natural_language_processing As a cost-effective alternative, learning-free PTQ schemes have been proposed.
__label__other Although this full network update method maximizes knowledge acquisition and sharing for each model layer, it prevents the layers of the global model from cooperating effectively to complete the tasks of each client, a challenge we refer to as layer mismatch.
__label__deep_learning_architectures Vision transformer model (ViT) is widely used and performs well in vision tasks due to its ability to capture long-range dependencies.
__label__infrastructure In this work, we provide a characterization of two MoE workloads, namely Language Modeling (LM) and Machine Translation (MT) and identify their sources of inefficiencies at deployment.
__label__diffusion_based_models From a spatial perspective, we propose Space Patched Distillation (SPD) to address the difficulty of matching binary features during distillation, focusing on the spatial locality of image generation tasks and noise estimation networks.
__label__machine_vision In this paper, we propose a novel unsupervised model selection approach for domain adaptive object detection, which is able to select almost the optimal model for the target domain without using any target labels.
__label__other We theoretically prove that *CSPG* can accelerate the existing graph-based ANNS algorithms by reducing unnecessary explorations.
__label__machine_vision Our method utilizes an image-conditioned 3D scene diffusion model to simultaneously denoise the 3D poses and geometries of all objects within the scene.
"__label__learning_theory We leverage this intuition to explain
the emergence of a bottleneck structure, as observed in previous work:
for large $\tilde{L}$ the potential energy dominates and leads to
a separation of timescales, where the representation jumps rapidly
from the high dimensional inputs to a low-dimensional representation,
move slowly inside the space of low-dimensional representations, before
jumping back to the potentially high-dimensional outputs."
__label__learning_theory Furthermore, given the same choice of experts, we demonstrate that the sigmoid gating function requires a smaller sample size than its softmax counterpart to attain the same error of expert estimation and, therefore, is more sample efficient.
__label__generative_models To address these challenges and promote the alignment of generative models through instruction tuning, we leverage multimodal large language models to create VisionPrefer, a high-quality and fine-grained preference dataset that captures multiple preference aspects.
__label__neuroscience_and_cognitive_science This variability can induce distribution shifts in the data $X$ and in the biomedical variables of interest $y$, thus limiting the application of supervised machine learning (ML) algorithms.
__label__machine_vision Additionally, we design a Query-aware Mamba module that decodes context features into object sets under the guidance of learnable queries.
__label__learning_theory Then, we design a two-stage training algorithm, where the pre-processing stage for training the feed-forward layer and the main stage for training the attention layer exhibit fast convergence performance.
__label__probabilistic_methods Recent works in Variational Inference have examined alternative criteria to the commonly used exclusive Kullback-Leibler divergence.
__label__machine_learning_for_other_sciences_and_fields In the sampling, PocketFlow leverages multi-granularity guidance (overall binding affinity and interaction geometry constraints) to facilitate generating high-affinity and valid pockets.
__label__machine_vision On the YFCC100M dataset, our matching accuracy is competitive with LoFTR, a state-of-the-art transformer-based architecture, while the inference speed is boosted to 4 times, even outperforming the CNN-based methods.Comprehensive evaluations on other open datasets such as Megadepth, ScanNet, and HPatches demonstrate our method's efficacy, highlighting its potential to significantly enhance a wide array of downstream applications.
__label__machine_vision Human visual search ability enables efficient and accurate tracking of an arbitrary moving target, which is a significant research interest in cognitive neuroscience.
__label__graph_neural_networks In detail, NIFA first designs two insightful principles for node injection operations, namely the uncertainty-maximization principle and homophily-increase principle, and then optimizes injected nodes’ feature matrix to further ensure the effectiveness of fairness attacks.
__label__other TAKFL also incorporates a KD-based self-regularization technique to mitigate the issues related to the noisy and unsupervised ensemble distillation process.
__label__natural_language_processing It is therefore crucial to detect contamination and estimate its impact on measured performance.
__label__optimization The former methods smooth the objective function, causing a biased gradient estimation, while the latter often enjoys more accurate estimates, at the cost of large amounts of samples and queries at each iteration to update variables.
"__label__other But at the same time, we observe a new problem that
the recent Transformer-based models are overly reliant on patching to achieve ideal
performance, which limits their applicability to some forecasting tasks unsuitable
for patching."
__label__probabilistic_methods Our work takes an important step towards understanding the expressive power of tree-structured PCs, and our techniques may be of independent interest in the study of structure learning algorithms for PCs.
__label__learning_theory Recently, the RKME (Reduced Kernel Mean Embedding) specification was proposed and most commonly utilized.
__label__diffusion_based_models Stable-Pose is designed to adeptly handle pose conditions within pre-trained Stable Diffusion, providing a refined and efficient way of aligning pose representation during image synthesis.
__label__safety_in_machine_learning However, the robustness of these models remains scarcely explored.
__label__reinforcement_learning We prove that a global coverage condition is both necessary and sufficient for offline contrastive methods to converge to the optimal policy, but a weaker partial coverage condition suffices for online RL methods.
__label__machine_vision Specifically, to enable GIMM as an effective motion modeling paradigm, we design a motion encoding pipeline to model spatiotemporal motion latent from bidirectional flows extracted from pre-trained flow estimators, effectively representing input-specific motion priors.
__label__machine_vision Our key insight is that 2D multi-view diffusion and 3D reconstruction models provide complementary information for each other.
__label__deep_learning_architectures This work presents a novel approach to neural architecture search (NAS) that aims to increase carbon efficiency for the model design process.
__label__online_learning Surprisingly, the analysis is short and elegant.
__label__machine_vision To address these issues, we introduce a debiased learning framework, namely **Happy**, characterized by **H**ardness-**a**ware **p**rototype sampling and soft entro**py** regularization.
__label__generative_models We prove that our solver provides a universal approximation of UEOT solutions and obtain its generalization bounds.
__label__machine_learning_for_other_sciences_and_fields We refer to this model as Fast T2T.
__label__neuroscience_and_cognitive_science In-context and in-weights cultural accumulation can be interpreted as analogous to knowledge and skill accumulation, respectively.
__label__interpretability_and_explainability Recent work has explored how individual components of the CLIP-ViT model contribute to the final representation by leveraging the shared image-text representation space of CLIP.
"__label__safety_in_machine_learning We present an approach, inspired by recent advances in representation engineering, that interrupts the models as they respond with harmful outputs with ""circuit breakers."""
__label__graph_neural_networks The pioneering work of Oono \& Suzuki [ICLR, 2020] and Cai \& Wang [arXiv:2006.13318] analyze the smoothness of graph convolutional network (GCN) features.
__label__machine_vision However, its potential in volumetric reconstruction tasks, such as X-ray computed tomography, remains under-explored.
__label__probabilistic_methods We develop a new geometric view of reparametrizations from which we explain the success of linearization.
__label__machine_learning_for_other_sciences_and_fields *f*-RAG is based on a pre-trained molecular generative model that proposes additional fragments from input fragments to complete and generate a new molecule.
__label__diffusion_based_models Adding additional guidance to pretrained diffusion models has become an increasingly popular research area, with extensive applications in computer vision, reinforcement learning, and AI for science.
__label__deep_learning_architectures These adversaries learn the marginal class probability functions over different data subspaces, while a single generator in the full space models the entire distribution of the inlier class.
__label__learning_theory In this work, we focus on understanding the *sample complexity* of training such a model; how many samples are needed to learn an accurate diffusion model using a sufficiently expressive neural network?
"__label__optimization This allows us to regulate the performance of the algorithm as a function of the prediction error, while simultaneously
maintaining the analytical notion of consistency/robustness tradeoffs, adapted to the profile setting."
__label__interpretability_and_explainability However, in certain datasets, there are spurious features non-causally correlated with the label and also get high mutual information, complicating the loss landscape of MMI.
__label__machine_learning_for_healthcare We expect this foundation model can promote the development of volumetric medical image analysis.
__label__other However, most FL approaches assume that clients possess labeled data, which is often not the case in practice.
__label__deep_learning_architectures VSAs support the commutativity and associativity of this binding operation, along with an inverse operation, allowing one to construct symbolic-style manipulations over real-valued vectors.
__label__diffusion_based_models Most existing theoretical investigations of the accuracy of diffusion models, albeit significant, assume the score function has been approximated to a certain accuracy, and then use this a priori bound to control the error of generation.
__label__neuroscience_and_cognitive_science Despite its widespread success among humans, the capacity for artificial learning agents to accumulate culture remains under-explored.
__label__natural_language_processing Then we exploit this by only computing a random subset of the scores and efficiently recover the missing entries in the matrix by applying the Alternating Least Squares (ALS) algorithm, thereby enabling fast approximation of the MBR decoding process.
__label__privacy We provide a theoretical analysis of our approach, which gives the noise strength needed for privacy protection, as well as the bound of mean squared error.
__label__machine_vision CHASE operates as a sample-adaptive normalization method to mitigate inter-entity distribution discrepancies, thereby reducing data bias and improving the subsequent classifier's multi-entity action recognition performance.
__label__robotics We propose Heterogeneous Pre-trained Transformers (HPT), which pre-train a large, shareable trunk of a policy neural network to learn a task and embodiment agnostic shared representation.
__label__machine_vision Moreover, our method reduces the computation in the context and avoid decreasing the vision tokens, thus preserving or even improving performance compared to the vanilla model.
__label__machine_vision Previous works have proposed a variety of test-time adaptation (TTA) methods to achieve strong generalization without any knowledge of the target domain.
__label__optimization To address the challenges posed by imbalanced data distributions, this study introduces a novel method utilizing density ratio estimation for dynamic class weight adjustment, termed as Re-weighting with Density Ratio (RDR).
__label__safety_in_machine_learning To meet the demand for effectively enhancing the model robustness under minimal assumptions about group annotation, we propose Environment-based Validation and Loss-based Sampling (EVaLS).
__label__machine_vision A competition-based sparsification mechanism is further proposed to avoid the storage of tunable weight indexes.
__label__natural_language_processing However, current reward models have limited generalization capabilities to unseen prompts and responses, which can lead to an unexpected phenomenon known as reward over-optimization, resulting in a decline in actual performance due to excessive optimization of rewards.
__label__graph_neural_networks We show that this open-book attention mechanism offers insights into the inherent relationships among various tasks in the benchmark and provides a robust tool for interpretable multi-task training.
__label__reinforcement_learning To this end, we first implement Decision Mamba (DM) by replacing the backbone of Decision Transformer (DT).
__label__interpretability_and_explainability The ICL inference process of the attention layer aligns with the training procedure of its  dual model, generating token representation predictions that are equivalent to the dual model's test outputs.
__label__generative_models In this work, we propose to model the per-token probability distribution using a diffusion procedure, which allows us to apply autoregressive models in a continuous-valued space.
__label__safety_in_machine_learning Experimental results show that our approach gives an average accuracy of more than 95.3% in spotting lip-syncing videos, significantly outperforming the baselines.
__label__reinforcement_learning Traditional approaches in this field have predominantly focused on the mean reward or utility criterion.
__label__machine_vision Extensive experiments on NuScenes and KITTI-360 datasets demonstrate the superiority of GeoNLF in both novel view synthesis and multi-view registration of low-frequency large-scale point clouds.
__label__natural_language_processing Nonetheless, LLMs are known to function as black boxes, making it challenging to comprehend their reasoning process.
__label__generative_models The code is available at https://github.com/sony/pagoda.
__label__other In this work, we first showcase the potential of low precision ensembling, where ensemble members are derived from a single model within low precision number systems in a training-free manner.
__label__machine_learning_for_other_sciences_and_fields Meanwhile, we introduce a Scoring Function to guide the model towards a more stable gradient descent.
__label__privacy Specifically, we design a coupled secured feature attribute to replace the original 3DGS's spherical harmonics coefficients and then use a scene decoder and a message decoder to disentangle the original RGB scene and the hidden message.
__label__diffusion_based_models A practical solution is to selectively removing target concepts from the model, but this may impact the remaining concepts.
__label__graph_neural_networks We show (empirically and with theoretical backing) that attention scores on graphs are usually quite consistent across network widths, and use this observation to propose a two-stage procedure, which we call Spexphormer: first, train a narrow network on the full augmented graph.
__label__machine_vision With the design, we also manually annotate 6K high-quality samples for the challenging graphical mathematical problems.
__label__safety_in_machine_learning In particular, we show high correlation and significantly reduced computation cost of GREAT Score when compared to the attack-based model ranking on RobustBench \cite{croce2021robustbench}.
__label__optimization The necessity to align two graphs, minimizing a structural distance metric, is prevalent in biology, chemistry, recommender systems, and social network analysis.
__label__natural_language_processing Contrastive preference optimization has shown promising results in aligning LLMs with available preference data by optimizing the implicit reward associated with the policy.
__label__interpretability_and_explainability Circuit analysis in this setting thus either yields intractably large circuits or fails to disentangle local and global behavior.
__label__algorithmic_game_theory This paper aims to develop the first such algorithm.
__label__machine_vision Then, GSFS dynamically shifts the testing sample toward the source domain, mitigating error accumulation in an online manner.
__label__optimization_for_deep_networks Contextual Sparsity (CS) is appealing for its training-free nature and its ability to reach a higher compression ratio seemingly without significant performance degradation.
__label__machine_vision Significant variations in the model output and granularity can occur with simply subtle changes in the prompt, contradicting the consensus requirement for the robustness of a model.
__label__machine_vision Specifically, we utilize off-the-shelf VFMs to generate semantic labels for weakly-supervised pixel-to-point contrastive distillation.
__label__learning_theory Motivated by the problem of detecting AI-generated text, we consider the problem of watermarking the output of language models with provable guarantees.
__label__natural_language_processing Recent studies disclose knowledge conflicts in LLM generation, wherein outdated or incorrect parametric knowledge (i.e., encoded knowledge) contradicts new knowledge provided in the context.
__label__probabilistic_methods Extensive experiments over many datasets demonstrate our proposed method outperforms existing OOD detection methods.
__label__optimization using CIFAR10, MNIST, and CelebA datasets.
__label__machine_vision The question-related selection reduces redundancy within the memories, enabling efficient and precise video understanding.
__label__learning_theory Our approach generalizes and improves all prior work on TDS learning: (1) we obtain *universal* learners that succeed simultaneously for large classes of test distributions, (2) achieve near-optimal error rates, and (3) give exponential improvements for constant depth circuits.
__label__fairness Next, we propose to alternatively update the prediction model and possible estimated causal effects, where the prediction model is trained via a min-max loss to control the worst-case fairness violations.
__label__natural_language_processing We have conducted extensive experiments on instruction tuning and unlearning tasks, demonstrating the effectiveness of CMC.
__label__natural_language_processing Specifically, we propose a tree-based data sampling method to generate supervised data and preference pairs derived from the evaluation tree.
"__label__learning_theory We examine two popular estimators whose accuracy and sample
complexity depend on their associated variances."
__label__graph_neural_networks However, practitioners often prefer using more complex aggregations and mixtures of diverse aggregations.
__label__machine_learning_for_other_sciences_and_fields In addition, we propose a Diffusion Communication Mechanism (DCM) to promote better communication and control performance under data-missing scenarios.
__label__diffusion_based_models Time series are associated with various attributes, including trends, seasonality, and external information such as location.
__label__optimization_for_deep_networks Our major observation shows that the {\it degree of sign consistency} between the noise and the convergence point is a critical indicator of valley symmetry.
__label__natural_language_processing Improving the reasoning capabilities of large language models (LLMs) has attracted considerable interest.
__label__interpretability_and_explainability However, the algorithmic aspects of measuring model calibration have remained relatively less well-explored.
__label__generative_models To simultaneously achieve high fidelity, consistency, and efficiency in single image-to-3D, we propose a novel framework Unique3D that includes a multi-view diffusion model with a corresponding normal diffusion model to generate multi-view images with their normal maps, a multi-level upscale process to progressively improve the resolution of generated orthographic multi-views, as well as an instant and consistent mesh reconstruction algorithm called ISOMER, which fully integrates the color and geometric priors into mesh results.
__label__evaluation In this work, we present RaVL, which takes a fine-grained perspective on VLM robustness by discovering and mitigating spurious correlations using local image features rather than operating at the global image level.
__label__other Our formulation turns the map equation compatible with any neural network architecture, enables end-to-end learning, incorporates node features, and chooses the optimal number of clusters automatically, all without requiring explicit regularisation.
__label__machine_learning_for_healthcare Notably, generating PS masks can be performed on the fly during VLP, which does not incur extra trainable parameters.
__label__reinforcement_learning A challenging problem in seeking to bring multi-agent reinforcement learning (MARL) techniques into real-world applications, such as autonomous driving and drone swarms, is how to control multiple agents safely and cooperatively to accomplish tasks.
__label__deep_learning_architectures We provide a formal rationale for why the proposed training method can reduce overall prediction errors while minimizing the impact of skipping sub-paths.
__label__diffusion_based_models Recently, diffusion denoising bridge models (DDBMs), a new formulation of generative modeling that builds stochastic processes between fixed data endpoints based on a reference diffusion process, have achieved empirical success across tasks with coupled data distribution, such as image-to-image translation.
__label__optimization Our proof relies on a novel lemma characterizing the dynamics of stochastic Nesterov accelerated gradient descent algorithm under distribution drift with high probability for the lower-level variable, which is of independent interest and also plays a crucial role in analyzing the hypergradient estimation error over time.
__label__diffusion_based_models Previous methods concentrate on image compositing at the 2D level, which fall short in handling complex spatial relationships ($\textit{e.g.
__label__machine_learning_for_physical_sciences However, being data-intensive, these methods still require a large amount of PDE data.
__label__online_learning Overall, this work provides insights into when continual learning is difficult and how to mitigate it.
__label__machine_vision Most existing methods address the USVI-ReID through cluster-based contrastive learning, which simply employs the cluster center to represent an individual.
__label__natural_language_processing The key idea of the proposed algorithm called aespa is to perform quantization layer-wise for efficiency while targeting attention-wise reconstruction to consider the cross-layer dependency.
__label__machine_learning_for_other_sciences_and_fields Then we provide three major insights for these challenges from extensive empirical analysis: 1) DNAS tends to overfit to too many skip-connections, consequently wasting a significant portion of the network's expressive capabilities; 2) DNAS suffers from the structure bias between the network architecture and the circuit inherent structure, leading to inefficient search; 3) the learning difficulty of different input-output examples varies significantly, leading to severely imbalanced learning.
__label__optimization It is unclear whether the strong convexity assumption can enable even better convergence results.
__label__algorithmic_game_theory We formalize this problem as one of *policy aggregation*, where the goal is to identify a desirable collective policy.
__label__privacy These include approximate certified unlearning for non-convex problems, complexity saving compared to retraining, sequential and batch unlearning for multiple unlearning requests.
__label__interpretability_and_explainability We find that there is a strong disconnect between the existing research and the practical requirements for AR.
__label__probabilistic_methods Although these have great potential, DDM priors yield complex posterior distributions that are challenging to sample from.
__label__fairness We introduce novel algorithms for tracing the complete trade-off curve, or Pareto front, between quality and fairness in clustering problems; that is, computing all clusterings that are not dominated in both objectives by other clusterings.
__label__safety_in_machine_learning Moreover, we show that AttnGCG is able to offer enhanced interpretability by visualizing models' attention scores across different input components, thus providing clear insights into how targeted attention manipulation contributes to more successful jailbreaking.
__label__machine_learning_for_healthcare Our TFM demonstrates competitive generalization and transferability performance over the existing TFMs on biologically important tasks including identifying novel cell types of unseen cells, prediction of cell-type-specific marker genes, and cancer drug responses.
__label__natural_language_processing To mitigate these limitations, we propose Chain-of-Agents (CoA), a novel framework that harnesses multi-agent collaboration through natural language to enable information aggregation and context reasoning across various LLMs over long-context tasks.
__label__machine_vision We first introduce a general mask editing method that combines rigid and non-rigid editing techniques to generate high-quality synthetic masks.
__label__optimization_for_deep_networks For the complex of multiple objects in an image, we propose Foreground Background Decoupling to centrally update the foreground of multiple instances and Incremental PatchExpand to further enhance the diversity of foregrounds.
__label__reinforcement_learning In this paper, we harness the dual role of text in this task, using it not only to provide constraint but also as a training signal.
__label__safety_in_machine_learning We explore the effects of representational alignment between humans and AI agents on learning human values.
__label__optimization And also, using an already different approach we provide the asymptotic convergence of _the first algorithm with the stochastic Order Oracle concept_.
__label__natural_language_processing To this end, we contribute context-conditioned preference datasets and accompanying experiments that investigate the ability of language models to evaluate context-specific preference.
__label__machine_vision However, inspection on sample data reveals that the vast majority of voxels is unoccupied.
__label__graph_neural_networks Conventional methods typically require numerous labels for node classification.
__label__optimization We numerically validated our convergence results using a synthetic function and demonstrated the effectiveness of our proposed methods using LSTM, Nano-GPT, and T5.
__label__machine_vision A natural expansion manner is to adopt a larger lexicon; however, the inevitable introduction of numerous synonyms and uncommon words fails to meet the above requirements, indicating that viable expansion manners move beyond merely selecting words from a lexicon.
__label__safety_in_machine_learning For instance, infrared images are typically grayscale, unlike visible images that contain color information.
__label__optimization_for_deep_networks Motivated by the gradient inconsistency observation, we propose a simple yet effective \underline{G}radient \underline{R}ewiring method for \underline{E}ditable graph neural network training, named \textbf{GRE}.
__label__online_learning To address the crucial need to keep models updated, online learning has emerged as a critical tool when utilizing LLMs for real-world applications.
__label__human-AI_interaction Our human study involving 50 subjects offers strong quantitative and qualitative evidence of the effectiveness of our approach.
__label__evaluation In this work, we propose and analyze a model that assumes that the attribute responsible for the shift is unknown in advance.
__label__online_learning This algorithm adopts a two-layer structure with a meta-algorithm running over a group of base-learners.
__label__optimization_for_deep_networks This mechanism enables matrics to set a higher initial rank, thus expanding the allocation space for ranks.
__label__generative_models This failure is highly related to the low quality of training data.
__label__generative_models While the continuous Entropic Optimal Transport (EOT) field has been actively developing in recent years, it became evident that the classic EOT problem is prone to different issues like the sensitivity to outliers and imbalance of classes in the source and target measures.
__label__diffusion_based_models DDMC is an oracle-based meta-algorithm, where its oracle is the assumed access to samples that generate a Monte Carlo score estimator.
__label__reinforcement_learning To address this challenge, we propose Text-Aware Diffusion for Policy Learning (TADPoLe), which uses a pretrained, frozen text-conditioned diffusion model to compute dense zero-shot reward signals for text-aligned policy learning.
__label__online_learning While existing prompt-based continual learning methods excel in leveraging prompts for state-of-the-art performance, they often lack a theoretical explanation for the effectiveness of prompting.
__label__machine_learning_for_social_sciences However, when agents' collaborations are leveraged to perform multi-person tasks, a new challenge arises due to information asymmetry, since each agent can only access the information of its human user.
__label__machine_learning_for_physical_sciences Experiments show that while EUNet effectively delivers the energy gradients due to the deformations, models constrained by EUNet achieve more stable and physically plausible performance comparing with those trained in garment-wise supervised manner.
"__label__other To identify concrete objects in complex scenes, humans instinctively rely on deictic descriptions in natural language, i.e., referring to something depending on the context such as ""The object that is on the desk and behind the cup.""."
__label__learning_theory We show that minimax risk is *tightly* characterized (up to a logarithmic factor of the hypothesis class size) by the *Hellinger gap* of the noisy label distributions induced by the kernel, *independent* of other properties such as the means and variances of the noise.
__label__machine_vision Our method explores the VFMs prior and how to harness them, aiming to inherit the recognition ability of VFMs.
__label__evaluation (3) Language model with large scale is more resistant to editing compared to small model.
__label__infrastructure These improvements translate into up to 104% improvement in inference and 39% improvement in training existing models based on neighborhood attention, and additionally extend its applicability to image and video perception, as well as other modalities.
__label__interpretability_and_explainability However, these studies have not yet been extended to Multi-modal Large Language Models (MLLMs).
__label__diffusion_based_models The creation of complex 3D scenes tailored to user specifications has been a tedious and challenging task with traditional 3D modeling tools.
__label__evaluation We introduce PromptEval, a method for estimating performance across a large set of prompts borrowing strength across prompts and examples to produce accurate estimates under practical evaluation budgets.
__label__reinforcement_learning We propose **M**ulti-step **A**ction **R**epre**S**entation (**MARS**), which encodes a sequence of actions from the original action space to a compact and decodable latent space.
__label__deep_learning_architectures Yet the results of NAS are fairly prosaic; they did not e.g.
__label__deep_learning_architectures We evaluate TAGI on the Super-Natural Instructions and P3 datasets.
__label__natural_language_processing This approach leverages extensively aligned pre-training data to enhance the effectiveness and usability of pre-trained models.
__label__algorithmic_game_theory We here focus on randomized algorithms for the fair matching of indivisible items, subject to various definitions of fairness.
__label__diffusion_based_models This paper presents innovative enhancements to diffusion models by integrating a novel multi-resolution network and time-dependent layer normalization.
__label__online_learning L-ARC updates a threshold function within a reproducing kernel Hilbert space (RKHS), with the kernel determining the level of localization of the statistical risk guarantee.
__label__machine_vision Our study reveals that 3D Gaussian Splatting (3DGS) is particularly susceptible to this noise, leading to numerous elongated Gaussian shapes that overfit the noise, thereby significantly degrading reconstruction quality and reducing inference speed, especially in scenarios with limited views.
__label__probabilistic_methods This work addresses these issues by introducing a variational spatio-temporal state-space GP that handles linear and non-linear physical constraints while achieving efficient linear-in-time computation costs.
__label__generative_models Additionally, the crystals generated by FlowLLM are much closer to their relaxed state when compared with another leading model, significantly reducing post-hoc computational cost.
__label__other It enhances LLMs' denoising-reasoning capabilities by contrasting noisy rationales with only one clean rationale, which can be the minimal requirement for denoising-purpose prompting.
__label__deep_learning_architectures The approach leverages a multi-exit architecture in MLLMs, which allows the model to cease processing once a proper size of the model has been activated for a specific situation, thus avoiding further redundant computation.
__label__interpretability_and_explainability In this paper, we review the recent development of classical federated primal dual methods and point out a serious common defect of such methods in non-convex scenarios, which we say is a ``dual drift'' caused by dual hysteresis of those longstanding inactive clients under partial participation training.
__label__machine_learning_for_healthcare Obtaining binding affinity data for complexes is highly expensive, resulting in a limited amount of available data that covers a relatively small chemical space.
__label__machine_vision In this context, task graphs have emerged as a human-understandable representation of procedural activities, encoding a partial ordering over the key-steps.
__label__generative_models Our distinctive geometric perspective of statistical manifolds allows us to apply optimal transport during training and interpret SFM as following the steepest direction of the natural gradient.
__label__reinforcement_learning However, standard RL algorithms like Q-learning learn a stationary policy.
__label__diffusion_based_models Importantly, this allows us to perform decoder computation in parallel, further accelerating the denoising process.
__label__learning_theory Our algorithm constitutes a fully polynomial-time approximation scheme (FPTAS) for any time-space recursive (TSR) cost criteria.
__label__machine_learning_for_healthcare Motion time series collected from low-power, always-on mobile and wearable devices such as smartphones and smartwatches offer significant insights into human behavioral patterns, with wide applications in healthcare, automation, IoT, and AR/XR.
__label__diffusion_based_models To comprehensively evaluate TwinAct, we construct a novel benchmark, which provides sample images with various forms of actions.
__label__natural_language_processing However, effective self-training remains a challenge regarding the unique visual perception and reasoning capability of LVLMs.
__label__machine_vision Without additional training, we connect these two generalized models with attention maps as the prompts.
__label__neuroscience_and_cognitive_science Using this approach, we find that the number of contexts storable by the hippocampus grows exponentially with the number of place cells, and calculate this exponent for environments of different sizes.
__label__generative_models Specifically, we present a novel framework, \textbf{Diffusion4D}, for efficient and scalable 4D content generation.
__label__safety_in_machine_learning We show that our novel construction is not only unelicitable thanks to using cryptographic techniques, but also has favourable robustness properties.
__label__safety_in_machine_learning Performative prediction aims to model scenarios where predictive outcomes subsequently influence the very systems they target.
__label__machine_vision With such a novel paradigm, we boost the SOTA query-based detector DINO from 49.0% AP to 51.9% AP (+2.9% AP) and further to 53.8% AP (+4.8% AP)  by integrating one or two foundation models respectively, on the COCO validation set after training for 12 epochs with R50 as the detector's backbone.
__label__reinforcement_learning Extensive experiments on simulation tasks and real-world robotic grasping tasks show that MARS significantly improves the learning efficiency and final performances compared with existing baselines.
__label__causal_inference Causal sufficiency is both unrealistic and empirically untestable.
__label__neuroscience_and_cognitive_science Experimental results show that the constructed spiking network can effectively segment the motion contained in event streams.
__label__reinforcement_learning Comprehensive evaluations on 18 tasks that vary in data quality and environment context demonstrate the superior performance of BECAUSE over existing offline RL algorithms.
__label__machine_vision Extensive experiments demonstrate that PHYRECON significantly improves the reconstruction quality.
__label__fairness We further estimate the regularizer in more general cases and explore the relationship between DRO and classical robust optimization.
__label__neuroscience_and_cognitive_science Through guiding the diffusion model to activate individual latent factors, we verify that the neural dynamics of latent factors in the disentangled neural subspace provide interpretable quantifications of the behaviors of interest.
__label__neuroscience_and_cognitive_science Linear models trained on data from one session successfully decode velocity, position, and direction in held-out test data from different dates and cortical areas (64\%, 88\%, and 90\%).
__label__causal_inference due to unmeasured confounding, distributional shift, or an adversarial environment.
__label__interpretability_and_explainability However, this paradigm cannot adaptively explore reasoning paths in KGs based on the question semantics and self-correct erroneous reasoning paths, resulting in a bottleneck in efficiency and effect.
__label__privacy We study the limits and capability of public-data  assisted differentially private (PA-DP) algorithms.
__label__deep_learning_architectures This indicates its ability to achieve the same level of performance with lower power consumption in synchronous scenarios.
__label__neuroscience_and_cognitive_science Our theory is validated by experiments on both linear and non-linear networks.
__label__reinforcement_learning By relying instead on a diameter estimation procedure, we propose the first algorithm for $(\varepsilon,\delta)$-PAC policy identification that does not need any form of prior knowledge on the MDP.
__label__optimization_for_deep_networks We show an implicit regularization towards flat minima: the sharpness of the minimizer is no more than a constant times the lower bound.
__label__safety_in_machine_learning Out-of-Distribution (OOD) generalization in machine learning is a burgeoning area of study.
__label__machine_vision First, the class token in foundation models provides an in-depth understanding of the complex scene, which facilitates decoding object queries in the detector's decoder by providing a compact context.
__label__machine_learning_for_other_sciences_and_fields The primary objective in biological-sequence editing is to determine the optimal modifications to a sequence which augment certain biological properties while adhering to a minimal number of alterations to ensure predictability and potentially support safety.
__label__interpretability_and_explainability We evaluate different resources of knowledge and recognition architectures on a broad range of domain shifts across 20 datasets.
__label__machine_vision To address the client gap, we design a local-synergistic contrastive learning approach that helps single-view clients and multi-view clients achieve consistency for mitigating heterogeneity among all clients.
__label__machine_learning_for_other_sciences_and_fields By processing cryo-EM movies into odd and even images and treating them as independent noisy observations, we apply a denoising-reconstruction hybrid training scheme.
__label__neuroscience_and_cognitive_science Then, we show that the balance and alignment of gradient noise can serve as a novel alternative mechanism for explaining important phenomena such as progressive sharpening/flattening and representation formation within neural networks and have practical implications for understanding techniques like representation normalization and warmup.
__label__machine_vision To enhance multiview feature extraction with 3D perception, we employ a self-supervised Vision Transformer (ViT) with cross-view completion pre-training on large-scale datasets.
__label__other RCC does not require any training and its worst-case complexity scales quasi-linearly with the size of the largest cluster.
__label__machine_learning_for_other_sciences_and_fields Furthermore, as a novel plug-and-play technique, the RCF can also significantly improve the prediction accuracy of existing models, including PatchTST and iTransformer.
__label__diffusion_based_models Taken together, our improvements set new benchmarks in one-step image generation, with FID scores of 1.28 on ImageNet-64×64 and 8.35 on zero-shot COCO 2014, surpassing the original teacher despite a 500X reduction in inference cost.
__label__optimization_for_deep_networks However, existing theoretical analyses for sketching-based distributed learning (sketch-DL) either incur a prohibitive dependence on the ambient dimension or need additional restrictive assumptions such as heavy-hitters.
__label__machine_learning_for_physical_sciences In this work, seeking data efficiency, we design unsupervised pretraining for PDE operator learning.
__label__diffusion_based_models The code and the arXiv version can be found on the [project website](https://chicychen.github.io/LOCO).
__label__graph_neural_networks Subsequently, the class prototypes are iteratively refined based on the learned node representations, initialized with the semantic vectors.
__label__graph_neural_networks Here, we propose and study unitary group convolutions, which allow for deeper networks that are more stable during training.
__label__deep_learning_architectures For example, our CRATE-$\alpha$-B substantially outperforms the prior best CRATE-B model accuracy on ImageNet classification by 3.7%, achieving an accuracy of 83.2%.
__label__generative_models Already, some theoretical results on self-consuming generative models (a.k.a., iterative retraining) have emerged in the literature, showcasing that either model collapse or stability could be possible depending on the fraction of generated data used at each retraining step.
__label__machine_vision We showcase exceptional results in both LiDAR semantic segmentation and 3D object detection tasks, under diverse weather and sensor failure conditions.
__label__natural_language_processing The eviction policy not only enables D-LLMs to be compatible with prevalent applications but also reduces considerable storage resources.
__label__machine_learning_for_physical_sciences We show that the learned generative priors lead to a versatile framework for accurately solving a wide range of PDEs under partial observation, significantly outperforming the state-of-the-art methods for both forward and inverse directions.
__label__evaluation Meanwhile, further analyses also indicate that CoPA can learn better representation clusters, enlarge the gap, and achieve the minimum validation loss at the enlarged gap.
__label__reinforcement_learning Despite the simplicity and practicality of $\texttt{RRL-MNL}$, its regret bound scales with $\kappa^{-1}$, which is potentially large in the worst case.
__label__natural_language_processing Additionally, a close connection between MoT and MoE is demonstrated through a novel technique we call *transition tuning*.
__label__machine_vision In this paper, we highlight a fundamental limitation of Gaussian Splatting: its inability to accurately render discontinuities and boundaries in images due to the continuous nature of Gaussian distributions.
__label__machine_vision Furthermore, PathWeave performs comparably to state-of-the-art MLLMs while concurrently reducing parameter training burdens by 98.73\%.
__label__machine_vision Specifically, our model is based on the pose-normalized query/patch pairs and enhanced by the proposed intrinsic patch geometry representation, modeling the intrinsic 3D patch geometry feature by learnable multi-head memory banks.
__label__natural_language_processing Current methods for large language model alignment typically use scalar human preference labels.
__label__machine_vision One main challenge in state-of-the-art retrieval methods lies in the modality gap, which stems from the substantial disparities between text and video and can persist in the joint space.
__label__learning_theory Our result refutes the previous hypothesis that LayerNorm plays no role in the rank collapse of self-attention and suggests that self-attention with LayerNorm constitutes a much more expressive, versatile nonlinear dynamical system than what was originally thought.
__label__online_learning However, a critical aspect often overlooked is the label delay, where new data may not be labeled due to slow and costly annotation processes.
__label__privacy Therefore, ensuring the security and fidelity of the original 3D scene while embedding information into the 3DGS point cloud files is an extremely challenging task.
__label__other We leverage this unique capability to propose NoMAD-Attention, an efficient attention algorithm that replaces MAD operations with in-register lookups.
__label__privacy In particular, we give an example of hyperparameters that result in $\varepsilon \approx 1$ for Poisson subsampling and $\varepsilon > 10$ for sampling without replacement.
__label__causal_inference However, the field has lacked feasible methods to integrate partial order constraints, a critical prior information typically used in real-world scenarios, into the differentiable structure learning framework.
__label__machine_vision To achieve this, we introduce a large-scale dataset named RefHuman, which substantially extends the MS COCO dataset with additional text and positional prompt annotations.
__label__natural_language_processing Our method outperforms strong baselines in dialogue tasks and achieves a 4 $\times$ speedup while reducing memory usage by 18 $\times$ compared to dense attention recomputation.
__label__machine_learning_for_other_sciences_and_fields Using this dataset, we apply DPO to optimize the original model, improving its performance in generating structures closely aligned with the desired reference distribution.
"__label__machine_vision Further
experiments on the Argoverse dataset using YOLOv8 confirm that RFPAR
effectively removed objects on a larger scale dataset."
__label__learning_theory Due to the utilization of solely unlabeled samples, there exists significant uncertainty in model updates, leading CTTA to encounter severe error accumulation issues.
__label__privacy Federated Learning (FL) is commonly used to collaboratively train models with privacy preservation.
__label__other Deep learning models can exhibit what appears to be a sudden ability to solve a new problem as training time, training data, or model size increases, a phenomenon known as emergence.
__label__fairness Federated learning (FL) offers a machine learning paradigm that protects privacy, allowing multiple clients to collaboratively train a global model while only accessing their local data.
__label__privacy This multiple model setup becomes infeasible for large open-source models.
__label__deep_learning_architectures GSAAL is specifically designed to address the MV limitation while also handling the IA and CD, being the only method to do so.
__label__generative_models Generative modeling over discrete data has recently seen numerous success stories, with applications spanning language modeling, biological sequence design, and graph-structured molecular data.
__label__machine_vision By utilizing the GSS block and the fusion module, Hamba effectively leverages the graph-guided state space features and jointly considers global and local features to improve performance.
__label__natural_language_processing Furthermore, recreating existing figures that are not stored in formats preserving semantic information is equally complex.
__label__privacy We empirically demonstrate that calibrating noise to attack sensitivity/specificity, rather than ε, when training privacy-preserving ML models substantially improves model accuracy for the same risk level.
__label__learning_theory We explore these problems in a setting where a predicted data distribution, possibly derived from historical data or predictive machine learning models, is available.
__label__natural_language_processing The project and data will be released at https://github.com/ydk122024/PediatricsGPT.
__label__evaluation The quality score is further optimized by maximum a posteriori estimation with the resulting probability matrix.
__label__learning_theory We study multi-agent reinforcement learning (RL) where agents cooperate through asynchronous communications with a central server to learn a shared environment.
__label__other This framework leverages the collaboration of various agents in the planning and coding process to unlock the potential of LLMs to resolve GitHub issues.
__label__natural_language_processing Efficient fine-tuning of large language models for task-specific applications is imperative, yet the vast number of parameters in these models makes their training increasingly challenging.
__label__interpretability_and_explainability We offer conceptual arguments grounded in identification issues such as distinguishing a model's knowledge from that of a simulated character's that are likely to persist in future unsupervised methods.
__label__generative_models Based on the coupled structures in LLMs, \model selects a few attention heads and channels in the MHA and FFN modules for each Transformer block, respectively.
__label__safety_in_machine_learning In contrast to the standard class-conditional CP (CCP) method that uniformly thresholds the class-wise conformity score for each class, the augmented label rank calibration step allows RC3P to selectively iterate this class-wise thresholding subroutine only for a subset of classes whose class-wise top-$k$ error is small.
__label__bandits We formally characterize the set of necessary and sufficient latent confounders one needs to detect or learn to ensure that all possibly optimal arms are identified correctly.
__label__machine_vision Source code will be made publicly available.
__label__optimization_for_deep_networks To address these challenges, we introduce the Block-Level Adaptive STructured (BLAST) matrix, designed to learn and leverage efficient structures prevalent in the weight matrices of linear layers within deep learning models.
__label__generative_models Latent-based image generative models, such as Latent Diffusion Models (LDMs) and Mask Image Models (MIMs), have achieved notable success in image generation tasks.
__label__learning_theory Theoretically explaining the benefits of complex parameterizations for SSMs is an open problem.
__label__deep_learning_architectures Theoretical upper bounds on the approximation error of $\rm CALDERA$ are established using a rank-constrained regression framework, and the tradeoff between compression ratio and model performance is studied by analyzing the impact of target rank and quantization bit budget.
__label__machine_learning_for_physical_sciences With the advent of advanced microscopy, detailed images of complex tissue formations involving multiple cell types have become more accessible in recent years.
__label__reinforcement_learning Reinforcement learning (RL) excels in optimizing policies for discrete-time Markov decision processes (MDP).
__label__graph_neural_networks Motivated by these limitations, we propose *Spatio-Spectral Graph Neural Networks (S²GNNs)* – a new modeling paradigm for Graph Neural Networks (GNNs) that synergistically combines spatially and spectrally parametrized graph filters.
__label__diffusion_based_models We attribute this to the issue called conditional image leakage, where the image-to-video diffusion models (I2V-DMs) tend to over-rely on the conditional image at large time steps.
__label__machine_vision Federated Learning (FL) is a form of distributed learning that allows multiple institutions or clients to collaboratively learn a global model to solve a task.
__label__bandits We also initiate the study of corrupted adversarial linear bandits, obtaining upper and lower bounds with matching dependencies on the corruption level.
__label__machine_vision DI-MaskDINO outperforms existing joint object detection and instance segmentation models on COCO and BDD100K benchmarks, achieving +1.2 $AP^{box}$ and +0.9 $AP^{mask}$ improvements compared to SOTA joint detection and segmentation model MaskDINO.
__label__natural_language_processing However, fine-tuning a large language model can be challenging.
__label__optimization An improved empirical Fisher (iEF) method is proposed to address this issue, which is motivated as a generalised NGD method from a loss reduction perspective, meanwhile retaining the practical convenience of EF.
__label__optimization Experimentally, ZO-GDEGA can generate more effective poisoning attack data with an average accuracy reduction of 5\%.
__label__machine_vision In this paper, we introduce a novel binarized diffusion model, BI-DiffSR, for image SR. First, for the model structure, we design a UNet architecture optimized for binarization.
__label__machine_vision Recent methods introduce a paradigm that leverages superpoints as geometric primitives and incorporates 2D multi-view masks from Segment Anything model (SAM) as merging guidance, achieving outstanding zero-shot instance segmentation results.
__label__machine_vision Based on our dataset, we propose a novel framework, De-MINDS, for capturing the intent humans aim to modify, thereby enhancing the ZS-CIR model's ability to understand human manipulation descriptions.
__label__learning_theory We validate the method with experiments, including an application to nearest neighbor search.
__label__learning_theory In this paper, we propose methodologies for aggregating prediction intervals to obtain one with minimal width and adequate coverage on the target domain under unsupervised domain shift, under which we have labeled samples from a related source domain and unlabeled covariates from the target domain.
__label__robotics Find the project at https://github.com/AIR-THU/V2X-Graph.
__label__safety_in_machine_learning When we allow them to instead choose which answer to argue for, we find judges are less frequently convinced by the wrong answer in debate than in consultancy.
__label__machine_vision In this paper, we develop a single any-to-any model trained on tens of highly diverse modalities and by performing co-training on large-scale multimodal datasets and text corpora.
__label__neuroscience_and_cognitive_science This finding reconciles computational analyses of brain activity using large language models with the classic observation from aphasic patients showing left hemisphere dominance for language.
__label__probabilistic_methods Bayesian optimization is a technique for efficiently optimizing unknown functions in a black-box manner.
__label__reinforcement_learning Additionally, our analysis of training dynamics reveals that CurrMask gradually acquires skills of varying complexity by dynamically adjusting its masking scheme.
__label__fairness Finally, we conduct experiments to evaluate the consistency of our theoretically derived egalitarian fairness bounds with the empirically achieved egalitarian fairness in fair FL settings.
"__label__natural_language_processing In this paper, for LLMs utilizing RoPE as position embeddings, we introduce a novel method called ""Mixture of In-Context Experts"" (MoICE) to address this challenge."
__label__machine_vision Our framework features a renaming model that enhances the quality of names for each visual segment.
__label__natural_language_processing Experiments show substantial quality improvements over the vanilla baselines, surpassing the previous state-of-the-art (SOTA) by wide margins.
__label__machine_vision Test-time adaptation, which enables models to generalize to diverse data with unlabeled test samples, holds significant value in real-world scenarios.
__label__graph_neural_networks To efficiently reduce the search space of potential molecules, we further introduce a Molecule Extraction Policy Network for molecule extraction.
__label__deep_learning_architectures Recent linear and transformer-based forecasters have shown superior performance in time series forecasting.
__label__optimization_for_deep_networks Previous studies in this field have concentrated on capturing in isolation either the inter-modality dependencies (the relationships between different modalities and the label) or the intra-modality dependencies (the relationships within a single modality and the label).
__label__machine_learning_for_physical_sciences We argue that multiple processes in natural sciences have to be represented as vector fields on the Wasserstein manifold of probability densities.
__label__generative_models To demonstrate the effectiveness of our method, we introduce a pre-training scheme inspired by mBART but adapted to leverage structure tokens.
__label__online_learning [2023]](https://openreview.net/forum?id=AA1xrgAP5z).
__label__graph_neural_networks The classifier based on graph attention network is applied to obtain predicted labels to enhance cross-modal feature representation.
__label__fairness While current approaches focus on creating fair synthetic representative samples by optimizing local properties relative to the original samples, their impact on downstream learning processes has yet to be explored.
__label__machine_vision To this end, we propose to explore the vulnerabilities of VIReID systems and prevent potential serious losses due to insecurity.
__label__safety_in_machine_learning helpful and often human-written) examples required in the standard alignment process.
__label__deep_learning_architectures Moreover, we design a learning-based integration strategy to integrate two losses dynamically, further improving the performance.
__label__evaluation This phenomenon is especially prominent in high-noise settings.
__label__machine_vision However, the disentanglement is challenging due to the complexity of glyphs, often resulting in glyphs that are influenced by the style of the source glyph and prone to artifacts.
__label__optimization_for_deep_networks In our paper, we perform an in-depth investigation of the initialization method of LoRA and show that careful initialization (without any change of the architecture and the training algorithm) can significantly enhance both efficiency and performance.
__label__fairness We accomplish this by extending multicalibration to incorporate grouping functions that consider covariates and labels jointly.
__label__deep_learning_architectures Moreover, we introduce a hardware-aware speculative decoding algorithm that accelerates the inference speed of Mamba and hybrid models.
__label__reinforcement_learning We present a practical instantiation of DiSPOs using diffusion models and show their efficacy as a new class of transferable models, both theoretically and empirically across various simulated robotics problems.
__label__machine_vision Specifically, we provide an optimal label pool to store the best pseudo-labels during network training, leveraging both global and local coherence to select high-quality candidates and assign weights to prioritize haze-free regions.
__label__diffusion_based_models The code is available at https://github.com/UniModal4Reasoning/AdaptiveDiffusion
__label__machine_vision Despite huge progress in skeleton-based action recognition, its generalizability to different domains remains a challenging issue.
__label__interpretability_and_explainability Thus we refer to these as ``multi-modal concepts''.
__label__optimization In this paper, we propose LORA-MOO, a surrogate-assisted MOO algorithm that learns surrogates from spherical coordinates.
__label__deep_learning_architectures While tremendous empirical advances have been attained by transformer-based neural networks, a theoretical understanding of their algorithmic reasoning capabilities in realistic parameter regimes is lacking.
__label__generative_models Our approach reveals hundreds of effective adversarial prompts, with an attack success rate exceeding 90% across all tested models.
__label__safety_in_machine_learning In particular, we consider the (standard) task of adversarial robustness where we need to decide if a point is robust at a certain radius or not using as few samples as possible while maintaining statistical guarantees.
__label__privacy This novel insight fuels the development of a new black box membership inference attack utilizing input loss curvature.
__label__safety_in_machine_learning They also prevent people from misusing images, especially those generated by AI models.
__label__machine_learning_for_physical_sciences We consider the problem of crystal materials generation using language models (LMs).
__label__deep_learning_architectures Besides, the design of various components in YOLOs lacks the comprehensive and thorough inspection, resulting in noticeable computational redundancy and limiting the model's capability.
__label__machine_vision The proposed method excels in synthesizing glyphs with neat and correct strokes, and enables the creation of new glyphs based on provided IDS.
__label__causal_inference In this paper, we propose a unified ITR estimation framework formulated as a constrained, weighted, and smooth convex optimization problem.
__label__optimization_for_deep_networks Therefore, these redundant features and relevant redundant parameters can be identified via the reactivation process.
__label__generative_models We begin with a comprehensive analysis of the Flag-DiT architecture and identify several suboptimal components, which we address by introducing the Next-DiT architecture with 3D RoPE and sandwich normalizations.
__label__machine_learning_for_social_sciences Our code is available at https://github.com/Yukayo/LoTNext.
__label__other Recent advancements in DETR-based visual grounding methods have attracted considerable attention, as they directly predict the coordinates of the target object without relying on additional efforts, such as pre-generated proposal candidates or pre-defined anchor boxes.
__label__algorithmic_game_theory The original No Free Lunch (NFL) theorems highlight the limitations of traditional black-box optimisation and learning algorithms, serving as a theoretical foundation for traditional optimisation.
__label__machine_vision Given a video, Artemis receives a natural-language question with a bounding box in any video frame and describes the referred target in the entire video.
__label__machine_vision Empowered by the largest real-world, multi-view dataset to date, ODIN is able to freely generate novel views of real-world scenes.
__label__learning_theory Towards that goal, we consider a regression framework in which the unknown regression function is modeled as a mixture of experts, and study the rates of convergence of the least squares estimator under the over-specified case in which the number of fitted experts is larger than the true value.
__label__safety_in_machine_learning When supervised learning (SL) algorithms have failed, a malicious data collector possibly resorts to contrastive learning (CL) algorithms to bypass the protection.
__label__machine_vision In response to these limitations, we introduce the **Di**screte **Di**ffusion **Pose** (**$\text{Di}^2\text{Pose}$**), a novel framework designed for occluded 3D HPE that capitalizes on the benefits of a discrete diffusion model.
__label__learning_theory The running time of our approach scales with the actual number of pieces that appear as opposed to worst case upper bounds on the number of pieces.
__label__bandits We propose MaxMinLCB, which emulates this trade-off as a zero-sum Stackelberg game and chooses action pairs that are informative and have favorable reward values.
__label__safety_in_machine_learning Extensive experiments on MultiOOD demonstrate that training with A2D and NP-Mix improves existing OOD detection algorithms by a large margin.
__label__reinforcement_learning To our knowledge, CP3ER is the first method to apply diffusion/consistency models to visual RL and demonstrates the potential of consistency models in visual RL.
__label__machine_vision We term this approach ZERO (TTA with “zero” temperature), whose design is both incredibly effective and frustratingly simple: augment N times, predict, retain the most confident predictions, and marginalize after setting the Softmax temperature to zero.
__label__learning_theory We uncover a surprising mechanism: in a simplified linear setting where both approaches learn similar representations, JEPAs are biased to learn high influence features, or features characterized by having high regression coefficients.
__label__graph_neural_networks Specifically, we develop the Maximum Rayleigh Quotient Subgraph Sampler (MRQSampler) that unifies multi-level formats by transferring objects at each level into graph-level tasks on subgraphs.
__label__probabilistic_methods As a remedy, we directly place a prior on function space.
__label__machine_vision This raises the question: can we achieve flexibility in the number of visual tokens to suit different tasks and computational resources?
__label__safety_in_machine_learning (2021), which demonstrate that ACL appears if the distributions only shift in mean and covariance scale in Gaussian data.
__label__machine_vision These scenarios often involve Spatio-Temporal Discontinuities (i.e., STDChallenge), prevalent in long-term tracking and global instance tracking.
__label__online_learning In doing so, we explicitly construct learning algorithms that can handle extremely large or unbounded label spaces.
__label__machine_learning_for_healthcare A digital twin is a virtual replica of a real-world physical phenomena that uses mathematical modeling to characterize and simulate its defining features.
__label__probabilistic_methods Extensive experiments, involving 17 baseline methods across 30 diverse datasets, validate the effectiveness and generalization capability of the proposed method, surpassing state-of-the-art methods.
__label__machine_vision It acts as a guide for effective sparsity learning and speeds up training.
__label__natural_language_processing One way to address safety risks from large language models (LLMs) is to censor dangerous knowledge from their training data.
__label__natural_language_processing Additionally, we designate functions via a consensus formed by identifying similarities in program behavior, mitigating error propagation.
__label__machine_learning_for_healthcare Deep learning-based methods significantly advance the exploration of associations among triple-wise biological entities (e.g., drug-target protein-adverse reaction), thereby facilitating drug discovery and safeguarding human health.
__label__machine_vision In light of this, we propose harmonizing the visual appearance, head motion, and 3D object to excavate the object interaction concept and subject intention, jointly inferring 3D human contact and object affordance from egocentric videos.
__label__optimization_for_deep_networks Our proposed analysis reveals a critical instability in common neural network parameterizations and normalizations during stochastic optimization, which impedes fast convergence and hurts generalization performance.
__label__algorithmic_game_theory This setting allows us to shed light on the deleterious effect of adverse selection in collaborative learning.
__label__machine_vision To utilize more appropriate ranking metric and leverage more comprehensive information among the alternative set, we propose a novel in-context example selection framework to approximately identify the global optimal prompt, i.e.
__label__probabilistic_methods Code and examples are available at our [webpage](https://eliasnehme.github.io/PosteriorTrees/).
__label__active_learning To overcome this challenge, the concept of active finetuning has emerged, aiming to select the most appropriate samples for model finetuning within a limited budget.
__label__fairness Existing definitions of fairness in image restoration are highly restrictive.
__label__natural_language_processing To address this, we introduce the Information Bottleneck Protector (IBProtector), a defense mechanism grounded in the information bottleneck principle, and we modify the objective to avoid trivial solutions.
__label__machine_vision The code is available here: https://github.com/mshukor/ima-lmms.
__label__causal_inference In this paper, we focus on permutation-based methods for learning causal graphs in Linear Gaussian Acyclic Models (LiGAMs), where the permutation encodes a causal ordering of the variables.
__label__evaluation (2) Establish the relationship between the discriminative and generative realms: the accuracy of the discriminative question type exhibits a strong positive correlation with its Consistency with the caption.
__label__deep_learning_architectures In this study, we extend the method for *vector-valued joint-group-equivariant* feature maps, so to cover such real networks.
__label__machine_learning_for_social_sciences Forecasting future events is important for policy and decision making.
__label__machine_learning_for_other_sciences_and_fields Addressing these challenges, the integration of artificial intelligence with materials science has opened avenues for accelerating the discovery process, though it also demands precise annotation, data extraction, and traceability of information.
__label__deep_learning_architectures While recent PEFT methods have narrowed this gap, they do so at the expense of additional learnable parameters.
__label__interpretability_and_explainability From a well-trained sparse and shallow neural network, one can interpret each layer and neuron through the language of logic rules, and a global explanatory rule set can be directly extracted.
__label__probabilistic_methods We demonstrate how our generic approach can be used to derive the optimal decision strategies for these diverse instances.
__label__diffusion_based_models Recent advances in diffusion models have demonstrated their strong capabilities in generating high-fidelity samples from complex distributions through an iterative refinement process.
__label__machine_vision This method not only incorporates long-term temporal dynamics into the streaming encoding process but also yields a fixed-length memory as a global representation for arbitrarily long videos.
__label__deep_learning_architectures By observing that composition of low frequency functions can effectively approximate a high-frequency function, we propose to learn a function containing high-frequency components by composing several SNNs, each of which learns certain low-frequency information from the given data.
__label__machine_learning_for_other_sciences_and_fields Learning subset-valued functions with neural networks has achieved great success by incorporating permutation invariance symmetry into the architecture.
__label__natural_language_processing First, it has a few-shot self-verification mechanism, which estimates the reliability of its own outputs without requiring extensive training.
__label__speech_and_audio However, these encoders demand high memory usage and computation complexity, making them impractical for resource-constrained wimpy devices.
__label__probabilistic_methods However, standard GP models assume homoskedastic Gaussian noise, while many real-world applications are subject to non-Gaussian corruptions.
__label__evaluation With the emergence of large pre-trained multimodal video models, multiple benchmarks have been proposed to evaluate model capabilities.
__label__reinforcement_learning We provide a near-optimal upper bound and a matching minimax lower bound for the $sa$-rectangular scenarios.
__label__diffusion_based_models This work not only pushes the boundaries of generative image compositing but also reduces reliance on expensive annotated datasets by effectively utilizing existing resources in innovative ways.
__label__optimization For more complex problems such as hypergraph minimum vertex cover (HMVC), numerous slack variables are introduced which drastically increase the search domain and reduce the effectiveness of the SNN solver.
__label__reinforcement_learning Despite various attempts to combine LLMs with RL, there is commonly a semantic gap between action signals and LLM tokens, which hinders their integration.
__label__deep_learning_architectures In this way, we obtain a meta-model, the meta-parameter of which is friendly to the test-time optimization.
__label__machine_vision Our work aims to help researchers understand the bias in existing large-scale pre-training datasets, and build more diverse and representative ones in the future.
__label__machine_vision Code is released at: https://github.com/zhengchen1999/BI-DiffSR.
__label__safety_in_machine_learning In this work, we propose a novel approach, namely $\textit{normalized outlier distribution adaptation}$ (AdaptOD), to tackle this distribution shift problem.
__label__algorithmic_game_theory We first demonstrate that in the absence of property rights in the considered online scenario, the social welfare breaks down.
__label__natural_language_processing Moreover, we also extend our standard D-CPT Law on cross-domain settings and propose the Cross-Domain D-CPT Law to predict the D-CPT law of target domains, where very small training costs (about 1\% of the normal training costs) are needed for the target domains.
__label__optimization_for_deep_networks Furthermore, we outperform QLoRA for fine-tuning LLaMA and show competitive performance against other memory-efficient pre-training methods on the large-scale C4 dataset.
__label__deep_learning_architectures To overcome the large computational overhead of standard NCAs, we propose Dynamic Interaction for more efficient interaction learning.
__label__probabilistic_methods Our experiments illustrate that the QB-Vine is appropriate for high dimensional distributions ($\sim$64), needs very few samples to train ($\sim$200) and outperforms state-of-the-art methods with analytical forms for density estimation and supervised tasks by a considerable margin.
__label__diffusion_based_models Guidance is a crucial technique for extracting the best performance out of image-generating diffusion models.
__label__natural_language_processing Direct Alignment Algorithms (DDAs), such as Direct Preference Optimization (DPO) have emerged as alternatives to the classical RLHF pipeline.
__label__machine_learning_for_physical_sciences The cross ST-Mamba blocks are then devised to integrate the adjacent transient features.
__label__machine_learning_for_healthcare Deep neural networks have demonstrated remarkable performance in various vision tasks, but their success heavily depends on the quality of the training data.
__label__diffusion_based_models Specifically, TwinAct involves three key steps: 1) Building common action space based on a set of representative action phrases; 2) Imitating the customized action within the action space; and 3) Generating highly adaptable customized action images in diverse contexts with action similarity loss.
__label__optimization_for_deep_networks Experiments on standard downstream vision tasks demonstrate that our method achieves promising fine-tuning performance.
__label__optimization We finally provide a repeated stochastic gradient descent scheme that converges to the PSC solution and analyze its non-asymptotic convergence rate.
__label__reinforcement_learning Despite its potential, many existing CRL methods struggle to efficiently guide agents toward desired outcomes, particularly in the absence of domain knowledge.
__label__learning_theory Given training data of $n$-dimensional LPs, we learn an $n\times k$ projection matrix with $n > k$.
__label__probabilistic_methods We illustrate the validity of our findings as $N$ gets larger,  in a teacher-student experimental setting, training a student NN to learn from a WI, SI  or arbitrary teacher model through various SL schemes.
__label__diffusion_based_models Moreover, we theoretically deduce the upper bound on the error of the reward model, which illustrates the potential confidence of reward estimation throughout the reinforcement alignment process, thereby facilitating accurate regularization.
__label__bandits In this paper, we study the contextual multinomial logit (MNL) bandit problem in which a learning agent sequentially selects an assortment based on contextual information, and user feedback follows an MNL choice model.
__label__interpretability_and_explainability This approach allows predictions to be traced back to specific concept patterns that users can understand and potentially intervene on.
__label__learning_theory The AGOP is defined with respect to a learned predictor and is equal to the uncentered covariance matrix of its input-output gradients averaged over the training dataset.
__label__learning_theory Ultimately, we obtain new matrix multiplication-type bit complexity upper bounds for PCA problems, including classical PCA and (randomized) low-rank approximation.
__label__other At the same time, we require that the learned semantic mask neurally collapses to the same simplex equiangular tight frame (ETF) in each environment after being applied to the original input.
__label__natural_language_processing Instead of optimizing these mandatory cloning objectives, we explore an imitation learning strategy for KD of LLMs.
__label__bandits Furthermore, we extend the aforementioned HierTS and HierBayesUCB algorithms to the multi-task combinatorial semi-bandit setting.
__label__machine_vision Sliced Wasserstein (SW) and Generalized Sliced Wasserstein (GSW) have been widely used in applications due to their computational and statistical scalability.
__label__reinforcement_learning The code to replicate these results can be found at \href{https://github.com/shshnkreddy/RLSF}{https://github.com/shshnkreddy/RLSF}
__label__reinforcement_learning Traditional information theory provides a valuable foundation for Reinforcement Learning (RL), particularly through representation learning and entropy maximiza tion for agent exploration.
__label__online_learning Furthermore, we introduce a text ensemble strategy, enhancing the overall test performance by aggregating diverse textual cues.
__label__natural_language_processing This assistant manages memory and knowledge through tool usage, action execution, memory building, and plan specification.
__label__diffusion_based_models We evaluate ZoDiac on three benchmarks, MS-COCO, DiffusionDB, and WikiArt, and find that ZoDiac is robust against state-of-the-art watermark attacks, with a watermark detection rate above 98% and a false positive rate below 6.4%, outperforming state-of-the-art watermarking methods.
__label__optimization_for_deep_networks However, current research on DC mainly focuses on image classification, with less exploration of object detection.
__label__generative_models With off-the-shelf multi-view diffusion models integrated, MVGamba unifies 3D generation tasks from a single image, sparse images, or text prompts.
__label__learning_theory Extending the classical wisdom of variance minimization in low dimensions to high-dimensional finetuning, our generalization analysis unveils the importance of additionally reducing bias induced by low-rank approximation.
__label__diffusion_based_models This module notably improves the representation of layout regions, particularly in scenarios where existing methods struggle with highly complex and detailed textual descriptions.
__label__interpretability_and_explainability This helps uncover the information content of activation vectors, and facilitates understanding of the algorithms implemented by transformer models.
__label__generative_models For instance, many interfaces of popular text-to-image generative models, such as Stable Diffusion or Midjourney, produce several variations of an image for a given query which can eventually be curated by the users.
__label__algorithmic_game_theory Tax mechanisms are a common method to alleviate this issue and induce socially optimal behavior.
__label__evaluation For example, while base models overfit badly early in training, both ensembling and SWA favor base models trained for more epochs.
__label__diffusion_based_models Semantic segmentation and semantic image synthesis are two representative tasks in visual perception and generation.
__label__natural_language_processing Rather than early straightforward approaches, ETP explicitly regularizes the semantic relations as optimal transport plans.
__label__active_learning We demonstrate DSPNs' efficacy in learning submodularity from a costly target submodular function and demonstrate its superiority both for experimental design and online streaming applications.
__label__generative_models Simultaneously generating images and texts typically results in performance degradation due to the inherent inconsistency between vision and language modalities.
__label__safety_in_machine_learning This implies that achieving robustness by maximizing knowledge continuity should not theoretically hinder inferential performance.
__label__generative_models Recently, reinforcement learning (RL) has been employed to fine-tune diffusion models with human preference data, but it requires at least two images (``winner'' and ``loser'' images) for each text prompt.
__label__deep_learning_architectures Our approach achieves baseline full-precision accuracy in ImageNet classification and surpasses state-of-the-art results in semantic segmentation, with notable performance in image super-resolution, and natural language understanding with transformer-based models.
__label__machine_vision Our evaluation yields key intriguing findings: Unsupervised image foundation models demonstrate superior overall performance, video models excel in object-level tasks, diffusion models benefit geometric tasks, language-pretrained models show unexpected limitations in language-related tasks, and the mixture-of-vision-expert (MoVE) strategy leads to consistent performance improvement.
__label__learning_theory By using the kernel complexity function that quantifies the complexity of the induced function space, we derive the upper bounds for both TKM and KM, and further reveal their dependencies on the degree of target-kernel alignment.
__label__safety_in_machine_learning Through theoretical analysis, we verify that our injected backdoor is provably undetectable and unremovable by various state-of-the-art defenses under mild assumptions.
__label__natural_language_processing Ms-PoE leverages the position indice rescaling to relieve the long-term decay effect introduced by RoPE, while meticulously assigning distinct scaling ratios to different attention heads to preserve essential knowledge learned during the pre-training step, forming a multi-scale context fusion from short to long distance.
__label__natural_language_processing Additionally, we release base, instruct and aligned versions on top of SaulLM-medium and SaulLM-large under the MIT License to facilitate reuse and collaborative research.
"__label__fairness When information diffusion occurs in a probabilistic manner, multiple
outreach scenarios can occur."
__label__learning_theory Our theory reveals several curious algorithmic insights.
__label__machine_vision In this paper, we introduce an image-text dataset incorporated with pseudo-manipulation intentions to enhance the training of ZS-CIR models in understanding human manipulation intents.
__label__machine_vision This allows the text model to learn complementary features from the infrared modality, ensuring the semantic structural consistency between the fusion modality and the visible modality.
__label__machine_vision Our method utilizes ray casting from the camera center to capture geometric and textured details, including depth, normal, and color, across all intersected surfaces.
__label__deep_learning_architectures We present Extended Linearized Contracting Dynamics (ELCD), the first neural network-based dynamical system with global contractivity guarantees in arbitrary metrics.
__label__generative_models Being unable to learn interaction semantics through supervised training, we instead leverage pre-trained large models, synergizing knowledge from a large language model and a text-to-motion model.
__label__natural_language_processing In this paper, we aim to further study the insight of the planning capability of LLMs by investigating the roles of LLMs in off-the-shelf planning frameworks.
__label__safety_in_machine_learning Systematic analysis also validates that the generated test scenarios represent meaningful use cases, as well as integrate enhanced measures to probe long-tail risks.
__label__reinforcement_learning To address these questions, we introduce a \textit{novelty-based sampling} mechanism that selectively involves the evaluator only when the the agent encounters a \textit{novel} trajectory, and discontinues querying once the trajectories are no longer \textit{novel}.
__label__machine_vision However, the criteria for abnormal events differ across VAD datasets, making it problematic to apply a single-domain model to other domains.
__label__natural_language_processing BAM makes full use of specialized dense models by not only using their feed-forward network (FFN) to initialize the MoE layers but also leveraging experts' attention weights fully by leveraging them as mixture-of-attention (MoA) layers.
__label__machine_learning_for_healthcare The PeskaVLP framework combines language supervision with visual self-supervision, constructing hard negative samples and employing a Dynamic Time Warping (DTW) based loss function to effectively comprehend the cross-modal procedural alignment.
__label__graph_neural_networks Graph neural networks (GNNs) employing message passing for graph classification are inherently limited by the expressive power of the Weisfeiler-Leman (WL) test for graph isomorphism.
__label__machine_vision By incorporating memory modeling to adjust static prompts, our approach can provide adaptive prompts for tracking guidance.
__label__neuroscience_and_cognitive_science NER also reveals distinct latent dynamics in S1 during consistent movements and in M1 during curved reaching tasks.
__label__generative_models By employing invertible transforms such as the delay embedding and the short-time Fourier transform, we unlock three main advantages: i) We can exploit advanced diffusion vision models; ii) We can remarkably process short- and long-range inputs within the same framework; and iii) We can harness recent and established tools proposed in the time series to image literature.
__label__machine_vision Moreover, we introduce a feature distillation method called WKD-F, which  uses a parametric method for modeling feature distributions and adopts continuous WD  for transferring knowledge from  intermediate layers.
__label__machine_vision We argue that large scale ODIN videos can address these limitations to provide scalable corresponding frames from diverse views.
__label__learning_theory Our experimental results on both synthetic and real-world datasets demonstrate the superiority of our method.
__label__machine_vision The experiments show that our approach achieves SOTA performance on both real-world and synthetic noisy smoke segmentation datasets.
__label__diffusion_based_models Our code is available at https://github.com/Darkbblue/generic-diffusion-feature.
__label__optimization_for_deep_networks SAMPa achieves a twofold speedup of SAM under the assumption that communication costs between devices are negligible.
__label__infrastructure Given this challenge, distribution and offloading the model states are two major solutions.
__label__infrastructure In this work, we aim to massively improve upon existing infrastructure by providing two new methods for implementing neighborhood attention.
__label__generative_models However, genomic sequences are functionally heterogeneous, consisting of multiple connected regions (e.g., Promoter Regions, Exons, and Introns) where elements within each region come from the same probability distribution, but the overall sequence is non-homogeneous.
__label__other We demonstrated its effectiveness and efficiency across diverse widely-used FL datasets.
__label__bandits As an extension, variants of TRIPLE are proposed to efficiently select examples for few-shot prompts, also achieving superior empirical performance.
__label__natural_language_processing Traditional survey paper creation faces challenges due to the vast volume and complexity of information, prompting the need for efficient survey methods.
__label__diffusion_based_models However, existing approaches only operate on static structures, neglecting the fact that physical systems are always dynamic in nature.
__label__neuroscience_and_cognitive_science Recent research has hinted at the affirmative, showing that human neural activity can be effectively predicted using the internal representations of language models (LMs).
__label__safety_in_machine_learning Our code is available at https://github.com/QingyangZhang/DUL.
__label__learning_theory With an arbitrary dataset corrupted with Massart noise at noise rate $\eta$, our algorithm uses only $\mathrm{polylog(1/\epsilon)}$ threshold statistical queries and computes an $(\eta + \epsilon)$-accurate labeling in polynomial time.
__label__reinforcement_learning POAD benefits from a finer-grained credit assignment process and lower optimization complexity, leading to enhanced learning efficiency and generalization abilities in aligning language agents with interactive environments.
__label__optimization However, existing search space transfer methods either lack an adaptive mechanism or are not flexible enough, making it difficult to efficiently identify promising search space during the optimization process.
__label__interpretability_and_explainability Transformers have shown impressive capabilities across various tasks, but their performance on compositional problems remains a topic of debate.
__label__optimization_for_deep_networks Experimental results show that with similar or less GPU memory consumption, LISA surpasses LoRA or even full parameter tuning in downstream fine-tuning tasks, where LISA consistently outperforms LoRA by over 10%-35% in terms of MT-Bench score while achieving on-par or better performance in MMLU, AGIEval and WinoGrande.
__label__diffusion_based_models Through extensive experiments across various IP tasks, including two linear and three nonlinear IPs, we demonstrate that DMPlug consistently outperforms state-of-the-art methods, often by large margins especially for nonlinear IPs.
__label__generative_models Diffusion policies (DP) have elevated BC performance to new heights, demonstrating robust efficacy across diverse tasks, coupled with their inherent flexibility and ease of implementation.
__label__machine_vision During training, large and scheduled drop path rates, and an auxiliary loss on globally pooled features for global understanding tasks are introduced.
__label__probabilistic_methods Our method outperforms well-tuned benchmarks and state-of-the-art posterior estimation methods on a large-scale real-world traffic network, as well as demonstrates a performance advantage over non-active counterparts on a suite of SBI benchmark environments.
__label__optimization The problem of finding suitable point embedding or geometric configurations given only Euclidean distance information of point pairs arises both as a core task and as a sub-problem in a variety of machine learning applications.
__label__diffusion_based_models We leverage the query-key self-attention mechanism of ViTs to explore the interconnections among different anatomical parts in human pose skeletons.
__label__machine_learning_for_other_sciences_and_fields Analytics on structured data is a mature field with many successful methods.
__label__graph_neural_networks We then analyze how different link prediction encoders and decoders adapt to varying levels of feature homophily and introduce designs for improved performance.
__label__deep_learning_architectures This misalignment arises because the model tends to prioritize textual information over visual input, even when both the language model and visual representations are of high quality.
__label__machine_vision We apply dispersion losses to ensure high inter-class discrepancy among child prompts while preserving semantic consistency between parent-child prompt pairs.
__label__machine_vision Furthermore, we utilize an MI estimator to estimate the tight upper bound of the actual MI and further minimize it to achieve explicit representation disentanglement.
__label__deep_learning_architectures Human Mesh Recovery (HMR) is the task of estimating a parameterized 3D human mesh from an image.
__label__machine_vision These gradients are projected to a lower dimension and then concatenated with the model's output embedding.
__label__natural_language_processing Additionally, combining LaPael with data-level paraphrasing further enhances performance.
__label__evaluation Our results show that RaVL accurately discovers (191% improvement over the closest baseline) and mitigates (8.2% improvement on worst-group image classification accuracy) spurious correlations.
__label__safety_in_machine_learning This allows us to reweight client parameter updates and identify those with large discrepancies as backdoor attackers.
__label__machine_vision Extensive experiments show that our method outperforms SOTA methods on three common benchmarks.
__label__reinforcement_learning This work investigates how existing UED methods select training environments, focusing on task prioritisation metrics.
__label__optimization_for_deep_networks Ensuring the convergence of optimization methods requires imposing specific structures on the objective function which often do not hold in practice.
__label__diffusion_based_models Despite extensive efforts, maintaining the temporal consistency of edited videos remains challenging due to the lack of temporal constraints in the regular T2I diffusion model.
__label__natural_language_processing CoA consists of multiple worker agents who sequentially communicate to handle different segmented portions of the text, followed by a manager agent who synthesizes these contributions into a coherent final output.
__label__robotics The integration of TorchRL with the LEGO hubs, via Bluetooth bidirectional communication, enables state-of-the-art reinforcement learning training on GPUs for a wide variety of LEGO builds.
__label__algorithmic_game_theory We further explore how different parameters impact the speed of convergence.
__label__learning_theory We study transfer learning for estimation in latent variable network models.
__label__learning_theory In recent years, algorithm unrolling has emerged as deep learning's answer to this age-old question: design a neural network whose layers can in principle simulate iterations of inference algorithms and train on data generated by the unknown prior.
__label__safety_in_machine_learning Our results demonstrate that BICCOS can generate hundreds of useful cuts during the branch-and-bound process and consistently increase the number of verifiable instances compared to other state-of-the-art neural network verifiers on a wide range of benchmarks, including large networks that previous cutting plane methods could not scale to.
__label__other Experiments validate our discoveries.
__label__neuroscience_and_cognitive_science UniAR leverages a multimodal transformer to predict subjective feedback, such as satisfaction or aesthetic quality, along with the underlying human attention or interaction heatmaps and viewing order.
__label__reinforcement_learning This leads to better sample complexity and improved performance compared to distributed PPO.
__label__human-AI_interaction Most importantly, we find that if the different tasks are sufficiently similar, the first-to-market model may become cost-ineffective on all tasks regardless of how this technology is priced.
__label__interpretability_and_explainability However, it is unclear whether LLMs accurately learn the underlying preferences in human feedback data.
__label__machine_vision All the observations demonstrate the effectiveness of FineCLIP.
__label__graph_neural_networks Empirical evaluations affirm the superiority of our proposed method, outperforming strong OOD detection baselines in various scenarios and settings.
__label__natural_language_processing Intrigued by the claims of emergent planning capabilities in large language models (LLMs), works have been proposed to investigate the planning effectiveness of LLMs, without considering any utilization of off-the-shelf planning techniques in LLMs.
__label__other We introduce SuperClass, a super simple classification method for vision-language pre-training on image-text data.
__label__fairness In high-stakes domains such as healthcare and hiring, the role of machine learning (ML) in decision-making raises significant fairness concerns.
__label__safety_in_machine_learning For instance, in radiology report generation, reports generated by a vision-language model must align with human evaluations before their use in medical decision-making.
__label__machine_learning_for_healthcare ATAC-Diff is the first diffusion model for the scATAC-seq data generation and analysis, composed of auxiliary modules encoding the latent high-level variables to enable the model to learn the semantic information to sample high-quality data.
__label__machine_learning_for_physical_sciences In previous works, the polynomial scaling in $d$ was addressed by amortizing the computation over the optimization process via randomization.
__label__machine_vision Our GIMM can be easily integrated with existing flow-based VFI works by supplying accurately modeled motion.
__label__machine_vision This characteristic renders them ideal decoders, capable of producing high-quality and aesthetically pleasing reconstructions.
__label__machine_vision However, typical supervised learning approaches face challenges in achieving satisfactory adaptation toward unseen and unlabeled target datasets (i.e., direct transfer) due to the inevitable geometric misalignment between the source and target domains.
__label__deep_learning_architectures However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale.
__label__machine_learning_for_other_sciences_and_fields In real-world applications, this assumption often fails due to sensor malfunctions or data loss, making TSC with missing data a critical challenge.
__label__optimization Furthermore, we prove that Nesterov's accelerated gradient (NAG) attains an iteration complexity of $O(\kappa\log\frac{1}{\epsilon})$, which is the best-known bound of first-order methods for rectangular matrix factorization.
__label__interpretability_and_explainability Even though neural networks have been long deployed in applications involving tabular data, still existing neural architectures are not explainable by design.
__label__neuroscience_and_cognitive_science However, existing methods of sequential memory suffer from catastrophic forgetting, limited capacity, slow iterative learning procedures, low-order Markov memory, and, most importantly, the inability to represent and generate multiple valid future possibilities stemming from the same context.
__label__algorithmic_game_theory To facilitate our investigation, we introduce significant modifications to the Sorted Deferred Acceptance algorithm proposed by ed by Ashlagi et al.
__label__reinforcement_learning OMIS leverages in-context learning-based pretraining to train a Transformer model for decision-making.
__label__generative_models Illustration is a fundamental mode of human expression and communication.
__label__machine_learning_for_other_sciences_and_fields We define a process for extracting a large, high-quality training dataset from the TabLib corpus, proposing methods for tabular data filtering and quality control.
__label__bandits We show that, with stochastic rewards, differently from what happens with non-ranking feedback, no algorithm can suffer a logarithmic regret in the time horizon $T$ in the instance-dependent case.
__label__reinforcement_learning While the statistical implications of distributional robustness in RL have been explored in some specific cases, the generalizability of the existing findings remains unclear, especially in comparison to standard RL.
__label__machine_vision Domain generalisation in computational histopathology is challenging because the images are substantially affected by differences among hospitals due to factors like fixation and staining of tissue and imaging equipment.
__label__machine_vision Subsequently, we present the Pixel-wise Triplet Fusion to eliminate redundancy of 3D Gaussians in overlapping view regions and to aggregate features observed across multiple views.
__label__graph_neural_networks We have witnessed the success of graph self-supervised learning on pre-training the parameters of GNNs, leading many not to doubt that whether the learned GNNs parameters are all useful.
__label__optimization We propose in this paper Alignment via  Optimal Transport (AOT), a novel method for distributional preference alignment of LLMs.
__label__deep_learning_architectures The source code is available at \url{https://github.com/yingchengy/AVMOE}.
__label__natural_language_processing Finally, we compare the results from the RAG-based evaluation and LLM self-evaluation to categorize four types of ambiguous answers that are beyond the knowledge boundary of the target LLM.
__label__neuroscience_and_cognitive_science This enables us to present the gradient to the shallow layers directly, thereby significantly mitigating the gradient vanishing problem.
__label__deep_learning_architectures We prove that logarithmic depth is necessary and sufficient for tasks like graph connectivity, while single-layer transformers with small embedding dimensions can solve contextual retrieval tasks.
__label__natural_language_processing Specifically, we show that simple supervised fine-tuning with GPT-4 as the teacher outperforms existing LAIF pipelines.
__label__machine_learning_for_other_sciences_and_fields Addressing this, few-shot molecular property prediction (FSMPP) has been developed.
__label__machine_vision Extensive experiment results show that our method achieves SOTA performance (94.7\% and 70.9\% average accuracy on common benchmarks and Union14M-Benchmark).
__label__generative_models The ability to invent novel and interesting problems is a remarkable feature of human intelligence that drives innovation, art, and science.
__label__graph_neural_networks Extensive experiments on common graphs show the proposed MSG achieves superior performance to previous spiking GNNs and energy efficiency to conventional GNNs.
__label__reinforcement_learning In terms of sample efficiency, PaMoRL maintains an MBRL-level sample efficiency that outperforms other no-look-ahead MBRL methods and model-free RL methods, and it even exceeds the performance of planning-based MBRL methods and methods with larger networks in certain tasks.
__label__privacy Langevin unlearning unifies the DP learning process and the privacy-certified unlearning process with many algorithmic benefits.
__label__generative_models To address this, one straightforward solution is to incorporate a personalized diffusion model with a text-driven editing framework.
__label__causal_inference We characterize the convergence rates of our method and validate its effectiveness through a simulation study.
__label__reinforcement_learning Previous work tackles this challenge by training on the source domain with modified rewards derived by matching distributions between the source and the target optimal trajectories.
__label__algorithmic_game_theory The learner can reveal truthful, yet not necessarily complete, information about the classifier to the agents, aiming to release just enough information to shape the agents' behavior and thus maximize accuracy.
__label__interpretability_and_explainability Chain-of-Thought (CoT) reasoning is known to improve Large Language Models both empirically and in terms of theoretical approximation power.
__label__natural_language_processing By contrast, previous works (i.e., SELF-INSTRUCT and WizardLM) obtain an average score exceeding 67, with a variance below 3.2.
__label__machine_vision Seams, distortions, wasted UV space, vertex-duplication, and varying resolution over the surface are the most prominent issues of the standard UV-based texturing of meshes.
__label__graph_neural_networks How to design a model that retains the ability of polynomial-based spectral GNNs to approximate filters while it possesses higher generalization and performance?
__label__machine_learning_for_other_sciences_and_fields We demonstrate the flexibility and effectiveness of our approach through multiple experiments with three benchmark models, namely Inception-V3, ResNet, and BERT.
__label__generative_models Ctrl-X designs feed-forward structure control to enable the structure alignment with a structure image and semantic-aware appearance transfer to facilitate the appearance transfer from a user-input image.
__label__safety_in_machine_learning We are also among the first to formulate the settings, goals, and evaluations in LLM unlearning.
__label__machine_learning_for_other_sciences_and_fields Mutual information (MI) is a general measure of statistical dependence with widespread application across the sciences.
__label__other With the emergence of various molecular tasks and massive datasets, how to perform efficient training has become an urgent yet under-explored issue in the area.
__label__learning_theory Prospective ERM, roughly speaking, incorporates time as an input in addition to the data.
__label__machine_vision To validate our approach, we construct a benchmark that includes both real-world and synthetic scenes, demonstrating its practical utility and effectiveness.
"__label__machine_vision Unlike previous works,
we consider the panoramic image as a composition of segment groups: oversampled
segments, representing planar structures such as floors and ceilings, and
under-sampled segments, representing other scene elements."
__label__learning_theory Our results suggest that nested feature selection may be an important inductive bias for finetuning neural networks.
__label__machine_vision Extensive experiments across diverse datasets, few-shot learning scenarios, and multiple $\texttt{PEFT}$ techniques demonstrate the outstanding prediction and calibration performance by $\texttt{Bayesian-PEFT}$.
__label__generative_models Going beyond mimicking limited human experiences, recent studies show initial evidence that, like humans, large language models (LLMs) are capable of improving their abilities purely by self-correction, i.e., correcting previous responses through self-examination, as seen in models like OpenAI o1.
__label__deep_learning_architectures With these metrics, we study how architectural and optimisation choices influence OFs, and provide practical insights to minimise OFs during training.
__label__learning_theory As an attempt to achieve better overall performance with less fine-tuning, we propose a softened, pointwise mechanism called SoftAD (soft ascent-descent) that downweights points on the borderline, limits the effects of outliers, and retains the ascent-descent effect of flooding, with no additional computational overhead.
__label__machine_learning_for_social_sciences We also explore the intrinsic properties of agent trust under conditions including external manipulations and advanced reasoning strategies.
__label__learning_theory 2021; Wang et al.
__label__machine_vision This paper investigates the feasibility and effectiveness of ingesting NeRF into MLLM.
__label__machine_vision For this purpose, we propose DiMoP3D, a diverse motion prediction framework with 3D scene awareness, which leverages the 3D point cloud and observed sequence to generate diverse and high-fidelity predictions.
__label__learning_theory When addressing a future LP instance, we reduce its dimensionality from $n$ to $k$ via the learned projection matrix, solve the resulting LP to obtain a $k$-dimensional solution, and apply the learned matrix to it to recover an $n$-dimensional solution.
__label__machine_vision Evaluations on benchmark datasets demonstrate the effectiveness and versatility of VFM-6D in various real-world scenarios.
__label__neuroscience_and_cognitive_science We demonstrate its efficiency in reconstructing dynamical systems, including their state space geometry and long-term temporal properties, from just short BOLD time series.
__label__other Our study demonstrates the effectiveness of LP-FT for fine-tuning language models.
__label__learning_theory Therefore, in this study, we mathematically prove the two-phase dynamics of interactions, providing a theoretical mechanism for how the generalization power of a DNN changes during the training process.
__label__machine_vision To address these issues, we propose a simple yet effective approach based on graph analysis.
__label__interpretability_and_explainability In label-noise learning, the noise transition matrix reveals how an instance transitions from its clean label to its noisy label.
__label__bandits We conclude by providing tight regret bounds when, after each interaction, the platform is allowed to observe the true traders' valuations.
"__label__learning_theory The phenomenon of ""model collapse"" refers to the situation whereby as a model is trained recursively on data generated from previous generations of itself over time, its performance degrades until the model eventually becomes completely useless, i.e."
__label__other To the best of our knowledge, ZOPP represents a pioneering effort in the domain of multi-modal panoptic perception and auto labeling for autonomous driving scenes.
__label__optimization_for_deep_networks Federated learning (FL) is a distributed learning framework that leverages commonalities between distributed client datasets to train a global model.
__label__optimization_for_deep_networks Compared with existing tools for deep networks (Lu et al., 2020) that demand homogeneity and global Lipschitz smoothness, we utilize a refined analysis assuming only $\textit{partial homogeneity}$ and $\textit{local Lipschitz smoothness}$.
__label__speech_and_audio State-of-the-art audio foundation models, such as CLAP, which learn to map between audio scenes and natural textual descriptions, are trained on non-spatial audio and text pairs, and hence lack spatial awareness.
__label__safety_in_machine_learning The second leverages the iterative nature of gradient-based learning to leverage incremental verification, reusing information from prior verification runs.
__label__safety_in_machine_learning We find that embedding space attacks circumvent model alignments and trigger harmful behaviors more efficiently than discrete attacks or model fine-tuning.
__label__machine_vision Our code, data and trained models are available at \url{https://github.com/chenkang455/S-SDM}.
__label__machine_vision On ImageNet and five derived distribution shifts, our VRF further improves the OOD accuracy by 1.5 - 2.0 pp over the ensemble baselines while maintaining or increasing ID accuracy.
__label__generative_models In this paper, we delve into understanding the mechanisms behind the effectiveness of NATs and uncover two important interaction patterns that naturally emerge from NAT’s paradigm: Spatially (within a step), although [MASK] and visible tokens are processed uniformly by NATs, the interactions between them are highly asymmetric.
__label__optimization It remains unclear how the other building blocks of the transformer contribute to ICL.
"__label__diffusion_based_models With this insight, we propose a variance-adaptive ODE solver that can adjust its step size in the inference stage, making
AdaFlow an adaptive decision-maker, offering rapid inference without sacrificing diversity."
__label__optimization The key strategy is to employ an adaptive stepsize tracking protocol involving the transmission of two extra (scalar) variables.
__label__generative_models Code is available at https://github.com/sangyun884/rfpp.
__label__interpretability_and_explainability We present cluster-based SEV and its variant tree-based SEV, introduce a method that improves credibility of explanations, and propose algorithms that optimize decision sparsity in machine learning models.
__label__safety_in_machine_learning Extensive theoretical and empirical results on multiple OoD data sets and network structures verify the superiority of our KPCA detector in  efficiency and efficacy with state-of-the-art detection performance.
__label__generative_models At the temporal level, we prioritize the computation of the critical tokens at each step, while maximally reusing previously computed token representations to supplement necessary information.
__label__natural_language_processing We design the Block Transformer to strategically mitigate these costs, by incorporating coarsity and locality into an integrated global-to-local architecture.
__label__optimization MFLD has gained attention due to its connection with noisy gradient descent for mean-field two-layer neural networks.
__label__optimization In the adversarial streaming model, the input is a sequence of adaptive updates that defines an underlying dataset and the goal is to approximate, collect, or compute some statistic while using space sublinear in the size of the dataset.
__label__machine_vision However, its potential has rarely been explored in low-level image restoration tasks.
__label__machine_vision At the other end, recent vision-language foundation models such as CLIP demonstrate superior open-vocabulary recognition capability.
__label__learning_theory We prove that KT-based regression estimators enjoy significantly superior computational efficiency over the full-data estimators and improved statistical efficiency over i.i.d.
__label__reinforcement_learning To address the need for pluralistic alignment, we develop a class of multimodal RLHF methods.
__label__deep_learning_architectures Meanwhile, a domain discriminator is designed to transfer the domain invariant knowledge.
__label__machine_vision Compared to traditional perturbations crafted specifically for each data point, Universal Adversarial Perturbations (UAPs) are input-agnostic and shown to be more practical in the real world.
"__label__learning_theory Our algorithm follows the primal-dual framework and is 
designed by directly bounding the risk with respect to the original, nonconvex $L_2^2$ loss."
__label__machine_vision To address these limitations, we generate the parameters of 3D Gaussians from a single image in a single forward pass.
__label__reinforcement_learning Model-based reinforcement learning (MBRL) is a promising route to sample-efficient policy optimization.
__label__safety_in_machine_learning Recent advances in out-of-distribution (OOD) detection on image data show that pre-trained neural network classifiers can separate in-distribution (ID) from OOD data well, leveraging the class-discriminative ability of the model itself.
__label__machine_vision For assessment, we introduce a new benchmark called EvalQABench, comprising 64,000 training samples (split evenly between positive and negative samples) and 5,000 testing samples.
__label__safety_in_machine_learning Additionally, we highlight the scalability of our method to 100 LLMs, ranging from 0.5B to 236B parameters, incurring no additional cost as the number of parameters increases.
__label__safety_in_machine_learning We introduce Hopfield Boosting, a boosting approach, which leverages modern Hopfield energy to sharpen the decision boundary between the in-distribution and OOD data.
__label__deep_learning_architectures In this paper, we propose Multi-Head Mixture-of-Experts (MH-MoE).
__label__causal_inference However, to make reliable inferences, medical practitioners require not only estimating averaged causal quantities, such as the conditional average treatment effect, but also understanding the randomness of the treatment effect as a random variable.
__label__optimization Moreover, part of the intermediate variables that contribute to the gradient estimation can be directly indexed, significantly reducing the computation complexity.
__label__learning_theory However, the absence of theoretical analysis on the error of OKRidge impedes its large-scale applications.
__label__learning_theory Our semirandom adversaries in particular are allowed to add edges inside clusters or increase the probability that an edge appears inside a cluster.
__label__machine_vision In this work, we propose an effective strategy for prioritizing tokens which allows training on longer video sequences (128 frames) and gets better performance than, more typical, random and uniform masking strategies.
__label__safety_in_machine_learning To address this challenge, we propose $\textsf{Safe LoRA}$, a simple one-liner patch to the original LoRA implementation by introducing the projection of LoRA weights from selected layers to the safety-aligned subspace, effectively reducing the safety risks in LLM fine-tuning while maintaining utility.
__label__machine_vision A well-known dilemma in large vision-language models (e.g., GPT-4, LLaVA) is that while increasing the number of vision tokens generally enhances visual understanding, it also significantly raises memory and computational costs, especially in long-term, dense video frame streaming scenarios.
__label__reinforcement_learning It promotes diversity among policy networks by encouraging discrepancy among these masks, without sacrificing the efficiencies of parameter sharing.
__label__machine_vision In this paper, we propose an orthogonal solution called the Retrieval-augmented Framework for Image Restoration (ReFIR), which incorporates retrieved images as external knowledge to extend the knowledge boundary of existing LRMs in generating details faithful to the original scene.
__label__natural_language_processing In this paper, we introduce Moderation Using LLM Introspection (MULI), which detects toxic prompts using the information extracted directly from LLMs themselves.
__label__probabilistic_methods The size-and-shape of $\mu$ is a geometric property invariant to a family of space-time unitary transformations, viewed as rotations of the Hilbert space, that jointly transform the $x$ and $y$ axes.
__label__bandits For the second setting, we design an algorithm RS-GLinCB that updates its policy $\tilde{O}(\log^2 T)$ times and achieves a regret of $\tilde{O}(\sqrt{T})$ even when the arm feature vectors are adversarially generated.
__label__natural_language_processing Even the strongest proprietary large language models (LLMs) do not quite exhibit the ability of continually improving their responses sequentially.
__label__reinforcement_learning Examples include frame stacking and recurrent neural networks.
__label__generative_models Gesture synthesis is a vital realm of human-computer interaction, with wide-ranging applications across various fields like film, robotics, and virtual reality.
__label__machine_vision We evaluated the proposed Segmentation with Uncertainty Model (SUM) on a diverse test set consisting of 14 public benchmarks, where it achieves state-of-the-art results.
__label__reinforcement_learning However, existing algorithms either suffer from sub-optimal regret guarantees or computational inefficiencies.
__label__safety_in_machine_learning We show that we can quickly identify this set of features and use this knowledge to speedup constructing adversarial examples.
__label__generative_models D3D-DiT models the distribution of encoded 3D latents and is specifically designed to fuse positional information from the three feature maps of the triplane latent, enabling a native 3D generative model scalable to large-scale 3D datasets.
__label__optimization We propose scalable and efficient algorithms for the functional bilevel optimization problem and illustrate the benefits of our approach on instrumental regression and reinforcement learning tasks.
__label__optimization Finally, we demonstrate this flexibility of our framework by presenting how it naturally addresses the important case of binary outcomes, which has received far less attention by recent developments in the NPIV literature.
__label__machine_learning_for_healthcare Results demonstrate Medformer's superiority over 10 baselines, achieving top averaged ranking across five datasets on all six evaluation metrics.
__label__safety_in_machine_learning We achieve state-of-the-art OOD detection results across multiple benchmarks of the OpenOOD framework, especially pronounced in difficult settings in which OOD samples are positioned close to the training set distribution.
__label__reinforcement_learning Past work has addressed this problem only under the assumption of full access to the expert's policy, guaranteeing transferability when learning from two experts with the same reward but different transition laws that satisfy a specific rank condition [Rolland et al., 2022].
__label__diffusion_based_models By incorporating a Lightning T2I branch with a standard diffusion one, PuLID introduces both contrastive alignment loss and accurate ID loss, minimizing disruption to the original model and ensuring high ID fidelity.
__label__natural_language_processing First, we find that while the typical approach of finetuning a model on synthetic correct or positive problem-solution pairs generated by capable models offers modest performance gains, sampling more correct solutions from the finetuned learner itself followed by subsequent fine-tuning on this self-generated data doubles the efficiency of the same synthetic problems.
__label__neuroscience_and_cognitive_science Morever, unlike most second-order optimizers which involve inherently sequential operations, SOFO's effective use of GPU parallelism yields a per-iteration wallclock time essentially on par with first-order gradient-based optimizers.
__label__learning_theory The sublinear convergence rate is due to the algorithmic nature of learning over-parameterized GMM with gradient EM.
__label__causal_inference Despite the multitude of identifiability results under various interventional CRL settings, the existing guarantees apply exclusively to the _infinite-sample_ regime (i.e., infinite observed samples).
__label__machine_vision The Zeroverse's procedural synthesis code and interactive visualization are available at: https://desaixie.github.io/lrm-zero/.
__label__graph_neural_networks Therefore, we propose a novel regularization method for TDB models that addresses this limitation.
__label__learning_theory The results above are achieved through a computationally inefficient algorithm.
"__label__generative_models Large Language Models (LLMs) have seen an impressive wave of advances, with
models now excelling in a variety of tasks, such as mathematical reasoning and
program synthesis."
__label__online_learning Under partial information on the probability transitions (uncertainty and non-stationarity coming only from external noise, independent of agent state-action pairs), we achieve optimal dynamic regret without prior knowledge of MDP changes.
__label__machine_learning_for_other_sciences_and_fields Experiments on RAbD benchmark show that our approach effectively optimizes the energy of generated antibodies and achieves state-of-the-art performance in designing high-quality antibodies with low total energy and high binding affinity simultaneously, demonstrating the superiority of our approach.
__label__natural_language_processing Building on this framework, we provide several theoretical results for multi-agent debate.
__label__machine_vision The expansive aerial observations make it difficult to generate precise Bird's Eye View (BEV) representations.
__label__probabilistic_methods However, there are many ways a user can transform a model that make inference more or less efficient.
__label__privacy In particular, our results determine the first types of gaps that are sufficient and necessary for estimating a subspace with an amount of points that is independent of the dimension.
__label__machine_vision Existing methods usually employ distinct constraint designs tailored to specific scenes, forming fixed fusion paradigms.
__label__generative_models Based on this, we further propose the LCGen method, which guides text-to-3D to obtain different priors with different certainty from various viewpoints, aiding in view-consistent generation.
__label__machine_learning_for_social_sciences However, existing MHeteroFL methods rely on training loss to transfer knowledge between the client model and the server model, resulting in limited knowledge exchange.
__label__reinforcement_learning }.
__label__reinforcement_learning It consists of an upper-level RL teacher agent that generates suitable training environments for a lower-level student agent.
__label__optimization_for_deep_networks Kronecker) approximations used or any damping-based interpolation towards first-order updates.
__label__diffusion_based_models The proposed TEdit is trained using a novel bootstrap learning algorithm that effectively enhances the coverage of the original data.
__label__safety_in_machine_learning Real-data experiments show that our method improves interval efficiency through model calibration and offers a practical alternative to feature-conditional validity.
__label__bandits One can even achieve best-both-worlds guarantees with logarithmic regret in the stochastic regime.
__label__optimization More recently, Newton-type methods using sparsified Hessian matrices have demonstrated promising results on OT computation, but there still remain a lot of unresolved open questions.
__label__privacy We say that an algorithm is instance-optimal if it is competitive with an algorithm that is given a constant multiplicative approximation of the density of the distribution.
__label__natural_language_processing Experimental results show that our methods can effectively extend the context window length.
__label__other Additionally, our proposed query adaption module can also act as an adapter, preserving the rich knowledge within CLIP without the need to tune the parameters of the backbone network.
__label__safety_in_machine_learning We additionally prove formally and demonstrate empirically that our targeting method, although inspired by linear predictors, also applies to non-linear models.
__label__deep_learning_architectures We conduct extensive experiments to evaluate the effectiveness of the proposed approach.
__label__algorithmic_game_theory Consequently, we focus on this setting and refine the analyses of the competitive ratios, with upper and lower bounds expressed as increasing functions of $\gamma$.
__label__algorithmic_game_theory The first key ingredient in our approach is a relaxation of the usual notion of a fixed point required in the framework of Gordon, Greenwald and Marks [2008].
__label__other Rather than focusing on subtasks, like individual relations between entities, we model entire subcomponents of the target ontology by finetuning an LLM with a custom regulariser that reduces overfitting on high-frequency concepts.
__label__learning_theory At the core of our approach is an improved analysis of spectral outlier-removal techniques from learning with nasty noise.
__label__safety_in_machine_learning However, lip-forgery videos, which neither change identity nor have discernible visual artifacts, present a formidable challenge to existing DeepFake detection methods.
__label__machine_learning_for_other_sciences_and_fields Our agent targets generating hard but provable conjectures --- a moving target, since its own theorem proving ability also improves as it trains.
__label__natural_language_processing To address this, we introduce a controlled framework that stimulates step-skipping behavior by iteratively refining models to generate shorter and accurate reasoning paths.
"__label__learning_theory We compute scaling laws empirically, and observe phase transitions
depending on whether $g$ or $h$ is harder to learn, as predicted
by our theory."
"__label__optimization We present adaptive gradient methods (both basic and accelerated) for solving
convex composite optimization problems in which the main part is approximately
smooth (a.k.a."
__label__machine_vision Despite its potential, transformer encoders are underutilized due to the collapsed self-attention map, having low representation capacity.
__label__machine_vision Nonetheless, their performance heavily depends on the specificity of the input text prompts, which requires skillful prompt template engineering.
__label__machine_vision Furthermore, the best-performing methods train object localization by a surrogate loss, that predicts a unit Gaussian at each object center.
__label__reinforcement_learning Implementing BAD within the PPO algorithm, we introduce Policy Optimization with Action Decomposition (POAD).
__label__optimization We then leverage these ingredients to give an improved algorithm for adversarially robust $L_p$ estimation on turnstile streams.
__label__deep_learning_architectures We show that a 850M decoder-only MatFormer language model (MatLM) allows us to extract multiple smaller models spanning from 582M to 850M parameters, each exhibiting better validation loss and one-shot downstream evaluations than independently trained counterparts.
__label__safety_in_machine_learning The main challenge in cross-modality ReID lies in effectively dealing with visual differences between different modalities.
__label__machine_vision In this paper, we delve into this phenomenon for an interpretation.
__label__natural_language_processing In particular, the instruction-tuned LLMs work surprisingly well by adding a small fraction of ranking data into the training blend, and outperform existing expert ranking models, including the same LLM exclusively fine-tuned on a large amount of ranking data.
__label__machine_vision Extensive experiments conducted on several challenging DA benchmarks, including the ImageCLEF-DA, Office-Home, VisDA 2017, and DomainNet datasets, demonstrate the superiority of our method over the state-of-the-art (SOTA) approaches.
__label__privacy Our DP pre-trained models are released in *fastDP* library (https://github.com/awslabs/fast-differential-privacy/releases/tag/v2.1)
__label__natural_language_processing However, recent research has found that edited models often exhibit varying degrees of performance degradation.
__label__reinforcement_learning In this study, we investigate the _Multi-Reward Best Policy Identification_ (MR-BPI) problem, where the goal is to determine the best policy for all rewards in a given set $\mathcal{R}$ with minimal sample complexity and a prescribed confidence level.
__label__probabilistic_methods We empirically demonstrate the competitive performance of the probabilistic prediction by Wasserstein gradient boosting in comparison with existing uncertainty quantification methods.
__label__privacy The code is available here: https://github.com/facebookresearch/VLMDejaVu.
__label__natural_language_processing However, existing routing models are ineffective when multiple LLMs perform well for a query.
__label__natural_language_processing To produce high-quality data, we incorporate a self-correct mechanism into our generalization framework and develop two models to predict prompt discrimination and difficulty score to facilitate our data synthesis framework, contributing valuable tools to evaluation data synthesis research.
__label__machine_vision We advance the development of object-centric occupancy perception from both data and algorithm perspectives.
__label__machine_vision Our network achieves accurate recovery that is superior to existing networks with significantly more complex architectures.
__label__learning_theory Cech Persistence diagrams (PDs) are topological descriptors routinely used to capture the geometry of complex datasets.
__label__infrastructure Large language models (LLMs) are increasingly used for complex tasks that require multiple generation calls, advanced prompting techniques, control flow, and structured inputs/outputs.
__label__privacy Third, for non-smooth loss functions, we obtain optimal excess risk in $n^{11/8} m^{5/4}$ gradient computations.
__label__natural_language_processing We hope this study can serve as a foundational guide for optimizing MM-ICL strategies in future research.
__label__safety_in_machine_learning a belief over trajectories, we prove conditions under which RLHF is guaranteed to result in policies that deceptively inflate their performance, overjustify their behavior to make an impression, or both.
__label__natural_language_processing This objective significantly improves reward model performance by up to 0.09 AUROC across challenging benchmarks, such as MATH and GSM8k.
__label__machine_vision Existing methods often rely on multiple task-specific encoder-decoder pairs, leading to high overhead of parameter and bitrate usage, or face challenges in multi-objective optimization under a unified representation, failing to achieve both performance and efficiency.
__label__generative_models Empirically, MuLAN sets a new **state-of-the-art** in density estimation on CIFAR-10 and ImageNet while matching the performance of previous state-of-the-art models with **50%** fewer steps.
__label__privacy As a result, PANORAMIA does not modify the model, training data, or training process, and only requires access to a subset of the training data.
__label__machine_vision Extensive experiments are performed on two benchmark datasets, EPIC-Kitchens and Human-Animal-Cartoon (HAC), with various modality combinations, demonstrating the effectiveness of our method under multi-source and single-source settings.
__label__probabilistic_methods This technique relaxes the constraints on kernel types and stationarity, allowing for more flexible modeling while reducing computational complexity to the linear level.
__label__causal_inference This can be modeled by a binomial thinning operator (for branching) and an additive independent Poisson distribution (for noising), known as Poisson Branching Structure Causal Model (PB-SCM).
__label__generative_models We then validate these findings by training the Bernoulli-Bernoulli RBM on real data sets.
__label__generative_models However, substantial challenges remain in precisely controlling individual elements within the generated video, such as the movement and appearance of specific characters and the manipulation of viewpoints.
__label__interpretability_and_explainability A particular 3D subspace in model activations in GPT-2 can be traversed to positionally index items in lists, and we show that this mechanism can explain an otherwise arbitrary-seeming sensitivity of the model to the order of items in the prompt.
__label__reinforcement_learning We define a novel MDP representation class, namely _Locally Linearizable MDPs_, generalizing other representation classes like Linear MDPs and MDPS with low inherent Belmman error.
__label__learning_theory Our result highlights the adaptivity of the pretrained transformer to low-dimensional structures of the function class, which enables sample-efficient ICL that outperforms estimators that only have access to the in-context data.
__label__optimization One can then use these losses to perform topological optimization via gradient descent routines.
__label__optimization Using an unbiassed compression technique, we develop a new method—Shadowheart SGD—that provably improves the time complexities of all previous centralized methods.
__label__machine_learning_for_other_sciences_and_fields Moreover, data owners may hesitate to share the access to local data due to privacy concerns and copyright protection, which makes it impossible to simply construct a FM on cross-domain training instances.
__label__privacy We show that these updates expose individuals to high-accuracy reconstruction attacks which allow the attacker to recover their data in its entirety, even when the original models are so simple that privacy risk might not otherwise have been a concern.
__label__online_learning ZAF utilizes new data for low-rank adaptation (LoRA), complemented by a zero-shot antidote on wild data, effectively decoupling learning from forgetting.
__label__optimization For code implementation, see https://github.com/ninja-wm/POM/.
__label__deep_learning_architectures This also indicates that our analysis has a certain guiding significance for architecture analysis, architecture improvement and architecture design.
__label__natural_language_processing A central piece in enabling intelligent agentic behavior in foundation models is to make them capable of introspecting upon their behavior, reasoning, and correcting their mistakes as more computation or interaction is available.
__label__machine_vision The key idea is to incorporate a public vision-language model (CLIP) to distill positive knowledge while refining negative knowledge for adaptation by self-promotion gradient direction alignment.
__label__machine_vision Given visual segments and textual descriptions of events, MECD requires identifying the causal associations between these events to derive a comprehensive, structured event-level video causal diagram explaining why and how the final result event occurred.
__label__generative_models By removing the latter ones from the activations, we reorient the behavior of the LLM towards the alignment goal.
__label__machine_vision We empirically demonstrate that SELMA significantly improves the semantic alignment and text faithfulness of state-of-the-art T2I diffusion models on multiple benchmarks (+2.1% on TIFA and +6.9% on DSG), human preference metrics (PickScore, ImageReward, and HPS), as well as human evaluation.
__label__speech_and_audio This paper addresses this gap by first analyzing the distribution gap in codec language models, highlighting how it leads to discrepancies between the training and inference phases, which negatively affects performance.
__label__reinforcement_learning We prove that our estimator is consistent and satisfies several desirable properties for policy evaluation.
__label__graph_neural_networks In particular, our FGSAM+ as a SAM variant offers a faster optimization than the base optimizer in most cases.
__label__infrastructure LoRANN is competitive with the leading graph-based algorithms and outperforms the state-of-the-art GPU ANN methods on high-dimensional data sets.
__label__causal_inference Remarkably, these guarantees match the best-known results for more restrictive single-node interventions.
__label__machine_vision While 3D object bounding box (bbox) representation has been widely used in autonomous driving perception, it lacks the ability to capture the precise details of an object's intrinsic geometry.
__label__natural_language_processing Large language models are usually fine-tuned to align with human preferences.
__label__reinforcement_learning The sim-to-real gap, which represents the disparity between training and testing environments, poses a significant challenge in reinforcement learning (RL).
__label__generative_models This approach enables efficient sampling of the equilibrium measure via a static Monte Carlo process.
__label__machine_vision Agent behavior simulation empowers robotics, gaming, movies, and VR applications, but building such simulators often requires laborious effort of manually crafting the agent's decision process and motion patterns.
__label__machine_learning_for_healthcare Our system is designed to safeguard both data integrity and patient privacy, facilitating collaborative efforts.
__label__machine_vision Despite recent advancements in high-fidelity human reconstruction techniques, the requirements for densely captured images or time-consuming per-instance optimization significantly hinder their applications in broader scenarios.
__label__machine_vision In this work, to maintain CLIP's simple architecture and objective while explicitly attending to fashion details, we propose $E^2$: Easy Regional Contrastive Learning of Expressive Fashion Representations.
__label__optimization_for_deep_networks An implementation of CoMERA is available at <https://github.com/ziyangjoy/CoMERA>.
__label__diffusion_based_models We validate our method through extensive experiments on diverse computational imaging tasks, including random inpainting, denoising, and deblurring, achieving new state-of-the-art performance.
__label__other An NCG is a group of neurons mapped to a specific class, implementing intra-class WTA and a novel competition regulation mechanism based on two-compartment thresholds.
__label__deep_learning_architectures This module adeptly aggregates both local and global information from individual views and interacting groups, enabling precise modeling of close physical interactions through dense point retrieval in small areas, supported by the implicit fields.
__label__safety_in_machine_learning Empirical results on various heterogeneous federated scenarios under backdoor attacks demonstrate the effectiveness of our method.
__label__online_learning Compared with the previous result, our regret bound is better for $d=O((n\bar{d})^{2/3}T^{1/3})$, and the delay-dependent part is tight in the worst case.
__label__evaluation Some recent works disclose the pitfalls of these editing methods such as knowledge distortion or conflict.
__label__machine_learning_for_other_sciences_and_fields In this work, we seek to narrow this gap and present TABULA-8B, a language model for tabular prediction.
__label__machine_learning_for_other_sciences_and_fields Building on this realistic and generalized contextual market model, we introduce MarketFCNet, a deep learning-based method for approximating market equilibrium.
__label__neuroscience_and_cognitive_science PAM is a streaming model that learns a sequence in an online, continuous manner by observing each input only once.
__label__reinforcement_learning In contrast to prior work in similar settings, our theoretical results require only the minimal assumption of an ERM oracle for value function approximation for the constituent policies (and not the global optimal policy or the max-following policy itself) on samplable distributions.
__label__generative_models First, we construct synthetic trajectories toward high-scoring regions using the dataset while injecting locality bias for consistent improvement directions.
"__label__machine_learning_for_physical_sciences This was subsequently improved
by [Lewis et al."
__label__neuroscience_and_cognitive_science The auditory attention detection (AAD) approach seeks to identify the attended speaker by analyzing brain signals, such as EEG signals.
__label__optimization We show that, under some smoothness conditions, this bias is of order $O(\alpha)$.
__label__neuroscience_and_cognitive_science Here, we study this issue using a neural network with a feedback alignment algorithm, demonstrating that pretraining neural networks with random noise increases the learning efficiency as well as generalization abilities without weight transport.
__label__learning_theory Compared to the classical approaches, our method achieves a *super-quadratic* speedup.
__label__diffusion_based_models We leverage dual-stream diffusion models as the backbone of our framework and carefully design two novel components for crucial local regions (i.e., hands and face) that can be easily integrated into our backbone.
__label__robotics Given a set of expert demonstrations, we jointly learn a latent inverse dynamics model and a forward dynamics model over a sequence of image embeddings, predicting the next frame in latent space, without augmentations, contrastive sampling, or access to ground truth actions.
__label__machine_vision The core idea is to model the interaction between normals and incident lighting using the physically-based rendering equation.
__label__machine_vision Extensive experiments demonstrate the strong universal ability of OneDet3D to utilize only one trained model for addressing almost all 3D object detection tasks (Fig.
__label__machine_learning_for_healthcare Despite the remarkable progress achieved in medical image segmentation, there is still an absence of a 3D foundation segmentation model that can segment a wide range of anatomical categories with easy user interaction.
__label__optimization_for_deep_networks In particular, we utilize core-feature descriptions generated by LLMs to induce core-based zero-shot predictions which then serve as proxies to estimate the worst-case risk.
__label__learning_theory Our work provides the first comprehensive characterization of noisy online classification with guarantees that apply to the *ground truth* while addressing *general* noisy observations.
__label__learning_theory Specifically, for the alignment with polynomial decay, the established results indicate that under the just-aligned and weakly-aligned regimes, TKM and KM share the same learning rate.
__label__reinforcement_learning To this end, we propose a novel UED algorithm, adversarial environment design via regret-guided diffusion models (ADD).
__label__neuroscience_and_cognitive_science A common source of anxiety for the computational neuroscience student is the question “will my recurrent neural network (RNN) model finally learn that task?”.
"__label__optimization Existing decentralized algorithms require knowledge of functions and network parameters, such as the Lipschitz constant of the global gradient and/or network connectivity, for
hyperparameter tuning."
__label__probabilistic_methods This work addresses the fundamental linear inverse problem in compressive sensing (CS) by introducing a new type of regularizing generative prior.
__label__other This allows personalization of the client models to fit their local distributions within the degrees of freedom in the solution simplex and homogenizes the update signals for the global simplex training.
__label__optimization It applies a Depth Value Network (DVN) based on graph convolution that exploits the symmetry property in $Q$ to auto-grasp value features.
__label__deep_learning_architectures Extensive experiments on five benchmarks demonstrate that START outperforms existing SOTA DG methods with efficient linear complexity.
__label__neuroscience_and_cognitive_science Therefore, we conclude that continuous attractors are functionally robust and remain useful as a universal analogy for understanding analog memory.
__label__diffusion_based_models In contrast to existing methods that necessitate training motion generation models on multi-human motion datasets with a fixed number of characters, our approach inherently possesses the flexibility to model human interactions involving an arbitrary number of individuals, thereby transcending the limitations imposed by the training data.
__label__optimization_for_deep_networks Addressing this, we propose Geometric Parameterization (GmP), a novel neural network parameterization technique that effectively separates the radial and angular components of weights in the hyperspherical coordinate system.
__label__bandits We treat the algorithm design problem as one of *mechanism design* under uncertainty and propose the Optimistic Grim Trigger Mechanism (OptGTM) that incentivizes the agents (i.e., arms) to report their contexts truthfully while simultaneously minimizing regret.
__label__optimization However, in many scenarios, the target customers comprise motifs, where activating only one or a few users within a motif is insufficient for effective viral marketing, which, nevertheless, receives little attention.
__label__neuroscience_and_cognitive_science These points are referred to as the noise equilibria because, at these points, noise contributions from different directions are balanced and aligned.
__label__online_learning Our approach is *simple* since it is algorithmically efficient-to-implement with a two-layer online ensemble structure and only $1$ gradient query per round, and theoretically easy-to-analyze with a novel and alternative analysis to the gradient-variation regret.
__label__neuroscience_and_cognitive_science This method works well in practice but lacks theoretical foundation.
__label__safety_in_machine_learning Motivated by this observation, we propose a ***B**ayesian-guided **L**abel **M**apping* (BLM) method.
__label__interpretability_and_explainability To counteract this, we propose a new approach, data-faithful feature attribution, which trains a confounder-free model using instrumental variables.
__label__learning_theory To optimize this loss function, we propose two families of surrogate losses: cost-sensitive comp-sum losses and cost-sensitive constrained losses.
__label__safety_in_machine_learning Existing research has mainly focused on unimodal scenarios on image data.
__label__machine_learning_for_healthcare Medical time series (MedTS) data, such as Electroencephalography (EEG) and Electrocardiography (ECG), play a crucial role in healthcare, such as diagnosing brain and heart diseases.
__label__learning_theory All of this is achieved by a novel set of analysis tools that characterize the algorithmic dynamics in a more deterministic manner.
__label__graph_neural_networks Here, we develop GraphMETRO, a Graph Neural Network architecture that models natural diversity and captures complex distributional shifts.
__label__probabilistic_methods Diffusion models have demonstrated competitive performance in missing data imputation (MDI) task.
__label__machine_vision Our new objective function can be viewed as a regularized maximum-likelihood estimation, constrained by a KL divergence penalty that integrates the text-encoder knowledge and guides the transductive learning process.
__label__neuroscience_and_cognitive_science The ablation of CMR-like heads suggests their causal role in in-context learning.
__label__natural_language_processing In contrast, MAGNET offers a customizable architecture where byte-level sequences are routed through language-script-specific predictors, each optimized for its respective language script.
__label__optimization_for_deep_networks Hence, the performance of LLMs in various NLP tasks depends significantly on the crucial role played by the attention mechanism with the softmax unit.
__label__natural_language_processing The code is available at [https://github.com/WooSunghyeon/dropbp](https://github.com/WooSunghyeon/dropbp).
__label__reinforcement_learning Our convergence rates match the familiar rates in the scalar reward setting, and additionally provide new insights into the fidelity of approximate return distribution representations as a function of the reward dimension.
__label__graph_neural_networks Leveraging this connection, we can efficiently compare the complexity of frames as well as determine the optimality of certain frames.
__label__optimization_for_deep_networks Code is available at https://github.com/jincan333/LoT.
__label__natural_language_processing This includes explicit markers of confidence (e.g.
__label__diffusion_based_models The project link and code is available at https://github.com/ai-med/StablePose.
__label__learning_theory Countermeasures, commonly referred to as preference alignment, include fine-tuning the pretrained LLMs with carefully crafted text examples of desired behaviour.
__label__human-AI_interaction In this work, we ask: Can LLMs and VLMs generate their own examples from generic, sub-optimal demonstrations?
__label__privacy Existing algorithms for user-level DP SCO are impractical in many large-scale machine learning scenarios because: (i) they make restrictive assumptions on the smoothness parameter of the loss function and require the number of users to grow polynomially with the dimension of the parameter space; or (ii) they are prohibitively slow, requiring at least $(mn)^{3/2}$ gradient computations for smooth losses and $(mn)^3$ computations for non-smooth losses.
__label__infrastructure A model matching technique is then applied to select the optimal personalized model, serving as a teacher model to guide each client’s training process.
__label__natural_language_processing The source code of \textsc{AutoPSV} is available at \url{https://github.com/rookie-joe/AutoPSV}.
__label__machine_vision Code will be available soon.
__label__other This auxiliary ranking classifier outputs a classification result, thus enabling integration with existing semi-supervised classification methods.
__label__machine_vision (iii) We find that features from Stable Diffusion and DINOv2 are good for discriminative learning of a number of properties, including scene geometry, support relations, shadows and depth, but less performant for occlusion and material, while outperforming DINOv1, CLIP and VQGAN for all properties.
__label__probabilistic_methods Circuits based on sum-product structure have become a ubiquitous representation to compactly encode knowledge, from Boolean functions to probability distributions.
__label__reinforcement_learning Our framework is a semi-supervised approach that leverages expert demonstrations as weak supervision to derive a set of candidate reward functions that align with the task rather than only with the data.
__label__other To address this gap, we present Coral, a $\underline{Co}$llabo$\underline{ra}$tive cognitive diagnosis model with disentang$\underline{l}$ed representation learning.
__label__machine_vision To address MECD, we devise a novel framework inspired by the Granger Causality method, using an efficient mask-based event prediction model to perform an Event Granger Test, which estimates causality by comparing the predicted result event when premise events are masked versus unmasked.
__label__machine_learning_for_healthcare Notably, the combination of moderator review and external medical knowledge in group collaboration resulted in an average accuracy **improvement of 11.8\%**.
__label__natural_language_processing Such re-training requires access to the data used to train the LoRA for the original base model.
__label__neuroscience_and_cognitive_science Second, although this effect is present in both hemispheres, it is stronger in the left than in the right hemisphere.
__label__optimization Relaxation onto the orthogonal group offers unique potential advantages such as a lower representation dimension and preservation of inner products; however, equally effective approaches remain unexplored.
__label__safety_in_machine_learning Experimental results demonstrate that our method is more effective and efficient than state-of-the-art token-level methods.
__label__probabilistic_methods These sorts of probabilistic inference questions are challenging when observations are high-dimensional.
__label__graph_neural_networks Through theoretical and empirical analysis, we demonstrate that text interpretability, a factor previously overlooked at the embedding level, plays a crucial role in attack strength.
__label__neuroscience_and_cognitive_science In stage two, we input both the high-level clip embedding, the blurry image and  caption from EEG latent to a pre-trained diffusion model.
__label__other Recently, the application of DDPMs has extended to time series generation tasks, where they have significantly outperformed other deep generative models, often by a substantial margin.
__label__machine_vision To initialize Gaussian primitives in such regions, we present a dynamic region densification method using the estimated depth maps and scene flow.
__label__reinforcement_learning The actions an agent has at its disposal often change the state of the environment in systematic ways.
__label__reinforcement_learning Despite the advancements in large language models (LLMs), recent agents built with these often neglect the control over discussion tactics, which are essential in communication scenarios and games.
__label__natural_language_processing Extensive evaluations are conducted across various model architectures and sizes (including a series of auto-regressive LLMs as well as BERT-like masking models) on a diverse set of NLP tasks (e.g., context-based question-answering, exhaustive hallucination evaluation, and passage-level dense retrieval).
__label__natural_language_processing Then a dataset with effective ICD sequences is constructed to train Lever-LM.
__label__bandits The softmax function is ubiquitous in machine learning and optimization applications.
__label__machine_vision Moreover, grouping learns to divide objects by considering inherent factors in a data-driven manner, without considering each factor separately as existing works.
"__label__optimization This improves upon the current best rate of
$\sqrt{\frac{\Tr(\Sigma)\ln(\tfrac{1}{\delta})}{T}}$ for Clipped-SGD, known \emph{only} for smooth and strongly convex objectives."
__label__algorithmic_game_theory We introduce and study the problem of detecting whether an agent is updating their prior beliefs given new evidence in an optimal way that is Bayesian, or whether they are biased towards their own prior.
__label__machine_learning_for_physical_sciences GenMS is able to generate complex structures such as double perovskites (or elpasolites), layered structures, and spinels, solely from natural language input.
__label__machine_vision However, this performance improvement has been accompanied by increases in memory usage, which stems from two factors, i.e., the storage of the whole weight matrix as learnable parameters in the optimizer and the additional storage of tunable weight indexes.
__label__learning_theory In addition to this theoretical contribution, we also perform the first empirical comparison of the proposed sample optimal Boosting algorithms.
__label__machine_learning_for_physical_sciences In recent years, machine learning has established itself as a powerful tool for high-resolution weather forecasting.
__label__machine_learning_for_other_sciences_and_fields In this paper, leveraging recent advances in LLMs, we propose an LLM-based approach (LLMLift) to building verified lifting tools.
__label__learning_theory data collected under different circumstances or synthesized by generative models.
__label__neuroscience_and_cognitive_science With the rapid advancements in cross-modality synthesis and brain decoding, the use of deep neural networks has emerged as a promising solution for inferring whole-brain, high-resolution fMRI features directly from electroencephalography (EEG), a more widely accessible and portable neuroimaging modality.
__label__human-AI_interaction Using a broad range of benchmark datasets and evaluation metrics, we bring to attention several important findings.
__label__deep_learning_architectures ECP-ViT, with the proposed algorithm-system co-optimizations, achieves a speedup of 4.6× to 26.9× on mobile GPUs across four datasets: STL-10, CIFAR100, TinyImageNet, and ImageNet.
__label__optimization However, in applications, it is common that good clusterings should optimize multiple objectives (e.g., visualizing data on a map by clustering districts into areas that are both geographically compact but also homogeneous with respect to the data).
__label__robotics These methods are susceptible to adverse weather conditions, limiting the all-weather deployment of self-driving cars.
__label__natural_language_processing Experimental results on three real-world simulated datasets with Mistral-7B, Gemma-7B, and Llama-3-8B demonstrate that our method can achieve superior performance compared to various strong baselines.
__label__machine_vision Extensive experiments on various real-world datasets demonstrate that our method achieves state-of-the-art rendering quality under a real-time level.
__label__diffusion_based_models Notably, on challenging inverse problems like 4x super-resolution on the ImageNet dataset, our method can generate high-quality samples in as few as *5* conditional sampling steps and outperforms competing baselines requiring 20-1000 steps.
__label__safety_in_machine_learning Based on this, we further propose the practical Query-based Reactivation Attack (QRA) which could effectively reactivate the backdoor by merely querying purified models.
__label__other Built upon our observation, we propose a novel learning approach to endow OFL with superb performance and low communication and storage costs, termed as FuseFL.
__label__interpretability_and_explainability When applied to testing samples, our method provides human-understandable explanations in the form of attribute-laden trees.
__label__privacy To achieve computational expediency, we propose an efficient Metropolis-Hastings algorithm and under certain regularity conditions, we establish that it enjoys polynomial mixing time to its stationary distribution.
__label__graph_neural_networks However, obtaining abundant labels is often challenging in practice, which makes graph few-shot incremental learning necessary.
__label__deep_learning_architectures The increasing size of neural networks has led to a growing demand for methods of efficient finetuning.
__label__bandits To the best of our knowledge, this is the first work in the contextual MNL bandit literature to prove minimax optimality --- for either uniform or non-uniform reward setting --- and to propose a computationally efficient algorithm that achieves this optimality up to logarithmic factors.
__label__generative_models Specifically, we design a visual masking data generation pipeline and create an IVM-Mix-1M dataset with 1 million image-instruction pairs.
__label__bandits Finally, we provide numerical simulations to assess our theoretical findings.
__label__machine_vision Recently, the success of unsupervised learning and graph neural networks has demonstrated the effectiveness of  data structure information.
__label__reinforcement_learning In this work, we take a close look at the learning process itself under the multi-level training in Procgen.
__label__robotics We identify three fundamental shortcomings in existing latent-space models that have so far prevented this powerful combination: (i) they lack the mathematical structure of a physical system, (ii) they do not inherently conserve the stability properties of the real systems, (iii) these methods do not have an invertible mapping between input and latent-space forcing.
__label__learning_theory We consider one of this setup's most fundamental and important manifestations where the output is a noisy linear combination of the inputs, and there are $k$ subgroups, each with its own regression vector.
"__label__machine_vision Our key insight is a ""decompose-recompose"" approach that factorizes the video scene into the background and object tracks, while also factorizing object motion into 3 components: object-centric deformation, object-to-world-frame transformation, and camera motion."
__label__neuroscience_and_cognitive_science These advances significantly broaden the applicability of non-invasive brain decoding in neuroscience and human-computer interaction, setting the stage for advanced brain-computer interfaces and cognitive models.
__label__natural_language_processing Experimental results show that (i) \textsc{DeePEn} achieves consistent improvements across six benchmarks covering subject examination, reasoning, and knowledge, (ii) a well-performing specialist model can benefit from a less effective LLM through distribution fusion, and (iii) \textsc{DeePEn} has complementary strengths with other ensemble methods such as voting.
__label__machine_learning_for_other_sciences_and_fields Specifically, we propose a fine-grained representation factorization module that sufficiently extracts valuable sentiment information by factorizing modality into sentiment-relevant and modality-specific representations through crossmodal translation and sentiment semantic reconstruction.
__label__machine_learning_for_physical_sciences Recent works proposed amortizing the cost by learning generalized wave functions across different structures and compounds instead of solving each problem independently.
__label__reinforcement_learning We show \emph{learning a generative model of human partners} can effectively address this issue.
__label__optimization_for_deep_networks However, the optimization for tasks performance often comes at the cost of generalizability in fine-tuned models.
__label__other However, are large-scale soft labels necessary for large-scale dataset distillation?
__label__machine_vision These accelerators, equipped with multiple parallel processors and dedicated per-processor memory instances, offer substantial performance improvements over traditional microcontroller units (MCUs).
__label__machine_vision In this paper, we present MoTE, a novel framework that enables generalization and specialization to be balanced in one unified model.
__label__natural_language_processing We also find that despite the recent focus on IO, how we select exemplars can outweigh how we optimize instructions, with EO strategies as simple as random search outperforming state-of-the-art IO methods with seed instructions without any optimization.
__label__robotics However, the heterogeneity between encoders and decoders complicates the models, and the manual separation of historical and future trajectories leads to low data utilization.
__label__algorithmic_game_theory We circumvent the aforementioned intractability by developing techniques that exploit the hidden structure of the objective function via a nonconvex--concave reformulation.
__label__optimization Recently, machine learning techniques have been adopted to expedite classic optimization algorithms.
__label__machine_vision Given two observations of an object in different articulation states, our method learns the geometry and appearance of object parts by using an implicit model from the first observation, distills the part segmentation and articulation from the second observation while rendering the latter observation.
__label__deep_learning_architectures Unlike conventional SSMs that maintain a monolithic state vector, SlotSSMs maintains the state as a collection of multiple vectors called slots.
__label__algorithmic_game_theory We observe that in QRJA, judges do not have to be people with subjective opinions; for example, a race can be viewed as a ``judgment'' on the contestants' relative abilities.
__label__machine_vision Under this view, we show that the 3D Gaussian updates can be converted as Stochastic Gradient Langevin Dynamics (SGLD) update by simply introducing noise.
__label__deep_learning_architectures Following this, we assign confidence levels to the initial imputations by correlating missing data with valid data.
__label__generative_models This is achieved through MIDGArD's modular approach that separates the problem into two primary components: structure generation and shape generation.
"__label__privacy Our algorithms match the lower bounds on privacy-utility trade-offs (including constants but ignoring $\delta$ factors) and we also present a new space lower bound
matching our constructions up to small constant factors."
__label__reinforcement_learning The cutting-edge research is focused on incorporating domain knowledge into rewards and introducing additional mechanisms to incentivize cooperation.
__label__optimization Further, we provide convolution-specific transformations based on the connectivity pattern which allow to simplify diagrams before evaluation.
__label__generative_models Extensive experiments demonstrate that MVGamba outperforms state-of-the-art baselines in all 3D content generation scenarios with approximately only $0.1\times$ of the model size.
__label__diffusion_based_models This strategy interpolates these regions using similar features from neighboring areas, thus enhancing semantic integrity.
__label__reinforcement_learning We empirically validate the correctness of the derived upper bounds and demonstrate the superior effectiveness of the proposed algorithm over benchmarks.
__label__safety_in_machine_learning The increasing compute demands of AI systems has led to the emergence of services that train models on behalf of clients lacking necessary resources.
__label__neuroscience_and_cognitive_science We show that a neural network trained to minimize the proposed objective learns place-like representations.
__label__diffusion_based_models We demonstrate that the proposed PaDIS approach enables solving various inverse problems in both natural and medical image domains, including CT reconstruction, deblurring, and superresolution, given only patch-based priors.
__label__natural_language_processing Open-source Large Language Models (LLMs) and their specialized variants, particularly Code LLMs, have recently delivered impressive performance.
__label__learning_theory We conjecture that larger gaps are possible for the agnostic case.
__label__probabilistic_methods This gives rise to the RTK-MALA and RTK-ULD algorithms for diffusion inference.
__label__machine_vision We show extensive results on challenging DAVIS, Kubric, and self-captured videos with quantitative comparisons and a user preference study.
__label__active_learning Our results demonstrate superior sample efficiency and generalization compared to non-contextual ranking approaches and active preference learning baselines.
__label__human-AI_interaction Both empirical results and user studies demonstrate CBPR's superior competitiveness compared to existing baselines.
__label__diffusion_based_models We also propose a novel parameterization technique for learning the forward process.
__label__active_learning We then propose the mean prediction (MP) algorithm and theoretically analyze it  in terms of the regret of predicted pairwise orderings between inputs.
__label__machine_learning_for_healthcare The variable-specific parameter spaces and dynamic graphs are injected into the graph convolutional recurrent network to capture intra-variable and inter-variable dependencies in ISMTS together.
__label__machine_vision To that end, knowledge dissemination, separation, and distillation are carried out successively.
__label__learning_theory Joint Embedding Predictive Architectures (JEPAs) is a class of architectures in which semantically similar inputs are encoded into representations that are predictive of each other.
__label__reinforcement_learning We analyze the key design choices that are required to make diffusion suitable for world modeling, and demonstrate how improved visual details can lead to improved agent performance.
__label__machine_learning_for_other_sciences_and_fields While large language models (LLMs) have shown some success in automatic code transpilation, none of them provide any functional correctness guarantees on the rewritten code.
__label__graph_neural_networks This severely limits their potential to solve complex tasks.
__label__generative_models We introduce a generative model for protein backbone design utilizing geometric products and higher order message passing.
__label__natural_language_processing However, transforming a chat LLM involves two critical issues: (1) How can we effectively transfer advanced abilities without their supervised data?
__label__diffusion_based_models In this paper, we reveal that the concrete score in absorbing diffusion can be expressed as conditional probabilities of clean data, multiplied by a time-dependent scalar in an analytic form.
__label__machine_learning_for_healthcare Protein language models (PLMs) have demonstrated strong capabilities in broad biological applications.
__label__reinforcement_learning To extract an efficient policy, it is necessary to \emph{stitch} the best behaviors from the dataset.
__label__generative_models We experiment on classic domains such as lists and strings, and an uncommon graphics programming domain not well represented in typical pretraining data.
__label__active_learning The theory is complemented by experiments on real as well as semi-synthetic datasets.
__label__machine_vision Few-shot 3D point cloud semantic segmentation aims to segment query point clouds with only a few annotated support point clouds.
__label__online_learning In each step, the framework reveals both unlabeled data from the current time step t and labels delayed with d steps, from the time step t−d.
__label__optimization We examine the effect of this cost for two distinct models in real-world and synthetic datasets.
__label__learning_theory Additionally, we give a conditional lower bound showing that the communication of our protocols is nearly optimal.
__label__algorithmic_game_theory Numerical experiments validate the effectiveness of our algorithm and theoretical results.
__label__probabilistic_methods As training objective, we derive a variational lower bound to the marginal likelihood.
__label__robotics SyntheOcc addresses the critical challenge of how to efficiently encode 3D geometric information as conditional input to a 2D diffusion model.
__label__diffusion_based_models Through an uncertainty quantification (UQ) perspective, we show that score-based generative models (SGMs) are provably robust to the multiple sources of error in practical implementation.
"__label__learning_theory It elucidates insightful theories and
useful tools to understand and optimize neural networks."
__label__optimization In this paper, we propose a DRO framework that relies on a new distance inspired by Unbalanced Optimal Transport (UOT).
__label__algorithmic_game_theory Our first result shows that this holds forall deterministictruthful auctions.
__label__causal_inference As many practical fields transition to provide personalized decisions, data is increasingly relevant to support the evaluation of candidate plans and policies (e.g., guidelines for the treatment of disease, government directives, etc.).
__label__generative_models In our work, we propose an efficient data attribution method by simulating unlearning the synthesized image.
__label__learning_theory Moreover, our flexible framework can be extended to different domains, tasks, and architectures.
__label__evaluation As such, lifelong benchmarks offer a robust, practical solution to the ``benchmark exhaustion'' problem.
__label__optimization This reformulation enables the use of first-order optimization algorithms in calculating the backward pass gradients, allowing our framework to potentially utilize any state-of-the-art solver.
__label__reinforcement_learning To the best of our knowledge, this is the **first** work that achieves **near-optimal** dynamic regret for adversarial linear mixture MDPs with the unknown transition without prior knowledge of the non-stationarity measure.
__label__reinforcement_learning Next, we conduct experiments on pluralistic language datasets representing diverse user preferences and demonstrate improved reward function accuracy.
__label__probabilistic_methods We improve the mixing time bound of prior best Dikin walk due to Narayanan and Rakhlin that mixes in $\widetilde O((n^2d^3+n^2dL^2R^2)\log(w/\delta))$ steps.
__label__machine_learning_for_social_sciences Finally, we extensively evaluate PRB in both online and offline settings, comparing it with bandit-based and graph-based methods.
__label__optimization_for_deep_networks We first study how distributed concept drift affects the model training and find that local classifier plays a critical role in drift adaptation.
__label__safety_in_machine_learning Ablation studies further validate our approach's effectiveness, robustness, and generalizability across different model backbones.
__label__diffusion_based_models Extensive experiment results demonstrate that COVE achieves the start-of-the-art performance in various video editing scenarios, outperforming existing methods both quantitatively and qualitatively.
__label__machine_learning_for_other_sciences_and_fields Computational methodsprimarily rely on evolutionary information (EI) encoded by protein languagemodels (PLMs) to predict fitness landscape for optimization.
__label__machine_vision SaSPA consistently outperforms all established baselines across multiple settings, including full dataset training and contextual bias.
__label__deep_learning_architectures Despite their benefits, MoE models face significant challenges during inference, including inefficient memory management and suboptimal batching, due to misaligned design choices between the model architecture and the system policies.
__label__machine_learning_for_healthcare The success of machine learning models relies heavily on effectively representing high-dimensional data.
__label__machine_vision This growth presents scalability challenges and hinders splatting efficiency.
__label__graph_neural_networks This gives rise to MolGPS, a new graph foundation model that allows to navigate the chemical space, outperforming the previous state-of-the-arts on 26 out the 38 downstream tasks.
__label__bandits 3.
__label__safety_in_machine_learning Specifically, we observe a probability curvature effect of MLLMs where clean and noisy examples reside on curvatures with different smoothness, further enabling the detection of label noise.
__label__natural_language_processing Extensive empirical studies across different models and datasets demonstrate that internal consistency effectively distinguishes between correct and incorrect reasoning paths.
__label__machine_vision Project page: https://xinhangliu.com/chatcam.
__label__interpretability_and_explainability In the second half of the model, the fine-tuned representations develop probability modes that better encode the identity of answers, while less-defined peaks characterize the landscape of ICL representations.
__label__machine_learning_for_physical_sciences Recently, deep learning-based methods have achieved impressive accuracy in dual-frame fluid motion estimation; however, they heavily depend on large volumes of labeled data.
__label__interpretability_and_explainability Experimental results demonstrate that our strategy improves fidelity between the target and surrogate model predictions on several datasets.
__label__deep_learning_architectures To achieve this, inspired by polynomial decomposition in calculus, where a function can be approximated by linearly combining several basic components, we propose to linearly decompose the ViT model into a set of components called learngenes during element-wise training.
__label__optimization Split federated learning (SFL) is a recent distributed approach for collaborative model training among multiple clients.
"__label__other A classifier is trained to
 make the final prediction based on the statistics of these losses."
__label__machine_learning_for_other_sciences_and_fields To evaluate the proposed ProSST, we conduct extensive experiments on the zero-shot mutation effect prediction and several supervised downstream tasks, where ProSST achieves the state-of-the-art performance among all baselines.
__label__machine_vision To minimize such a need, task-generic promptable segmentation has been introduced, which employs a single task-generic prompt to segment various images of different objects in the same task.
__label__learning_theory Lastly, we numerically compare our theoretical bounds with the empirical performance of LSE and SME on a pendulum example and a quadrotor example.
__label__diffusion_based_models Numerous methods have been proposed to ensure these models generate safe images.
__label__deep_learning_architectures Finally, we empirically validate the bounds and uncover valuable insights into the influence of the analyzed architectural components.
__label__machine_vision By imposing the CCA-loss, we enable the neural network to spontaneously adjust important information into the early stage of the autoregressive entropy model.
__label__safety_in_machine_learning However, detecting LGTs is challenging due to the vast number of LLMs, making it impractical to account for each LLM individually; hence, it is crucial to identify the common characteristics shared by these models.
__label__infrastructure Recently, various strategies for distributed training of large language models (LLMs) have been proposed.
__label__interpretability_and_explainability This effectively offers practitioners a valuable tool for prioritizing model trials.
__label__learning_theory Moreover, we demonstrate that such a preprocess-then-optimize algorithm can significantly outperform naive gradient descent and ridge regression algorithms.
__label__reinforcement_learning While theoretical understanding has been obtained for ICL in reinforcement learning (RL), the previous results are largely confined to the single-agent setting.
__label__interpretability_and_explainability A commonly used framework involves defining a predictiveness criterion, applying a cross-fitting procedure to estimate the predictiveness, and utilizing the difference in estimated predictiveness between two models as the test statistic.
__label__infrastructure Our empirical analysis encompasses a variety of downstream training modalities, including Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Reward Model (RM).
__label__machine_vision We present a novel OCR-free document understanding framework based on pretrained Multimodal Large Language Models (MLLMs).
__label__generative_models Like GPT, we extend the higher-order graph transformer to support parallel training of all blocks.
__label__machine_vision Specifically, we divide the point cloud into different patches and use a lightweight yet effective Inner Mamba to capture local geometric information.
__label__optimization However, there has been no progress since, either in terms of achievability or impossibility.
__label__deep_learning_architectures Most VSAs were developed before deep learning and automatic differentiation became popular and instead focused on efficacy in hand-designed systems.
__label__natural_language_processing This separated architecture restricts knowledge sharing and deep collaboration between them.
__label__generative_models Towards this, we introduce Time Reversed Language Models (TRLMs), which can score and generate queries when conditioned on responses, effectively functioning in the reverse direction of time.
__label__generative_models Additionally, to eliminate extra computational costs caused by PTQ4DiT during inference, we design an offline re-parameterization strategy for DiTs.
__label__evaluation We extend the specification to better leverage model pseudo-labels and subsequently enrich the unified embedding space for better specification evolvement.
__label__machine_vision Additionally, MonoMAE learns generalizable representations that can work well in new domains.
__label__machine_vision Then, we present a high-pass filter-based photovoltage estimation module, which effectively reduces noise in event data to improve the robustness of our method in real-world scenarios.
__label__natural_language_processing Concretely, we collect a few data samples, and perform singular value decomposition for each linear layer of a pre-trained LLM multiplied by the covariance matrix of the input activation using these samples.
__label__robotics Our proposed open-source environments and comprehensive analysis show promising boost to future research in garment manipulation by unlocking the full potential of these methods.
__label__optimization This efficiency is quantified by defining a *cost* function between source and target data.
__label__natural_language_processing Our experiments reveal that fine-tuning as well as advanced pre-training strengthens LLM-built representations' tendency of maintaining goal-oriented abstractions during decoding, prioritizing task completion over recovery of the world's state and dynamics.
__label__other We characterize the achievable bit rates as a function of cluster sizes and number of elements, showing RCC consistently outperforms previous methods while requiring less compute and memory resources.
__label__natural_language_processing Our extensive experiments reveal that these aligned models show a marked increase in honesty, as indicated by our proposed metrics.
__label__machine_vision However, a limited amount of research focuses on a more challenging continual learning problem in SGG.
__label__natural_language_processing Our method introduces an iterative refinement process that generates multiple problem PDDL candidates and progressively refines the domain PDDL based on feedback obtained from interacting with the environment.
__label__natural_language_processing These models, which feature architectures of 54 billion and 140 billion parameters, respectively, are based on the Mixtral architecture.
__label__learning_theory The proposed risk estimates serve as effective proxies for the actual generalization error, allowing us to determine the optimal stopping iteration that minimizes the generalization error.
__label__machine_vision In this paper, we propose a novel learning method to overcome this weakness by designing the rate-distortion optimal lattice vector quantization (OLVQ) codebooks with respect to the sample statistics of the latent features to be compressed.
__label__machine_vision Recently, there have been explorations of generalist segmentation models that can effectively tackle a variety of image segmentation tasks within a unified in-context learning framework.
__label__deep_learning_architectures We provide a theoretical analysis of spectral fine-tuning and show that our approach improves the rank capacity of low-rank adapters given a fixed trainable parameter budget.
__label__online_learning Somewhat surprisingly, despite NSW being a concave function, we prove that no algorithm can achieve sublinear regret.
__label__machine_vision This paper addresses these challenges by proposing a unified approach that integrates the strengths of both paradigms.
__label__human-AI_interaction We evaluate our method on a set of standard Markov Decision Process (MDP) benchmarks.
__label__generative_models Our code is available at https://github.com/justkolesov/EnergyGuidedBarycenters.
__label__generative_models The results demonstrate that our method exhibits out-of-domain generalization and interpretability.
__label__safety_in_machine_learning To address this issue, we introduce a novel framework of distributionally robust performative prediction and study a new solution concept termed as distributionally robust performative optimum (DRPO).
__label__machine_vision In this work, we propose a novel image quantization model named VQGAN-LC (Large Codebook), which extends the codebook size to 100,000, achieving an utilization rate exceeding 99%.
__label__neuroscience_and_cognitive_science We investigate how to generate maximal neural variability while at the same time having high network performance.
__label__machine_vision We develop an unsupervised object detection architecture and prove that the learned variables correspond to the true object positions up to small shifts related to the encoder and decoder receptive field sizes, the object sizes, and the widths of the Gaussians used in the rendering process.
__label__natural_language_processing Knowledge Graph Embedding (KGE) techniques are crucial in learning compact representations of entities and relations within a knowledge graph, facilitating efficient reasoning and knowledge discovery.
__label__generative_models Qualitative and quantitative evaluations demonstrate that our model outperforms existing methods in both in-domain and out-of-domain scenarios.
__label__active_learning This paper investigates ML systems serving a group of users, with multiple models/services, each aimed at specializing to a sub-group of users.
__label__machine_vision We propose to explore an interesting and promising problem, Cloud Object Detector Adaptation (CODA), where the target domain leverages detections provided by a large cloud model to build a target detector.
__label__machine_vision In the paper, we first justify the use of the Gauss-Laguerre quadrature and then demonstrate this plug-and-play attribute by implementing it in two different NeRF models.
__label__online_learning The trichotomy is then determined by a combination of the Level-constrained Littlestone and Branching dimensions.
__label__machine_vision Furthermore, we demonstrate that not all soft labels are created equal; they must contain *structured information* to be beneficial.
__label__deep_learning_architectures In this work we present two sampling strategies for such sharding, obtained as solutions to specific optimization problems.
__label__online_learning We conduct various ablations and sensitivity experiments, demonstrating the effectiveness of our approach.
__label__natural_language_processing Improvements include gains of up to 30 points on logical reasoning benchmarks, up to 10 points on math and coding benchmarks, and 5 points on the benchmark suite BBH.
__label__speech_and_audio Specifically, we propose a novel LLM-driven audio codec model, LLM-Codec, which transfers the audio modality into textual space by representing audio tokens with words or sub-words from the LLM vocabulary, while maintaining high audio reconstruction quality.
__label__causal_inference We show the identification properties of our compositional model, inspired by advances in causal matrix factorization methods.
__label__learning_theory We address this question based on the framework of *data-driven algorithm design*, which connects the amount of data sufficient for establishing generalization bounds to the *pseudo-dimension* of performance metrics.
__label__safety_in_machine_learning This indicates a negative trade-off between adversarial robustness and privacy in general deep learning models.
__label__interpretability_and_explainability It draws inspiration from the strategy of additional splitting of testing data, but encourages an overlap between two testing data splits in predictiveness evaluation.
__label__machine_learning_for_other_sciences_and_fields Our algorithm can well transfer the knowledge gained in single-target pretraining to dual-target scenarios in a zero-shot manner.
__label__deep_learning_architectures We theoretically demonstrate that quadratic neurons inherently capture correlation within structured data, a feature that grants them superior generalization abilities over traditional neurons.
__label__safety_in_machine_learning Generating synthetic fake faces, known as pseudo-fake faces, is an effective way to improve the generalization of DeepFake detection.
__label__natural_language_processing These losses are designed to effectively cluster related entities (input as texts) and organise them hierarchically.
"__label__fairness On the other hand, fine-tuning-free methods typically utilize a ``one-size-fits-all"" approach that assumes that correlation with the spurious attribute can be explained using a single linear direction across all possible inputs."
__label__reinforcement_learning By exploiting the representation power of diffusion models, ADD can directly generate adversarial environments while maintaining the diversity of training environments, enabling the agent to effectively learn a robust policy.
__label__deep_learning_architectures We empirically validate the efficacy of MatFormer across different model classes (decoders and encoders) and modalities (language and vision), demonstrating its potential for real-world deployment.
__label__reinforcement_learning Our work makes the initial step to uncovering the inherent difficulty of robust RL via interactive data collection and sufficient conditions for designing a sample-efficient algorithm accompanied by sharp sample complexity analysis.
__label__machine_vision To ensure that the one-step diffusion model could yield HQ Real-ISR output, we apply variational score distillation in the latent space to conduct KL-divergence regularization.
__label__graph_neural_networks Furthermore, we introduced a global uniformity constraint to replace negative sampling, reducing the time complexity from $O(n^2)$ to $O(n)$, and by explicitly defining positive samples, FUG avoids the substantial memory requirements of data augmentation.
__label__reinforcement_learning We demonstrate that our strategy surpasses existing behaviour cloning and online algorithms, as well as online-offline baselines for multi-armed bandits, Markov decision processes (MDPs), and partially observable MDPs, showcasing the broad reach and utility of ExPerior in using expert demonstrations across different decision-making setups.
__label__probabilistic_methods These distributions enable practitioners to quantify uncertainty, compute risk, and detect outliers.
__label__diffusion_based_models Our thorough toy experiments thus contribute a deeper understanding of how diffusion models capture compositional structure in data, paving the way for future research aimed at enhancing factorization and compositional generalization in generative models for real-world applications.
__label__generative_models The project page: https://linshan-bin.github.io/GeoLRM/.
__label__infrastructure This paper introduces FlashMask, a simple yet effective \emph{Exact} attention algorithm designed to substantially reduce both the computational complexity and memory requirements of attention computations.
__label__machine_vision Yet, the outputs of current models are limited to point estimates or tight distributions around single modes, which prevent them from capturing these effects.
__label__probabilistic_methods We benchmark various baselines based on direct adaptations of existing federated model aggregation techniques and introduce a new probabilistic prompt aggregation method that substantially outperforms these baselines.
__label__graph_neural_networks This work introduces Graph Reference Distribution Learning (GRDL), an efficient and accurate graph classification method.
__label__reinforcement_learning Such decision problems can be modeled as Latent Markov Decision Processes (LMDPs), where a latent variable is selected at the beginning of an interaction and is not disclosed to the agent initially.
__label__neuroscience_and_cognitive_science Here, we ask whether we can obtain interpretable embeddings through LLM prompting.
__label__natural_language_processing In this work, we propose Graph Uncertainty -- which represents the relationship between LLM generations and claims within them as a bipartite graph and estimates the claim-level uncertainty with a family of graph centrality metrics.
__label__other Additionally, a thorough ablation study is conducted to dissect the pivotal elements in DUSA.
__label__machine_vision Then, to fully mining the available WSIs, we use all the patches and available patch labels to build a cache branch, which utilizes the labeled patches to learn the labels of unlabeled patches and through knowledge retrieval for patch classification.
__label__natural_language_processing Our experimental results in mathematical reasoning tasks demonstrate that AlphaLLM significantly enhances the performance of LLMs without additional annotations, showing the potential for self-improvement in LLMs.
__label__machine_learning_for_other_sciences_and_fields These sequences offer valuable insights into users’ intentions and preferences.
__label__safety_in_machine_learning We refer to the weight attribution-guided LLM unlearning method as WAGLE, which unveils the interconnections between 'influence' of weights and 'influence' of data to forget and retain in LLM generation.
__label__machine_vision Both reconstruction quality and throughput substantially surpass the state-of-the-art methods (e.g., Quanta Burst Photography (QBP)).
__label__machine_vision However, a notable drawback of this approach is its limited practical value in gaze applications, as mere localization may not fully capture our primary interest — understanding the underlying semantics, such as the nature of the gaze target, rather than just its 2D pixel location.
__label__bandits We discuss how CBA can be applied to the problem of adversarial contextual bandits with the option of abstaining from selecting any action.
__label__graph_neural_networks We address this fundamental question for message passing GNNs under ReLU activations, i.e., the de-facto choice for most GNNs.
__label__causal_inference For another, we can capture important aspects of causality using compatibility: we can use compatibility to understand cyclic causal graphs, and to demonstrate structural compatibility, we must essentially produce a causal model.
__label__other In this paper, we analyze the training dynamics of LP-FT for classification tasks on the basis of the neural tangent kernel (NTK) theory.
__label__optimization Our analysis is extended to a lazy deployment scheme where models are deployed once per several SGD updates, and we show that it converges to a bias-free SPS solution.
__label__probabilistic_methods By maximizing a novel anomaly-aware data likelihood, representation learning and clustering can effectively reduce the adverse impact of anomalous data and collaboratively benefit anomaly detection.
__label__graph_neural_networks We theoretically demonstrate that the learned representations are divided into distinct partitions based on the number of classes and exhibit enhanced generalization ability across tasks.
__label__machine_vision Parameter efficient fine-tuning ($\texttt{PEFT}$) provides promising generalization performance in adaptation while incurring minimum computational overhead.
__label__optimization This paper introduces a novel optimization algorithm named KATE, which presents a scale-invariant adaptation of the well-known AdaGrad algorithm.
__label__machine_vision Such an issue is hard to be addressed by enlarging the group size with existing serialization-based methods due to the quadratic complexity of Transformers with feature sizes.
__label__optimization For $p-1=\Omega(1)$, this rate is near optimal due to existing lower bounds.
__label__learning_theory Moreover, through the lens of the pruning ratio threshold,  we can provide rigorous interpretations on several heuristics in existing pruning algorithms.
__label__machine_vision The lack of object-level labels presents a significant challenge for 3D object retrieval in the open-set environment.
__label__generative_models To address these challenges, we present FouRA, a novel low-rank method that learns projections in the Fourier domain along with learning a flexible input-dependent adapter rank selection strategy.
"__label__speech_and_audio For a machine to have the same degree of comprehension,  the machine must know what a lion is (semantic attribute), what the concept of ""behind"" is (spatial attribute) and how these pieces of linguistic information align with the semantic and spatial attributes of the sound (what a roar sounds like when its coming from behind)."
__label__reinforcement_learning In this paper, we propose the Decomposed Prompt Decision Transformer (DPDT) that adopts a two-stage paradigm to efficiently learn prompts for unseen tasks in a parameter-efficient manner.
__label__privacy Next, we propose a simple data model which is based on a mixture of Dirichlet distributions, to formally motivate our non-private algorithm and demonstrate some properties of its components.
__label__robotics These results provide compelling evidence that world models can significantly enhance the precision of robot action prediction.
__label__machine_vision SeCo formulates pseudo-labels at the connectivity level, which makes it easier to locate and correct closed and open set noise.
__label__reinforcement_learning Several OPE estimators have been proposed in the last decade, many of which have hyperparameters and require training.
__label__machine_vision Extensive experiments on five datasets demonstrate that SGNet achieves competitive performance and consistent improvements across a variety of general segmentation models, surpassing the traditional ultra image segmentation methods by a large margin.
__label__neuroscience_and_cognitive_science Altogether, LoRaFB-SNet is highly competent in capturing both dynamic and static representations of the mouse visual cortex and contributes to the understanding of movie processing mechanisms of the visual system.
__label__reinforcement_learning The resulting algorithm is easy to implement, requiring only a few lines of code modification to existing methods.
__label__deep_learning_architectures While recent studies have experimentally demonstrated that the non-one-hot ECOCs with multi-bits error correction ability, could be a better solution, there is a notable absence of theoretical foundations that can elucidate the relationship between codeword design, weight-error magnitude, and network characteristics, so as to provide robustness guarantees.
__label__diffusion_based_models We introduce a formal approach to analyze the uniqueness, redundancy, and synergy terms by applying PID to the denoising model at both the image and pixel level.
__label__other Extensive empirical evaluations demonstrate that NoMAD-Attention maintains the quality of the original LLMs well and speeds up the 4-bit quantized LLaMA-7B-based model by up to $2 \times$ at 16k context length.
__label__interpretability_and_explainability Utilizing only noisy data, our method can effectively learn the latent causal structure.
__label__learning_theory Although numerous DMVC approaches have been proposed, the collaboration role of individual views have not been well investigated in existing literature.
__label__optimization Uncertain perturbations in dynamical systems often arise from diverse resources, represented by latent components.
__label__neuroscience_and_cognitive_science Overall, our approach -- inspired by neuroscience findings and capitalizing on region-level representations from specific brain regions -- is suitable for invasive brain modeling and represents a promising neuro-inspired AI approach in brain-computer interfaces.
__label__machine_vision An inter-query contrastive loss further encourages the diversity of object queries.
__label__robotics To address this problem, we introduce DeMo, a framework that decouples multi-modal trajectory queries into two types: mode queries capturing distinct directional intentions and state queries tracking the agent's dynamic states over time.
__label__machine_learning_for_healthcare Dynamic sparse feature fusion (DSFF) mechanism: it adaptively learns to fuse informative multi-scale features while reducing redundancy.
__label__learning_theory We first obtain an exact decomposition of the large matrix verification problem into smaller sub-problems.
__label__causal_inference Unlike hard interventions, which directly force changes in a causal variable, soft interventions exert influence indirectly by affecting the causal mechanism.
__label__diffusion_based_models First, we propose to start the generation process from an earlier time step to avoid the unreliable large-time steps of I2V-DMs, as well as an initial noise distribution with optimal analytic expressions (Analytic-Init) by minimizing the KL divergence between it and the actual marginal distribution to bridge the training-inference gap.
__label__interpretability_and_explainability This paper introduces MeLLoC(Mechanism Learning for Lossless Compression), a novel approach that combines high-order mechanism learning with classical encoding to enhance lossless compression for scientific data.
__label__reinforcement_learning Motivated by this paradigm shift, we introduce DIAMOND (DIffusion As a Model Of eNvironment Dreams), a reinforcement learning agent trained in a diffusion world model.
__label__robotics Motion forecasting for agents in autonomous driving is highly challenging due to the numerous possibilities for each agent's next action and their complex interactions in space and time.
__label__reinforcement_learning Then, we propose to combine a set of established and novel solution ideas to yield the e-COP  algorithm that is easy to implement and numerically stable, and provide a theoretical guarantee on optimality under certain scaling assumptions.
__label__natural_language_processing This paper takes a first step in this direction by considering families of functions (i.e.
__label__other While reshuffling leads to test performances that are competitive with using fixed splits, it drastically improves results for a single train-validation holdout protocol and can often make holdout become competitive with standard CV while being computationally cheaper.
__label__machine_vision With noisy depth predictions as input, SDDR generates low-noise depth edge representations as pseudo-labels by coarse-to-fine self-distillation.
__label__fairness This can be useful to find a practical compromise between efficiency---by providing informative predictions---and algorithmic fairness---by ensuring equalized coverage for the most sensitive groups.
__label__learning_theory To overcome the challenge above, we derive new lower bounds on testing errors that are adaptive to the model complexity.
__label__safety_in_machine_learning This yields the conclusion that, to achieve truly *privacy-preserving LLM adaptations* that yield high performance and more privacy at lower costs, taking into account current methods and models, one should use open LLMs.
__label__deep_learning_architectures To mitigate this gap, we theoretically analyze the optimization objective of dataset condensation for TS-forecasting and propose a new one-line plugin of dataset condensation for TS-forecasting designated as Dataset \textbf{Cond}ensation for \textbf{T}ime \textbf{S}eries \textbf{F}orecasting (CondTSF) based on our analysis.
__label__machine_learning_for_other_sciences_and_fields By aggregating dynamics across diverse temporal dependencies and channeling them into the GFN, the SICSM adeptly approximates the posterior distribution of the system's structure.
__label__other Essentially, to start from the pretrained model, one can either initialize $B$ to zero and $A$ to random, or vice-versa.
__label__other To this end, we release ChessBench, a large-scale benchmark dataset of 10 million chess games with legal move and value annotations (15 billion data points)  provided by Stockfish 16, the state-of-the-art chess engine.
__label__other With a long history in network science, community detection typically relies on objective functions, optimised with custom-tailored search algorithms, but often without leveraging recent advances in deep learning.
__label__generative_models However, these strategies cannot fully harness the potential of the similar feature patterns across adjacent timesteps.
__label__other A key challenge in applying OT to massive datasets is the quadratic scaling of the coupling matrix with the size of the dataset.
__label__deep_learning_architectures }, attention, STAR employs a centralized strategy to improve efficiency and reduce reliance on the quality of each channel.
__label__reinforcement_learning However, outside of specific applications, existing approaches for interacting with unknown environments are not well-adapted to these observations.
__label__machine_learning_for_physical_sciences We propose the first learning scheme for functional differential equations (FDEs).
__label__probabilistic_methods Among these Bayesian models, Poisson-Gamma Dynamical Systems (PGDSs) are proven to be effective in capturing the evolving dynamics underlying observed count sequences.
__label__machine_vision However, the VLAD-based aggregation methods usually learn a large number of (e.g., 64) clusters and their corresponding cluster centers, which directly leads to a high dimension of the yielded global features.
__label__reinforcement_learning We show that OTaCoS enjoys sublinear regret for systems with sufficiently smooth dynamics and empirically results in further sample-efficiency gains.
__label__natural_language_processing Under this view, existing uncertainty estimation methods based on the concept of self-consistency can be viewed as using degree centrality as an uncertainty measure, and we show that more sophisticated alternatives such as closeness centrality provide consistent gains at claim-level uncertainty estimation.
__label__diffusion_based_models Due to semantic sharing, when a smaller patch is jointly denoised alongside a generatable-size image, the two become semantic clones.
__label__graph_neural_networks However, a major issue of those generalizations is that the calculation of such paths is computationally expensive.
__label__optimization_for_deep_networks The results suggest the effectiveness and efficiency of LoT in identifying generalizable information at the right scales while discarding spurious data correlations, thus making LoT a valuable addition to current machine learning.
__label__machine_learning_for_physical_sciences equilibrium structure, demand more cost to compute than others, e.g.
__label__optimization_for_deep_networks To reduce the computation budget of transformer-based DPMs, this work proposes the Efficient Diffusion Transformer (EDT) framework.
__label__optimization Second, we investigate the problem of learning a single neuron with the bilevel approach and obtain local exponential convergence rates that depend polynomially on the dimension and noise level (to compare with the exponential dependence that would result from prior analyses).
__label__machine_vision For example, on the recent TAP-Vid benchmark, our framework consistently improves all baselines, e.g., up to 13.5% improvement on the average Jaccard metric.
__label__deep_learning_architectures The MVF module fuses radar and image features within both the camera view and bird's-eye view, thereby generating a more precise unified BEV representation.
__label__machine_learning_for_other_sciences_and_fields Moreover, these methods employ separate training schemes and ignore the interdependencies between the dividing and conquering strategies, often leading to sub-optimal solutions.
__label__human-AI_interaction The RG-SAN consists of the Text-driven Localization Module (TLM) and the Rule-guided Weak Supervision (RWS) strategy.
__label__machine_vision Designing single-task image restoration models for specific degradation has seen great success in recent years.
__label__natural_language_processing In this work, we tackle a task which we call *data mixture inference*, which aims to uncover the distributional make-up of the pretraining data.
__label__machine_vision To address the scarcity of labeled 3D semantic data and enable natural language-driven scene manipulation, we incorporate a pre-trained 2D language-based segmentation model into a 3D-consistent semantic feature field.
__label__other However, challenges remain due to multi-agent scene uncertainty and heterogeneous interaction.
__label__safety_in_machine_learning Large language models (LLMs) have the potential to generate texts that pose risks of misuse, such as plagiarism, planting fake reviews on e-commerce platforms, or creating inflammatory false tweets.
__label__probabilistic_methods We examine these joint predictive distributions, which we call LLM Processes, over arbitrarily-many quantities in settings such as forecasting, multi-dimensional regression, black-box optimization, and image modeling.
__label__deep_learning_architectures Moreover, this task poses additional difficulties regarding the re-productivity of such editing.
__label__safety_in_machine_learning The trustworthiness of the filtering step may vary based on the topic of the response.
__label__diffusion_based_models However, designing such an approach specifically for facial parts swapping is challenged by a reasonable multiple reference feature fusion, which needs to be both efficient and effective.
__label__machine_vision The code is available at https://github.com/ZhangYushan3/DiffSF.
__label__reinforcement_learning Achieving the no-regret property for Reinforcement Learning (RL) problems in continuous state and action-space environments is one of the major open problems in the field.
__label__evaluation We apply our DARG framework to diverse reasoning tasks in four domains with 15 state-of-the-art LLMs.
__label__diffusion_based_models At each step, much subtler yet effective adversarial guidance is crafted using only the attacked model without any additional network, which gradually leads the end of diffusion process from the original image to a desired imperceptible adversarial example.
__label__speech_and_audio Moreover, we show that the representation-space of ELSA is structured, enabling swapping of direction of audio via vector arithmetic of two directional text embeddings.
__label__natural_language_processing We conduct extensive experiments on 10 challenging reasoning-intensive tasks, and achieve significant performance improvements over previous SOTA methods: 11\% on Game of 24, 20\% on Geometric Shapes and 51\% on Checkmate-in-One.
__label__evaluation The predominant *de facto* paradigm of testing ML models relies on either using only held-out data to compute aggregate evaluation metrics or by assessing the performance on different subgroups.
__label__natural_language_processing In this work, we identify that naive epistemic uncertainty estimation leads to the acquisition of redundant samples.
__label__safety_in_machine_learning To alleviate this problem, we propose \emph{ZeroMark} to conduct ownership verification without disclosing dataset-specified watermarks.
__label__other A common  paradigm is converting user behavior sequences into instruction data, and fine-tuning the LLM with parameter-efficient fine-tuning (PEFT) methods like Low-Rank Adaption (LoRA).
__label__reinforcement_learning Reinforcement Learning from Human Feedback (RLHF) has become the standard approach for aligning Large Language Models (LLMs) with human preferences, allowing LLMs to demonstrate remarkable abilities in various tasks.
__label__natural_language_processing In this study, we explore the positional information within and beyond the context window for deciphering the underlying mechanism of LLMs.
__label__machine_learning_for_physical_sciences When solving partial differential equations (PDEs), classical numerical methods often require fine mesh grids and small time stepping to meet stability, consistency, and convergence conditions, leading to high computational cost.
__label__other It then breaks these trajectories down into actionable subtasks, executes them sequentially, and refines the results to ensure optimal outcomes.
__label__deep_learning_architectures In this paper, we take a step forward to close the gap between the linear and Softmax attention with novel theoretical analyses, which demystify the core factors behind the performance deviations.
__label__generative_models By using up to 8 sampling steps, we obtain distilled models that outperform not only their one-step versions but also their original many-step teacher models, obtaining new state-of-the-art results on the Imagenet dataset.
__label__other We propose federated learning over connected modes (\textsc{Floco}), where clients are assigned local subregions in this simplex based on their gradient signals, and together learn the shared global solution simplex.
__label__machine_vision To validate RCDN, we create OPV2V-N, a new large-scale dataset with manual labelling under different camera failed scenarios.
__label__machine_vision Large vision-language models (LVLMs) have recently achieved rapid progress, sparking numerous studies to evaluate their multi-modal capabilities.
__label__learning_theory The prevailing theoretical framework for studying machine learning, namely probably approximately correct (PAC) learning, largely ignores time.
__label__safety_in_machine_learning Experimental results demonstrate that our approach improves test-time accuracy under distribution shifts while maintaining accuracy and calibration in their absence, outperforming leading entropy minimization methods across various scenarios.
__label__generative_models In this paper, we introduce FlowLLM, a novel generative model that combines large language models (LLMs) and Riemannian flow matching (RFM) to design novel crystalline materials.
__label__learning_theory Our paper provides theoretical insights into the phenomenon of preference alignment and jailbreaking from a statistical perspective.
__label__reinforcement_learning By integrating both regret and novelty as complementary objectives for curriculum design, CENIE facilitates effective exploration across the state-action space while progressively increasing curriculum complexity.
__label__online_learning Our work provides new insights into non-asymptotic analyses of controlling continuous-time systems.
__label__learning_theory Next, ignoring constants and focusing on behavior near zero, we identify *minimizability gaps* as the key differentiating factor in these bounds.
__label__machine_vision Besides 4D scene generation, DreamScene4D obtains accurate 2D persistent point track by projecting the inferred 3D trajectories to 2D.
__label__learning_theory This opens the doors to machine learning studies based on full on-line information processing that are based on the replacement of Backpropagation with the proposed spatiotemporal local algorithm.
__label__probabilistic_methods The variational perspective shows that the previous types of inference for planning are only adequate in environments with low stochasticity, and allows us to characterize each type by its own merits, disentangling the type of inference from the additional approximations that its practical use requires.
__label__safety_in_machine_learning Our approach involves adaptively picking labeled data to create a calibration set based on the stability of the selection rule.
__label__graph_neural_networks This paper introduces a new Subgraph GNN framework to address these issues.
__label__machine_vision On certain tasks such as ScienceQA and MMMU, we can even go down to only 2 visual tokens with performance drops of just 3\% and 6\% each.
__label__deep_learning_architectures This is achieved by using *low displacement rank matrices* (LDRMs), which hasn't been used in this context before.
__label__optimization One of the common objectives in the design and analysis of such algorithms is to attain (Pareto) optimal tradeoffs between the {\em consistency} of the algorithm, i.e., its performance assuming perfect predictions, and its {\em robustness}, i.e., the performance of the algorithm under adversarial predictions.
__label__safety_in_machine_learning We demonstrate our theoretical findings through experiments with super-resolution and inpainting algorithms.
__label__diffusion_based_models However, these models are limited by non-contextualized noise, which fails to fully consider the characteristics of Seq2Seq tasks.
__label__diffusion_based_models In latent diffusion models (LDMs), denoising diffusion process efficiently takes place on latent space whose dimension is lower than that of pixel space.
__label__machine_vision We also show AAPE is particularly helpful to handle non-canonical and OOD examples.
__label__machine_learning_for_other_sciences_and_fields Due to data heterogeneity, negative transfer may occur in the FL training process.
__label__deep_learning_architectures Implicit neural networks including deep equilibrium models have achieved superior task performance with better parameter efficiency in various applications.
__label__machine_learning_for_healthcare It achieves significant performance improvements over both hand-crafted and automated state-of-the-art methods, also maintains a feasible search cost at the same time.
__label__other Motivated by the empirical findings, we propose a novel LLM-based **M**ulti-**A**gent framework for **G**itHub **I**ssue re**S**olution, **MAGIS**, consisting of four agents customized for software evolution: Manager, Repository Custodian, Developer, and Quality Assurance Engineer agents.
__label__machine_learning_for_healthcare To address this problem, we propose a semi-supervised knowledge transfer framework named Dual label scArcity elimiNation with Cross-omic multi-samplE Mixup (DANCE).
__label__natural_language_processing Experiments show that our method outperforms all traditional algorithms on GLMs under mathematical reasoning scenarios and can be extended to more applications with high-density features in output spaces, such as multiple-choice questions.
__label__machine_vision In this work, we introduce StreamDSGN, the first real-time stereo-based 3D object detection framework designed for streaming perception.
__label__algorithmic_game_theory However, in terms of last-iterate convergence in two-player zero-sum games, an increasingly popular topic in this area, OGDA guarantees that the duality gap shrinks at a rate of $(1/\sqrt{T})$, while the best existing last-iterate convergence for OMWU depends on some game-dependent constant that could be arbitrarily large.
__label__machine_learning_for_other_sciences_and_fields This tokenizer transforms molecules into sequences of molecule tokens with causal dependency, encapsulating high-level molecular and textual information.
__label__deep_learning_architectures Increasing the throughput of the Transformer architecture, a foundational component used in numerous state-of-the-art models for vision and language tasks (e.g., GPT, LLaVa), is an important problem in machine learning.
__label__natural_language_processing This perspective enables us to detect *any* model with inflated performance, i.e., performance that does not generalize to rephrased samples, synthetic samples from the same distribution, or different benchmarks for the same task.
"__label__machine_vision Extensive experiments demonstrate that our approach produces high-quality reconstructions of both deformable and non-deformable objects in complex interactions, with improved
training efficiency compared to existing methods."
__label__privacy Experimental results demonstrate that our method significantly outperforms previous methods across various data distributions and dataset scales.
__label__machine_vision Then, we present a Label-Efficient Domain Adaptation approach to handle unfamiliar targets with significantly fewer amounts of labels (i.e., 1$\%$ and 5$\%)$, while preserving well-defined source knowledge for training efficiency.
__label__privacy Specifically, AdaSCP evaluates the importance of parameters with the gradients in dominant timesteps of the diffusion model.
__label__natural_language_processing Developing intelligent pediatric consultation systems offers promising prospects for improving diagnostic efficiency, especially in China, where healthcare resources are scarce.
__label__natural_language_processing In this work, we introduce Sketchpad, a framework that gives multimodal LMs a visual sketchpad and tools to draw on the sketchpad.
__label__machine_vision In order to apply this scheme for domains that lack program annotations, we develop a self-supervised learning approach that integrates this edit network into a bootstrapped finetuning loop along with a network that predicts entire programs in one-shot.
__label__natural_language_processing We show that RBRs are an effective training method, achieving an F1 score of 97.1, compared to a human-feedback baseline of 91.7, resulting in much higher safety-behavior accuracy through better balancing usefulness and safety.
__label__interpretability_and_explainability The cluttered effects of unobservable confounders in a model trained as such are decoupled from input features, thereby aligning the output of the model with the contribution of input features to the target feature in the data generation.
__label__machine_vision This study pioneers the application of Mamba to multi-class unsupervised anomaly detection, presenting MambaAD, which consists of a pre-trained encoder and a Mamba decoder featuring (Locality-Enhanced State Space) LSS modules at multi-scales.
__label__machine_learning_for_other_sciences_and_fields Leveraging the dataset's ultra-high resolution, which details proof states at the sub-type level, we propose a novel neural architecture targeted at faithfully representing dependently-typed programs on the basis of structural rather than nominal principles.
__label__machine_vision Next, SELMA adapts the T2I model to the new skills by learning multiple single-skill LoRA (low-rank adaptation) experts followed by expert merging.
__label__machine_learning_for_physical_sciences This work opens the possibility of using high-order differential operators in large-scale problems.
__label__machine_vision Moreover, exploiting the thin and long characteristics of lane lines, we propose a novel Curve-IoU loss to supervise the fit of lane lines.
__label__machine_vision Specifically, we propose  a logit distillation method called WKD-L based on discrete WD, which performs cross-category comparison of  probabilities and thus can explicitly leverage  rich interrelations among categories.
__label__optimization Our algorithm and data structure extend to higher dimensions as well as to $p$-Wasserstein problem for any $p \ge 1$.
__label__human-AI_interaction First, we collect hundreds of minutes of gaze data to demonstrate that we can mimic human gaze modalities using localized narratives.
__label__graph_neural_networks We introduce graphcodes, a novel multi-scale summary of the topological properties of a dataset that is based on the well-established theory of persistent homology.
__label__generative_models By employing an innovative perturbation technique, we achieve lower FDR and higher power.
__label__natural_language_processing We assessed MaskLLM using 2:4 sparsity on various LLMs, including LLaMA-2, Nemotron-4, and GPT-3, with sizes ranging from 843M to 15B parameters, and our empirical results show substantial improvements over state-of-the-art methods.
__label__probabilistic_methods We derive bounds that quantify the deviation of the regularised KKL to the original one, as well as concentration bounds.
__label__machine_learning_for_healthcare While the development of machine learning algorithms for this task has seen significant success, the prevailing approaches, which predominantly employ a discriminative formulation, frequently encounter the error accumulation issue and often fail to capture the extensive variety of plausible sequences.
__label__neuroscience_and_cognitive_science Despite its role in exploring and learning, the function and neural basis of this variability is still not well understood.
__label__machine_vision To tackle this problem, we simply introduce a 3D spatial feature descriptor and integrate it into the linear group RNN operators to enhance their spatial features rather than blindly increasing the number of scanning orders for voxel features.
__label__probabilistic_methods Empirically, Trifle achieves $7$ state-of-the-art scores and the highest average scores in $9$ Gym-MuJoCo benchmarks against strong baselines.
__label__machine_vision Intuitively, as deepfakes also contain additional informative forgery clues ($\textit{e.g.,}$ deep generative artifacts), excluding all deepfake data in training deepfake detectors seems counter-intuitive.
__label__machine_vision However, incomplete observations of interacting parties in the egocentric view introduce ambiguity between visual observations and interaction contents, impairing their efficacy.
__label__safety_in_machine_learning By demonstrating the feasibility of seamlessly integrating backdoors into transformer models, this paper fundamentally questions the efficacy of pre-deployment detection strategies.
__label__generative_models We prove that, if the data is curated according to a reward model, then the expected reward of the iterative retraining procedure is maximized.
__label__machine_vision Specifically, we introduce an iterative \textbf{Pro}mpt-\textbf{Ma}sk \textbf{C}ycle generation framework (ProMaC) with a prompt generator and a mask generator.
__label__reinforcement_learning To seamlessly integrate both modalities, we introduce a two-level hierarchical framework, RL-GPT, comprising a slow agent and a fast agent.
__label__deep_learning_architectures However, the current quantized/binarized training approaches are limited by: (1) significant performance loss due to arbitrary approximations of the latent weight gradient through its discretization/binarization function, and (2) training computational intensiveness due to the reliance on full-precision latent weights.
__label__machine_learning_for_other_sciences_and_fields Sequential recommender systems are designed to capture users' evolving interests over time.
__label__deep_learning_architectures The self-decoder efficiently encodes global key-value (KV) caches that are reused by the cross-decoder via cross-attention.
__label__optimization This paper introduces Language Hyper-Heuristics (LHHs), an emerging variant of Hyper-Heuristics that leverages LLMs for heuristic generation, featuring minimal manual intervention and open-ended heuristic spaces.
__label__graph_neural_networks While generally effective, these methods often rely on a fixed graph coarsening routine, leading to overly homogeneous cluster representations and loss of node-level information.
__label__other To deal with this shift, we theoretically show that the AUC on the test distribution can be expressed by using the positive and marginal training densities and the marginal test density.
__label__optimization Our study incorporates time-dependent bias and emphasizes the importance of controlling the bias of the gradient estimator.
__label__probabilistic_methods By allowing diversification on intra-block variance and inter-block correlation matrices, we effectively address the sensitivity issue of existing block sparse  learning methods to pre-defined block information, which enables adaptive block estimation while mitigating the risk of overfitting.
__label__deep_learning_architectures Particularly, we encode these scaling/sign-flipping symmetries by designing our corresponding equivariant and invariant layers.
__label__evaluation Such incoherence creates fragility: using a generative model to solve related but subtly different tasks can lead to failures.
__label__machine_learning_for_physical_sciences We present the Graph Diffusion Transformer (Graph DiT) for multi-conditional molecular generation.
__label__generative_models Our key insight in achieving this is that interaction semantics and dynamics can be decoupled.
__label__learning_theory We also provide an $\Omega(nk)$ lower bound, implying our result is tight up to an $\tilde{\mathrm{O}}(k)$ factor.
__label__other This holds true for both in-distribution (ID) and out-of-distribution (OOD) data.
__label__machine_vision Zero-shot (ZS) 3D anomaly detection is a crucial yet unexplored field that addresses scenarios where target 3D training samples are unavailable due to practical concerns like privacy protection.
__label__deep_learning_architectures The challenge of few-shot learning with tabular data stands as a crucial problem in both industry and academia, due to the high cost or even impossibility of annotating additional samples.
__label__machine_learning_for_healthcare We create a vector database consisting of local structure motif embeddings from a pre-trained protein structure encoder, which allows for efficient retrieval of similar local structure motifs during mutation effect prediction.
__label__neuroscience_and_cognitive_science We introduce a novel self-supervised modeling approach for population activity in which the model alternates between masking out and reconstructing neural activity across different time steps, neurons, and brain regions.
__label__learning_theory We expect this deterministic equivalent to hold broadly beyond our theoretical analysis, and we empirically validate its predictions on various real and synthetic datasets.
__label__reinforcement_learning Most importantly, we show that the pre-existing COMRL algorithms are essentially optimizing the same mutual information objective between the task variable $M$ and its latent representation $Z$ by implementing various approximate bounds.
__label__diffusion_based_models Extensive experiments demonstrate that our ReVideo has promising performance on several accurate video editing applications, i.e., (1) locally changing video content while keeping the motion constant, (2) keeping content unchanged and customizing new motion trajectories, (3) modifying both content and motion trajectories.
__label__learning_theory For weight and feature matrices of bounded operator norms that are infinitesimally free with respect to (normalized) trace functionals, we derive equivalence paths connecting different weighting matrices and ridge regularization levels.
__label__machine_learning_for_other_sciences_and_fields Next, we propose an efficient method to estimate the loss function of the training algorithm unbiasedly, enabling us to optimize the network parameters through gradient descent.
__label__neuroscience_and_cognitive_science Here, we develop methods to efficiently select which neurons to stimulate such that the resulting neural responses will best inform a dynamical model of the neural population activity.
__label__generative_models To address this issue, we leverage the scene graph, a powerful structured representation, for complex image generation.
__label__reinforcement_learning We introduce a new method called optimization-based AIL (OPT-AIL), which centers on performing online optimization for reward functions and optimism-regularized Bellman error minimization for Q-value functions.
__label__diffusion_based_models Recently, diffusion models have emerged as a powerful class of generative models.
__label__generative_models In this paper, we theoretically study the impact of data curation on iterated retraining of generative models and show that it can be seen as an implicit preference optimization mechanism.
__label__machine_vision In this way, we convert the modeling of MSC for each correspondence into a BFS traversal with pruning of a K-ary tree rooted at the superpoint, with its K nearest neighbors in the feature pyramid serving as child nodes.
__label__safety_in_machine_learning We propose a method that combines training in a higher precision than the target model, rounding after intermediate computation steps, and storing rounding decisions based on an adaptive thresholding procedure, to successfully control for  nondeterminism.
__label__reinforcement_learning This problem is further modelled as a two-step iterative optimization problem, where the first step is TD learning in the delay-free environment with a small state space, and the second step is behaviour cloning which can be addressed much more efficiently than TD learning.
__label__graph_neural_networks The code is available at https://github.com/changyi7231/IVR.
__label__graph_neural_networks Specifically, we utilize GNNs for parameter perturbation while employing MLPs to minimize the perturbed loss so that we can find a flat minimum with good generalization more efficiently.
__label__optimization_for_deep_networks We utilize Householder transformations to construct orthogonal matrices that efficiently mimic the unitary matrices, requiring only a vector.
__label__neuroscience_and_cognitive_science Although their asymptotic behaviors to maintain memory are categorically distinct, their finite-time behaviors are similar.
__label__reinforcement_learning MBTL models the generalization performance in two parts: 1) the performance set point, modeled using Gaussian processes, and 2) performance loss (generalization gap), modeled as a linear function of contextual similarity.
__label__neuroscience_and_cognitive_science This also enables the network robustly to generalize a novel, out-of-distribution dataset.
__label__speech_and_audio Finally, the speech waveforms are produced using a HiFi-GAN model.
__label__infrastructure The increased adoption of Internet of Things (IoT) devices has led to the generation of large data streams with applications in healthcare, sustainability, and robotics.
"__label__interpretability_and_explainability We
introduce a novel objective based on the graph information bottleneck theory (GIB)
and a new mix-up framework, which can support various GNNs and explainers
in a model-agnostic manner."
__label__causal_inference First, we generalize the existing results of identifiability with the score to additive noise models with minimal requirements on the causal mechanisms.
__label__learning_theory We consider a planted model with two datasets $X,Y$ that consist of $n$ datapoints in $\mathbb{R}^d$, where $Y$ is a noisy version of $X$, up to an orthogonal transformation and a relabeling of the data points.
__label__optimization We also establish complexity results when the upper- and lower-level objectives are general nonsmooth functions.
__label__reinforcement_learning We evaluate the effectiveness of our algorithm AgA through benchmark environments for testing mixed-motive collaboration with small-scale agents such as the two-player public good game and the sequential social dilemma games, Cleanup and Harvest, as well as our self-developed large-scale environment in the game StarCraft II.
__label__probabilistic_methods The metric backbone of a weighted graph is the union of all-pairs shortest paths.
__label__generative_models Certain types of motion that accompany speech can provide this illustrative mode of communication.
__label__generative_models Our objective has a simple form&mdash;it is a mixture of classical masked language modeling losses&mdash;and can be used to train encoder-only language models that admit efficient samplers, including ones that can generate arbitrary lengths of text semi-autoregressively like a traditional language model.
__label__generative_models Specifically, we first introduce pre-trained text semantics as prior knowledge, then design two novel alignment modules (\emph{i.e.
"__label__machine_learning_for_healthcare Extensive experiments demonstrate the effectiveness and superiority of the proposed method, 
with the discovered subgraphs and encoding functions highlighting the model’s adaptability."
__label__machine_learning_for_other_sciences_and_fields Increasing the training set with additional synthetic data, similar to data augmentation in images, is commonly believed to improve downstream tabular classification performance.
__label__machine_vision The recent advances in 3D Gaussian Splatting (3DGS) show promising results on the novel view synthesis (NVS) task.
__label__learning_theory This contrasts dramatically with negative results due to Gold and Angluin in a well-studied model of language learning where the goal is to identify an unknown language from samples; the difference between these results suggests that identifying a language is a fundamentally different problem than generating from it.
__label__diffusion_based_models In this work, we present a new form of editing, termed imitative editing, to help users exercise their creativity more conveniently.
__label__fairness Our approach involves the integration of the pairwise graph disparity error and a tailored loss function into a nonsmooth multi-objective optimization problem, striving to achieve fairness across different sensitive groups while maintaining the effectiveness of the GMs.
"__label__diffusion_based_models We further introduce a theoretically grounded ""denoising schedule"" to improve sampling and learning efficiency."
__label__optimization For this purpose, we propose a data structure that allows us to efficiently sample from the tensor with time complexity logarithmic in the product of the tensor dimensions.
__label__reinforcement_learning We develop a continuous-time stochastic dynamics framework to quantify the impact of these errors, by examining the regularization effects for both cases with zero-drift representation errors and  non-zero-drift representation  errors.
__label__probabilistic_methods State-space graphical models and the variational autoencoder framework provide a principled apparatus for learning dynamical systems from data.
__label__evaluation However, the general abilities of post-edited language models remain unexplored.
__label__diffusion_based_models DMPlug addresses the issues of manifold feasibility and measurement feasibility in a principled manner, and also shows great potential for being robust to unknown types and levels of noise.
__label__diffusion_based_models Our experiments show that our RGBA diffusion model is capable of generating diverse and high quality instances with precise control over object attributes.
__label__machine_vision To address above problems, we propose RCDN, a Robust Camera-insensitivity collaborative perception with a novel Dynamic feature-based 3D Neural modeling mechanism.
__label__machine_learning_for_healthcare We reassess and recalibrate performance expectations from classical and DLIR methods under access to label supervision, training time, and its generalization capabilities under minor domain shifts.
__label__machine_vision Previous works perform a two-stage paradigm, first conducting language-agnostic instance segmentation then matching with given text query.
__label__natural_language_processing Text embeddings are essential for tasks such as document retrieval, clustering, and semantic similarity assessment.
__label__learning_theory Stochastic dominance is an important concept in probability theory, econometrics and social choice theory for robustly modeling agents' preferences between random outcomes.
__label__machine_vision Extensive experiments demonstrate the efficacy of our framework.
__label__machine_learning_for_healthcare Conditional 3D structure-based drug design (3D-SBDD) models, which take into account complex three-dimensional interactions and molecular geometries, are particularly promising.
__label__deep_learning_architectures By integrating spectral geometry principles into neural modeling, we show that this problem can be better addressed in the functional domain, mitigating complexity, while enhancing interpretability and performances on downstream tasks.
__label__reinforcement_learning In this paper, we study GAIL’s optimization from a control-theoretic perspective.
__label__graph_neural_networks We theoretically demonstrate that PSNR can alleviate the drawbacks of previous residual methods.
__label__generative_models The optimal training configurations of large language models (LLMs) with respect to model sizes and compute budgets have been extensively studied.
__label__machine_vision We propose UAD, a method for vision-based end-to-end autonomous driving (E2EAD), achieving the best open-loop evaluation performance in nuScenes, meanwhile showing robust closed-loop driving quality in CARLA.
__label__machine_vision Additionally, a hash-based representation is proposed for memory-efficient spherical frustum storage.
__label__optimization We propose adaptive, line-search-free second-order methods with optimal rate of convergence for solving convex-concave min-max problems.
__label__bandits (3) We introduce a procedure relating standard regret with drifted regret that does not rely on boundedness of delays.
__label__natural_language_processing Our approach is based on the observation that KV cache states exhibit high similarity between the adjacent layers in the middle-to-deep portion of LLMs.
__label__machine_learning_for_physical_sciences Our code is available at https://github.com/Siddharth-Rout/deepADRnet.
__label__machine_vision The proposed size targets can be seen as an extension of the standard class tags, which correspond to non-zero size targets in each image.
__label__machine_vision In this paper, we systematically explore anchoring as a general protocol for training vision models, providing fundamental insights into its training and inference processes and their implications for generalization and safety.
__label__optimization However, some important problems -- such as risk minimization for infinite width two-layer neural networks, or sparse deconvolution -- are originally defined over the set of signed, rather than probability, measures.
__label__reinforcement_learning Regarding efficiency, the online testing of DM-H in the long-term task is 28$\times$ times faster than the transformer-based baselines.
__label__generative_models Furthermore, we introduce a consistency alignment loss to encourage coherent subgoal images and align with their corresponding instructions, mitigating potential hallucinations and semantic conflicts between the two planning manners.
__label__machine_learning_for_other_sciences_and_fields Key problems in genomics intrinsically involve multiple modalities, but it remains unclear how to adapt general-purpose sequence models to those cases.
__label__neuroscience_and_cognitive_science To the end, we propose NeuroClips, an innovative framework to decode high-fidelity and smooth video from fMRI.
__label__neuroscience_and_cognitive_science Our experiments demonstrate that NeuroBOLT effectively reconstructs unseen resting-state fMRI signals from primary sensory, high-level cognitive areas, and deep subcortical brain regions, achieving state-of-the-art accuracy with the potential to generalize across varying conditions and sites, which significantly advances the integration of these two modalities.
__label__diffusion_based_models Despite these advances, text-driven image editing, a key area in computer graphics, faces unique challenges.
__label__machine_vision Targeting at expanding receptive filed, spatial-shift operator tailored for efficient spatial communication and  has achieved remarkable advances in high-level image classification tasks, like $S^2$-MLP and ShiftVit.
__label__machine_vision Moreover, overfitting often happens, where the trained models can only generate images based on the layout data from the validation set of the same dataset.
__label__other Our code is available at: \url{https://github.com/LINs-lab/ReLA}.
__label__diffusion_based_models The form of our inference process is consistent with the DDPM.
__label__generative_models However, generating diverse embodied environments with realistic detail and considerable complexity remains a significant challenge.
__label__interpretability_and_explainability Our investigation suggests that incorporating model uncertainty can help EDL methods faithfully quantify uncertainties and further improve performance on representative downstream tasks, albeit at the cost of additional computational complexity.
__label__generative_models Comprehensive experiments demonstrate the superior generation power of Era3D- it can reconstruct high-quality and detailed 3D meshes from diverse single-view input images, significantly outperforming baseline multiview diffusion methods.
__label__probabilistic_methods Our pretrained model is available online.
__label__natural_language_processing Experimental results demonstrate the effectiveness of our framework, outperforming GPT-4o by $\sim$30\% in accuracy.
__label__interpretability_and_explainability We focus on the input saliency maps, as the input gradient field is decisive to the models' mathematical essence.
__label__safety_in_machine_learning Our work offers a scalable, efficient solution for OOD detection, setting a new state-of-the-art in this area.
__label__deep_learning_architectures Pre-trained language models have been proven to possess strong base capabilities, which not only excel in in-distribution language modeling but also show powerful abilities in out-of-distribution language modeling, transfer learning and few-shot learning.
__label__diffusion_based_models This failure stems from the models' difficulty in learning robust color predictions from limited matting datasets.
__label__deep_learning_architectures Furthermore, visual-only experimental results also indicate that our approach can tackle challenging scenes where modality information is missing.
__label__causal_inference This problem has been addressed when all the variables in the system are observable.
__label__neuroscience_and_cognitive_science By examining the RNN’s latent space, we found that: 1) Multi-task RNNs represent both task-relevant and irrelevant information simultaneously while performing tasks; 2) While the latent subspaces used to maintain specific object properties in vanilla RNNs are largely shared across tasks, they are highly task-specific in gated RNNs such as GRU and LSTM; 3) Surprisingly, RNNs embed objects in new representational spaces in which individual object features are less orthogonalized relative to the perceptual space; 4) Interestingly, the transformation of WM encodings (i.e., embedding of visual inputs in the RNN latent space) into memory was shared across stimuli, yet the transformations governing the retention of a memory in the face of incoming distractor stimuli were distinct across time.
__label__evaluation Our localization methods are independent of the downstream task, do not require any label information, and can be performed in a forward pass.
__label__machine_vision This paper studies the problem of estimating physical properties (system identification) through visual observations.
__label__natural_language_processing To this end, we develop a set of practical and effective **compression best practices** for LLMs that combine depth, width, attention and MLP pruning with knowledge distillation-based retraining; we arrive at these best practices through a detailed empirical exploration of pruning strategies for each axis, methods to combine axes, distillation strategies, and search techniques for arriving at optimal compressed architectures.
__label__robotics Our experiments showcase the promising capability of building embodied agents that can adapt to diverse multi-modal in a unified framework.
__label__graph_neural_networks Recently, data pruning, distillation, and coreset selection have been developed to streamline data volume by \textit{retaining}, \textit{synthesizing}, or \textit{selecting} a small yet informative subset from the full set.
__label__graph_neural_networks (2) Building on our geometric insights, we augment the message-passing process of graph convolutional layers (GCLs) with a learnable term to modulate the smoothness of node features with computational efficiency.
__label__learning_theory In contrast, model learned using pooled SGD over all data would simultaneously learn both the invariant and spurious signals.
__label__causal_inference Standard imitation methods do not generally apply when the learner and the expert's sensory capabilities mismatch and demonstrations are contaminated with unobserved confounding bias.
__label__deep_learning_architectures Our Elliptical Attention provides two benefits: 1) reducing representation collapse and 2) enhancing the model's robustness as the Elliptical Attention pays more attention to contextually relevant information, rather than focusing on some small subset of informative features.
__label__machine_learning_for_other_sciences_and_fields As a result, our machine learning procedure for model checking is entirely unsupervised, formally sound, and practically effective.
__label__machine_vision Project page is available at https://sadilkhan.github.io/text2cad-project/.
__label__causal_inference We confirm our findings through empirical simulations and real-world experiments and demonstrate how the test error of existing robustness methods grows increasingly suboptimal as the proportion of previously unseen test directions increases.
__label__reinforcement_learning We propose a method of offline reinforcement learning (RL) featuring the performance guarantee without any assumptions on the data support.
__label__optimization_for_deep_networks Here, key-query, as well as value-projection parameter matrices, are multiplied directly with each other: $W_K^TW_Q$ and $PW_V$.
__label__machine_vision Traditional 3D shape reconstruction techniques from multi-view images, such as structure from motion and multi-view stereo, primarily focus on opaque surfaces.
__label__evaluation Then, RaVL mitigates the identified spurious correlation with a novel region-aware loss function that enables the VLM to focus on relevant regions and ignore spurious relationships during fine-tuning.
__label__natural_language_processing Additionally, we propose incorporating the data construction process into the fine-tuning data as code comprehension tasks, including instruction evolution, data filtering, and code review.
__label__generative_models To do so, we formulate data selection for task-specific finetuning as an optimization problem with a distribution alignment loss based on optimal transport to capture the discrepancy between the selected data and the target distribution.
__label__machine_learning_for_healthcare The project code is available at https://github.com/mrirecon/aid.
__label__diffusion_based_models Addressing these imperfections, we present Face2QR—a novel pipeline specifically designed for generating personalized QR codes that harmoniously blend aesthetics, face identity, and scannability.
__label__reinforcement_learning In this paper, we propose a new method for constrained allocation tasks based on an autoregressive process to sequentially sample allocations for each entity.
__label__safety_in_machine_learning Nonetheless, CLIP has been observed to be susceptible to adversarial examples.
"__label__evaluation In Natural Language Processing (NLP), the Elo rating system, originally designed for ranking players in dynamic games such as chess, is increasingly being used to evaluate Large Language Models (LLMs) through ""A vs B"" paired comparisons."
__label__diffusion_based_models To bridge this gap, we propose two metrics to assess L2I performance in open-vocabulary scenarios.
__label__deep_learning_architectures Extensive experiments across various models, datasets, and scale factors demonstrate that our method achieves comparable or superior performance to existing approaches with about 34\% reduction in computational cost.
__label__human-AI_interaction Extensive experiments demonstrate that IDC could remarkably improve the performance of various pre-trained clustering models, at the expense of low user interaction costs.
__label__optimization This paper studies decentralized bilevel optimization, in which multiple agents collaborate to solve problems involving nested optimization structures with neighborhood communications.
__label__machine_learning_for_other_sciences_and_fields For the second issue, PepGLAD explores a receptor-specific affine transformation to convert the 3D coordinates into a shared standard space, enabling better generalization ability across different binding shapes.
__label__machine_vision In this paper, to solve the skeleton action generalization problem, we present a recover-and-resample augmentation framework based on a novel complete action prior.
__label__probabilistic_methods Symmetries have proven useful in machine learning models, improving generalisation and overall performance.
__label__robotics Given a few seed task descriptions and the robot APIs, Robo-Instruct is capable of generating a training dataset using only a small open-weight model.
__label__diffusion_based_models First, we develop Abstract 3D Models to represent and manipulate articulated objects efficiently and arbitrarily.
__label__optimization_for_deep_networks During the course of training, models pass through multiple phases: after an initial stage in which predictions are uniform, they learn to sub-optimally predict using in-context single-token statistics (unigrams); then, there is a rapid phase transition to the correct in-context bigram solution.
__label__learning_theory While existing results along these lines focus on sparsity or manifold assumptions, we introduce a new graphical quantity called ``graph resilience'' and show that it dictates the optimal sample complexity.
__label__reinforcement_learning In this paper, we propose an automatic swift sampler search algorithm, SS, to explore automatically learning effective samplers efficiently.
__label__machine_vision In particular, SpelsNet is composed of two main components; (1) a supervised 3D spatial segmentation head that outputs B-Rep element types and memberships; (2) a graph-based head that leverages the proposed topological supervision.
__label__diffusion_based_models Experimental results demonstrate that PFDiff exhibits flexible applicability across various pre-trained DPMs, particularly excelling in conditional DPMs and surpassing previous state-of-the-art training-free methods.
__label__safety_in_machine_learning Recent advancements in generative AI suggest the potential for large-scale interaction between autonomous agents and humans across platforms such as the internet.
__label__deep_learning_architectures Yet with the remarkable progress, current solutions concern little on the holistic function features--both global and local information-- during the process of solving PDEs.
__label__safety_in_machine_learning Our code will be made publicly available.
__label__deep_learning_architectures However, previous adaptive depth networks do not provide general principles and a formal explanation on why and which layers can be skipped, and, hence, their approaches are hard to be generalized and require long and complex training steps.
__label__probabilistic_methods When solving ill-posed inverse problems, one often desires to explore the space of potential solutions rather than be presented with a single plausible reconstruction.
__label__learning_theory Since using i.i.d.
__label__probabilistic_methods This finding has important theoretical and practical implications, as it demonstrates that the ability to perform well on complex tasks like image classification is not unique to neural networks.
"__label__natural_language_processing Moreover, we experiment with MOD on combining three fully-finetuned 
LMs of different model sizes, each aimed at different objectives such as safety, coding, and general user preference."
__label__robotics On 30 real-world manipulation tasks, given an average of just 17 demonstrations per task, BAKU achieves a 91% success rate.
__label__probabilistic_methods Second, a neural recognition model that processes subsets of the simulated observations, and that is trained to output the initial condition and rate matrix of the target MJP in a supervised way.
__label__deep_learning_architectures The key feature of ELCD is a parametrization of the extended linearization of the nonlinear vector field.
__label__diffusion_based_models This makes it challenging to achieve precise and disentangled image generation without additional training, especially in an unsupervised way.
__label__fairness We propose MC-Pseudolabel, a post-processing algorithm to achieve both extended multicalibration and out-of-distribution generalization.
__label__machine_vision Specially, rigid editing leverages geometric priors from diffusion models to achieve precise viewpoint transformations under zero-shot conditions, while non-rigid editing employs adversarial training and self-attention mechanisms for complex, topologically consistent modifications.
__label__diffusion_based_models To alleviate this problem, we employ the flow matching framework for simulation-free training of NODEs, which directly regresses the parameterized dynamics function to a predefined target velocity field.
__label__active_learning Without the need of ground-truth labels, our method can successfully identify pseudo-class centers, apply a novel denoising technique, and iteratively select boundary samples  with designed evaluation metric.
__label__machine_vision Most incremental learners excessively prioritize object classes while neglecting various kinds of states (e.g.
__label__evaluation Dimension reduction (DR) is an important and widely studied technique in exploratory data analysis.
__label__diffusion_based_models Specifically, we show that such speedup achieves almost-linear time latent DiTs training by casting the DiTs gradient as a series of chained low-rank approximations with bounded error.
__label__machine_vision Our proposed architecture is designed with two key principles in mind: (1) it takes into account the inherent symmetries present in the input point tracks data, and (2) it assumes that the movement patterns can be effectively represented using a low-rank approximation.
__label__optimization_for_deep_networks The convergence of this typical interactive teaching is achieved by continuously optimizing a variational lower bound on the log marginal likelihood.
__label__diffusion_based_models To address this issue, we develop constrained diffusion models by imposing diffusion constraints based on desired distributions that are informed by requirements.
__label__deep_learning_architectures Results illustrate that compressing LlaMa-$2$ $7$B/$13$B/$70$B and LlaMa-$3$ $8$B models obtained using $\rm CALDERA$ outperforms existing post-training LLM compression techniques in the regime of less than $2.5$ bits per parameter.
__label__generative_models The perceptual weights generator is designed to convert visual features to perceptual weights with low-rank property, exhibiting a form similar to LoRA.
__label__safety_in_machine_learning Then, to model high-order dependencies among these components, we propose a hypergraph-based relational reasoning module that models the intricate relations of nodes (slots) with structural constraints.
__label__learning_theory The resulting stability guarantee places no distributional assumptions on the data, does not depend on the number of classes or dimensionality of the covariates, and holds for any base classifier.
"__label__diffusion_based_models Code
  is available at this repository: https://github.com/thuml/Diffusion-Tuning."
__label__speech_and_audio Furthermore, we propose two separated encoders to preserve the speaker’s voice characteristics and isochrony from the source speech during the translation process, making it highly suitable for scenarios such as video dubbing.
__label__optimization_for_deep_networks This paper addresses a crucial aspect that has largely been overlooked in HPO: the impact of uncertainty in ML model training.
__label__graph_neural_networks Moreover, we introduce new synthetic benchmark graph datasets to show how to integrate expert knowledge into RuleGNNs making them more powerful than ordinary graph neural networks.
__label__safety_in_machine_learning Experimental results show the superior certified robustness of these Noised Diffusion Classifiers (NDCs).
__label__machine_vision In this paper, we propose eFreeSplat, an efficient feed-forward 3DGS-based model for generalizable novel view synthesis that operates independently of epipolar line constraints.
__label__generative_models In this work, we show that, on text generation tasks, noisy annotations significantly hurt the performance of in-context learning.
__label__learning_theory Besides, a theoretical guarantee is given for the convergence by \textit{regret} argument.
__label__optimization_for_deep_networks To the best of our knowledge, we are the first to obtain generalization bound via minima stability in the non-interpolation case and the first to show ReLU NNs without regularization can achieve near-optimal rates in nonparametric regression.
__label__generative_models We demonstrate that RNNs, LSTMs, GRUs, Linear RNNs, and linear gated architectures such as Mamba and Hawk/Griffin can also serve be universal in-context approximators.
__label__causal_inference QWO has a speed-up of $O(n^2)$ ($n$ is the number of variables) compared to the state-of-the-art BIC-based method, making it highly scalable.
__label__machine_vision Image Restoration (IR), a classic low-level vision task, has witnessed significant advancements through deep models that effectively model global information.
__label__machine_vision Extensive experiments on the ScanNet dataset demonstrate state-of-the-art performance in 3D planar reconstruction, underscoring the great potential of AlphaTablets as a generic 3D plane representation for various applications.
__label__machine_vision While images typically require 2D non-causal modeling, texts utilize 1D causal modeling.
__label__machine_vision In contrast, we propose a sample-wise ensembling technique that can simultaneously attain the best ID and OOD accuracy without the trade-offs.
__label__learning_theory Finally, we show that any pure DP learner can be transformed in a black-box manner to a replicable learner, with time complexity polynomial in the confidence and accuracy parameters, but exponential in the representation dimension of the underlying hypothesis class.
__label__machine_vision We conclude with out-of-the-box applications of superior uncertainty estimation abilities of CLAP including novel data detection and exemplar selection within the existing CL setups.
__label__probabilistic_methods Gradient boosting is a sequential ensemble method that fits a new weaker learner to pseudo residuals at each iteration.
"__label__natural_language_processing The code and data are publicly available at
https://github.com/JianGuanTHU/AMOR."
__label__learning_theory Experimental results on three datasets demonstrate the method's effectiveness in mitigating error accumulation within the CTTA framework.
__label__deep_learning_architectures To allow for contraction with respect to more general metrics in the data space, we train diffeomorphisms between the data space and a latent space and enforce contractivity in the latent space, which ensures global contractivity in the data space.
"__label__graph_neural_networks The most widely used homophily metrics, such as edge or node homophily, quantify such ""similarity"" as label consistency across the graph topology."
__label__neuroscience_and_cognitive_science Current methods for visual grounding of dynamics either use pure neural-network-based simulators (black box), which may violate physical laws, or traditional physical simulators (white box), which rely on expert-defined equations that may not fully capture actual dynamics.
__label__speech_and_audio Unsupervised automatic speech recognition (ASR) aims to learn the mapping between the speech signal and its corresponding textual transcription without the supervision of paired speech-text data.
__label__reinforcement_learning The empirical results demonstrate that our proposed CTR yields significant performance improvement over the state-of-the-art methods.
__label__machine_vision Tracking points in video frames is essential for understanding video content.
__label__reinforcement_learning Previous return-conditioned diffusion models manifest comparable performance but rely on well-defined reward functions, which requires amounts of human efforts and faces challenges in multi-task settings.
__label__optimization_for_deep_networks Recognizing that **$\epsilon$-softmax**-enhanced losses may slightly reduce fitting ability on clean datasets, we further incorporate them with one symmetric loss, thereby achieving a better trade-off between robustness and effective learning.
__label__reinforcement_learning We implement CTR on top of QMIX and evaluate its performance in various cooperative multi-agent tasks.
__label__machine_vision One can exclude pixels using an estimate of perceptual groupings, such as superpixels, but the naive use of superpixels can be theoretically and empirically worse than standard attention.
__label__learning_theory Inspired by the variance-bias tradeoff in high dimensions from the theory, we introduce Sketchy Moment Matching (SkMM), a scalable data selection scheme with two stages.
__label__learning_theory Firstly, we prove that while a moderate dimension suffices in order for a complex SSM to express all mappings of a real SSM, a much higher dimension is needed for a real SSM to express mappings of a complex SSM.
__label__generative_models Finally, our method can identify an optimal SubNet through few-step gradient optimization and a simple post-processing procedure.
__label__generative_models Recent advancements have utilized the diffusion model to improve gesture synthesis.
__label__generative_models This is achieved by integrating surface-based volumetric rendering within a structured tetrahedral grid while preserving the desired ability of precise mesh extraction, and a tile-based differentiable tetrahedron rasterizer.
__label__machine_vision Existing studies have predominantly concentrated on the dimensional collapse of representations, neglecting whether this can sufficiently prevent the dimensional collapse of the weight matrices and hidden features.
__label__natural_language_processing It employs four LLM-based agents: strategy generator, executor, optimizer, and evaluator, working together to generate, evaluate, and select promising strategies for a given task.
__label__natural_language_processing Further analysis substantiates our hypothesis that our improvement can be attributed to reduced overfitting to instruction tuning datasets.
__label__learning_theory In particular, we consider the use of stochastic gradient descent (SGD) on a linear model initialized with pretrained weights and using a small training data set from the target distribution.
__label__generative_models Extensive experiments show that our TeT-Splatting strikes a superior tradeoff among convergence speed, render efficiency, and mesh quality as compared to previous alternatives under varying 3D generation settings.
__label__other Recent work [Zeng et al., 2022] proposes using this structured information explicitly, but the use of Euclidean distance may distort the underlying semantic context [Chen et al., 2013].
__label__neuroscience_and_cognitive_science These are composed into a single vector representing position by a similarity-preserving, conjunctive vector-binding operation.
__label__diffusion_based_models We conduct additional analysis to substantiate the theoretical stability and global convergence property of the proposed optimal sampler.
__label__machine_vision To enhance the transferability of UAPs in FSL, we propose a unifying attacking framework addressing these two shifts.
__label__natural_language_processing To make model parameters directly interpretable, we introduce a family of statistical models---including clustering, time series, and classification models---parameterized by *natural language predicates*.
__label__natural_language_processing We will release a dataset of over 3,000 carefully crafted prompts to facilitate evaluation research of LLMs.
__label__machine_vision In certain scenarios, we find that training with LOFF-TA yields better results than directly fine-tuning the foundation model.
__label__other Through hardware-aware algorithmic designs, NoMAD-Attention achieves the computation of attention scores using repeated fast accesses to SIMD registers.
__label__machine_vision Therefore, in this work, we introduce the problem of **U**niversal **U**nsupervised **C**ross-**D**omain **R**etrieval (U^2CDR) for the first time and design a two-stage semantic feature learning framework to address it.
__label__other This paper investigates an under-explored challenge in large language models (LLMs): chain-of-thought prompting with noisy rationales, which include irrelevant or inaccurate reasoning thoughts within examples used for in-context learning.
__label__machine_vision This paper builds on these properties of task vectors and aims to answer (1) whether components of task vectors, particularly parameter blocks, exhibit similar characteristics, and (2) how such blocks can be used to enhance knowledge composition and transfer.
__label__infrastructure However, the prohibitive costs of tensor communication render it a theoretically plausible yet practically inefficient solution.
__label__other FedLPA aggregates local models to obtain a more accurate global model without requiring extra auxiliary datasets or exposing any private label information, e.g., label distributions.
__label__probabilistic_methods Our findings reveal that our method performs comparably to a baseline that hierarchically clusters samples from a diffusion-based posterior sampler, yet achieves this with orders of magnitude greater speed.
__label__fairness Our results show more nuanced interactions of modern finetuned models with group robustness than was previously known.
__label__optimization Additionally, under a novel empirical evaluation framework, the proposed iEF method shows consistently better approximation quality to exact Natural Gradient updates than both the EF and the more expensive sampled Fisher methods, meanwhile demonstrating the superior property of being robust to the choice of damping across tasks and training stages.
__label__machine_vision \emph{The question is how to align the PV features with the generative models to facilitate the map estimation}.
__label__graph_neural_networks Polynomial-based learnable spectral graph neural networks (GNNs) utilize polynomial to approximate graph convolutions and have achieved impressive performance on graphs.
__label__machine_vision However, existing generalizable 3D Gaussian Splatting methods are largely confined to narrow-range interpolation between stereo images due to their heavy backbones, thus lacking the ability to accurately localize 3D Gaussian and support free-view synthesis across wide view range.
__label__interpretability_and_explainability Furthermore, the model inherits enhanced interpretability and generalizability from MLLMs.
__label__reinforcement_learning World models empower model-based agents to interactively explore, reason, and plan within imagined environments for real-world decision-making.
__label__probabilistic_methods We also propose a novel exploration strategy for off-policy methods, based on local search in the target space with the use of a replay buffer, and show that it improves the quality of samples on a variety of target distributions.
__label__other Extensive experiments conducted on a wide range of time series datasets and five common applications demonstrate the state-of-the-art performance of ACON.
__label__machine_vision Edge-based guidance with edge-guided gradient loss and edge-based fusion loss serves as the optimization objective equivalent to Poisson fusion.
__label__infrastructure We assess the performance of FlashMask in a variety of masking scenarios, including causal and customized attention masks, demonstrating its versatility and robustness across a wide range of attention patterns and models.
__label__natural_language_processing \name can also use specialist vision models during the sketching process (e.g., draw bounding boxes with object detection models, draw masks with segmentation models), to further enhance visual perception and reasoning.
__label__safety_in_machine_learning Recently, without poisoned data, unlearning models with clean data and then learning a pruning mask have contributed to backdoor defense.
__label__machine_vision We provide a  closed form solution of the procedure and adjust it for robust stochastic training while computing everything efficiently.
__label__diffusion_based_models Our approach introduces Multi-stage Hairstyle Blend (MHB), effectively separating control of hair color and hairstyle in diffusion latent space.
__label__optimization_for_deep_networks Fine-tuning pre-trained models for downstream tasks is a widely adopted technique known for its adaptability and reliability across various domains.
__label__optimization_for_deep_networks Using these insights, we show how $\eta_{\text{init}}$ can be properly chosen by utilizing the loss catapult mechanism, which saves on the number of warmup steps, in some cases completely eliminating the need for warmup.
__label__diffusion_based_models We prove that this can obtain a quadratic speed up of LMC under very weak assumptions.
__label__other Moreover, AlphaRec introduces a new text-based CF paradigm with several desirable advantages: being easy to implement, lightweight, rapid convergence, superior zero-shot recommendation abilities in new domains, and being aware of user intention.
__label__reinforcement_learning The algorithm relies on a carefully crafted off-policy evaluation procedure to evaluate the policy using historical data, which informs policy updates through policy gradients and conserves samples.
__label__optimization_for_deep_networks Specifically, we show that MicroAdam can be implemented efficiently on GPUs: on both million-scale (BERT) and billion-scale (LLaMA) models, MicroAdam provides practical convergence competitive to that of the uncompressed Adam baseline, with  lower memory usage and similar running time.
__label__diffusion_based_models Crucially, we introduce text-motion and video-motion supervision to improve the model's understanding and generation of motion.
__label__generative_models To exploit the complementary nature of image and video data, we further propose a progressive training strategy, where OmniTokenizer is first trained on image data on a fixed resolution to develop the spatial encoding capacity and then jointly trained on image and video data on multiple resolutions to learn the temporal dynamics.
__label__deep_learning_architectures Neural ODEs (NODEs) are continuous-time neural networks (NNs) that can process data without the limitation of time intervals.
__label__deep_learning_architectures This explicitly represents the naturally occurring pyramid-like properties in time series, where the top level is the original time series and lower levels consist of periodic components with gradually shorter periods, which we call the periodic pyramid.
__label__deep_learning_architectures Our code is available at https://github.com/intell-sci-comput/OPDF.
__label__natural_language_processing In this study, we propose \underline{\textbf{L}}earnable \underline{\textbf{I}}n-Context \underline{\textbf{Ve}}ctor (LIVE) to distill essential task information from demonstrations, improving ICL performance in LMMs.
__label__other We present an optimal method for encoding cluster assignments of arbitrary data sets.
__label__machine_learning_for_other_sciences_and_fields In general, traditional methods based on data-matching are hindered by the impracticality of storing adequate visual records of global landmarks.
__label__machine_learning_for_physical_sciences In order to improve interpretrability and provide insight into the causality of a chemical reaction, we train various machine learning frameworks on the PMechDB dataset.
__label__interpretability_and_explainability In SCBMs, a single-concept intervention affects all correlated concepts, thereby improving intervention effectiveness.
__label__safety_in_machine_learning Recent work has shown it is possible to construct adversarial examples that cause aligned language models to emit harmful strings or perform harmful behavior.
__label__learning_theory We prove that the risk of prospective ERM converges to the Bayes risk under certain assumptions on the stochastic process  generating the data.
__label__machine_vision To tackle this, we present Hierarchical Sinkhorn Tree (HST), a pruned tree structure designed to hierarchically measure the local consistency of each coarse correspondence across multiple feature scales, thereby filtering out the local dissimilar ones.
__label__diffusion_based_models Moreover, their capabilities to deal with noisy IPs with unknown types and levels of measurement noise are unknown.
__label__safety_in_machine_learning LLM unlearning is designed to reduce the impact of undesirable data influences and associated model capabilities without diminishing the utility of the model if unrelated to the information being forgotten.
__label__reinforcement_learning Theoretically, we prove that OPT-AIL achieves polynomial expert sample complexity and interaction complexity for learning near-expert policies.
__label__graph_neural_networks Most advanced GCL methods fall into three main frameworks: node discrimination, group discrimination, and bootstrapping schemes, all of which achieve comparable performance.
__label__reinforcement_learning Offline model-based reinforcement learning (MBRL) enhances data efficiency by utilizing pre-collected datasets to learn models and policies, especially in scenarios where exploration is costly or infeasible.
"__label__learning_theory More specifically, we derive a generalization bound that combines
a covering number argument for compositionality, and the $F_{1}$-norm
(or the related Barron norm) for large width adaptivity."
__label__reinforcement_learning To the best of our knowledge, this work is the first to delve into the regret and constraint violation analysis of average reward CMDPs with a general policy parametrization.
__label__neuroscience_and_cognitive_science The successful combination of learning and visual guidance could provide a new view on combining BU and TD processing in human vision and suggests possible directions for both biologically plausible models and artificial instructed models, such as vision-language models (VLMs).
__label__natural_language_processing SFT improves the pass@1 by up to 15.92\% and pass@10 by 9.30\% over four benchmarks.
__label__other To address this, we initiate a topological formation that serves as a compliant behavioral foreground to guide downstream trajectory generations.
__label__human-AI_interaction The resulting examples, when used as exemplars in the prompt, significantly improve decision-making in retrieval-augmented LLM and VLM agents.
__label__learning_theory However, most existing anchor-based KGE models select the anchors in a primitive manner, which limits their performance.
__label__natural_language_processing Our experimental results on machine translation tasks demonstrate that the proposed method requires 1/16 utility metric computations compared to the vanilla MBR decoding while achieving equal translation quality measured by COMET on the WMT22 dataset (en<>de, en<>ru).
__label__reinforcement_learning This work introduces the first oracle-free and computationally-tractable algorithms for provably convergent multivariate *distributional* dynamic programming and temporal difference learning.
__label__reinforcement_learning We show both theoretically and empirically that this method converges quickly to an optimal coupling, essentially at the same computational cost of running vanilla Sinkhorn in each pair of states.
__label__learning_theory We provide explicit generalization error estimates for iterates generated from GD and SGD, or from proximal SGD in the presence of a non-smooth regularizer.
__label__reinforcement_learning Since solving the problem with joint chance constraints is challenging in practice, we then prove that joint chance constraints can be approximated into Expected Cumulative Safety Constraints (ECSCs) and that there exists a flipping-based policy in the optimal solution sets for constrained MDPs with ECSCs.
__label__causal_inference Building on the theory of partial identification and transportability, this paper introduces new results for bounding the value of a functional of the target distribution, such as the generalization error of a classifiers, given data from source domains and assumptions about the data generating mechanisms, encoded in causal diagrams.
__label__probabilistic_methods In this work, we leverage information theory to connect conformal prediction to other notions of uncertainty.
__label__natural_language_processing To enhance their inferential capabilities, current research has delved into prompting engineering, exemplified by methodologies such as the Tree of Thought and Graph of Thought.
__label__machine_vision Our work pioneers the context model in the anchor level for 3DGS representation, yielding an impressive size reduction of over 100 times compared to vanilla 3DGS and 15 times compared to the most recent state-of-the-art work Scaffold-GS, while achieving comparable or even higher rendering quality.
"__label__interpretability_and_explainability Existing studies typically build the relation by explicitly defining the similarity between the estimated noise transition matrices of ""special"" instances and those of other instances."
__label__diffusion_based_models This paper introduces the Transfer Guided Diffusion Process (TGDP), a novel approach distinct from conventional finetuning and regularization methods.
__label__deep_learning_architectures However, existing methods predominantly concentrate on classification tasks, posing challenges in their adaptation to time series forecasting (TS-forecasting).
__label__learning_theory Over the past decade, there is a growing interest in collaborative learning that can enhance AI models of multiple parties.
__label__machine_vision Extensive experiments on binary and multi-class datasets demonstrate that our proposed method significantly surpasses existing few-shot classification methods and approaches the accuracy of fully supervised methods with only 0.22% annotation costs.
__label__learning_theory In this paper, we focus on the PQC expressivity for general multivariate function classes.
__label__privacy However, it remains uncertain whether MU can be effectively applied to Multimodal Large Language Models (MLLMs), particularly in scenarios of forgetting the leaked visual data of concepts.
__label__safety_in_machine_learning While the vanilla few-shot jailbreaking may be inefficient, we propose improved techniques such as injecting special system tokens like [/INST] and employing demo-level random search from a collected demo pool.
__label__safety_in_machine_learning To address these security vulnerabilities, various backdoor purification methods have been proposed to purify compromised models.
__label__machine_vision The pretext models the driving scene by predicting the angular-wise spatial objectness and temporal dynamics, without manual annotation.
__label__interpretability_and_explainability Embedders play a central role in machine learning, projecting any object into numerical representations that can, in turn, be leveraged to perform various downstream tasks.
"__label__optimization_for_deep_networks LoRA **freezes the original model $W$** and **updates the ""Noise \& Zero"" adapter**, which may lead to slow convergence."
__label__deep_learning_architectures There is a lack of channel strategy that effectively balances individual channel treatment for improved forecasting performance without overlooking essential interactions between channels.
__label__deep_learning_architectures Our approach relies on an additional averaging step after each transformer block, which computes a weighted average of current and past representations---we refer to this operation as Depth-Weighted-Average (DWA).
__label__optimization Such a surrogate modeling method makes it possible to use a single ordinal surrogate to do the surrogate-assisted search, and the remaining surrogates are used to select solution for expensive evaluations, which enhances the optimization efficiency.
__label__algorithmic_game_theory Our key insight is that social choice methods can be reinterpreted by identifying ordinal preferences with volumes of subsets of the *state-action occupancy polytope*.
__label__machine_vision Experiments on COCO and LVIS v1.0 datasets demonstrate the effectiveness of our method, particularly in improving the mAP/AP scores for tail classes.
__label__reinforcement_learning Unlike previous approaches that relied on tracking-based methods for multi-humanoid HOI, CooHOI is inherently efficient, does not depend on motion capture data of multi-humanoid interactions, and can be seamlessly extended to include more participants and a wide range of object types.
__label__optimization In this paper, we propose a combinatorial framework for the semi-discrete OT, which can be viewed as an extension of the combinatorial framework for the discrete OT but requires several new ideas.
__label__machine_vision Generalizable 3D Gaussian splitting (3DGS) can reconstruct new scenes from sparse-view observations in a feed-forward inference manner, eliminating the need for scene-specific retraining required in conventional 3DGS.
__label__machine_learning_for_physical_sciences When data clearly reveals underlying symmetry, leveraging this symmetry can naturally inform the design of model architectures or learning strategies.
__label__machine_learning_for_healthcare Along with this, we systematically investigate the scaling law within molecular pretraining models, examining the power-law correlations between validation loss and model size, dataset size, and computational resources.
__label__probabilistic_methods Our approach is agnostic to both the choice of symmetry group and model architecture, making it widely applicable.
__label__machine_vision This process efficiently condenses the whole 3D object into a multi-frame video format, motivating the utilize of a network architecture similar to those in video diffusion models.
__label__deep_learning_architectures We propose MatrixNet, a neural network architecture that learns matrix representations of group element inputs instead of using predefined representations.
__label__machine_learning_for_other_sciences_and_fields Structure-based drug design (SBDD), which aims to generate 3D ligand molecules binding to target proteins, is a fundamental task in drug discovery.
__label__probabilistic_methods However, when modelling real-world data, learning problems are often not *exactly* equivariant, but only approximately.
__label__safety_in_machine_learning To address this challenge, this paper makes the first attempt to present a new framework, called \textit{GREAT Score}, for global robustness evaluation of adversarial perturbation using generative models.
__label__optimization The universality of the provided convergence analysis based on inexact gradient descent frameworks (Khanh et al., 2023b) allows its extensions to the normalized versions of SAM such as F-SAM (Li et al.
__label__privacy Current computable guarantees for fixed-size subsampling are not tight and do not consider both add/remove and replace-one adjacency relationships.
__label__machine_vision However, this task remains challenging due to its inherently ill-posed nature: event cameras only detect the edge and motion information locally.
__label__generative_models Large language models (LLMs), although having revolutionized many fields, still suffer from the challenging extrapolation problem, where the inference ability of LLMs sharply declines beyond their max training lengths.
__label__other However, these methods are not directly applicable to regression tasks.
__label__robotics arms, legs, or fingers), but each robot must be trained from scratch to control all the actuators of all the parts together.
__label__learning_theory The fundamental goal of deep multi-view clustering is to achieve preferable task performance through inter-view cooperation.
__label__generative_models The current state of the art hairstyle transfer methods use an optimization process for different parts of the approach, making them inexcusably slow.
__label__neuroscience_and_cognitive_science Moreover, the actual deployment on the CPU demonstrated a latency below 100ms, well within real-time requirements.
__label__graph_neural_networks The implementation codes are available at https://github.com/ArefEinizade2/CITRUS.
__label__other Multi-instance partial-label learning (MIPL) is an emerging learning framework where each training sample is represented as a multi-instance bag associated with a candidate label set.
__label__machine_vision 3D reconstruction and relighting of objects made from scattering materials present a significant challenge due to the complex light transport beneath the surface.
__label__graph_neural_networks The implementation code is available at \url{https://github.com/zknus/NeurIPS-2024-DRAGON}.
__label__evaluation However, major dataset imperfections often prove insurmountable.
__label__algorithmic_game_theory Classical algorithms for market equilibrium computation such as proportional response dynamics face scalability issues with Internet-based applications such as auctions, recommender systems, and fair division, despite having an almost linear runtime in terms of the product of buyers and goods.
__label__optimization We develop a theoretically-grounded learning method for the Generalized Linear Programming Value Function (GVF), which models the optimal value of a linear programming (LP) problem as its objective and constraint bounds vary.
__label__natural_language_processing In this article, we propose a method for making predictions purely from the representation of data inside the LLM.
__label__machine_vision Large vision language models (LVLMs) often suffer from object hallucination, producing objects not present in the given images.
__label__deep_learning_architectures We demonstrate that it is feasible to distill large Transformers into linear RNNs by reusing the linear projection weights from attention layers with academic GPU resources.
__label__natural_language_processing Given a set of outputs from different LLMs, Smoothie constructs a latent variable graphical model over embedding representations of observable LLM outputs and unknown “true” outputs.
"__label__optimization Our approach employs an operator splitting technique with a novel variable
metric, enabling a local backtracking line-search to adaptively select the stepsize
without global information or extensive communications."
__label__deep_learning_architectures We introduce UniTS, a unified multi-task time series model that utilizes task tokenization to integrate predictive and generative tasks into a single framework.
__label__reinforcement_learning To better understand the benefits of privileged information, we revisit and examine several simple and practically used paradigms in this setting, with both computation and sample efficiency analyses.
__label__natural_language_processing Trained on vast corpora of human language, language models demonstrate emergent human-like reasoning abilities.
__label__optimization_for_deep_networks The efficacy of the proposed framework is demonstrated through several applications.
__label__reinforcement_learning RL algorithms rely on observing rewards to train the agent, and if informative rewards are sparse the agent learns slowly or may not learn at all.
__label__safety_in_machine_learning Additionally, we establish a relation between distortion, uncertainty and perception, through which we prove the aforementioned uncertainly-perception tradeoff induces the well-known perception-distortion tradeoff.
__label__learning_theory We provide the first algorithm that operates in almost linear time ($\tilde{O}(n/\epsilon^3)$ time) and achieves $\alpha = 3$.
__label__algorithmic_game_theory This phenomenon is formulated by a decision-dependent distribution mapping within the recently proposed framework of performative prediction (PP) Perdomo et al.
__label__online_learning Typically, networks suffer from catastrophic interference when training on a sequence of documents; however, we discover a curious and remarkable property of LLMs finetuned sequentially in this setting: they exhibit *anticipatory* behavior, recovering from the forgetting on documents *before* seeing them again.
__label__optimization Specifically, we base our algorithms on the optimistic method and appropriately combine it with second-order information.
__label__reinforcement_learning Based on this, we then establish two key results: 1) a sufficient condition for transferability to any transition laws when learning from at least two experts with sufficiently different transition laws, and 2) a sufficient condition for transferability to local changes in the transition law when learning from a single expert.
__label__optimization It is only very recently that SGD using sampling without replacement -- shuffled SGD -- has been analyzed with matching upper and lower bounds.
__label__machine_learning_for_healthcare Machine learning catalyzes a revolution in chemical and biological science.
__label__infrastructure However, managing long contexts brings substantial challenges due to the expansion of key-value cache (KV cache).
__label__safety_in_machine_learning Notably, we achieve the best attack performance with an average of 58.3% on three defended CNNs.
__label__natural_language_processing Furthermore, we introduce a token retention strategy to keep highly distinct state pairs unmerged, thus preserving the information with minimal additional storage overhead.
__label__diffusion_based_models Extensive experiments demonstrate that DiffPano can generate scalable, consistent, and diverse panoramic images with given unseen text descriptions and camera poses.
__label__machine_learning_for_other_sciences_and_fields To verify the effectiveness and versatility of our proposed enhancement framework, we conduct extensive experiments on three real-world datasets using three popular SRS models.
__label__optimization These matrices are integral to optimization under J-orthogonal constraints, which have widespread applications in statistical learning and data science.
__label__diffusion_based_models We propose a novel family of bespoke ODE solvers to the continuous adjoint equations for diffusion models, which we call *AdjointDEIS*.
__label__graph_neural_networks Our empirical evidence, leveraging algorithms from the CLRS-30 benchmark, validates that one can train a network to solve algorithmic problems by directly finding the equilibrium.
__label__learning_theory In this paper, we theoretically analyze the reversal curse via the training dynamics of (stochastic) gradient descent for two auto-regressive models: (1) a bilinear model that can be viewed as a simplification of a one-layer transformer; (2) one-layer transformers under certain assumptions.
__label__machine_vision DiPEx progressively learns to expand a set of distinct, non-overlapping hyperspherical prompts to enhance recall rates, thereby improving performance in downstream tasks such as out-of-distribution OD.
__label__evaluation Standardized benchmarks drive progress in machine learning.
__label__machine_vision Finally, we fuse complementary 2D and 3D mask features, resulting in competitive performance across multiple benchmarks for 3D open vocabulary semantic segmentation.
__label__machine_vision Remarkably, ZERO requires a single batched forward pass through the vision encoder only and no backward passes.
__label__learning_theory Having an (occasionally) instance-dependent confusion matrix across data samples is apparently more realistic, but inevitably introduces outliers to the model.
__label__reinforcement_learning We then show that the use of suboptimal but high-coverage data or test-time policy training techniques can address this generalization issue in practice.
__label__other Most previous work on federated learning assumes that clients possess static batches of training data.
__label__fairness SFID is versatile, maintaining the semantic integrity of outputs and costly effective by eliminating the need for retraining.
__label__probabilistic_methods To overcome these limitations, we propose a novel approach utilizing the sparse spectral representation of nonstationary kernels.
__label__machine_vision Our approach, Metric from Human (MfH), bridges from generalizable MRDE to zero-shot MMDE in a generate-and-estimate manner.
__label__machine_vision Finally, our experiments on the widely-used DIS5K dataset benchmark demonstrate superior performance in quality and efficiency compared to existing methods.
__label__learning_theory In this work, we ask whether there is a price to pay for testably learning more complex concept classes.
__label__diffusion_based_models Our results indicate that the weight space of fine-tuned diffusion models can behave as an interpretable $\textit{meta}$-latent space producing new models.
__label__robotics Finally, we introduce an instance-level 3D-2D feature association method that links 3D points to 2D masks, which are further associated with 2D CLIP features.
__label__human-AI_interaction We build upon prior work that studies assistance through the lens of empowerment: an assistive agent aims to maximize the influence of the human's actions such that they exert a greater control over the environmental outcomes and can solve tasks in fewer steps.
__label__generative_models Our results show that this strategy successfully trains RBMs to capture the full diversity of data in datasets where previous methods fail.
__label__machine_vision This paper introduces CoFie, a novel local geometry-aware neural surface representation.
__label__machine_vision Empirical results on a series of vision-language benchmarks reveal that the pre-train acceleration through Chain-of-Sight is achieved without sacrificing performance, matching or surpassing the standard pipeline of utilizing all visual tokens throughout the entire training process.
__label__generative_models In these methods, an input view is geometrically warped to novel views with estimated depth maps, then the warped image is inpainted by T2I models.
__label__natural_language_processing Our work, KVQuant, facilitates low precision KV cache quantization by incorporating several novel methods: (i) Per-Channel Key Quantization, where we adjust the dimension along which we quantize the Key activations to better match the distribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations before the rotary positional embedding to mitigate its impact on quantization; (iii) Non-Uniform KV Cache Quantization, where we derive per-layer sensitivity-weighted non-uniform datatypes that better represent the distributions; and (iv) Per-Vector Dense-and-Sparse Quantization, where we isolate outliers separately for each vector to minimize skews in quantization ranges.
__label__graph_neural_networks To this end, we follow the idea of estimating the underlying density of the training data to decide whether a given input is close to the in-distribution (IND) data and adopt Energy-based models (EBMs) as density estimators.
__label__machine_vision To curate our benchmark, we built an automatic pipeline using large language models and diffusion models, followed by a rigorous 4-step verification process by human annotators.
__label__safety_in_machine_learning In this paper, we propose RLbreaker, a black-box jailbreaking attack driven by deep reinforcement learning (DRL).
__label__natural_language_processing Transformer-based large language models (LLMs) typically have a limited context window, resulting in significant performance degradation when processing text beyond the length of the context window.
__label__interpretability_and_explainability In each iteration, our testing framework computes optimistic and pessimistic safety estimates.
__label__other We propose two algorithms for non-parametric classification using such EaS representation.
__label__machine_vision Importantly, the image pair (as well as the caption pair) contains minimal changes, i.e., between the two images (as well as between the two captions), only one aspect changes at a time from among the following possible types of changes: object, attribute, count, and spatial relation.
__label__evaluation We propose the Area under the Generalized Risk Coverage curve ($\mathrm{AUGRC}$), which meets all requirements and can be directly interpreted as the average risk of undetected failures.
"__label__generative_models We develop Gorilla, a
finetuned LLaMA model that surpasses the performance of GPT-4 on writing API
calls."
__label__safety_in_machine_learning Our code is available at https://github.com/CGCL-codes/UnlearnablePC.
__label__optimization The optimization of such functions is often a non-trivial task given their combinatorial, black-box and expensive-to-evaluate nature.
__label__deep_learning_architectures At test time, after several test-time optimization steps starting from the meta-parameter, we obtain much higher HMR accuracy than the test-time optimization starting from the simply pretrained regression model.
__label__diffusion_based_models The generated motion-music samples are available at https://momu-diffusion.github.io/.
__label__reinforcement_learning Plasticity loss, trainability loss, and primacy bias have been identified as issues arising when training deep neural networks on sequences of tasks---referring to the increased difficulty in training on new tasks.
__label__learning_theory Despite the remarkable success of transformer-based models in various real-world tasks, their underlying mechanisms remain poorly understood.
__label__natural_language_processing In the rapidly evolving field of Large Language Models (LLMs), ensuring safety is a crucial and widely discussed topic.
__label__optimization We instantiate two uniformly meta-stable learning algorithms based on regularized empirical risk minimization and gradient descent and give explicit generalization bounds for convex learning problems with smooth losses and for weakly convex learning problems with non-smooth losses.
__label__machine_vision Even in new classes, the distribution of normal residual features would not remarkably shift from the learned distribution.
__label__machine_learning_for_other_sciences_and_fields The stable periodic patterns present in time series data serve as the foundation for conducting long-horizon forecasts.
__label__diffusion_based_models We introduce a novel technique of measuring the rate-of-change and curvature of the diffusion paths connecting samples to the standard normal.
__label__generative_models More precisely, it establishes bounds on the Kullback-Leibler divergence between the target distribution and the one generated by such DFM models under moment conditions on the score of $\nu^\star$, $\mu$ and $\pi$, and a standard $\mathrm{L}^2$-drift-approximation error assumption.
__label__machine_vision Empirical experiments reveal that current VLMs exhibit notable deficiencies in understanding spatial relationships and counting abilities.
__label__machine_vision Extensive experiments on multiple downstream tasks show that SNELL achieves state-of-the-art performance with low memory usage, endowing PEFT with sparse tuning to large-scale models.
__label__graph_neural_networks In this paper, we introduce a novel framework called General Retrieval-Augmented Graph Learning (RAGraph), which brings external graph data into the general graph foundation model to improve model generalization on unseen scenarios.
__label__generative_models Nevertheless, to date, DRE methods have failed in accurately capturing the distributions of complex high-dimensional data, like images, and have thus been drawing reduced research attention in recent years.
__label__reinforcement_learning The diffusion policy maintains expressiveness, while the trust region loss directs the one-step policy to explore freely and seek modes within the region defined by the diffusion policy.
__label__machine_vision Our new regularization method enhances normal and geometry reconstruction while reducing needle-like artifacts.
__label__machine_learning_for_physical_sciences Hitherto, it has been separately addressed by employing operator learning surrogates for field prediction while using simulation-based inference (and its variants) for statistical parameter determination.
__label__machine_vision The basic idea is to enrich the representation of the infrared modality with textual descriptions automatically generated by VLMs.
__label__interpretability_and_explainability Planning is a crucial element of both human intelligence and contemporary large language models (LLMs).
__label__safety_in_machine_learning However, an increasing privacy concern exists regarding training large-scale image segmentation models on unauthorized private data.
__label__machine_vision We show that modeling per-point provenance during the NeRF optimization enriches the model with information on triangulation leading to improvements in novel view synthesis and uncertainty estimation under the challenging sparse, unconstrained view setting against competitive baselines.
__label__machine_learning_for_physical_sciences Poseidon also generalizes very well to new physics that is not seen during pretraining.
__label__learning_theory In addition to this algorithm, we provide another (almost) linear-time algorithm with better dependency on the additive accuracy parameter $\epsilon$, albeit with a slightly worse accuracy parameter, $\alpha = 4$.
__label__machine_vision Although achieving groundbreaking progress, such design has certain drawbacks: 1) preceding subtasks require massive high-quality 3D annotations as supervision, posing a significant impediment to scaling the training data; 2) each submodule entails substantial computation overhead in both training and inference.
__label__machine_learning_for_other_sciences_and_fields We theoretically prove that the FL-PT coalitions formed are optimal since no coalitions can collaborate together to improve the utility of any of their members.
__label__privacy In contrast, the (sorted) index of the discretization point changes only by $1$ between neighboring instances, and so we use a novel algorithm that set the discretization points using random Bernoulli noise, resulting in only a few buckets being affected under the right coupling.
__label__diffusion_based_models Firstly, we introduce a novel image-to-text concept activation module to guide the diffusion model in revisiting ignored concepts.
__label__graph_neural_networks Then, we propose two message passing neural networks to perform prompt encoding and KG reasoning, respectively.
__label__deep_learning_architectures Addressing this gap, this paper presents an efficient MLP-based model, the Series-cOre Fused Time Series forecaster (SOFTS), which incorporates a novel STar Aggregate-Redistribute (STAR) module.
__label__probabilistic_methods Notably, this optimization reduces to solving a minimum eigenvalue problem, so that EigenVI effectively sidesteps the iterative gradient-based optimizations that are required for many other BBVI algorithms.
__label__causal_inference Notably, it can seamlessly incorporate expert knowledge as constraints within the optimization process, which enhances the interpretability of the outcomes.
__label__machine_learning_for_healthcare EHR can be continuously collected but CXR is generally taken with a much longer interval due to its high cost and radiation dose.
__label__machine_vision It starts from the top of the hierarchy with the coarsest granularity and then identifies the potential patches likely to contain objects of interest.
"__label__other Over strings, UHAT without positional encodings capture only
    regular languages."
__label__bandits Second, we propose a new strategy called \wrtinf that achieves this optimal regret and improves over the state-of-the-art both in $K$ and the optimality gap.
__label__privacy Additionally, we propose an inefficient algorithm based on the inverse smooth sensitivity mechanism, which satisfies the more restrictive notion of pure DP.
__label__generative_models To enhance the compression ratio, we propose normalized attention score as an effective metric for identifying salient tokens by considering the lower triangle characteristics of the attention matrix.
"__label__deep_learning_architectures We prove a property of continuous-discrete equivalence to show the capability of
MambaNO in approximating operators arising from universal PDEs to desired accuracy."
__label__deep_learning_architectures We then formulate the exact formula for the value matrix in self-attention, theoretically and empirically demonstrating that this value matrix captures the eigenvectors of the Gram matrix of the key vectors in self-attention.
__label__privacy On this dataset, we evaluate 7 state-of-the-art VLMs, finding that they can infer various personal attributes at up to 77.6% accuracy.
"__label__optimization As an example, we introduce the Fitzpatrick logistic loss and the
Fitzpatrick sparsemax loss, counterparts of the logistic and the sparsemax
losses."
__label__interpretability_and_explainability By allowing flexibility in that reference and by considering how distances along the hypercube translate to distances in feature space, we can derive sparser and more meaningful explanations for various types of function classes.
__label__natural_language_processing By training alongside a frozen template LLM, the tiny model gains the capability to alter the logits output by the LLMs.
__label__diffusion_based_models In this paper, we present DoSSR, a $\textbf{Do}$main $\textbf{S}$hift diffusion-based SR model that capitalizes on the generative powers of pretrained diffusion models while significantly enhancing efficiency by initiating the diffusion process with low-resolution (LR) images.
__label__optimization Furthermore, the downstream performance evaluation based on MT-bench and math benchmarks shows that BAdam outperforms existing memory efficient baselines such as LoRA.
__label__machine_vision The metric validates the intrinsic nature of the compositional relations among parts, objects, and scenes in a hierarchy-agnostic domain.
__label__fairness Code is released at https://github.com/Scarelette/CultureLLM.
__label__machine_vision The latest methods learn neural SDFs using either a data-driven based or an overfitting-based strategy.
__label__other While its superiority lies in communication efficiency and privacy preservation compared to iterative FL, one-shot FL often compromises model performance.
__label__diffusion_based_models Datasets, model, and code are released at https://sites.google.com/view/mole4diffuser/.
__label__robotics To our knowledge, this paper proposes the first on-road object importance estimation model that fuses multi-fold top-down guidance factors with bottom-up feature.
__label__other Extensive experiments across multiple datasets and state-of-the-art MTS forecasting models demonstrate the effectiveness, versatility, and stealthiness of BackTime attacks.
__label__optimization_for_deep_networks Recent years have witnessed significant breakthroughs in first-order methods, such as gradient descent, for tackling this non-convex optimization problem.
__label__privacy Experiments on three datasets against four types of removal attacks show that ADV-TRA exhibits superior performance in distinguishing between infringing and innocent models, outperforming the state-of-the-art comparisons.
__label__optimization_for_deep_networks Extensive experiments demonstrate the effectiveness and efficiency of our approach.
__label__optimization Notably, our analysis identifies different properties of the sub-populations that drive bias at different timescales and hence shows a shifting preference of our classifier during training.
__label__probabilistic_methods Moreover, we establish the global and local optimality theory of our model.
__label__machine_learning_for_healthcare The algorithm incorporates both undersampled k-space and pre-existing information.
__label__learning_theory Then, we theoretically demonstrate the existence of favorable finite-sample and asymptotic statistical guarantees on the performance of the robust optimization procedure.
"__label__algorithmic_game_theory We extend our results to a natural ""balanced"" variant of the $k$-facility case, and show that without balancedness, robustness completely breaks down even for $k=2$ facilities on a line."
__label__machine_vision However, when faced with deformable objects, the keypoints they identify do not preserve semantic consistency well.
__label__machine_vision Our results indicate that some level of disentanglement, transferring a representation from synthetic to real data, is possible and effective.
__label__diffusion_based_models Consequently, each component is facilitated to compute in parallel on separate devices.
__label__other Conventional learning algorithms for neural networks, however, suffer from catastrophic forgetting (CF)---previously acquired skills are forgotten when a new task is learned.
__label__natural_language_processing In this work we consider Code World Models, world models generated by a Large Language Model (LLM) in the form of Python code for model-based Reinforcement Learning (RL).
__label__interpretability_and_explainability If these assumptions fail, both noise transition matrices and clean labels cannot be accurately estimated.
__label__reinforcement_learning Our key idea is to penalize the Q-values of OOD actions that correspond to high uncertainty.
__label__learning_theory However, their improvement over linear samples and time is only by subpolynomial factors.
__label__reinforcement_learning This combination allows for the generation of challenging yet achievable goals, enabling agents to learn effectively without relying on domain knowledge.
__label__reinforcement_learning Recent studies have revealed that focusing the optimization of Bellman equations solely on in-sample actions tends to result in more stable optimization, especially in the presence of function approximation.
__label__graph_neural_networks In particular, there is no non-transferable energy under the convergence we consider here.
__label__optimization_for_deep_networks Attempting to complement this deficiency, we investigate the layerwise properties of LoRA on fine-tuning tasks and observe an unexpected but consistent skewness of weight norms across different layers.
__label__generative_models Despite this equivalence, attempts to train GFlowNets using traditional divergence measures as learning objectives were unsuccessful.
__label__generative_models Large, autoregressive models trained on databases of chemical compounds have yielded powerful generators, but we still lack robust strategies for generating molecules with desired properties.
__label__neuroscience_and_cognitive_science Moreover, we introduce a learned receptive field layer that sheds light on the CNN-based model's data processing during training, enhancing understanding of its structure and interpretive capacity.
__label__algorithmic_game_theory We focus on two learning tasks; in the first, the input is vectors of utilities of an action (decision or policy) for individuals in a group and their associated social welfare as judged by a policy maker, whereas in the second, the input is pairwise comparisons between the welfares associated with a given pair of utility vectors.
__label__probabilistic_methods In networks with well-separated communities, the metric backbone tends to preserve many inter-community edges, because these edges serve as bridges connecting two communities, but tends to delete many intra-community edges because the communities are dense.
__label__machine_vision We have included the code in supplementary materials.
__label__algorithmic_game_theory When learning in strategic environments, a key question is whether agents can overcome uncertainty about their preferences to achieve outcomes they could have achieved absent any uncertainty.
__label__machine_learning_for_physical_sciences Despite their remarkable empirical performance in various scientific computing tasks, PINNs often fail to generate reasonable solutions, and such pathological behaviors remain difficult to explain and resolve.
__label__speech_and_audio Automatic and human evaluations on the DailyTalk dataset demonstrate that our approach effectively generates natural-sounding spoken responses, surpassing previous and cascaded baselines.
__label__optimization_for_deep_networks Spry achieves a low memory footprint, high accuracy, and fast convergence.
__label__learning_theory A long line of work established that uniformity testing has sample complexity $\Theta(\sqrt{n}\varepsilon^{-2})$.
__label__graph_neural_networks These metrics are believed to be able to reflect the performance of GNNs, especially on node-level tasks.
__label__other Our experiments highlighted the trade-offs between MDP rewards and receiver accuracy across various compression rates, showcasing the efficacy of our method compared to conventional compression baseline.
__label__machine_vision To resolve the $\ell_0$-minimization, we develop a novel two-stage decoupling strategy, which first decouples the alignment error into a rotation fitting error and a translation fitting error.
__label__machine_vision Additionally, we introduce a progressive training scheme and a point-backtracking technique that improves Ex4DGS's convergence.
__label__machine_vision We conduct detailed experiments on photo finishing tuning and photo stylization tuning tasks, demonstrating the advantages of our method.
__label__deep_learning_architectures In this formulation the number of network's parameters remains fixed.
__label__causal_inference This mismatch translates into a lack of guarantees in various tasks such as generative modeling, decision-making, fairness, and generalizability, to cite a few.
__label__learning_theory Experiments show that our learning scheme substantially improves outlier detection and the classifier's testing accuracy.
__label__causal_inference Finally, compatibility has deep connections to information theory.
__label__machine_vision Unfortunately, the benefits of the prior and representation do not involve novel view synthesis for dynamic motions.
__label__safety_in_machine_learning This approach departs from the conventional self-training method, which focuses on minimizing the classifier's entropy.
__label__diffusion_based_models Without any regularization term in the loss function, this framework achieves superior disentanglement performance on the benchmark datasets, surpassing all previous methods with intricate designs.
__label__probabilistic_methods Such a regularization provides the upper bound estimation over the distribution of possible conjugate potentials and makes the learning stable, completely eliminating the need for additional extensive fine-tuning.
__label__deep_learning_architectures This data-reliance may lead to low generalization of the learned alignment relationships.
__label__generative_models Experiments reveal that our approach surpasses the existing encoder training methods qualitatively and quantitatively.
__label__machine_vision To this end, we introduce novel methods related to poses, scales, and occlusion parsing which are keys to enable deep priors to work together in a robust way.
__label__diffusion_based_models Our method achieves state-of-the-art performance in both Chinese and English text generation.
__label__graph_neural_networks Graph Contrastive Learning (GCL) has emerged as a powerful approach for generating graph representations without the need for manual annotation.
__label__diffusion_based_models Diffusion Probabilistic Models (DPMs) have been recently utilized to deal with various blind image restoration (IR) tasks, where they have demonstrated outstanding performance in terms of perceptual quality.
__label__natural_language_processing The simplicity and scalability of maximum likelihood estimation (MLE) for next token prediction led to its role as predominant paradigm.
__label__machine_learning_for_other_sciences_and_fields By eliminating the need for iterative parameter tuning, SuperEncoder represents a pioneering step towards iteration-free approximate QSP.
__label__reinforcement_learning DDiffPG forms a multimodal training batch and utilizes mode-specific Q-learning to mitigate the inherent greediness of the RL objective, ensuring the improvement of the diffusion policy across all modes.
__label__safety_in_machine_learning Most existing approaches that seek to certify robustness, especially Lipschitz continuity, lie within the continuous domain with norm and distribution-dependent guarantees.
__label__reinforcement_learning To further enhance the exploration capability of the diffusion policy, we design a special entropy regularization term.
__label__learning_theory To avoid trivial solutions and ill-posedness, we introduce a natural linear normalization constraint.
__label__neuroscience_and_cognitive_science Compared to traditional Artificial Neural Network (ANN), Spiking Neural Network (SNN) has garnered widespread academic interest for its intrinsic ability to transmit information in a more energy-efficient manner.
__label__active_learning The pretraining-finetuning paradigm has gained widespread adoption in vision tasks and other fields.
__label__privacy On the negative side, Dwork, Talwar, Thakurta, and Zhang (STOC 2014) proved that privately estimating subspaces, in general, requires an amount of points that has a polynomial dependency on the dimension.
__label__reinforcement_learning We first establish the converse result, where we show that any Federated Q-learning that offers a linear speedup with respect to number of agents in sample complexity needs to incur a communication cost of at least $\Omega(\frac{1}{1-\gamma})$, where $\gamma$ is the discount factor.
__label__machine_learning_for_physical_sciences Furthermore, we demonstrate that the SEA module alone can reduce errors by 97% for state variables that are highly dependent on other states of the system.
__label__optimization In this paper, we show that applying adaptive methods directly to distributed minimax problems can result in non-convergence due to inconsistency in locally computed adaptive stepsizes.
__label__machine_vision To validate the superiority of our FineCLIP and the rationality of each design, we conduct extensive experiments on challenging dense prediction and image-level tasks.
__label__interpretability_and_explainability We also provide a similar framework for discovering discrete symmetry.
__label__probabilistic_methods We consider Bayesian algorithm execution (BAX), a framework for efficiently selecting evaluation points of an expensive function to infer a property of interest encoded as the output of a base algorithm.
__label__diffusion_based_models However, achieving this effectively and efficiently is highly nontrivial.
__label__machine_vision Our pilot study reveals that object hallucination is closely tied with Rotary Position Encoding (RoPE), a widely adopted positional dependency modeling design in existing LVLMs.
__label__machine_vision Extensive experiments on various datasets, including UCF Sports, KITTI and Cityscapes, highlight the strong representative ability of motion graph.
__label__machine_vision Current methods either require impractical instance-level training or are confined to predefined categories, limiting their applicability.
__label__optimization Yet, these results still lack a solid theoretical understanding, and it is unclear whether they can be improved by leveraging connections to the wealth of work on sparse recovery algorithms.
__label__machine_learning_for_other_sciences_and_fields We present a new class of fast polylog-linear algorithms based on the theory of structured matrices (in particular *low displacement rank*) for integrating tensor fields defined on weighted trees.
__label__algorithmic_game_theory Our study explores how the allocated budget enhances the success probability of online selection algorithms.
__label__machine_learning_for_healthcare To address this challenge, we propose DDL-CXR, a method that dynamically generates an up-to-date latent representation of the individualized CXR images.
__label__natural_language_processing To facilitate merging, we propose disentangling the states into the magnitude and direction components, interpolating the directions of the state vectors while preserving their lengths unchanged.
__label__generative_models Leveraging MOFT, we propose a novel training-free video motion control framework.
__label__probabilistic_methods Kohler and Mossel constructed chains such that for trees with $N$ leaves, recovering the root better than random requires a polynomial of degree $N^{\Omega(1)}$.
__label__optimization Quantization is a fundamental optimization for many machine learning (ML) use cases, including compressing gradients, model weights and activations, and datasets.
__label__natural_language_processing In this paper, we investigate the integration of LLMs with KGs by introducing a specialized KG Language (KGL), where a sentence precisely consists of an entity noun, a relation verb, and ends with another entity noun.
__label__natural_language_processing Qualitatively, we find that a LACIE-trained model hedges more when uncertain and adopts implicit cues to signal certainty when it is correct, such as using an authoritative tone or including details.
__label__machine_learning_for_physical_sciences Our main contribution is to define and analyze generalization of a broad suite of neural representations of classes of ergodic systems, including chaotic systems, in a way that captures emulating underlying invariant, physical measures.
__label__optimization_for_deep_networks We extend previous results and show on one hand that any local minimum of a $L2$-regularized loss of the form $L(AB^\top) + \lambda (\|A\|^2 + \|B\|^2)$ coincides with a minimum of the nuclear norm-regularized loss $L(AB^\top) + \lambda\|AB^\top\|_*$, and on the other hand that the 2 losses become identical exponentially quickly during training.
__label__safety_in_machine_learning To further systematically evaluate the safety risks of text-to-image models, we propose a novel Automatic Red-Teaming framework, ART.
__label__robotics Next, we canonicalize the input point cloud to estimate constrained parts' poses by predicting the joint parameters and states as replacements.
__label__reinforcement_learning We investigate Reinforcement Learning from Human Feedback (RLHF) in the context of a general preference oracle.
__label__learning_theory We revisit the apparent gap between offline and online IL from a learning-theoretic perspective, with a focus on general policy classes up to and including deep neural networks.
__label__natural_language_processing Current Multi-modal Large Language Models (MLLMs), constrained by their training data, lack the capability to function effectively as operation assistants.
__label__natural_language_processing Additionally, we empirically show that introspective planning, in combination with conformal prediction, achieves tighter confidence bounds, maintaining statistical success guarantees while minimizing unnecessary user clarification requests.
__label__causal_inference However, traditional clinical trials are not customized for the goal of identifying best-performing subgroups because they typically pre-define subgroups at the beginning of the trial and adhere to a fixed subgroup treatment allocation rule, leading to inefficient use of experimental efforts.
__label__machine_vision From the egocentric view, humans integrate the visual cortex, cerebellum, and brain to internalize their intentions and interaction concepts of objects, allowing for the pre-formulation of interactions and making behaviors even when interaction regions are out of sight.
__label__algorithmic_game_theory Finally, we design practical algorithms for these tasks and evaluate their performance.
__label__machine_vision We show that while synthetic data can benefit some downstream tasks, it is universally matched or outperformed by real data from the simple retrieval baseline.
__label__probabilistic_methods Empirically, DCPC achieves better numerical performance than competing algorithms and provides accurate inference in a number of problems not previously addressed with predictive coding.
__label__deep_learning_architectures We extend work done by Garg et al.
__label__natural_language_processing Recently, Large Language Models (LLMs) have demonstrated extensive knowledge and remarkable proficiency in temporal reasoning.
__label__machine_learning_for_healthcare To address these challenges in cross-domain disease grading, we propose a Severity-aware Recurrent Modeling (Samba) method in this paper.
__label__diffusion_based_models We conduct a comprehensive study of the UNet encoder and empirically analyze the encoder features.
__label__optimization_for_deep_networks Noisy labels pose a common challenge for training accurate deep neural networks.
__label__optimization Our experimental results show that HyperPrism can improve the convergence speed up to 98.63% and scale well to more devices compared with the state-of-the-art, all with little additional computation overhead compared to traditional linear aggregation.
__label__other In experiments, we employ the SWE-bench benchmark to compare MAGIS with popular LLMs, including GPT-3.5, GPT-4, and Claude-2.
__label__diffusion_based_models In addition, we conducted a thorough analysis that sheds light on how it improves diffusion training speed while improving fidelity.
__label__interpretability_and_explainability Most importantly, the grounding and validation of algorithmic recourse in real-world contexts remain underexplored.
__label__reinforcement_learning Our empirical results demonstrate that TTCT effectively comprehends textual constraint and trajectory, and the policies trained by TTCT can achieve a lower violation rate than the standard cost function.
__label__machine_learning_for_other_sciences_and_fields To address this challenge, we propose an end-to-end method that integrates data imputation with anomaly detection into a unified optimization problem.
__label__machine_vision To address these challenges, we propose GOMAA-Geo -- a goal modality agnostic active geo-localization agent -- for zero-shot generalization between different goal modalities.
__label__machine_learning_for_healthcare The code is available at https://github.com/prescient-design/funcmol.
__label__optimization To address these limitations, we utilize Bayesian Optimization (BO), a sample-efficient black-box solver, and propose a novel framework for combinatorial optimization on graphs.
__label__other As a result, we can increase within-class diversity and reduce the size of required soft labels.
__label__interpretability_and_explainability Our approach reveals the diverse computational strategies developed inside LLMs to solve the same task across different conditions, allowing us to make a step towards designing optimal methods to extract information from language models.
__label__machine_vision To address this issue, we propose the Tree of Attributes Prompt learning (TAP), which first instructs LLMs to generate a tree of attributes with a ``concept - attribute - description'' structure for each associated category name, and then learn the hierarchy with vision and text prompt tokens.
__label__optimization_for_deep_networks However, current approaches only pertain to first-order optimizers.
__label__algorithmic_game_theory Our result applies to optimistic gradient and extra-gradient descent/ascent, as well as a certain iterative variant of Nesterov's smoothing technique.
__label__learning_theory In this general setting, we provide rates on~the excess adversarial (transfer) risk for Lipschitz losses and smooth nonnegative losses.
__label__safety_in_machine_learning Despite their ease of implementation and computational efficiency, current logit-based methods are vulnerable to overconfidence issues, leading to prediction bias, especially under the natural shift.
__label__probabilistic_methods As an exemplar, we focus on how polar codes can be used to efficiently simulate i.i.d.
__label__reinforcement_learning To overcome these challenges, we introduce the **E**ffective **M**etric-based **E**xploration-bonus (EME).
__label__interpretability_and_explainability A key goal in mechanistic interpretability is circuit analysis: finding sparse subgraphs of models corresponding to specific behaviors or capabilities.
__label__natural_language_processing How do language models use information provided as context when generating a response?
__label__machine_vision However, since more than one point can be projected onto the same 2D position but only one point can be preserved, the previous 2D projection-based segmentation methods suffer from inevitable quantized information loss, which results in incomplete geometric structure, especially for small objects.
__label__neuroscience_and_cognitive_science In this work, we propose a similarity-based objective function that translates proximity in space, to proximity in representation.
__label__bandits Furthermore, we examine the setting where the function class additionally provides distributional information of the reward, as studied by Wang et al.
__label__machine_vision In experiments, we surpass the state-of-the-art by over 3 dB in PSNR and more than 200x in rendering speed.
__label__machine_vision Multi-instance point cloud registration aims to estimate the pose of all instances of a model point cloud in the whole scene.
__label__machine_vision To facilitate geometry-aware guidance in physical property estimation, we introduce a novel hybrid framework that leverages 3D Gaussian representation to not only capture explicit shapes but also enable the simulated continuum to render object masks as 2D shape surrogates during training.
__label__safety_in_machine_learning For the first time, we propose a benchmark for certified robust regression in visual positioning systems using the Cambridge Landmarks dataset where robustness analysis is essential for autonomous navigation of AI agents and self-driving cars.
__label__machine_vision Moreover, we show that supervised SAM can also benefit from our self-supervised labels.
__label__machine_vision Additionally, we propose Pattern Observer (PrObe) to address these challenges.
__label__machine_vision Codes are available in https://github.com/BeierZhu/VRF.
__label__machine_vision To this end, we first learn precise audio-visual speech representations on real videos via a self-supervised masked prediction task, which encodes both local and global semantic information simultaneously.
__label__machine_vision Disturbed Taylor pruning is also proposed to address the misalignment between the pruning objective and training target, thereby boosting the post-distillation after pruning.
__label__reinforcement_learning Theoretical and empirical results show that SCAS also exhibits the effect of suppressing OOD actions.
__label__online_learning In practice, predictive features may be expensive, so we allow the decision maker to issue at most $k$ such queries.
__label__diffusion_based_models Extensive experiments on benchmark datasets, including MNIST, FashionMNIST, CIFAR-10, CIFAR-100, and NIPD, demonstrate the effectiveness of CRFed in improving accuracy, convergence speed, and overall robustness in federated learning scenarios with severe data heterogeneity.
__label__learning_theory The classical iteratively reweighted least-squares (IRLS) algorithm aims to recover an unknown signal from linear measurements by performing a sequence of weighted least squares problems, where the weights are recursively updated at each step.
__label__generative_models Extensive experimental results demonstrate that GeoLRM significantly outperforms existing models, especially for dense view inputs.
__label__deep_learning_architectures However, in many sequence modeling problems, the underlying process is inherently modular and it is of interest to have inductive biases that mimic this modular structure.
__label__machine_vision In particular, for the contrast corruption that was found problematic in prior work we achieve an accuracy that exceeds the $L^p$- and the LPIPS-based adversarially trained neural networks by a margin of 27.16\% on the CIFAR-10-C corruption data set.
__label__machine_vision This task introduces three challenges: (1) detecting behavior errors in novel environments, (2) identifying behavior errors that occur without revealing notable changes, and (3) lacking complete temporal information of the rollout due to the necessity of online detection.
__label__optimization_for_deep_networks We study a class of optimization problems motivated by automating the design and update of AI systems like coding assistants, robots, and copilots.
__label__diffusion_based_models We discern a phenomenon of attribute bias in the text space and highlight a contextual issue in padding embeddings that entangle different concepts.
__label__generative_models We show that this method *provably* eliminates order dependency, and that it can be applied to *any* transformer-based LLM to enable text generation that is unaffected by re-orderings.
__label__natural_language_processing These results significantly surpass not only the prior best agents, including AppAgent with GPT-4V (8.3\% success rate) and the 17B CogAgent trained with AitW data (14.4\%), but also our implementation of prior best autonomous RL approach based on filtered behavior cloning (57.8\%), thereby establishing a new state-of-the-art for digital agents for in-the-wild device control.
__label__privacy We conduct extensive experiments on various datasets and models, including both vision-language models (CLIP) and large language models, demonstrating the broad applicability and effectiveness of such an attack.
__label__diffusion_based_models In the first stage, a large language model is used to rewrite the prompts.
__label__optimization Finally, we propose adaptive variants of both methods using line search, obtaining the first provably efficient adaptive algorithms that could exploit local second-order similarity without the prior knowledge of any parameters.
__label__probabilistic_methods More precisely, we prove three different ways to upper bound the intrinsic uncertainty, as described by the conditional entropy of the target variable given the inputs, by combining CP with information theoretical inequalities.
__label__machine_vision Additionally, we introduce an Iterative Cross-view Gaussians Alignment method to ensure consistent depth scales across different views.
__label__probabilistic_methods Deep neural networks (DNNs) lack the precise semantics and definitive probabilistic interpretation of probabilistic graphical models (PGMs).
__label__natural_language_processing Specifically, IRCAN first identifies neurons that significantly contribute to context processing, utilizing a context-aware attribution score derived from integrated gradients.
__label__machine_vision Traditional UDF learning methods typically require extensive training on large datasets of 3D shapes, which is costly and often necessitates hyperparameter adjustments for new datasets.
__label__infrastructure We first present the problems of scaling MoE training in existing systems and highlight the potential of exploiting token similarity to facilitate data compression.
__label__reinforcement_learning We evaluated RESeL in 18 POMDP tasks, including classic, meta-RL, and credit assignment scenarios, as well as five MDP locomotion tasks.
__label__machine_learning_for_other_sciences_and_fields An implementation of LMI is available at *latentmi.readthedocs.io.
__label__deep_learning_architectures Currently, vision encoder models like Vision Transformers (ViTs) typically excel at image recognition tasks but cannot simultaneously support text recognition like human visual recognition.
__label__machine_learning_for_physical_sciences However, current RL-based placement methods suffer from long training times, low generalization ability, and inability to guarantee PPA results.
__label__deep_learning_architectures In this work, we adopt a distributional approach to the problem of identifying important components, leveraging the recently proposed discriminative filters hypothesis, which states that well-trained (convolutional) models possess discriminative filters that are essential to prediction.
__label__optimization Our theoretical analysis proves that CRONOS converges to the global minimum of the convex reformulation under mild assumptions.
__label__interpretability_and_explainability However, the reliability and generalisation properties of this approach are unknown.
__label__learning_theory We propose the first communication-efficient protocols that achieve near-optimal regret in these settings, even against a strong adversary who can choose the inputs adaptively.
__label__reinforcement_learning EMIT constructs a sequence of empirical MDPs using data from the growing replay memory.
__label__reinforcement_learning Our analysis is made possible through C-Procgen, a benchmark we build upon Procgen that enables explicit control of the contexts.
__label__neuroscience_and_cognitive_science The present study formulates the motion planning as a Lie group operator search problem, and uses the 1D rotation group as an example to provide insight into general operator search in neural circuits.
__label__learning_theory The key argument is to replace the empirical risk (seen as a function of hypotheses) in the generalisation bound by its projection onto a constructible low dimensional functional space: these projections can be queried much more efficiently than the initial risk.
__label__machine_vision Our method employs a two-phase alternating optimization framework, similar to Expectation-Maximization (EM), where one phase reduces the geometry gap and the other addresses the modality gap.
__label__natural_language_processing Upon well-designed PedCorpus, we propose PediatricsGPT, the first Chinese pediatric LLM assistant built on a systematic and robust training pipeline.
__label__machine_learning_for_physical_sciences Exploiting symmetry inherent in data can significantly improve the sample efficiency of a learning procedure and the generalization of learned models.
__label__machine_vision Following this, accurate sparse reconstructions are necessary for further dense modeling, which is then input into task-specific neural networks.
__label__machine_learning_for_physical_sciences Inferring these multimodal functions from experimental data is a central task in modern protein engineering.
__label__optimization We propose an algorithm that features a combination of a bootstrapping stage and a mirror-descent stage.
__label__safety_in_machine_learning (2023) in order to adaptively issue weaker guarantees when they are required to preserve the utility of the output.
__label__natural_language_processing Specifically,  our Llama3-RankRAG-8B and Llama3-RankRAG-70B significantly outperform Llama3-ChatQA-1.5-8B and Llama3-ChatQA-1.5-70B, respectively, on nine general knowledge-intensive benchmarks for RAG.
__label__learning_theory While such estimators exist for MDPs and POMDPs can be converted to history-based MDPs, their estimation errors depend on the state-density ratio for MDPs which becomes history ratios after  conversion, an exponential object.
__label__safety_in_machine_learning Recent research has uncovered that federated learning (FL) systems are vulnerable to various security threats.
"__label__algorithmic_game_theory We receive a prediction for each agent's location, and these predictions are crucially allowed to be only ""mostly"" and ""approximately"" correct (MAC for short): a $\delta$-fraction of the predicted locations are allowed to be arbitrarily incorrect, and the remainder of the predictions are required to be correct up to an $\varepsilon$-error."
__label__natural_language_processing Creating high-quality scientific figures can be time-consuming and challenging, even though sketching ideas on paper is relatively easy.
"__label__machine_vision Sketch Re-identification (Sketch Re-ID), 
which aims to retrieve target person from an image gallery based on a sketch query, is crucial for criminal investigation, law enforcement, and missing person searches."
__label__causal_inference We conduct extensive experiments on synthetic data and demonstrate the effectiveness of LiNGCReL in the finite-sample regime.
__label__reinforcement_learning As a by-product, we show that Reward-Free Exploration (RFE) enjoys the same worst-case rate, improving over the state-of-the-art lower bound.
__label__interpretability_and_explainability We show that, for data with *underlying community structure*, PCA significantly reduces the distance of data points belonging to the same community while reducing inter-community distance relatively mildly.
__label__human-AI_interaction In particular, we reveal the value of market information by showing that a company who deploys later after knowing their competitor’s price can always secure cost-effectiveness on at least one task, whereas the company who is the first-to-market must price their model in a way that incentivizes higher prices from the latecomer in order to gain revenue.
__label__other The key elements of our approach involve quantifying the uncertainty of response predictions via predictive inference and addressing individual and interactive constraints in a sequential manner.
__label__online_learning Additionally, the proposed method can be seamlessly integrated into existing continual learning methods yielding significant performance improvement.
__label__reinforcement_learning With this paper, we present a novel exploration strategy that overcomes the limitations of existing methods and guarantees convergence to an optimal policy even when rewards are not always observable.
__label__machine_learning_for_other_sciences_and_fields Progress in autoformalization research is hindered by the lack of a sizeable dataset consisting of informal-formal pairs expressing the same essence.
__label__optimization_for_deep_networks We propose a new variant of the Adam optimizer called MicroAdam that specifically minimizes memory overheads, while maintaining theoretical convergence guarantees.
__label__diffusion_based_models In contrast, synthesis-based methods generate entirely new content, greatly enhancing informativeness.
__label__neuroscience_and_cognitive_science In our work, we develop Switching Recurrent Neural Networks (SRNN), RNNs with weights that switch across time, to reconstruct switching dynamics of neural time-series data.
__label__machine_vision We show that MooG can provide a strong foundation for different vision tasks when compared to “on-the-grid” baselines.
__label__algorithmic_game_theory All else equal, monoculture (1) selects less-preferred applicants when noise is well-behaved, (2) matches more applicants to their top choice, though individual applicants may be worse off depending on their value to decision-makers and risk tolerance, and (3) is more robust to disparities in the number of applications submitted.
__label__diffusion_based_models In our approach, we strive to explore this problem by synthesizing human motion with interactions for a group of characters of any size in a zero-shot manner.
"__label__optimization Preliminary numerical experiments support our theoretical
findings, demonstrating superior performance in convergence speed and scalability."
__label__robotics Our framework exhibits notable advancement in real-world robotic tasks and achieves state-of-the-art on CALVIN benchmark, improving by 8% over previous open-loop counterparts.
__label__generative_models We apply our approach at scale to two very different games: chess (47,864 players) and Rocket League (2,000 players).
__label__robotics Vision-language navigation (VLN) requires an agent to execute actions following human instructions.
__label__safety_in_machine_learning In this work, we propose the Adversarial Compression Ratio (ACR) as a metric for assessing memorization in LLMs.
__label__machine_vision While optimizing prompts on downstream labeled data has proven effective in improving performance, these methods entail labor costs for annotations and are limited by their quality.
__label__learning_theory On the practical side, we explore two simple methods for learning projection matrices: PCA- and gradient-based methods.
__label__generative_models This debiasing strategy contributes to advancing the reliability and applicability of synthetic data in statistical inference.
__label__safety_in_machine_learning This new attack does not require parameter tuning and further degrades the accuracy, up to 81\% points compared to the previous gradient attacks.
__label__machine_learning_for_other_sciences_and_fields Compared to the strongest baselines, it improves accuracy from 0.688 to 0.744 and ROC AUC from 0.786 to 0.832 while maintaining stronger calibration.
__label__natural_language_processing We demonstrate that existing models exhibit a significant *alignment gap* -- *i.e.
__label__probabilistic_methods At the same time, by leveraging the notion of conditional Gaussianity, it also incorporates the adaptability from generative models to training data.
__label__machine_learning_for_physical_sciences We also find that consistency training can directly leverage force and off-equilibrium structure data to improve structure prediction, demonstrating a broad capability for integrating heterogeneous data.
__label__safety_in_machine_learning This paper addresses this gap by systematically analyzing the Rashomon effect and predictive multiplicity in gradient boosting algorithms.
__label__machine_learning_for_other_sciences_and_fields To address these challenges, we propose AdaNovo, a novel and domain knowledge-inspired framework that calculates Conditional Mutual Information (CMI) between the mass spectra and amino acids or peptides, using CMI for robust training against above biases.
__label__machine_learning_for_physical_sciences L-GATr represents high-energy data in a geometric algebra over four-dimensional space-time and is equivariant under Lorentz transformations, the symmetry group of relativistic kinematics.
__label__privacy Our approach to the problem relies on discovering and exploiting similar subpopulations of users which are often present and latent in real-world data, while minimizing user privacy leakage at the same time.
__label__machine_learning_for_other_sciences_and_fields To address temporal distribution shifts, we present Drift-Resilient TabPFN, a fresh approach based on In-Context Learning with a Prior-Data Fitted Network that learns the learning algorithm itself: it accepts the entire training dataset as input and makes predictions on the test set in a single forward pass.
__label__natural_language_processing We investigate the radioactivity of text generated by large language models (LLM), \ie whether it is possible to detect that such synthetic input was used to train a subsequent LLM.
__label__natural_language_processing In particular, we boost the open-sourced LLaMA2-70B model to achieve comparable performance against GPT-3.5-Turbo as the halluci- nation evaluator.
__label__optimization We address the challenges of Byzantine-robust training in asynchronous distributed machine learning systems, aiming to enhance efficiency amid massive parallelization and heterogeneous compute resources.
__label__diffusion_based_models Our evaluation demonstrates that our method effectively removes target classes from recent diffusion-based generative models and concepts from stable diffusion models while maintaining close alignment with the models' original trained states, thus outperforming state-of-the-art baselines.
__label__graph_neural_networks By utilizing a small subset of samples generated by various graph adversarial attack methods, we reconstruct the attack policy, closely approximating the performance of the original attack method.
__label__generative_models Motivated by this, we explore the question of whether LLMs can be empowered to think (predict and score) backwards to provide unsupervised feedback that complements forward LLMs.
__label__diffusion_based_models Can we generate a control policy for an agent using just one demonstration of desired behaviors as a prompt, as effortlessly as creating an image from a textual description?
__label__safety_in_machine_learning We first propose a novel \textbf{T}ri-level learning framework for \textbf{T}ime \textbf{S}eries \textbf{O}OD generalization, termed TTSO, which considers both sample-level and group-level uncertainties.
__label__natural_language_processing Existing methodologies usually prioritize a global optimization for finding the global optimum, which however will perform poorly in certain tasks.
__label__machine_vision Spike cameras, sampling at rates up to 40,000 Hz, have proven effective in capturing motion features and beneficial for solving this ill-posed problem.
__label__natural_language_processing To solve this problem, we introduce the MoGU framework, designed to enhance LLMs' safety while preserving their usability.
__label__generative_models Recently, it was shown that GFlowNets can be framed as a hierarchical variational inference (HVI) method for discrete distributions.
__label__natural_language_processing Furthermore, KG-FIT yields substantial performance gains of 12.6\%, 6.7\%, and 17.7\% compared to the structure-based base models upon which it is built.
__label__probabilistic_methods Precision matrix estimation is a ubiquitous task featuring numerous applications such as rare disease diagnosis and neural connectivity exploration.
__label__probabilistic_methods Our framework encompasses a wide range of well-known and recently proposed decision making problems as specific cases.
__label__machine_learning_for_other_sciences_and_fields Our software is available at https://github.com/qlu-lab/psps.
__label__machine_vision As such, it is hard to further use the fine-tuned model when it encounters classes beyond the fine-tuning data.
__label__optimization_for_deep_networks Despite fine-tuning techniques represented by visual prompt tuning (VPT) achieving substantial performance improvement by leveraging pre-trained knowledge, models still exhibit unsatisfactory generalization performance on tail classes.
__label__other Our approach consistently outperforms fourteen state-of-the-art baselines, being robust against symmetric and random Gaussian label noise.
__label__machine_learning_for_healthcare On the other hand, textual descriptions of proteins, which could be annotated by human experts or a pretrained protein sequence-to-text model, provide meaningful context that could assist in the functional annotations, such as the localization of active sites.
__label__optimization This paper proposes a novel framework that navigates MO to SOI by directly leveraging human feedback without being restricted by a predefined reward model nor cumbersome model selection.
__label__safety_in_machine_learning Our analysis leads to an upper bound on transferability in terms of task-relatedness, quantified using the difference between the class priors, label sets, and features of the two tasks.Our experiments using state-of-the-art pre-trained models show the effectiveness of task-relatedness in explaining transferability on various vision and language tasks.
__label__natural_language_processing Categorizing the situations into 36 factors, we conduct a human evaluation involving more than 1,200 subjects worldwide.
__label__machine_vision To address these issues, we develop a pixel-level AUC loss function and conduct a dependency-graph-based theoretical analysis of the algorithm's generalization ability.
__label__diffusion_based_models First, we experimentally confirm the suboptimal performance of existing APO methods on unseen models.
__label__machine_vision We identify that this discrepancy arises due to the overlooked impact of fragment count per Gaussian (i.e., the number of pixels each Gaussian is projected onto).
__label__machine_learning_for_other_sciences_and_fields However, the transition of experimental protocols, originally crafted for human comprehension, into formats interpretable by machines presents significant challenges, which, within the context of specific expert domain, encompass the necessity for structured as opposed to natural language, the imperative for explicit rather than tacit knowledge, and the preservation of causality and consistency throughout protocol steps.
__label__safety_in_machine_learning However, the safety of cross-modality scenarios, which are more commonly encountered in practical applications involving images captured by infrared cameras, has not received adequate attention.
__label__causal_inference Specifically, first, we theoretically demonstrate that when unobserved confounders (UCs) exist in an MDP, the learner is generally unable to imitate expert performance.
__label__optimization_for_deep_networks Several challenges make it difficult for sparse neural networks to compete with dense models.
__label__machine_vision Code is available at https://github.com/beautyremain/ProDet.
__label__graph_neural_networks We then devise an efficient graph learning algorithm operating directly on ICG in linear memory and time with respect to the __number of nodes__ (rather than edges).
__label__other Additionally, we introduce a sequence representation guided gate function that generates customized expert participation weights for each user sequence, which allows dynamic parameter adjustment for instance-wise recommendations.
__label__safety_in_machine_learning Fine-tuning experiments on GSM8K, ChatDoctor, and OpenOrca show that PTST significantly reduces the rise of unsafe behaviors.
__label__deep_learning_architectures This score identifies large clusters of similar tokens as high-energy, indicating potential candidates for merging, while smaller (unique and isolated) clusters are considered as low-energy and preserved.
__label__natural_language_processing Experiments suggest that InfoNCA/NCA surpasses various preference baselines when reward datasets are available.
__label__machine_vision Knowledge dissemination combines knowledge from cloud detector and CLIP model to initialize a target detector and a CLIP detector in target domain.
__label__diffusion_based_models Our LDM-based model incorporates an effective and efficient mechanism, CacheKV, for conditioning on reference images.
__label__reinforcement_learning Addressing the limitations of traditional Chain of Thought (CoT) methods, we introduce the Chain of Summarization (CoS) method, enhancing LLMs' capabilities in rapid and effective decision-making.
__label__learning_theory In this work, we study SSL for high dimensional sparse Gaussian classification.
__label__machine_vision Event cameras harness advantages such as low latency, high temporal resolution, and high dynamic range (HDR), compared to standard cameras.
__label__machine_learning_for_other_sciences_and_fields Priority queues are one of the most fundamental and widely used data structures in computer science.
__label__optimization Our findings indicate the significance of accounting for architecture-specific invariances in achieving LMC.
__label__generative_models We theoretically prove that transcendence is enabled by low-temperature sampling, and rigorously assess this experimentally.
__label__bandits Ours is the first result in any structured setting not to require the size of the ensemble to scale linearly with $T$---which defeats the purpose of ensemble sampling---while obtaining  near $\smash{\sqrt{T}}$ order regret.
__label__robotics Specifically, in the first stage, VidMan is pre-trained on the Open X-Embodiment dataset (OXE) for predicting future visual trajectories in a video denoising diffusion manner, enabling the model to develop a long horizontal awareness of the environment's dynamics.
__label__generative_models Accompanying this, we propose a stitching framework on the triplane domain to get the best predictions from both.
__label__machine_vision LLM and LVLM could still answer some visual-necessary questions without visual content, indicating the memorizing of these samples within large-scale training data.
__label__bandits We propose the three-batch best arm identification (Tri-BBAI) algorithm, which is the first batched algorithm that achieves the optimal sample complexity in the asymptotic setting (i.e., $\delta\rightarrow 0$) and runs in $3$ batches in expectation.
__label__machine_learning_for_other_sciences_and_fields Designing a fast and effective entropy model is challenging but essential for practical application of neural codecs.
__label__optimization_for_deep_networks Sharpness-aware minimization (SAM) improves generalization of various deep learning tasks.
__label__causal_inference Additionally, we prove the statistical validity of our estimator for the best subgroup treatment effect, demonstrating its asymptotic normality and semiparametric efficiency.
__label__interpretability_and_explainability Extensive experiments and ablation studies demonstrate that our model outperforms state-of-the-art models by up to 10.1\% in prediction accuracy across a wide range of tasks.
__label__optimization To our best knowledge, D-AdaST is the *first* distributed adaptive method achieving near-optimal convergence without knowing any problem-dependent parameters for nonconvex minimax problems.
__label__natural_language_processing While several approaches have been proposed to enhance LLMs' context awareness, achieving both effectiveness and efficiency remains challenging.
__label__reinforcement_learning In popular MuJoCo and DeepMind Control Suite (DMC) environments, we find common phenomena for TD3 and RAD agents: (1) the activity of policy network parameters is highly asymmetric and policy networks advance monotonically along a very limited number of major parameter directions; (2) severe detours occur in parameter update and harmonic-like changes are observed for all minor parameter directions.
__label__speech_and_audio To address this gap, we introduce a novel model for simulating the spatio-temporal motion of nonlinear strings, integrating modal synthesis and spectral modeling within a neural network framework.
__label__machine_learning_for_other_sciences_and_fields While several verified lifting tools have been developed for various application domains, they are specialized for specific source-target languages or require significant expertise in domain knowledge to make the search efficient.
__label__diffusion_based_models Additionally, an attribute concentration module is proposed to map the text conditions of each entity to its corresponding image area correctly.
__label__natural_language_processing To relieve this problem, we tailor an abstract reasoning dataset (AbsR) together with a meaningful learning paradigm to teach LLMs how to leverage generic facts for reasoning purposes.
__label__machine_vision However, these NeRF methods suffer from time-consuming training and inference, as well as limited scene-editing capabilities of implicit representations.
__label__diffusion_based_models Our method yields diffreps with substantially **improved quality and diversity** for images, panoramas, and 3D NeRFs compared to existing techniques.
__label__deep_learning_architectures Nevertheless, the inherent autoregressive property and decoder-only architecture of LLMs have not been fully considered, resulting in insufficient utilization of LLM abilities.
__label__optimization_for_deep_networks While on-device inference is a well-explored topic in recent research, backpropagation remains an open challenge due to its prohibitive computational and memory costs compared to the extreme resource constraints of embedded devices.
__label__machine_vision The recent advancements in large-scale pre-training techniques have significantly enhanced the capabilities of vision foundation models, notably the Segment Anything Model (SAM), which can generate precise masks based on point and box prompts.
__label__diffusion_based_models In this work, we propose DomainGallery, a few-shot domain-driven image generation method which aims at finetuning pretrained Stable Diffusion on few-shot target datasets in an attribute-centric manner.
__label__learning_theory In the literature of studying training dynamics of transformers, several simplifications are commonly adopted such as weight reparameterization, attention linearization, special initialization, and lazy regime.
__label__learning_theory Heterogeneity, e.g., due to different types of layers or multiple sub-models, poses key challenges in analyzing the generalization behavior of several modern architectures.
__label__diffusion_based_models To address these limitations, we introduce Neural Flow Diffusion Models (NFDM), a novel framework that enhances diffusion models by supporting a broader range of forward processes beyond the standard Gaussian.
__label__optimization Moreover, we prove convergence of our methods assuming only local Lipschitzness of the gradient.
__label__probabilistic_methods Mean-field variational inference (VI) is computationally scalable, but its highly-demanding independence requirement hinders it from being applied to wider scenarios.
__label__learning_theory Specifically, we monitor the evolution of signal learning and noise memorization in prompt-based federated learning, demonstrating that performance can be assessed by the ratio of task-relevant to task-irrelevant coefficients.
__label__causal_inference We theoretically demonstrate the correctness of the proposed algorithm.
__label__human-AI_interaction Moreover, existing Multimodal Large Language Models (MLLMs) face challenges in integrating audio and recognizing subtle facial micro-expressions.
__label__interpretability_and_explainability Second, we propose a rationale-informed optimization method to guide the model in disentangling and localizing visual evidence for each rationale, without requiring manual annotations.
__label__generative_models Disabled people constitute a significant part of the global population, deserving of inclusive consideration and empathetic support.
__label__machine_vision Deep supervised models possess significant capability to assimilate extensive training data, thereby presenting an opportunity to enhance model performance through training on multiple datasets.
__label__deep_learning_architectures Through analysis, we found the contribution ratio of Multi-Head Attention (a combination function) to pre-trained language modeling is a key factor affecting base capabilities.
__label__generative_models This tailored diffusion process is anchored by initial and target geometric states as fixed endpoints and governed by equivariant transition kernels.
__label__machine_vision The dynamic prototype is used to encourage variety.
__label__fairness We also provide some theoretical analysis of the generalization error bound, and based on this bound we give a strategy to set the hyper-parameter, which makes the proposed methods easy to use.
__label__machine_learning_for_other_sciences_and_fields Extensive experiments using diverse real-world time-series datasets confirm the superiority of RECURVE over state-of-the-art methods.
__label__natural_language_processing The code and demo are available at https://github.com/sail-sg/scaling-with-vocab and https://hf.co/spaces/sail/scaling-with-vocab-demo.
__label__generative_models This enables to construct geometrically expressive messages between residues, including higher order terms, using the bilinear operations of the algebra.
__label__machine_learning_for_other_sciences_and_fields Concretely, we introduce a lightweight personalized adapter into PLMs and endows it with weather pattern awareness.
__label__natural_language_processing It is worth noting that we are not proposing \ours as a replacement for the current instruction tuning process.
__label__machine_learning_for_physical_sciences We demonstrate the accuracy, force computation efficiency, and robustness of our method on learning macroscopic closure models from a variety of microscopic systems, including those modeled by partial differential equations or molecular dynamics simulations.
__label__other Though Bayesian inference can alleviate this issue, a direct posterior inference at clients may result in biased local posterior estimates due to data heterogeneity, leading to a sub-optimal global posterior.
__label__natural_language_processing The characters in the TV-series Friends are more difficult to distinguish using this method (AUCs range from 0.61 to 0.66).
__label__bandits These bounds have matching behavior in $T$, exponential dependence on $L$, and polynomial dependence on $d$ (with the gap $d\ $).
"__label__optimization_for_deep_networks We find that classical learning rate tuners
may yield greater one-step loss reduction, yet they ultimately underperform in
the long term when compared to constant learning rates in the full batch regime."
__label__other While the revision is minimal, we highlight three merits of PSL: 1) it serves as a tighter surrogate for DCG with suitable activation functions; 2) it better balances data contributions; and 3) it acts as a specific BPR loss enhanced by Distributionally Robust Optimization (DRO).
__label__optimization_for_deep_networks We view the DPS problem as posterior inference in a novel Bayesian model where the posterior distributions of the instance-wise weights and the main neural network parameters are inferred under a reasonable prior and likelihood model.
__label__causal_inference Causal effect identification and estimation are two crucial tasks in causal inference.
__label__diffusion_based_models The extracted poses are also used to train a cascaded diffusion model to enable the generation of novel poses.
__label__reinforcement_learning Exploration in reinforcement learning (RL) remains an open challenge.
__label__machine_vision Source code and demos are available at: https://yxlao.github.io/lit.
__label__generative_models Recent advancements in 2D/3D generative techniques have facilitated the generation of dynamic 3D objects from monocular videos.
__label__machine_learning_for_other_sciences_and_fields First, by using the learning scheme, NeuralSteiner ensures the connectivity of generated routes while significantly reducing congestion.
__label__neuroscience_and_cognitive_science How reliable are the mechanistic insights derived from this procedure?
__label__machine_learning_for_healthcare Despite the adoption of various techniques, multi-modal biological data, which can provide crucial insights into a patient's overall health, is often overlooked.
__label__learning_theory To address these issues, we introduce two important FL features into the model.
__label__natural_language_processing The advent of Large Language Models (LLMs) presents new avenues for automating EA with annotations, inspired by their comprehensive capability to process semantic information.
__label__machine_learning_for_physical_sciences We demonstrate the ability of our method to find feasible transition paths on real-world molecular simulation and protein folding tasks.
__label__safety_in_machine_learning This issue is exacerbated for the setting of class-conditional coverage on imbalanced classification tasks with many and/or imbalanced classes.
__label__human-AI_interaction Given these characteristics, we explore the problem of how developers of new generative AI software can release and price their technology.
__label__safety_in_machine_learning When such backdoors exist, they allow the designer of the model to sell information on how to slightly perturb their input to change the outcome of the model.
__label__reinforcement_learning Given the simplicity of our method, skills extracted from 1\% of the demonstrations in one task can be transferred to a new loosely related task.
__label__machine_vision Our approach learns features within a specific radius around each query point and utilizes an attention mechanism to focus on the crucial features for UDF estimation.
__label__privacy By contrast, previous audits were only (relatively) tight in stronger white-box models, where the adversary can access the model's inner parameters and insert arbitrary gradients.
__label__machine_vision Finally, the explored pseudo-labels can be adopted to guide the optimization of aerial object detector in a closed-looping progressive fashion.
__label__learning_theory This resolves an open problem by Schlisserman et al.23 and Amir er Al.21, and improves over previous lower bounds that demonstrated that the sample size must be at least square root of the dimension.
__label__algorithmic_game_theory We then show how to stabilize PTB$^+$ with a stepsize, resulting in an algorithm with a state-of-the-art $O(1/T)$ convergence rate.
__label__machine_learning_for_other_sciences_and_fields Notably, when transferred to public datasets, PowerPM retains its edge, showcasing its remarkable generalization ability across various tasks and domains.
__label__machine_vision the joint angles of a predefined skeleton.
__label__learning_theory These findings shed light on the effectiveness of few-shot prompting and the roles of task diversity and representation learning for ICL.
__label__machine_vision The supplementary material includes the source code for reproducibility.
__label__machine_vision However, representations obtained from contrastive learning enhance the discriminability of the model but often lack fine-grained information, which is suboptimal in the pixel-level prediction tasks.
__label__natural_language_processing Project is available at https://minicache.vmv.re .
__label__machine_vision In this paper, we propose a conditional sample weighting (CoSW).
__label__natural_language_processing The experimental results on both in-domain and out-of-domain datasets demonstrate that even without GPT-4 or human-annotated process supervision, our AlphaMath framework achieves comparable or superior results to previous state-of-the-art methods.
__label__diffusion_based_models Articulated object manipulation in real images is a fundamental step in computer and robotic vision tasks.
__label__privacy In this paper, we introduce a new family of adversarial attacks that strike a balance between effectiveness and computational efficiency.
__label__learning_theory Additionally, we demonstrate the key role of minimizability gaps in comparing excess error bounds and $H$-consistency bounds.
__label__other Experimentally, we extend pre-trained audio-text and 3D-image representations to the existing vision-text space.
__label__deep_learning_architectures We propose a new perspective to reconsider the Fourier transform from a basis functions perspective.
__label__learning_theory Furthermore, we describe how various forms of data balancing in contrastive multimodal learning and self-supervised clustering can be better understood, and even improved upon, owing to our variance reduction viewpoint.
__label__machine_vision A hierarchical loss that incorporates human semantic information is devised to achieve high-fidelity texture modeling and impose stronger constraints on the estimated multiple views.
__label__learning_theory \textbf{Under that same framework, we then introduce a statistical notion of alignment, and lower-bound the jailbreaking probability, showing that it is unpreventable under reasonable assumptions.}
"__label__bandits By leveraging the interpretation of Prod as a first-order OMD approximation, we present the following surprising results:
1."
__label__machine_learning_for_healthcare FoldFlow++ presents substantial new architectural features over the previous FoldFlow family of models including a protein large language model to encode sequence, a new multi-modal fusion trunk that combines structure and sequence representations, and a geometric transformer based decoder.
__label__generative_models In this paper, we extend these guidance methods to generalized guidance formulation for MGMs and propose a self-guidance sampling method, which leads to better generation quality.
__label__safety_in_machine_learning forgetting undesirable (mis)behaviors, on large language models (LLMs).
__label__machine_vision More importantly, our FAM is fully-automated without the need for pre-cutting and can deal with highly-complex topologies, since its learning process adaptively finds reasonable cutting seams and UV boundaries.
__label__natural_language_processing Using only a zero-shot setting, our method achieves an average absolute improvement of 4\% across all tasks, highlighting its potential to improve the reasoning performance of LLMs.
__label__natural_language_processing Analysis of Neural Tangent Kernels (NTKs) reveals that those long-tail data are commonly overlooked in the model's gradient updates and, consequently, are not effectively memorized, leading to poor domain-specific downstream performance.
__label__learning_theory In addition, a finite training set limits the resolution of correlations to an effective range, whose size grows with that of the training set.
__label__machine_vision In this paper, we exploit advancements in Multimodal Large Language Models (MLLMs), particularly GPT-4V, to present a novel approach, Make-it-Real: 1) We demonstrate that GPT-4V can effectively recognize and describe materials, allowing the construction of a detailed material library.
__label__online_learning Finally, we apply our results to give offline oracle-efficient algorithms for interactive decision making.
__label__neuroscience_and_cognitive_science We find that learned plasticity adjusts both excitation and inhibition in response to manipulations, outperforming rules applied only to excitatory connections.
__label__natural_language_processing Despite recent progress made by large language models in code generation, they still struggle with programs that meet complex requirements.
__label__learning_theory Using RSS, we establish nearly matching upper and lower bounds on the excess risk.
__label__machine_learning_for_physical_sciences Consequently, the limitations in the temporal scale of datasets prevent these models from forecasting at finer time scales.
__label__generative_models In contrast, our approach adopts the **feature reconstruction** objective, where tokenizers are trained by distilling knowledge from pretrained IU encoders.
__label__bandits In these settings, naively comparing treatment and control groups that may result from self-selection can lead to biased estimates of underlying treatment effects.
__label__algorithmic_game_theory This problem was recently revisited through the learning-augmented framework, aiming to move beyond worst-case analysis and design truthful mechanisms that are augmented with (machine-learned) predictions.
__label__reinforcement_learning BRO is the first model-free algorithm to achieve near-optimal policies in the notoriously challenging Dog and Humanoid tasks.
__label__machine_learning_for_physical_sciences Species range maps (SRMs) are essential tools for research and policy-making in ecology, conservation, and environmental management.
__label__machine_vision This gap hinders accurate sensory grounding in real-world scenarios.
__label__machine_learning_for_social_sciences Our experiments with two real-world trajectory datasets demonstrate that LoTNext significantly surpasses existing state-of-the-art works.
__label__machine_vision We show via a generalization bound that the flatness can be deemed as model variance, while the minima depend on the domain distribution distance for the DAOD task.
__label__natural_language_processing However, these methods fail to allocate adaptive layer-wise sparsities, leading to performance degradation in challenging tasks.
__label__algorithmic_game_theory In this work, we demonstrate, both theoretically and empirically, that a purely relevance-driven policy with low exploration strength boosts short-term user satisfaction but undermines the long-term richness of the content pool.
__label__privacy We consider the \emph{slicing privacy mechanism} that injects noise into random low-dimensional projections of the private data, and provide strong privacy guarantees for it.
__label__machine_vision The code and distilled datasets will be made public.
__label__learning_theory In multi-output regression, we identify a previously neglected challenge that arises from the inability of training distribution to cover all combinations of input features, leading to combinatorial distribution shift (CDS).
__label__reinforcement_learning Goal-conditioned reinforcement learning is a powerful way to control an AI agent's behavior at runtime.
__label__natural_language_processing While they deliver powerful performance, they also face a set of common optimization needs to meet specific requirements or standards, such as instruction following or avoiding the output of sensitive information from the real world.
__label__machine_vision Empirical evaluations confirm the capacity of the model to generate convincing guide hair and dense strands, complete with nuanced high-frequency details.
__label__learning_theory In experiments, we show that NCP with a 2-hidden-layer network matches or outperforms leading methods.
__label__machine_vision Extensive experiments demonstrate that our proposed UPS achieves state-of-the-art performance relative to leading lightweight SISR methods, as verified by various popular benchmarks.
__label__natural_language_processing Overall, our results make substantial strides toward detaching LMs from their tokenizer.
__label__neuroscience_and_cognitive_science However, there is still a lack of dimensionality reduction methods that can effectively align low-dimensional latent dynamics with movements.
__label__diffusion_based_models Existing methods, though effective in various individual applications, often lack theoretical grounding and rigorous testing on extensive benchmarks.
__label__machine_vision Semi-supervised learning (SSL) offers a robust framework for harnessing the potential of unannotated data.
__label__privacy Finally, we perform some illustrative experiments on simulated data showing that our algorithm can quickly identify active features under reasonable privacy budget constraints.
__label__learning_theory We additionally prove results about stable-rank preserving functions that are potentially useful in algorithmic design, and more.
__label__machine_vision We introduce GeCo, a novel low-shot counter that achieves accurate object detection, segmentation, and count estimation in a unified architecture.
__label__safety_in_machine_learning On Harmbench, our approach achieves the highest attack success rate on seven out of eight LLMs compared to the latest jailbreak methods.
__label__generative_models Text-conditioned human motion generation has experienced significant advancements with diffusion models trained on extensive motion capture data and corresponding textual annotations.
__label__algorithmic_game_theory Nonetheless, this fundamental result relies on the assumption that bargainers possess perfect knowledge of the underlying game.
__label__privacy Then, we propose a novel MIA pipeline specifically designed for token-level image detection.
__label__human-AI_interaction Here, the price optimization problem becomes piecewise continuous where the companies must choose a subset of the tasks on which to be cost-effective and forgo revenue for the remaining tasks.
__label__machine_vision However, PTMs are often ''black-box,'' where information on such models is unavailable for commercial reasons or social responsibilities.
__label__other For our second algorithm, we use *empirical $k$-thresholding* operation for the sparsification step, and under the assumption that data lie on a low dimensional manifold of dimension $d_0\ll d$, we show that the convergence rate of this classifier depends only on $d_0$ and is again minimax-optimal.
__label__deep_learning_architectures The other branch employs patches to capture short-term information and aggregate the global representation of the series.
__label__optimization For both \textbf{GS-JOBCD} and \textbf{VR-J-JOBCD}, we establish the oracle complexity under mild conditions and strong limit-point convergence results under the Kurdyka-Lojasiewicz inequality.
__label__natural_language_processing In this paper, we perceive the LLMs' knowledge boundary with semi-open-ended questions by discovering more ambiguous answers.
"__label__machine_vision In this paper, rather than focusing on representation architectures, which
is a common focus in many existing works, we propose a novel INR-based video
compression framework, Neural Video Representation Compression (NVRC),
targeting compression of the representation."
__label__probabilistic_methods To this end, we propose a novel approach FasMe for Fast and Sample-efficient Meta Precision Matrix Learning, which first extracts meta-knowledge through a multi-task learning diagram.
__label__learning_theory There is a recent and growing body of literature that proposes the framework of fractals to model optimization trajectories of neural networks, motivating generalization bounds and measures based on the fractal dimension of the trajectory.
__label__learning_theory Experiments demonstrate that learning projection matrices from data is indeed beneficial: it leads to significantly higher solution quality than the existing random projection while greatly reducing the time for solving LPs.
__label__learning_theory they must fully recover the correct partition, even if up to $\ell$ answers are incorrect, for some error-tolerance parameter $\ell$.
__label__online_learning While we obtain those positive results for the stochastic setting, another main result of our work is an impossibility in the adversarial setting.
__label__machine_learning_for_other_sciences_and_fields Specifically, we introduce a strategy of diversifying hyper latent representations for forward adaptation, i.e., using two additional types of contexts along with the existing single type of context.
__label__optimization_for_deep_networks Class incremental learning (CIL) trains a network on sequential tasks with separated categories in each task but suffers from catastrophic forgetting, where models quickly lose previously learned knowledge when acquiring new tasks.
__label__natural_language_processing Specifically, in grammar-constrained decoding (GCD), the LLM's output must follow a given grammar.
__label__machine_vision Our method demonstrates robust performance in completing object shapes under noisy detection and tracking conditions.
__label__causal_inference The model's inference capabilities allow for the immediate identification of intervention targets on unseen samples and novel causal graphs, circumventing the need for retraining.
__label__diffusion_based_models Our study aims to improve video diffusion distillation and meanwhile enabling the student model to improve frame appearance using the abundant high-quality image data.
__label__optimization_for_deep_networks Our work thereby underscores the generality of $\mathcal{NC}$ as it extends to the novel and more challenging setting of language modeling.
__label__interpretability_and_explainability We analyze this algorithm and show that it is a novel approach incorporating momentum and adaptive rescaling based on noise levels.
__label__reinforcement_learning In contrast to both intuition and theory, if the learned dynamics model is replaced by the true error-free dynamics, existing model-based methods completely fail.
__label__machine_vision Extensive experiments on a range of benchmark datasets demonstrate the superiority of PCD over state-of-the-art methods.
__label__reinforcement_learning During the decomposed prompt tuning phase, we learn both cross-task and task-specific prompts on training tasks to achieve prompt decomposition.
__label__privacy In this model, algorithms must always guarantee differential privacy with respect to the private samples while also ensuring learning guarantees when the private data distribution is sufficiently close to that of the public data.
__label__fairness However, real-life applications often do not require such centroids.
__label__deep_learning_architectures Code is available at https://github.com/LeapLabTHU/MLLA.
__label__machine_vision Moreover, the adapted models lack accurate fine-grained uncertainty quantification capabilities limiting their broader applicability in critical domains.
__label__optimization_for_deep_networks By combining three closely coupled components namely *cross attention projector*, *dual-view feature mimicking* and *teacher parameter perception* tailored to address the alignment problems stated above, we present a simple and effective knowledge distillation method, called *ScaleKD*.
__label__online_learning In this paper we show that /for linear losses, dynamic regret minimization is equivalent to static regret minimization in an extended decision space/.
__label__machine_vision By contrast, with simple architecture (comprising only two unimodal encoders) and just the contrastive objective, popular pre-trained VL models (e.g., CLIP) achieve superior performance in general domains, which are further easily extended to downstream tasks.
__label__diffusion_based_models We hope that our study provides new insights into understanding the data and pre-training processes of DMs.
__label__reinforcement_learning See our project website at https://sites.google.com/view/nips2024-omis.
__label__diffusion_based_models An appropriate discretisation schedule is crucial to obtain high quality samples.
__label__machine_learning_for_healthcare ii.
__label__active_learning However, many existing techniques are not robust to class imbalance and distributional shifts, and can suffer from poor worst-class generalization performance.
__label__machine_learning_for_other_sciences_and_fields We also suggest an efficient prototype-based negative sampling method to maintain high-quality negative samples for each label.
__label__optimization_for_deep_networks Neural Architecture Search (NAS) methods seek effective optimization toward performance metrics regarding model accuracy and generalization while facing challenges regarding search costs and GPU resources.
"__label__safety_in_machine_learning It
can measure goal-directedness with respect to a known utility function, a hypothesis
class of utility functions, or a set of random variables."
__label__online_learning The classical theory of statistical estimation aims to estimate a parameter of interest under data generated from a fixed design (''offline estimation''), while the contemporary theory of online learning provides algorithms for estimation under adaptively chosen covariates (''online estimation'').
__label__machine_vision We introduce a novel statistical reasoning algorithm in local regions which is able to finetune data-driven based priors without signed distance supervision, clean point cloud, or point normals.
__label__other To address this limitation, we propose an automatic outlier rectification mechanism that integrates rectification and estimation within a joint optimization framework.
__label__graph_neural_networks Utilizing long-range dependency, a concept extensively studied in homogeneous graphs, remains underexplored in heterogeneous graphs, especially on large ones, posing two significant challenges: Reducing computational costs while maximizing effective information utilization in the presence of heterogeneity, and overcoming the over-smoothing issue in graph neural networks.
__label__reinforcement_learning The associated estimation problems are thus easy when working with trajectory data, while they remain nontrivial when working with individual transitions.
__label__graph_neural_networks Our method minimizes the norms of intermediate variables involved in the different ways of computing the predicted tensor.
__label__learning_theory Our results build on a connection between testable learning and *fooling*.
__label__deep_learning_architectures Additionally, we design the system optimizations for intensive data transformation in pruned models.
__label__reinforcement_learning Then, we observe that the recently proposed cross-play minimization (XP-min) technique produces diverse and specialized partners.
__label__deep_learning_architectures Additionally, the balance between high performance and efficiency remains an under-explored problem for exposure correction task.
__label__machine_learning_for_physical_sciences Code is available at https://github.com/L-I-M-I-T/LatentNeuralOperator.
__label__reinforcement_learning Website, code and data: https://mazpie.github.io/genrl/
__label__safety_in_machine_learning Drawing inspiration from the generalization and the universal computation ability of pre-trained transformer models, we propose a novel method termed \textbf{CeTaD} (\textbf{C}onsidering Pr\textbf{e}-trained \textbf{T}ransformers \textbf{a}s \textbf{D}efenders) for RaPiD, optimized for efficient computation.
__label__reinforcement_learning Among which, context-based OMRL (COMRL) as a popular paradigm, aims to learn a universal policy conditioned on effective task representations.
__label__machine_learning_for_physical_sciences In this work, we formulate end-to-end language-to-structure generation as a multi-objective optimization problem, and propose Generative Hierarchical Materials Search (GenMS) for controllable generation of crystal structures.
__label__optimization These guarantees are worst-case and in particular, neither the analysis, ${\textit{nor the algorithm}}$, takes into account any potential structural information of the data.
__label__safety_in_machine_learning Finally, we identify two cases where ICL fails: one in logic reasoning tasks that require generalizing to new, unseen patterns, and another in analogy completion where relevant word pairs appear only in fixed training positions.
__label__machine_vision For this reason, instead of generating textures in automatically generated UV-planes like most state-of-the-art methods, we propose to represent textures as coloured point-clouds whose colours are generated by a denoising diffusion probabilistic model constrained to operate on the surface of 3D objects.
__label__deep_learning_architectures APM can process patches of an image one at a time in any order asymmetrically, and still encode semantic-awareness in the net.
__label__bandits In bandit best-arm identification, an algorithm is tasked with finding the arm with highest mean reward with a specified accuracy as fast as possible.
__label__machine_vision To address these limitations, we propose AltO, an unsupervised learning framework for estimating homography in multimodal image pairs.
__label__machine_vision This paper explores the weakly-supervised referring image segmentation (WRIS) problem, and focuses on a challenging setup where target localization is learned directly from image-text pairs.
__label__diffusion_based_models To numerically simulate the sampling process, a discretisation schedule from the reference back towards clean data must be chosen.
__label__other We propose a new method named LoD-Loc for visual localization in the air.
__label__deep_learning_architectures In this work, we present a novel FL framework grounded in Bayesian learning to address these challenges.
__label__other A key benefit of improved image diversity is that soft label compression can be achieved through simple random pruning, eliminating the need for complex rule-based strategies.
__label__interpretability_and_explainability However, it is crucial to recognize that the quality of cells within a single data point can vary greatly in practice.
__label__machine_vision In our approach, we developed a mask generator based on the denoising UNet from a pre-trained diffusion model, leveraging its capability for precise textual control over dense pixel representations and enhancing the open-world adaptability of the generated masks.
__label__other Our code is released at https://github.com/MMorafah/TAKFL and the project website is available at https://mmorafah.github.io/takflpage .
__label__machine_learning_for_other_sciences_and_fields For samples where all annotators agree, an aggregating strategy is designed to mitigate potential noise.
__label__machine_vision Recently, many works project the point cloud onto the 2D image and adopt the 2D Convolutional Neural Networks (CNNs) or vision transformer for LiDAR point cloud semantic segmentation.
__label__graph_neural_networks We illustrate that selecting appropriate initialization not only ensures performance on clean datasets but also enhances model robustness against adversarial perturbations, with observed gaps of up to 50\% compared to alternative initialization approaches.
__label__machine_learning_for_other_sciences_and_fields Our main technical tool is a novel non-uniform online learning framework, which may be of independent interest.
__label__deep_learning_architectures TPGN achieves a theoretical complexity of $\mathcal{O}(\sqrt{L})$, ensuring efficiency in its operations.
__label__robotics The experimental results show that, with the support of SCaR, the robot achieves a higher success rate in long-horizon tasks compared to relevant baselines and demonstrates greater robustness to perturbations.
__label__safety_in_machine_learning Recent advancements focus on utilizing easily accessible auxiliary outliers (e.g., data from the web or other datasets) in training.
__label__generative_models In this work, we present MuDI, a novel framework that enables multi-subject personalization by effectively decoupling identities from multiple subjects.
__label__machine_learning_for_other_sciences_and_fields However, it is difficult to identify, combine, and refine complementary information in an increasingly large and diverse knowledge base.
__label__machine_vision *, superior, medium, inferior) is better for two benefits: 1) It narrows the gap between adjacent levels, thereby encouraging MLLMs to discern subtle differences.
__label__other The community has designed various techniques to tackle this issue, among which Knowledge Distillation (KD)-based techniques are common.
__label__other This outcome suggests a homomorphic relationship between the language representation space and an effective recommendation space, implying that collaborative signals may indeed be encoded within advanced LMs.
"__label__learning_theory Our framework extends well-studied notions of stability, including Differential Privacy ($k = 0$), differentially private learning with public data (where the $k$ public datapoints are fixed in advance),
and stable sample compression (where the $k$ datapoints are selected adaptively by the algorithm)."
__label__machine_learning_for_other_sciences_and_fields We first develop a theoretical framework for characterizing the performance of such UNN-based methods.
__label__evaluation Surprisingly, practitioners cannot currently measure the uncertainty that any mismatch between human and model preferences may introduce in the constructed rankings.
__label__other Then, we select the best distribution within the rectification set to perform the estimation task.
__label__optimization_for_deep_networks While Forward-mode Auto-Differentiation (AD) can significantly reduce memory footprint from activations, we observe that directly applying it to LLM finetuning results in slow convergence and poor accuracy.
__label__optimization We apply our algorithms on different synthetic and real-world functions, and the results show the effectiveness of our method.
__label__machine_vision We introduce Cambrian-1, a family of multimodal LLMs (MLLMs) designed with a vision-centric approach.
__label__deep_learning_architectures This paper bridges this gap through the design and evaluation of the Elastic Time-Series Transformer (ElasTST).
__label__machine_learning_for_social_sciences Additionally, CogMir framework demonstrates its potential as a valuable platform for encouraging more research into the social intelligence of LLM Agents.
__label__learning_theory Our findings offer insights into the benefits of multi-head attention and contribute to understanding the more intricate mechanisms hidden within trained transformers.
__label__privacy Finally, we show theoretically that much larger batches can be reconstructed with high probability given exponential time.
__label__machine_vision Specifically, we design a Graph-guided State Space (GSS) block that learns the graph-structured relations and spatial sequences of joints and uses 88.5\% fewer tokens than attention-based methods.
__label__natural_language_processing Specifically, to handle diverse attribute knowledge descriptions, we design alignment-augmented abstract representation that incorporates the large language model and in-context learning into attribute alignment and filtering for generating and embedding the attribute abstract.
__label__bandits That results in a sublinear regret bound significantly lower than its counterpart in the independent task learning setting.
__label__other The results demonstrate that our approach outperforms previous approaches by 2.08\% (absolute) on average regarding cross-scene detection Accuracy.
__label__generative_models However, the preference datasets used in DAP methods are usually collected ahead of training and never updated, thus the feedback is purely offline.
__label__optimization_for_deep_networks To the best of our knowledge, this is the first result that matches the SQ lower bound for solving $k$-sparse parity problem using gradient-based methods.
__label__other Further, global thresholds are used to update model parameters by extracting aggregated parameter importance.
__label__machine_learning_for_healthcare Diverging from traditional approaches based on pixel-level classification, our proposed method, named GraphMorph, focuses on branch-level features of tubular structures to achieve more topologically accurate predictions.
__label__graph_neural_networks In this paper we address both by identifying and manipulating the key contributors to a problem's ``hardness'', known as cores.
__label__generative_models Further investigation reveals that this degradation is primarily due to the distortion of high-frequency components in long videos, characterized by a decrease in spatial high-frequency components and an increase in temporal high-frequency components.
__label__deep_learning_architectures Recent advancements have shown shifted focus towards element-wise multiplication to facilitate higher-dimensional feature interaction space for better information transformation.
__label__machine_vision 1) To identify the most effective configurations for multi-LiDAR systems, we introduce the Surrogate Metric of the Semantic Occupancy Grids (M-SOG) to evaluate LiDAR placement quality.
__label__fairness Inspired by cognitive theories on social communication, this paper introduces CulturePark, an LLM-powered multi-agent communication framework for cultural data collection.
__label__machine_vision Vanilla pixel-level classifiers for semantic segmentation are based on a certain paradigm, involving the inner product of fixed prototypes obtained from the training set and pixel features in the test image.
__label__optimization Our method ensures that each batch of synthetic data mirrors the characteristics of a large, varying subset of the original dataset.
__label__safety_in_machine_learning Our physical world experiments demonstrate the effectiveness of our FDA patterns across various detection models like YOLOv5, Deformable-DETR, and Mask RCNN.
__label__graph_neural_networks And a tailored Transformer-based backbone is adopted to learn meaningful node representations from these generated token sequences.
__label__optimization_for_deep_networks In this paper, we propose a theoretically motivated optimization framework for BNN training based on Gaussian variational inference.
__label__machine_vision Through extensive experiments, our model demonstrated a significant improvement over the state-of-the-art models on the Pascal-Part-116, ADE20K-Part-234, and PartImageNet datasets.
__label__causal_inference Most works in the literature design causal discovery policies with perfect interventions, i.e., they have access to infinite interventional samples.
__label__machine_vision Experimental results show that PDF-Embedding surpasses protocol-driven methods and non-PDF choices on the D-Rep test set.
__label__machine_vision The AGL task is associated with two important challenges.
"__label__fairness However, most existing methods overlook intersectional groups determined by combinations of
group attributes, such as gender, race, and ethnicity."
__label__safety_in_machine_learning However, principled decoding methods rely on oracle access to an optimal Q-function ($Q^*$), which is often unavailable in practice.
__label__graph_neural_networks To deal with this challenge, in this paper, we propose an end-to-end graph attention network hashing (EGATH) for cross-modal retrieval, which can not only capture direct semantic associations between images and texts but also match semantic content between different modalities.
__label__probabilistic_methods Our theoretical analysis demonstrates that GMDI achieves a more stringent evidence lower bound, closer to the log-likelihood.
__label__natural_language_processing Unlike conventional methods, MoT assigns mixtures of tokens from different examples to each expert.
__label__machine_vision Visible-Infrared Person Re-identification (VI-ReID) is a challenging cross-modal retrieval task due to significant modality differences, primarily caused by the absence of detailed color information in the infrared modality.
__label__bandits We deploy LLMs as agents in simple multi-armed bandit environments, specifying the environment description and interaction history entirely in-context, i.e., within the LLM prompt.
__label__evaluation Existing large language models (LLMs) evaluation methods typically focus on testing the performance on some closed-environment and domain-specific benchmarks with human annotations.
__label__machine_learning_for_other_sciences_and_fields While most ML models expect independent and identically distributed data, this assumption is often violated in real-world scenarios due to distribution shifts, resulting in the degradation of machine learning model performance.
__label__deep_learning_architectures We investigate this question in terms of the network’s depth, width, and number of extra tokens for algorithm execution.
__label__fairness Then, we theoretically propose the  optimal egalitarian fairness bounds that an FL coalition can achieve while maintaining core stability under various types of altruistic behaviors.
__label__reinforcement_learning We provide an $\tilde{O}(A_{\text{tot}}[(1 - \gamma)^{-3}\epsilon^{-2} + (1 - \gamma)^{-2}])$-time algorithm in the sampling setting, where the probability transition matrix is unknown but accessible through a generative model which can be queried in $\tilde{O}(1)$-time, and an $\tilde{O}(s +  (1-\gamma)^{-2})$-time algorithm in the offline setting where the probability transition matrix is known and $s$-sparse.
__label__active_learning Experiments on synthetic datasets show the effectiveness of evidential uncertainty prediction and EMM's capability to capture label correlations through predicted components.
__label__generative_models What may be an effective approach to consutrct a hybrid-DGM with theoretically-proven identifiability?
__label__diffusion_based_models Saving and transferring them is a major bottleneck for various applications, especially those running on resource-constrained devices.
__label__probabilistic_methods Finally, we demonstrate the ability to usefully incorporate text into numerical predictions, improving predictive performance and giving quantitative structure that reflects qualitative descriptions.
__label__machine_vision However, the emergence of open-world SSL (OwSSL) introduces a more practical challenge, wherein unlabeled data may encompass samples from unseen classes.
__label__algorithmic_game_theory The simultaneous version was first considered by Aumann and Maschler.
__label__graph_neural_networks In this paper, we propose a novel dynamic message-passing mechanism for GNNs.
__label__learning_theory Contrastive learning has been a leading paradigm for self-supervised learning, but it is widely observed that it comes at the price of sacrificing useful features (\eg colors) by being invariant to data augmentations.
__label__neuroscience_and_cognitive_science We propose UniAR -- a unified model of human attention and preference behavior across diverse visual content.
__label__diffusion_based_models We propose an implementation of FG-DMs by adapting a pre-trained Stable Diffusion (SD) model to implement all FG-DM factors, using only COCO dataset, and show that it is effective in generating images with 15\% higher recall than SD while retaining its generalization ability.
__label__natural_language_processing Furthermore, our enhanced LLM shows exceptional competence in generating accurate three-word sentences from an initial entity and interpreting new unseen terms out of KGs.
__label__reinforcement_learning Under mild assumptions, our theoretical analysis provides the upper bounds on optimality and safety constraint violation.
__label__generative_models Extensive experiments with the various sampling hyperparameters confirm the effectiveness of the proposed self-guidance.
__label__safety_in_machine_learning The final result then is decoded from the collective outputs of the worker nodes.
__label__diffusion_based_models Each patch is denoised in its own branch of the generation process and then transplanted into the corresponding region of the original noisy image at each timestep, resulting in robust spatial grounding for each bounding box.
__label__machine_learning_for_healthcare Moreover, the function of certain proteins is highly dependent on the granular aspects of their surface topology, which have been overlooked by prior models.
__label__probabilistic_methods (2022) introduced a simple and inexpensive method for training CNFs in generative modeling, termed flow matching (FM).
__label__privacy We present excess empirical risk bounds and computational complexities for the scalable kernel DP ERM, KME algorithms, contrasting them with established methodologies.
__label__graph_neural_networks So far, existing spiking GNNs consider graphs in Euclidean space, ignoring the structural geometry, and suffer from the high latency issue due to Back-Propagation-Through-Time (BPTT) with the surrogate gradient.
__label__reinforcement_learning The exploration-exploitation dilemma has been a central challenge in reinforcement learning (RL) with complex model classes.
__label__online_learning First-order methods (FOMs) are arguably the most scalable algorithms for equilibrium computation in large extensive-form games.
__label__deep_learning_architectures These expert modules are combined in parameter space, enabling systematic generalization to new domains without increasing inference time.
__label__natural_language_processing Our code, dataset, benchmark, and models are available at https://lklab.kaist.ac.kr/Janus/.
__label__interpretability_and_explainability By introducing a novel representation for time-series data, we forge a connection between the latent space of VQShape and shape-level features.
__label__reinforcement_learning Existing works on interpretable reinforcement learning have shown promise in extracting decision tree (DT) based policies from DRL policies with most focus on the single-agent settings while prior attempts to introduce DT policies in multi-agent scenarios mainly focus on heuristic designs which do not provide any quantitative guarantees on the expected return.
__label__reinforcement_learning Recent advancements in metric learning, such as deep bisimulation metric approaches, have shown promising results in learning structured low-dimensional representation space from pixel observations, where the distance between states is measured based on task-relevant features.
__label__machine_vision This is the first attempt to deeply and explicitly embed information fusion within the diffusion process, effectively addressing compound degradation in image fusion.
__label__evaluation Experiments demonstrate that, even without a model explicitly tailored to user tasks, the system can effectively handle tasks by leveraging models from diverse feature spaces.
__label__machine_learning_for_healthcare However, the practical application of 3D-SBDD generative models is hampered by their slow processing speeds.
__label__deep_learning_architectures However, existing research primarily focuses on reducing the theoretical computational complexity through methods such as local attention and model pruning, rather than considering realistic performance on mobile hardware.
__label__learning_theory In this paper, we shed light on the efficacy of ICL from the viewpoint of statistical learning theory.
__label__machine_vision Furthermore, NeRFs tend to overfit individual frames and easily get stuck in local minima under sparse-view inputs.
__label__learning_theory Despite recent progress, in agnostic boosting, where assumptions on the conditional distribution of labels given feature descriptions are absent, ERM outstrips the agnostic boosting methodology in being quadratically more sample efficient than all known agnostic boosting algorithms.
__label__interpretability_and_explainability Data debugging is to find a subset of the training data such that the model obtained by retraining on the subset has a better accuracy.
__label__probabilistic_methods However, the design of methods for learning credal set predictors remains a challenging problem.
__label__interpretability_and_explainability To overcome these limitations, we propose Relational Concept Bottleneck Models (R-CBMs), a family of relational deep learning methods providing interpretable task predictions.
__label__natural_language_processing While many contemporary large language models (LLMs) can process lengthy input, they still struggle to fully utilize information within the long context, known as the *lost-in-the-middle* challenge.
__label__machine_learning_for_physical_sciences However, contemporary quantum computers experience errors that often cause quantum programs run on them to fail.
__label__machine_learning_for_physical_sciences To address this, we present a framework that achieves high-precision modeling for a wide range of dynamical systems from the numerical aspect, by enforcing Time-Reversal Symmetry (TRS) via a novel regularization term.
__label__other The potential benefits and capabilities of representing graph structures as visual images (i.e., $\textit{visual graph}$) are still unexplored.
__label__machine_vision Finally, comprehensive experiments across various benchmarks confirm the effectiveness of our proposed AUCSeg method.
__label__reinforcement_learning We showcase the efficiency of our method through experimentation on several benchmark Safety Gymnasium environments and realistic self-driving scenarios.
__label__deep_learning_architectures This work introduces $\rm CALDERA$ -- a new post-training LLM compression algorithm that harnesses the inherent low-rank structure of a weight matrix $\mathbf{W}$ by approximating it via a low-rank, low-precision decomposition as $\mathbf{W} \approx \mathbf{Q} + \mathbf{L}\mathbf{R}$.
__label__optimization Moreover, HyperPrism could adaptively choose different mapping for different layers of the local model with a dedicated hypernetwork per device, achieving automatic optimization of DML in high divergence settings.
__label__graph_neural_networks We theoretically prove the superiority of HDSE in terms of expressivity and generalization.
__label__safety_in_machine_learning Large Language Models (LLMs) have demonstrated remarkable capabilities, surpassing human experts in various benchmark tests and playing a vital role in various industry sectors.
__label__machine_vision Furthermore, we integrate causal inference techniques such as front-door adjustment and counterfactual inference to address challenges in MECD like causality confounding and illusory causality.
__label__machine_vision Current approaches for open-vocabulary scene graph generation (OVSGG) use vision-language models such as CLIP and follow a standard zero-shot pipeline – computing similarity between the query image and the text embeddings for each category (i.e., text classifiers).
__label__machine_vision In this work, we propose a novel method, PixelCLIP, to adapt the CLIP image encoder for pixel-level understanding by guiding the model on where, which is achieved using unlabeled images and masks generated from vision foundation models such as SAM and DINO.
__label__machine_vision VFM-6D is trained on cost-effective synthetic data and exhibits superior generalization capabilities.
__label__diffusion_based_models Analysis shows that our training objective is closely related to the generation quality and our proposed generation framework enjoys ideal invariant/equivariant properties concerning the permutation of node ordering.
__label__machine_vision To further enhance rendering results, we apply a patch-based hybrid loss to optimize NeRF.
__label__reinforcement_learning We show that SPO provides robust policy improvement and efficient scaling properties.
__label__natural_language_processing Through hybrid instruction fine-tuning with augmented trajectories on Llama 2 and Llama 3 family, we obtain UrbanKGC agent family, consisting of UrbanKGent-7/8/13B version.
__label__fairness Our experiments demonstrate the effectiveness of our method across multiple benchmarks.
__label__optimization Experiments on neuromorphic hardware show that our method consistently yielded high quality solutions for HMVC on real and synthetic instances where the SNN-based QUBO solver often failed, while consuming measurably less energy than global solvers on CPU.
__label__privacy We then generalize our attack to other loss functions and architectures,  and empirically demonstrate the effectiveness of our attacks across a wide range of datasets (capturing both tabular and image data).
__label__privacy By exploiting properties of the standard matrix square root, BSR allows to efficiently handle also large-scale problems.
__label__optimization The empirical Fisher (EF) method approximates the Fisher information matrix empirically by reusing the per-sample gradients collected during back-propagation.
"__label__learning_theory To achieve such provable ""forward-error"" guarantees, our methods rely on a new $O(n^{\omega+\eta})$ stability analysis for the Cholesky factorization, and a smoothed analysis for computing spectral gaps, which can be of independent interest."
__label__reinforcement_learning Partial observability in environments poses significant challenges that impede the formation of effective policies in reinforcement learning.
__label__interpretability_and_explainability Singly or multiply imputing missing values complicates the model’s mapping from features to labels.
__label__machine_vision Compared to existing methods that use text representation for domain difference, this method utilizes pixel-level representation with higher granularity, enabling efficient domain adaptation guidance for SR networks.
__label__diffusion_based_models However, such interleaving methods struggle to produce final results that look like natural objects of interest (i.e., manifold feasibility) and fit the measurement (i.e., measurement feasibility), especially for nonlinear IPs.
__label__machine_learning_for_physical_sciences Incorporating physics-informed priors, such as in Hamiltonian Neural Networks (HNNs), achieves high-precision modeling for energy-conservative systems.
__label__machine_learning_for_other_sciences_and_fields In addition to being intractable with existing approaches to semantic routing, our benchmark poses a significant scaling challenge for graph learning methods.
__label__generative_models In this work, we propose InteractTraj, the first language-driven traffic trajectory generator that can generate interactive traffic trajectories.
__label__learning_theory The key is to guarantee that reciprocal learning contracts such that the Banach fixed-point theorem applies.
__label__machine_learning_for_physical_sciences At the core of Stormer is a randomized forecasting objective that trains the model to forecast the weather dynamics over varying time intervals.
__label__machine_learning_for_other_sciences_and_fields Time series forecasting has played a pivotal role across various industries, including finance, transportation, energy, healthcare, and climate.
__label__reinforcement_learning Building upon these aspects, MAST utilizes gradient-based topology evolution to exclusively train multiple MARL agents using sparse networks.
__label__machine_learning_for_physical_sciences However, they frequently result in blurry predictions which provide limited utility to forecasting operations.
__label__safety_in_machine_learning Existing methods often struggle to generalize to unseen domains due to the diverse nature of facial manipulations.
__label__safety_in_machine_learning Experiments on various datasets show that our proposed DAT leads to significantly improved robustness against diverse adversarial attacks.
__label__graph_neural_networks Specifically, SGA first integrates a structure augmentation module to detect candidate edges solely based on network information.
__label__learning_theory [STOC, 2022].
__label__natural_language_processing Further, as test generation is a similar but more structured task than code generation, it allows for a more fine-grained analysis using issue reproduction rate and coverage changes, providing a dual metric for analyzing systems designed for code repair.
__label__human-AI_interaction This approach requires inferring intentions, which can be difficult in high-dimensional settings.
__label__safety_in_machine_learning We develop the first scalable Branch-and-Bound-based relational verifier, RABBit, which efficiently combines branching over multiple executions with cross-executional bound refinement to utilize relational constraints, gaining substantial precision over SOTA baselines on a wide range of datasets and networks.
__label__probabilistic_methods They are tractable if LVs can be analytically integrated out, otherwise they can be approximated by tractable probabilistic circuits (PC) encoding a hierarchical numerical quadrature process, called QPCs.
__label__machine_learning_for_other_sciences_and_fields To meet the needs of practical applications, we introduce DiffLight, a novel conditional diffusion model for TSC under data-missing scenarios in the offline setting.
__label__natural_language_processing We also present the ChatRAG Bench, which encompasses ten datasets covering comprehensive evaluations on RAG, table-related QA, arithmetic calculations, and scenarios involving unanswerable questions.
__label__learning_theory In contrast, this work provides a fine-grained mathematical analysis to show how transformers leverage the multi-concept semantics of words to enable powerful ICL and excellent out-of-distribution ICL abilities, offering insights into how transformers innovate solutions for new unseen tasks encoded with multiple cross-concept semantics.
__label__neuroscience_and_cognitive_science Inferring the synaptic plasticity rules that govern learning in the brain is a key challenge in neuroscience.
__label__safety_in_machine_learning This is because the art necessitates modifications to the diffusion training and sampling procedures.
__label__optimization For expensive MOO problems, due to their costly function evaluations, computationally cheap surrogates have been widely used in MOO to save evaluation budget.
__label__machine_vision Video samples are available at https://mimictalk.github.io .
__label__learning_theory The Transformer architecture is widely applied in sequence modeling applications, yet the theoretical understanding of its working principles remains limited.
__label__reinforcement_learning Nevertheless, its performance often suffers from the objective mismatch between model and policy learning, resulting in inferior performance despite accurate model predictions.
__label__deep_learning_architectures The results show that LLM-AutoDA outperforms state-of-the-art data augmentation methods and other re-balancing methods significantly.
__label__safety_in_machine_learning Given any pre-trained model and new units with model-generated outputs, Conformal Alignment leverages a set of reference data with ground-truth alignment status to train an alignment predictor.
__label__machine_vision For both third-person and first-person (egocentric) videos, state-of-the-art (SOTA) methods aim at finding correspondences across videos in time to accomplish procedure learning.
__label__machine_vision In this paper, we introduce LexVLA, a more interpretable VLA framework by learning a unified lexical representation for both modalities without complex design.
__label__deep_learning_architectures Experimental evaluations conducted on various synthetic and real-world noisy datasets demonstrate significant improvements over existing transition matrix-based methods.
__label__machine_vision Key-Grid achieves the state-of-the-art performance on the semantic consistency and position accuracy of keypoints.
"__label__learning_theory We design a computationally efficient algorithm that recovers a vector $ \hat{\mathbf{w}}$
satisfying 
$\mathbb{E}\_{p^*} (\sigma(\hat{\mathbf{w}} \cdot \mathbf{x}) - y)^2 \leq C \hspace{0.2em}  \mathbb{E}\_{p^*} (\sigma(\mathbf{w}^* \cdot \mathbf{x}) - y)^2 + \epsilon$, where $C>1$ is a dimension-independent constant and $(\mathbf{w}^*, p^*)$ is the witness attaining the min-max risk
$\min_{\mathbf{w}:\|\mathbf{w}\| \leq W} \max\_{p} \mathbb{E}\_{(\mathbf{x}, y) \sim p} (\sigma(\mathbf{w} \cdot \mathbf{x}) - y)^2 - \nu \chi^2(p, p_0)$."
__label__privacy DP-GLMtron is based on a generalized linear model perceptron approach, integrating adaptive clipping and Gaussian mechanism for enhanced privacy.
__label__machine_learning_for_other_sciences_and_fields Within this framework, the timestamps are modeled individually to capture the global dependencies.
__label__diffusion_based_models Then, we propose a novel text-driven panoramic generation framework, termed DiffPano, to achieve scalable, consistent, and diverse panoramic scene generation.
__label__generative_models The extensive results show that CATOD significantly outperforms the prior approaches with an 11.10 boost on the CLIP score and a 33.08% decrease on the CMMD metric.
__label__robotics In the second stage, a flexible yet effective layer-wise self-attention adapter is introduced to transform VidMan into an efficient inverse dynamics model that predicts action modulated by the implicit dynamics knowledge via parameter sharing.
__label__bandits We propose a novel piecewise stationary linear bandit (PSLB) model, where the environment randomly samples a context from an unknown probability distribution at each changepoint, and the quality of an arm is measured by its return averaged over all contexts.
__label__generative_models }, text-to-image, image captioning) due to the existence of modal gaps.
__label__privacy In this paper, we present an improved algorithm that achieves time and space complexity of $\tilde{O}(d + k^2)$.
__label__causal_inference We show that the ATE estimator under our proposed design strategy attains this semiparametric efficiency bound and achieves asymptotic normality.
__label__safety_in_machine_learning However, fine-tuning LLMs for alignment often suffers from unstable training and requires substantial computing resources.
__label__learning_theory On the algorithmic side, we merge the re-solving heuristic with distribution estimation skills and propose an algorithm that achieves an $\widetilde{O}(1)$ regret as long as the fluid LP has a unique and non-degenerate solution.
__label__graph_neural_networks Our code is available at:  [https://github.com/Zhengsh123/IntraMix](https://github.com/Zhengsh123/IntraMix).
__label__other This paper presents a case study to reveal this critical vulnerability in KD-based FL systems.
__label__probabilistic_methods Our approach implements an automatic Occam's razor that selects the most sparsifiable model that still explains the data well, both for structured and unstructured sparsification.
__label__machine_vision Notably, APM introduces only 0.01% additional parameters but improves the average performance by over 10%, and ACPA imports only 2.5% parameters but further improves the performance by over 1.5%, which significantly surpasses the state-of-the-art CD-FSS methods.
__label__deep_learning_architectures We propose DenseFormer, a simple modification to the standard architecture that improves the perplexity of the model without increasing its size---adding a few thousand parameters for large-scale models in the 100B parameters range.
__label__bandits And by trivially treating every set as a unique arm one deduces that $\sqrt{ {n \choose k} T }$ is also achievable using standard multi-armed bandit algorithms.
__label__natural_language_processing In this work, we show that localized fine-tuning serves as an effective alternative to such representation intervention methods.
__label__reinforcement_learning We further provide tight bounds for the ratio given the worst-case dynamics.
__label__safety_in_machine_learning The mismatch between the learned and assumed distributions motivates us to raise a fundamental yet under-explored question: \textit{Is it possible to deterministically model the feature distribution while pre-training a discriminative model?}
__label__privacy For the key scenario of stochastic gradient descent with momentum and weight decay, we even derive analytical expressions for BSR that render the computational overhead negligible.
__label__generative_models Extensive experiments conducted on unconditional and class-conditioned object generation, digital avatar creation, and text-to-3D synthesis all show that our model achieves state-of-the-art generation results both qualitatively and quantitatively, underscoring the potential of GaussianCube as a highly accurate and versatile radiance representation for 3D generative modeling.
__label__natural_language_processing Our approach reveals three main findings.
__label__diffusion_based_models CRE can handle complex scenarios, including incomplete or blurred representations of unsafe concepts, offering a promising solution to challenges in managing harmful content generation in diffusion-based models.
__label__interpretability_and_explainability We then successfully train transcoders on language models with 120M, 410M, and 1.4B parameters, and find them to perform at least on par with SAEs in terms of sparsity, faithfulness, and human-interpretability.
__label__causal_inference Causal representation learning promises to extend causal models to hidden causal variables from raw entangled measurements.
__label__optimization_for_deep_networks Specifically, IRE decouples the dynamics of flat and sharp directions, which boosts the sharpness reduction along flat directions while maintaining the training stability in sharp directions.
__label__optimization_for_deep_networks Federated Learning (FL) is a prevalent machine learning paradigm designed to address challenges posed by heterogeneous client data while preserving data privacy.
__label__speech_and_audio We conduct experiments demonstrating performance remarkably close to the state of the art, including a special inference configuration enabling long-form recognition.
__label__machine_learning_for_other_sciences_and_fields Then, using a learned stochastic policy, the algorithm makes edits at these identified locations, offering diverse modifications for each sequence to enhance the desired property.
__label__safety_in_machine_learning Through experimental analysis, we have observed a phenomenon wherein adversarial perturbations induce shifts in text-guided attention.
__label__machine_learning_for_other_sciences_and_fields Biological sequences encode fundamental instructions for the building blocks of life, in the form of DNA, RNA, and proteins.
__label__safety_in_machine_learning We focus on post-processing a pre-trained model $f$ to better align with $\mathcal{P}$ using conformal risk control.
__label__generative_models Moreover, these methods heavily depended on camera poses, limiting their real-world applications.
__label__bandits In this work, we present the first nontrivial result for sequential multi-task linear bandits without the task diversity assumption.
__label__learning_theory We establish a near-optimal sample complexity bound of $\tilde{\Theta}(\sqrt{k}/\varepsilon + {1}/{\varepsilon^2}$) for this problem, and show how to apply it to the problem of identity testing for in-degree-$d$ $n$-dimensional Bayesian networks, obtaining an upper bound of $\tilde{O}\left( {2^{d / 2} n}/{\varepsilon^2} + {n^2}/{\varepsilon^4} \right)$.
__label__generative_models This complexity presents challenges for modeling and generalizing trajectory prediction.
__label__machine_vision Our approach involves the creation of two supplementary training tasks GenQA and EvalQA, aiming at fostering the skills of asking and assessing questions in the context of images.
__label__probabilistic_methods However, optimizing a black-box which is also a function of time (*i.e.
__label__machine_learning_for_other_sciences_and_fields We demonstrate the effectiveness of the proposed method on six inverse problems (three linear and three nonlinear), including a real-world black hole imaging problem.
__label__safety_in_machine_learning Github page: \url{https://github.com/showlab/watermark-steganalysis}.
__label__bandits Bandits with preference feedback present a powerful tool for optimizing unknown target functions when only pairwise comparisons are allowed instead of direct value queries.
__label__online_learning Notably, CAMS requires substantially less labeling effort (less than 10%) compared to existing methods on CIFAR10 and DRIFT benchmarks, while achieving similar or better accuracy.
__label__human-AI_interaction Recent advances in AI models have increased the integration of AI-based decision aids into the human decision making process.
__label__machine_vision There are two challenges in this direction: First, rendering error gradients are often insufficient to recover fast object motion, and second,  view predictive generative models work much better for objects than whole scenes, so, score distillation objectives cannot currently be applied at the scene level directly.
__label__interpretability_and_explainability We demonstrate the intelligibility of GNANs in a series of examples on different tasks and datasets.
__label__reinforcement_learning In many reinforcement learning (RL) applications, incorporating heuristic rewards alongside the task reward is crucial for achieving desirable performance.
__label__machine_vision The directional attention enables Grid4D to more accurately fit the diverse deformations across distinct scene components based on the spatial encoded features.
__label__machine_learning_for_healthcare Deep Learning in Image Registration (DLIR) methods have been tremendously successful in image registration due to their speed and ability to incorporate weak label supervision at training time.
__label__machine_learning_for_healthcare By scaling up training data to 90K unlabeled Computed Tomography (CT) volumes and 6K labeled CT volumes, this foundation model supports the segmentation of over 200 anatomical categories using semantic and spatial prompts.
__label__robotics Extensive qualitative and quantitative evaluations of SyntheOcc on the nuScenes dataset prove its effectiveness in generating controllable occupancy datasets that serve as an effective data augmentation to perception models.
__label__neuroscience_and_cognitive_science Moreover, we benchmark all AI models in point-light displays of two standard video datasets in computer vision.
__label__algorithmic_game_theory We study online Bayesian persuasion problems in which an informed sender repeatedly faces a receiver with the goal of influencing their behavior through the provision of payoff-relevant information.
__label__machine_learning_for_other_sciences_and_fields To address these pitfalls, we introduce FlowDPO, a novel framework that explores various probability paths with flow matching models and further suppresses hallucinations using Direct Preference Optimization (DPO) for structure generation.
__label__reinforcement_learning Diffusion Q-Learning (DQL), introducing diffusion models as a powerful and expressive policy class, significantly boosts the performance of offline RL.
__label__machine_learning_for_physical_sciences We discover new hypotheses using a mix of standard evolutionary steps and LLM-guided steps (obtained through zero-shot LLM queries) conditioned on discovered concepts.
__label__machine_learning_for_physical_sciences Conventional sophisticated mesh movement methods are extremely expensive and struggle to handle scenarios with complex boundary geometries.
__label__machine_vision To this end, we introduce a new, challenging benchmark termed Visual Minimal-Change Understanding (VisMin), which requires models to predict the correct image-caption match given two images and two captions.
__label__graph_neural_networks The main focus of the paper are graph neural networks, where we show that unitary graph convolutions provably avoid over-smoothing.
__label__machine_vision The Object-Slots branch focuses on extracting object-centric slots from features of high spatial resolution but low frame sample rate, emphasizing detailed object information.
__label__learning_theory We prove our results through a novel application of the hemisphere transform.
__label__graph_neural_networks Understanding this concept and its related metrics is crucial for designing effective Graph Neural Networks (GNNs).
__label__online_learning In addition to rewarding accurate predictions (completeness) and penalizing incorrect ones (soundness), an important desideratum of calibration measures is *truthfulness*, a minimal condition for the forecaster not to be incentivized to exploit the system.
__label__neuroscience_and_cognitive_science However, fMRI is constrained by issues such as high operation costs and immobility.
__label__natural_language_processing First, our Polar Probe successfully recovers the type and direction of syntactic relations, and substantially outperforms the Structural Probe by nearly two folds.
__label__learning_theory That means that, in contrast with other algorithms, GD has no advantage over naive ERMs.
__label__natural_language_processing We denote these activations as activation spikes.
__label__privacy Moreover, we present a kernel-based estimator for this divergence, circumventing the need for adversarial training.
__label__reinforcement_learning BRO achieves state-of-the-art results, significantly outperforming the leading model-based and model-free algorithms across 40 complex tasks from the DeepMind Control, MetaWorld, and MyoSuite benchmarks.
__label__active_learning We also define a novel suite of automatic sampling strategies for training, including active-learning inspired submodular feedback.
__label__causal_inference In this work, we consider the task of learning causal representation learning with data collected from general environments.
__label__machine_vision A pivotal challenge is the development of an effective method to encapsulate video content into a set of representative tokens to align with LLMs.
__label__diffusion_based_models SIM shows strong empirical performances for one-step generators: on the CIFAR10 dataset, it achieves an FID of 2.06 for unconditional generation and 1.96 for class-conditional generation.
__label__machine_vision For each input AV data pair, our AVAgent uses a multi-modal LLM to convert audio and visual data into language descriptions separately (i.e., tool use).
__label__learning_theory In this paper, we investigate the fundamental limits of learning with label DP under both central and local models.
__label__online_learning Experimental results confirm that SER effectively enhances the performance (in some cases up to about twenty percent points) of state-of-the-art continual learning methods, both in class-incremental and task-incremental settings.
__label__robotics This offers a flexible and cost-efficient approach for scaling and also provides a robust infrastructure for robot-environment-algorithm communication.
__label__machine_vision However, the effectiveness of this capacity often degrades when there are shifts in data distribution during testing compared to the training data.
__label__speech_and_audio Thus, the audio representation can be viewed as a new \textit{foreign language}, and LLMs can learn the new \textit{foreign language} with several demonstrations.
__label__natural_language_processing This may shed some light on how  next-token prediction can capture the structure of text across multiple levels of granularity, from words and clauses to broader contexts and intents.
__label__neuroscience_and_cognitive_science Here, exploiting the recent success of control techniques for training SSMs, we propose a novel algorithm that solves this problem and scales exceptionally well with model dimensionality and filter length.
__label__learning_theory However, existing results require distributional assumptions on the data and are limited to a high-dimensional setting, where the input dimension $d_0$ scales at least logarithmically in the number of samples $n$.
__label__machine_learning_for_healthcare Experiments on a pharmacokinetic Warfarin dataset reveal that D3 identifies a new plausible model that is well-fitting, highlighting its potential for precision dosing in clinical applications.
__label__machine_learning_for_physical_sciences Our approach goes beyond the popular KFAC for traditional deep learning problems as it captures contributions from a PDE's differential operator that are crucial for optimization.
__label__generative_models We develop a new method that takes an ICL problem and estimates the probability that a CGM will generate a hallucination.
__label__deep_learning_architectures Motivated by our observation of a correlation between the time series model's performance boost against channel mixing and the intrinsic similarity on a pair of channels, we developed a novel and adaptable \textbf{C}hannel \textbf{C}lustering \textbf{M}odule (CCM).
__label__active_learning Compared to the best baseline using only OpenAI's CLIP-L/14, our methods achieve a 5.3\% improvement on ImageNet-1k and a 2.8\% improvement on 38 downstream evaluation tasks.
__label__machine_vision We further demonstrate the improvements brought by SimGen for synthetic data augmentation on the BEV detection and segmentation task and showcase its capability in safety-critical data generation.
__label__probabilistic_methods Our experimental results show that the metalearned neural circuit achieves comparable or better performance than particle filter-based methods for inference in these models while being faster and simpler to use than methods that explicitly incorporate Bayesian nonparametric inference.
__label__reinforcement_learning Using context encoders based on contrastive learning to enhance the generalisability of Meta-RL agents is now widely studied but faces challenges such as the requirement for a large sample size, also referred to as the $\log$-$K$ curse.
__label__safety_in_machine_learning We develop new conformal inference methods for obtaining validity guarantees on the output of large language models (LLMs).
__label__learning_theory However, ignorance of the variance is a frequent issue in applications and basic theoretical questions still remain open in this setting.
"__label__graph_neural_networks With the growing interest in architectures that process NNs, a recurring design principle has permeated the field: adhering to the permutation symmetries arising from the connectionist structure of
NNs."
__label__machine_learning_for_healthcare Disease grading is a crucial task in medical image analysis.
__label__interpretability_and_explainability Extensive experimental results demonstrate that DDPath can significantly reduce noise in the attributions—resulting in clearer explanations—and achieves better quantitative results than traditional path-based methods.
__label__natural_language_processing Concretely, we craft a set of prompts based on human education stages to guide GPT-4, to synthesize problems covering diverse math knowledge and difficulty levels.
__label__machine_vision KptLLM underscores the initial discernment of semantics in keypoints, followed by the precise determination of their positions through a chain-of-thought process.
__label__robotics Simulating realistic behaviors of traffic agents is pivotal for efficiently validating the safety of autonomous driving systems.
__label__deep_learning_architectures Experimental results on MNIST, FashionMNIST, CIFAR10, CIFAR100, and STL-10 datasets using multi-layer perceptrons and convolutional neural networks demonstrate that CCL achieves comparable performance to other biological plausible algorithms while offering a more biologically realistic learning mechanism.
__label__machine_vision In detail, a novel Adapter-in-Adapter (AnA) framework is introduced, in which uni-modal and cross-modal adapters are seamlessly integrated to facilitate efficient modality alignment and collaboration.
__label__natural_language_processing It is noteworthy that, on the task of long-text image retrieval, we beat the competitor using long captions with 11.1% improvement (i.e., from 72.62% to 83.72%).
__label__machine_vision Prior works use diffusion models to generate driving images conditioned on the 3D object layout.
"__label__learning_theory We deepen this line of work by finding an exact formula for the likelihood ratio norm which proves that statistical
distinguishability requires $n\gtrsim d$ samples, while distinguishing the two
distributions in polynomial time requires $n \gtrsim d^2$ samples for a wide
class of algorithms, i.e."
__label__diffusion_based_models A key component of our algorithm is our new type of diffusion sampler, Time Correction Sampler (TCS), which is used to control guidance and ensure that the generated molecules remain on the correct manifold at each reverse step of the diffusion process at the same time.
__label__machine_vision These findings lead to several **implications.
__label__other Our largest models learn to predict action-values for novel boards quite accurately, implying highly non-trivial generalization.
__label__machine_vision Besides, based on our detailed analysis, we propose a token importance evaluation method adapted for SSM models, to guide the token pruning.
__label__deep_learning_architectures We then introduce a continuous approximation of the dictionary readout operation in order to derive two energy functions that are Lyapunov functions of the dynamics.
__label__machine_vision The resulting framework can naturally supports generation and recognition, and more importantly is robust even when visual inputs contain limited information.
__label__reinforcement_learning The results showcase that the discussion greatly changes players' utilities by affecting their beliefs, emphasizing the significance of discussion tactics.
__label__algorithmic_game_theory We show that partial information release can counter-intuitively benefit the learner’s accuracy, allowing qualified agents to pass the classifier while preventing unqualified agents from doing so.
__label__natural_language_processing To verify $\texttt{MWork}$, we introduce Parallel Language-specific Neuron Detection ($\texttt{PLND}$) to identify activated neurons for inputs in different languages without any labeled data.
__label__optimization In this paper, we consider the complementary question of using learned information to overcome computational barriers in the form of approximation hardness of polynomial-time algorithms for NP-hard (offline) problems.
__label__learning_theory Much of Bayesian inference centers around the design of estimators for inverse problems which are optimal assuming the data comes from a known prior.
__label__deep_learning_architectures We observe much faster convergence of state activations in certain dimensions therefore indicating the dimensionality of the underlying dynamics of the forward pass is much lower than the defined dimension of the states.
__label__causal_inference Theoretically, we identify three types of indeterminacy for the parameters in partially observed linear causal models.
__label__optimization_for_deep_networks Overall, we present a unifying perspective from ResNets on vision tasks to LLMs: weight decay is never useful as an explicit regularizer but instead changes the training dynamics in a desirable way.
__label__bandits Interestingly, $\beta_M(G)$ interpolates between $\alpha(G)$ (the independence number of the graph) and $\mathsf{m}(G)$ (the maximum acyclic subgraph (MAS) number of the graph) as the number of contexts $M$ varies.
__label__optimization_for_deep_networks It is often unclear whether the observed generalization behaviour arises specifically from the second-order nature of the parameter updates, or instead reflects the specific structured (e.g.
__label__optimization We propose to convert the m-sparse fractional optimization problem into an equivalent m-sparse quadratic programming problem.
__label__machine_vision Presently, all these MLLMs integrate visual recognition and understanding in a same sequential manner in the LLM head, i.e., generating the response token-by-token for both recognition and understanding.
__label__machine_vision This modification ensures that the $\mathbf{w}$MSE component is always effective during training, providing extra constructive cues.
__label__generative_models Equipped with the parameter-efficient fine-tuning method and high-temperature sampling, MGMs with the proposed self-guidance achieve a superior quality-diversity trade-off, outperforming existing sampling methods in MGMs with more efficient training and sampling costs.
__label__neuroscience_and_cognitive_science Specifically, the left-right difference in brain correlation follows a scaling law with the number of parameters.
__label__reinforcement_learning Recent advances in data-driven imitation learning and offline reinforcement learning have highlighted the use of expert data for skill acquisition and the development of hierarchical policies based on these skills.
__label__interpretability_and_explainability Notably, this finding is universal and holds for various LLMs, including Gemma-7B, LLaMA2-13B, Mistral-7B and LLaMA3-8B.
__label__graph_neural_networks In real-world scenarios, networks (graphs) and their tasks possess unique characteristics, requiring the development of a versatile graph augmentation (GA) to meet the varied demands of network analysis.
__label__generative_models Furthermore, we propose a set of data augmentation techniques within this metric space to create new data samples.
__label__reinforcement_learning In the rapidly evolving domain of vision-based deep reinforcement learning (RL), a pivotal challenge is to achieve generalization capability to dynamic environmental changes reflected in visual observations.
__label__deep_learning_architectures This adaptation enhances the versatility of our model, enabling it to learn a broader range of scenarios prevalent in the real world, where groups can act on latent factors.
__label__probabilistic_methods This work presents general upper and lower bounds on the Kullback-Leibler (KL) divergence of coreset approximations that reflect the full range of applicability of Bayesian coresets.
__label__other In this paper, we bridge this gap and propose a novel method to extract node-level topological features from complex point clouds using discrete variants of concepts from algebraic topology and differential geometry.
__label__learning_theory An Implementation of our algorithm can be accessed at the following link: https://github.com/shayankiyani98/CP.
__label__machine_vision Unlike the aforementioned models, diffusion models excel in discerning mid/low-level visual concepts as generative models, and possess strong compositionality to handle novel concepts expressed in text inputs.
__label__other We conduct comprehensive experiments on both synthetic and real-world datasets.
__label__machine_vision This paper demonstrates the ineffectiveness of traditional UAPs in open-set scenarios like Few-Shot Learning (FSL).
__label__optimization Integer linear programs (ILPs) are commonly employed to model diverse practical problems such as scheduling and planning.
__label__probabilistic_methods In this paper, we developed the Tree-structured Variational Inference (TreeVI), which uses a tree structure to capture the correlation of latent variables in the posterior distribution.
__label__generative_models More importantly, the high-accuracy fitting of the Gaussians allows us to achieve a high-quality representation with orders of magnitude fewer parameters than previous structured representations for comparable quality, ranging from one to two orders of magnitude.
__label__machine_learning_for_healthcare To overcome the label scarcity in scRNA-seq data, we generate pseudo-labels based on optimal transport and merge them into the labeled scRNA-seq data.
__label__privacy We characterize the instance-optimal estimation rates in both these settings and show that they are uniformly achievable (up to polylogarithmic factors).
__label__machine_vision Recent open-world 3D representation learning methods using Vision-Language Models (VLMs) to align 3D point clouds with image-text information have shown superior 3D zero-shot performance.
__label__deep_learning_architectures FBM extracts explicit frequency features while preserving temporal characteristics, enabling the mapping network to capture the time-frequency relationships.
__label__machine_vision Thereby, the new class features are clustered in the reserved space to minimize the shock of the new classes on the former classes.
__label__algorithmic_game_theory We extend Team-FP dynamics to multi-team Markov games for model-based and model-free cases.
__label__machine_vision Meta-learning offers a promising avenue for few-shot learning (FSL), enabling models to glean a generalizable feature embedding through episodic training on synthetic FSL tasks in a source domain.
__label__other In this paper, we propose a method based on discrete token modeling technique called Similarity-driven Discrete Transformer (SDformer).
__label__algorithmic_game_theory Reward allocation, also known as the credit assignment problem, has been an important topic in economics, engineering, and machine learning.
__label__machine_learning_for_other_sciences_and_fields The complexity is exacerbated by the presence of unlabeled data and the opaque nature of black-box anomaly detection models, which obscure the rationale behind their predictions.
__label__machine_learning_for_social_sciences We analyze these NN-based planners for opinion networks governed by two dynamic propagation models.
__label__other Yet, it is unclear whether TTA methods can maintain their adaptability over prolonged periods.
__label__safety_in_machine_learning \textit{Attack} uses an evolutionary algorithm to attack the crucial regions, where the attacks are semantically related to the target texts of \textit{Ask}, thus achieving targeted attacks without semantic loss.
__label__machine_vision To facilitate cross-hardware learning, previous efforts attempt to directly collect multi-hardware data and perform centralized training, which is impractical due to severe user data privacy concerns and hardware heterogeneity across different platforms/institutions.
__label__generative_models However, there is no research discussing how text embedding contributes to T2I models, especially when generating more than one object.
__label__active_learning Learning an ordering of items based on pairwise comparisons is useful when items are difficult to rate consistently on an absolute scale, for example, when annotators have to make subjective assessments.
__label__machine_learning_for_healthcare Among them, some studies advocate for multi-task learning (MTL) to jointly predict multiple target diseases for improving the prediction performance over single task learning.
__label__graph_neural_networks However, characterizing the class of functions that they learn has remained unresolved.
__label__machine_vision We show that GIMM performs better than the current state of the art on standard VFI benchmarks.
__label__machine_vision (iv) It is observed that different time steps of Stable Diffusion features, as well as different transformer layers of DINO/CLIP/VQGAN, are good at different properties, unlocking potential applications of 3D physical understanding.
__label__natural_language_processing In this paper, we propose a Data-Adaptive Positional Encoding (DAPE) method, which dynamically and semantically adjusts based on input context and learned fixed priors.
__label__interpretability_and_explainability This deﬁnition implies, in contrast to encoding explanations, that non-encoding explanations contain all the informative inputs used to produce the explanation, giving them a “what you see is what you get” property, which makes them transparent and simple to use.
__label__online_learning A new buyer appears and may choose to purchase some amount of data.
__label__graph_neural_networks Unsupervised Multiplex Graph Learning (UMGL) aims to learn node representations on various edge types without manual labeling.
__label__diffusion_based_models This enables ClavaDDPM to capture long-range dependencies effectively.
__label__generative_models To solve a Sudoku, the model is first required to search over all empty cells of the puzzle to decide on a cell to fill and then apply an appropriate strategy to fill the decided cell.
__label__graph_neural_networks LGD is also capable of conditional generation through a specifically designed cross-attention mechanism.
__label__natural_language_processing Please find the dataset and codes in our [project page](https://microsoft.github.io/visualization-of-thought).
__label__algorithmic_game_theory Recent breakthrough results by Dagan, Daskalakis, Fishelson and Golowich [2023] and Peng and Rubinstein [2023] established an efficient algorithm attaining at most $\epsilon$ swap regret over extensive-form strategy spaces of dimension $N$ in $N^{\tilde O(1/\epsilon)}$ rounds.
__label__deep_learning_architectures We benchmark on the traveling salesperson problem to evaluate the capabilities of the modified system in an NP-hard problem where DT fails to learn.
__label__diffusion_based_models In this paper, we propose a framework called FlowTurbo to accelerate the sampling of flow-based models while still enhancing the sampling quality.
__label__natural_language_processing Large Language Models (LLMs) have made significant progress in assisting users to query databases in natural language.
__label__machine_learning_for_healthcare Due to the continuous progression of diseases, i.e., the variability within the same level and the similarity between adjacent stages, accurate grading is highly challenging.
__label__machine_learning_for_other_sciences_and_fields Solving large-scale sparse linear systems is essential in fields like mathematics, science, and engineering.
__label__other This limitation is only partially addressed by existing knowledge distillation (KD) techniques, which often fail to transfer knowledge effectively across a broad spectrum of device prototypes with varied capabilities.
__label__generative_models This enables them to act as intelligent agents interacting with the real world.
__label__machine_learning_for_other_sciences_and_fields However, their iterative sampling process requiring denoising across multiple noise levels incurs substantial overhead.
__label__generative_models However, its performance is still limited as it lacks high-quality annotated datasets for training.
__label__machine_vision In this paper, we find language naturally conveys abundant semantic information, rendering it stunningly superior in ensuring semantic consistency for E2V reconstruction.
__label__machine_vision Our method is content-aware, requiring no tuning for different datasets, and fast, incurring negligible overhead.
__label__generative_models Overall, our work systematically elucidates the synergy between Hamiltonian dynamics, force fields, and generative models, thereby opening new avenues for applications of machine learning in physical sciences and dynamical systems.
__label__safety_in_machine_learning Given the theoretical justification of models' biased learning behavior on different spatial frequency components, which is based on the dataset frequency properties, we argue that the learning behavior on various frequency components could be manipulated by changing the dataset statistical structure in the Fourier domain.
__label__evaluation 2) Human Perception Alignment: To ensure the alignment of our benchmark with human perception, we conducted an extensive user study for each evaluation dimension.
__label__machine_vision We present TransCLIP, a novel and computationally efficient transductive approach designed for Vision-Language Models (VLMs).
__label__active_learning We present Causal Amortized Active Structure Learning (CAASL), an active intervention design policy that can select interventions that are adaptive, real-time and that does not require access to the likelihood.
__label__reinforcement_learning Moreover, as the new metric generalizes the conventional one, the algorithm can address standard offline RL tasks without modification.
"__label__interpretability_and_explainability Distinct from previous work, \method is task-agnostic and can be used, without training, 
to explain and even replace traditional dense CLIP representations, maintaining high downstream performance while significantly improving their interpretability."
__label__generative_models Firstly, we aim to create identifiers that are both semantically relevant and sufficiently distinct to represent individual documents effectively.
__label__machine_vision The development of large foundation models like Large Language Models (LLMs) and Vision Language Models (VLMs) motivates us to investigate a feasible solution to empower VI-ReID performance with off-the-shelf large foundation models.
__label__learning_theory Despite empirical successes, the theoretical understanding of SSL is still far from complete.
__label__bandits This paper introduces the confounded pure exploration transductive linear bandit (CPET-LB) problem.
__label__reinforcement_learning In each setting, we introduce the transfer-ability coefficient $\alpha$ that measures the difficulty of representational transfer.
__label__evaluation We further characterize the game-theoretic properties of LLMs, such as equilibrium and Pareto Efficiency in repeated games.
__label__deep_learning_architectures The code is available on GitHub: \href{https://github.com/Ronglong-Fang/AddressingSpectralBiasviaMGDL}{\texttt{Addressing Spectral Bias via MGDL}}.
__label__natural_language_processing Leveraging these insights, we introduce a new language model called Rho-1.
__label__privacy Considering the existence of largely redundant data samples, pruning them will significantly speed up the training, as proven in plain non-FHE training.
__label__speech_and_audio Through both subjective and objective evaluations, we show that SpeechAlign can bridge the distribution gap and facilitating continuous self-improvement of the speech language model.
__label__machine_learning_for_healthcare Comprehensive numerical experiments and real brain MRI analysis using an ADNI dataset demonstrated the superior performance of our model.
__label__optimization_for_deep_networks Instead of focusing on commonly used sharpness, this work introduces a concept termed *balancedness*, defined as the difference between the squared norm of two variables.
__label__machine_vision Despite its successes, a notable research challenge persists within almost all PEFT approaches: significant performance degradation is observed when there is a substantial disparity between the datasets applied in pretraining and finetuning phases.
__label__machine_learning_for_physical_sciences Our experimental results corroborate our theoretical findings.
__label__generative_models Finally, we provide a latency- and memory-efficient SHiRA implementation based on Parameter-Efficient Finetuning (PEFT) Library which trains at nearly the same speed as LoRA while consuming up to 16% lower peak GPU memory, thus making SHiRA easy to adopt for practical use cases.
__label__interpretability_and_explainability Through experimentation with open-source models, we observe that this ability to retrieve facts can be easily manipulated by changing contexts, even without altering their factual meanings.
__label__diffusion_based_models The project page is at https://seqml.github.io/tse.
__label__machine_vision By conditioning on the low-frequency content retrieved in the first stage, the diffusion model effectively reconstructs the high-frequency details that are typically lost in the lensless imaging process, while also maintaining image fidelity.
__label__neuroscience_and_cognitive_science SynCx also avoids certain systematic grouping errors of current models, such as the inability to separate similarly colored objects without additional supervision.
__label__machine_learning_for_physical_sciences In modern chip design, placement aims at placing millions of circuit modules, which is an essential step that significantly influences power, performance, and area (PPA) metrics.
__label__machine_vision Existing optimizations of the Mamba model, especially when applied in the visual domain, have primarily relied on predefined methods such as improving scanning mechanisms or integrating other architectures, often requiring strong priors and extensive trial and error.
__label__machine_vision Extensive experiments demonstrate that our AdaptIR achieves stable performance on single-degradation tasks, and excels in hybrid-degradation tasks, with training only 0.6% parameters for 8 hours.
__label__learning_theory Much remains to be understood, however, in statistical learning under distribution shifts.
__label__generative_models This field, a collection of interacting SCS feature vectors, skillfully captures both contact and non-contact interactions between body geometries.
__label__optimization To achieve linear running time while maintaining better approximation using a local search strategy, we propose a local search-based approximation algorithm for the CSS problem with exactly k columns selected.
__label__safety_in_machine_learning Drawing inspiration from the difficulties of discrete token optimization, our method relaxes the discrete jailbreak optimization into a continuous optimization process while gradually increasing the sparsity of the optimizing vectors.
__label__machine_learning_for_physical_sciences Dynamic reconstruction in confocal non-line-of-sight imaging encounters great challenges since the dense raster-scanning manner limits the practical frame rate.
__label__machine_learning_for_physical_sciences Finally, UPTs allow for queries of the latent space representation at any point in space-time.
__label__natural_language_processing This oversight could lead to a superficial understanding and interaction with the context, potentially undermining the quality and reliability of the reasoning outcomes.
__label__neuroscience_and_cognitive_science Our method directly converts the well-trained transformer without modifying its attention architecture.
__label__optimization_for_deep_networks Our initial theoretical analysis indicates that SGD performs worse because it applies one single learning rate to all blocks, which cannot handle the heterogeneity among blocks.
__label__generative_models This method only requires the noisy measurements and a forward operator, relying solely on deep networks initialized with random noise to learn and restore the structure of the data.
__label__deep_learning_architectures Further, we construct a Pedestrian Pre-collision Pose Estimation Network (PPSENet) to estimate the collision pose and shape sequence of pedestrians from pedestrian-vehicle accident videos.
__label__neuroscience_and_cognitive_science Code is available at https://github.com/hzc1208/LMHT_SNN.
__label__reinforcement_learning We then provide a pair of efficient reductions to no-regret online convex optimization that are capable of minimizing the regret gap *(a)* under a coverage assumption on the expert (MALICE) or *(b)* with access to a queryable expert (BLADES).
__label__natural_language_processing Our results demonstrate that SimPO consistently and significantly outperforms existing approaches without substantially increasing response length.
__label__diffusion_based_models This paper studies the challenging task of makeup transfer, which aims to apply diverse makeup styles precisely and naturally to a given facial image.
__label__optimization The impacts of heterogeneity and non-stationarity on client unavailability can be significant, as we illustrate using FedAvg, the most widely adopted federated learning algorithm.
__label__machine_vision Code and models are available at: https://github.com/modelscope/facechain/tree/main/face_module/TopoFR.
__label__probabilistic_methods Our code for the sampling methods and benchmarks studied is made public at [this link](https://github.com/GFNOrg/gfn-diffusion) as a base for future work on diffusion models for amortized inference.
__label__graph_neural_networks In this paper, we propose a novel Generative GAD approach (namely GGAD) for the semi-supervised scenario to better exploit the normal nodes.
__label__machine_vision Existing image-based methods, such as LISA, struggle with video tasks due to the additional temporal dimension, which requires temporal dynamic understanding and consistent segmentation across frames.
__label__natural_language_processing A primary challenge in learning a truthfulness classifier is the lack of a large amount of labeled truthful and hallucinated data.
__label__optimization_for_deep_networks Notably, DAMP is able to train a ViT-S/16 on ImageNet from scratch, reaching the top-1 error of 23.7% which is comparable to ResNet50 without extensive data augmentations.
__label__natural_language_processing This paper investigates this challenge by focusing on comics with contradictory narratives, where each comic consists of two panels that create a humorous contradiction.
__label__natural_language_processing It can reduce up to 45\% computational cost and KV storage on Q\&A, summarization, and math solving tasks, 50\% on commonsense reasoning tasks.
__label__graph_neural_networks We establish an exact correspondence between the expressivity of GNNs using diverse activation functions and arithmetic circuits over real numbers.
__label__reinforcement_learning We leverage memoroids to propose a batching method that improves sample efficiency, increases the return, and simplifies the implementation of recurrent loss functions in reinforcement learning.
__label__reinforcement_learning By leveraging flow-based models, PFM transforms less preferred data into preferred outcomes, and effectively aligns model outputs with human preferences without relying on explicit or implicit reward function estimation, thus avoiding common issues like overfitting in reward models.
__label__machine_vision Our project page is at  rccchoudhury.github.io/projects/rlt.
__label__machine_vision Inspired by Matryoshka Representation Learning, we introduce the Matryoshka Query Transformer (MQT), capable of encoding an image into $m$ visual tokens during inference, where $m$ can be any number up to a predefined maximum.
__label__algorithmic_game_theory In the strategic facility location problem, a set of agents report their locations in a metric space and the goal is to use these reports to open a new facility, minimizing an aggregate distance measure from the agents to the facility.
__label__other Current OOV recommendation models often generate 'makeshift' embeddings for OOV items from content features and then jointly recommend with the `makeshift' OOV item embeddings and the behavioral IV item embeddings.
__label__learning_theory Bounds on the smallest eigenvalue of the neural tangent kernel (NTK) are a key ingredient in the analysis of neural network optimization and memorization.
__label__natural_language_processing AlphaLLM is comprised of prompt synthesis component, an efficient MCTS approach tailored for language tasks, and a trio of critic models for precise feedback.
__label__graph_neural_networks In the following analysis, we find that incorporating global label statistics in posterior computation is the key to the success of label smoothing.
__label__causal_inference 2023] which is basically unavoidable in our setting.
__label__optimization_for_deep_networks We explore this assumption by decomposing the weights of fine-tuned models into their pre-trained components and an additional delta.
__label__interpretability_and_explainability In this paper, we propose a novel model-based method to rigorously compute a ranking of state importance across the entire state space.
__label__online_learning For smooth and exp-concave time-varying functions, we achieve an $\mathcal{O}(d \log (\sigma_{1:T}^2 + \Sigma_{1:T}^2))$ bound where $d$ denotes the dimensionality.
__label__natural_language_processing In this work, we propose to use distillation to combat overconfidence: we train the LM to match the reward distribution induced by a model trained on the preference data.
__label__natural_language_processing We reveal the theoretical foundations of techniques for editing large language models, and present new methods which can do so without requiring retraining.
__label__generative_models Thanks to these improvements, Lumina-Next not only improves the basic text-to-image generation but also demonstrates superior resolution extrapolation capabilities as well as multilingual generation using decoder-based LLMs as the text encoder, all in a zero-shot manner.
__label__neuroscience_and_cognitive_science Such transformations of the original trained representation (e.g.
__label__evaluation Vision Language Models (VLMs) demonstrate remarkable proficiency in addressing a wide array of visual questions, which requires strong perception and reasoning faculties.
__label__learning_theory This is a novel statistical-computational trade-off for density estimation, demonstrating that any data structure must use close to a linear number of samples or take close to linear query time.
__label__algorithmic_game_theory committees) through a sequence of minor yet impactful modifications, called reconfiguration path.
__label__machine_vision Inspired by these, we develop a novel Dual-Space Representation Learning (DSRL) method for weakly supervised VVD to utilize the strength of both Euclidean and hyperbolic geometries, capturing the visual features of events while also exploring the intrinsic relations between events, thereby enhancing the discriminative capacity of the features.
__label__machine_learning_for_other_sciences_and_fields Detailed theoretical analysis and abundant empirical evidence consistently show that Attraos outperforms various LTSF methods on mainstream LTSF datasets and chaotic datasets with only one-twelfth of the parameters compared to PatchTST.
__label__machine_vision We validate our design choices through exhaustive ablations and observe improved performance of the resulting long-video (128 frames) encoders over short-video (32 frames) counterparts.
__label__natural_language_processing Our allocation functions involve two steps: reducing element-wise metrics to per-layer importance scores, and modelling layer importance to sparsity ratios.
__label__diffusion_based_models Extensive experiments on the image restoration tasks and source separation and partial generation tasks demonstrate that ProjDiff exhibits superior performance across various linear and nonlinear inverse problems, highlighting its potential for practical applications.
__label__natural_language_processing In this paper, we develop $\textbf{RISE:}$ $\textbf{R}$ecursive $\textbf{I}$ntro$\textbf{S}$p$\textbf{E}$ction, an approach for fine-tuning LLMs to introduce this capability, despite prior work hypothesizing that this capability may not be possible to attain.
__label__interpretability_and_explainability With a multiple-choice-list experiment, we initially estimate the degree of risk preference, probability weighting, and loss aversion in a context-free setting for three commercial LLMs: ChatGPT-4.0-Turbo, Claude-3-Opus, and Gemini-1.0-pro.
__label__diffusion_based_models Towards this goal, we propose a modular diffusion probabilistic IR framework (DP-IR), which allows us to combine the performance benefits of existing pre-trained state-of-the-art IR networks and generative DPMs, while it requires only the additional training of a small module (0.7M params) related to the particular IR task of interest.
__label__machine_vision As shown in Fig.1, a lightweight transformer interface without tuning any foundation model weights is enough for segmentation, grounding, and retrieval in an interleaved manner.
__label__machine_learning_for_healthcare There is therefore a need for models that can identify interactions between drugs and contextual covariates.
__label__machine_learning_for_physical_sciences Experimentally, DeepLag excels in three challenging fluid prediction tasks covering 2D and 3D, simulated and real-world fluids.
__label__generative_models How does the existing theory of the un-identifiability of general DGMs apply to hybrid-DGMs?
__label__neuroscience_and_cognitive_science A self-supervised learning objective is arguably a more biologically plausible organizing principle, as the optimization does not require a large number of labeled examples.
__label__machine_learning_for_other_sciences_and_fields It employs a customized learning-based framework with tailored pretraining and training objectives that enable it to effectively capture code semantics and parallel structural nuances, allowing for bidirectional code translation.
__label__other The results demonstrate significant improvements over current state-of-the-art self-supervised methods, establishing new benchmarks in this field.
__label__machine_vision Self-supervised learning (SSL) has rapidly advanced in recent years, approaching the performance of its supervised counterparts through the extraction of representations from unlabeled data.
__label__other We further identify the common behavior among successful PLL methods as a progressive transition from uniform to one-hot pseudo-labels, highlighting the critical role of mini-batch PL purification in achieving top performance.
__label__diffusion_based_models We develop a reparametrized sampling scheme and a noise cancellation technique that together stabilizes the distillation process.
__label__generative_models We also show promising results on a large text-to-image model where we achieve fast generation of high resolution images directly in image space, without needing autoencoders or upsamplers.
__label__machine_vision 1).
__label__safety_in_machine_learning Based on a provably safe control envelope in dL, we derive a specification for the NN which is proven with NN verification tools.
__label__deep_learning_architectures Recently, a series of SNN projects have achieved tremendous success, significantly improving the SNN's performance.
__label__machine_vision Our codes/models are released at [https://github.com/zyf-815/VSOR/tree/main](https://github.com/zyf-815/VSOR/tree/main).
__label__learning_theory (2023)), our results are independent of any previous findings from regret guarantees of online gambling algorithms.
__label__machine_learning_for_healthcare The learning process of ProtoSurv is not only driven by data but also incorporates pathological domain knowledge, including the awareness of tissue heterogeneity, the emphasis on prior knowledge of prognostic-related tissues, and the depiction of spatial interaction across multiple tissues.
__label__machine_learning_for_physical_sciences In MPP, rather than training one model on a specific physical system, we train a backbone model to predict the dynamics of multiple heterogeneous physical systems simultaneously in order to learn features that are broadly useful across systems and facilitate transfer.
__label__machine_learning_for_other_sciences_and_fields Our codes are available at https://anonymous.4open.science/r/Double-Rounding-EF78/README.md.
__label__fairness Specifically, the algorithm first scores each new example by its influence on fairness and accuracy evaluated on the validation dataset, and then selects a certain number of examples for training.
__label__interpretability_and_explainability We present a case study of a fundamental, yet surprisingly difficult, relational reasoning task: judging whether two visual entities are the same or different.
__label__natural_language_processing Position embedding is a core component of current Large Language Models (LLMs).
__label__learning_theory Experimental validations on both synthetic and real-world datasets in regression and multivariate time series forecasting demonstrate improvements on univariate models, incorporating our method into the training loss and thus leveraging multivariate information.
__label__machine_vision However, the geometry extracted from NeRF is generally inaccurate, which leads to sub-optimal detection performance.
__label__probabilistic_methods In this work, we present a novel method for eigenvector estimation that avoids this dependence on coherence.
__label__learning_theory To address this gap, this work delves deeply into the \textit{benign overfitting} perspective of transformers in vision.
__label__reinforcement_learning However, the concept of subtasks is not sufficiently understood and modeled yet, and existing works often overlook the true structure of the data generation process: subtasks are the results of a *selection* mechanism on actions, rather than possible underlying confounders or intermediates.
__label__machine_vision Deep learning-based feature matching has shown great superiority for point cloud registration in the absence of pose priors.
__label__robotics First, we perform spatial prediction on the masked current frame for learning content features.
__label__privacy We will release the code and benchmark in the near future.
__label__generative_models Despite the widespread use of statistical prior models in various fields, such models for neural network gradients have long been overlooked.
__label__optimization Furthermore, we show that the time-averaged bias is equal to $\alpha V + O(\alpha^2)$, where $V$ is a constant characterized by a Lyapunov equation, showing that $E[\bar{\theta}_n] \approx \theta^*+V\alpha + O(\alpha^2)$, where $\bar{\theta}_n$ is the Polyak-Ruppert average.
__label__optimization Though the model appears frequently in practice, such as for policy problems, it lacks specific analysis in the general setting.
__label__causal_inference Since we are solving a causal problem, where labels don’t exist, we use a causal model to learn costs which are robust to a bounded degree of hidden confounding.
__label__generative_models Our approach involves adjusting visual tokens from the MLP output during inference, controlling the attention response to ensure text prompt tokens attend to visual tokens in referring regions.
__label__probabilistic_methods As a significant application, we demonstrate the potential of our models for phylogenetic analysis of lineages, which involve coalescence, recombination, multiple ancestors, and mutation.
__label__diffusion_based_models DDIM).
__label__probabilistic_methods To overcome this problem, this paper proposes Trifle (Tractable Inference for Offline RL), which leverages modern tractable generative models to bridge the gap between good sequence models and high expected returns at evaluation time.
__label__fairness Multicalibration is shown to be associated with robustness of statistical inference under covariate shift.
__label__evaluation But are language models actually useful for time series?
__label__natural_language_processing Notably, our process-supervised 7b RL model and 34b model (reranking@1024) achieves an accuracy of 34.0% and 52.5% on MATH500, respectively, despite only using human supervision on easy problems.
__label__learning_theory In this paper, we respect the discrete-time nature of training trajectories and investigate the underlying topological quantities that can be amenable to topological data analysis tools.
__label__machine_learning_for_physical_sciences The spatio-temporal relations of impacts of extreme events and their drivers in climate data are not fully understood and there is a need of machine learning approaches to identify such spatio-temporal relations from data.
__label__machine_learning_for_physical_sciences Here, we introduce the State-Exchange Attention (SEA) module, a novel transformer-based module enabling information exchange between encoded fields through multi-head cross-attention.
__label__machine_vision First, we model the text attribute and the positive sample probability, obtaining their empirical probability, which can be seen as the detector's estimation of the likelihood of the target with certain known attributes being predicted as the foreground.
__label__deep_learning_architectures The code is available at: https://github.com/runze1223/Fourier-Basis-Mapping.
__label__speech_and_audio To address this, disentanglement-based encoders have been proposed to remove sensitive information from speech signals without compromising the speech understanding functionality.
__label__bandits ** an estimator of $r(n)$ from feedback collected from any arm $k$, **2.
__label__optimization_for_deep_networks We demonstrate this rank deficiency by studying the time evolution of the *determinant* of a matrix of parameters.
__label__interpretability_and_explainability Due to its powerful representational capabilities, Transformers have gradually become the mainstream model in the field of machine vision.
__label__machine_vision Extensive experiments reveal the vulnerability of current methods and demonstrate the effectiveness of PCRIL, achieving an average 12\% mAP improvement to the current SOTA across all datasets.
"__label__diffusion_based_models Based on the former, the latter addresses ""how to drag"" by collaboratively integrating existing editing guidance with the newly proposed semantic guidance and quality guidance."
__label__reinforcement_learning In this paper, we present the e-COP algorithm, the first policy optimization algorithm for constrained Reinforcement Learning (RL) in episodic (finite horizon) settings.
__label__graph_neural_networks Recently, there has been an emerging trend to integrate persistent homology (PH) into graph neural networks (GNNs) to enrich expressive power.
__label__optimization The Sharpe ratio is an important and widely-used risk-adjusted return in financial engineering.
__label__causal_inference To achieve this, we explore a tensor rank condition on contingency tables for an observed variable set $\mathbf{X}_p$, showing that the rank is determined by the minimum support of a specific conditional set (not necessary in $\mathbf{X}_p$) that d-separates all variables in $\mathbf{X}_p$.
__label__other Our main result is a theoretical justification for this finding: we show that, for a natural class of rational functions, Lanczos-FA matches the error of the best possible Krylov subspace method up to a multiplicative approximation factor.
__label__machine_learning_for_other_sciences_and_fields Label distribution learning is a powerful learning paradigm to deal with label polysemy and has been widely applied in many practical tasks.
__label__machine_vision Vision Transformers (ViT) is known for its scalability.
__label__other Federated learning is a distributed machine learning paradigm designed to protect user data privacy, which has been successfully implemented across various scenarios.
__label__optimization This enables the usage of off-the-shelf MIPS solvers for multi-vector retrieval.
__label__other However, this approach does not inform outlier removal with the estimation task, leaving room for improvement.
__label__speech_and_audio Since supervised data for training the segmentation model is not available, we use reinforcement learning to train the segmentation model to favor segmentations that yield phoneme sequence predictions with a lower perplexity.
__label__machine_learning_for_other_sciences_and_fields In reality, the actual time intervals of user interactions vary dramatically.
__label__reinforcement_learning We formulate CEURL as a novel Controlled Embodiment Markov Decision Process (CE-MDP) and systematically analyze CEURL's pre-training objectives under CE-MDP.
__label__natural_language_processing Motivated by the observation that the adaptation of fully continuous methods has been an overarching trend in Deep Learning, we develop Mixture of Tokens (MoT), a simple, continuous architecture that is capable of scaling the number of parameters similarly to sparse MoE models.
__label__natural_language_processing Finally, LFMs can be modified to provide human-interpretable feedback without performance loss, allowing human verification of desirable behaviour for imitation learning.
__label__diffusion_based_models During diffusion training, current methods diffuse each image across the entire noise space, resulting in a mixture of all images at every point in the noise layer.
__label__safety_in_machine_learning Our method could generate unsafe content through two commercial deep generation models including GPT-4 and DALL·E 2.
__label__other However, existing client selection techniques either introduce significant computation overhead or perform well only in the scenarios where clients have data with similar heterogeneity profiles.
__label__generative_models Generative models are trained with the simple objective of imitating the conditional probability distribution induced by the data they are trained on.
__label__reinforcement_learning Empirical results demonstrate that RC-PPO learns policies with comparable goal-reaching rates to while achieving up to 57% lower cumulative costs compared to existing methods on a suite of minimum-cost reach-avoid benchmarks on the Mujoco simulator.
__label__natural_language_processing We provide empirical evidence that the efficacy of our defence lies in its ``depth'': the degree to which information about harmful representations is removed across {\em all layers} of the LLM.
__label__natural_language_processing Unfortunately, the vocabulary discrepancy between heterogeneous LLMs directly makes averaging the distributions unfeasible due to the token misalignment.
__label__human-AI_interaction Our EAL framework not only acts as an explanatory framework for existing works but also provides us with concrete insights into the limitations of existing methods to handle reward misspecification and novel solution strategies.
__label__machine_vision Gaussian Splatting became widely adopted due to its efficiency and ability to render novel views accurately.
__label__learning_theory We also demonstrate a similar phenomenon in the multivariate-input case; specifically, we show that neural network training problems with a large number of diverse tasks are approximately equivalent to an $\ell^2$ (Hilbert space) minimization problem over a fixed kernel determined by the optimal neurons.
__label__machine_vision Crucially, the frequency transformation segregates the depth information into various frequency components, with low-frequency components encapsulating the core scene structure and high-frequency components detailing the finer aspects.
__label__machine_learning_for_other_sciences_and_fields When data gathered from the real world is polluted, the absence of global information will damage the robust prediction capability of these algorithms.
__label__reinforcement_learning The code is publicly available at \url{https://github.com/LXXXXR/Kaleidoscope}.
__label__safety_in_machine_learning As for the data quality indicator, we compute the per-sample gradients with respect to the private data and the anchor dataset, and use the trace of the accumulated inner products as a measurement of data quality.
__label__machine_vision Over multiple domains, we experimentally compare our method against the alternative of using only the one-shot model, and find that even under equal search-time budgets, our editing-based paradigm provides significant advantages.
__label__probabilistic_methods We further extend our result to show how to sample from a $d$-dimensional spectrahedron, the constrained set of a semidefinite program, specified by the set $\{x\in \mathbb{R}^d: \sum_{i=1}^d x_i A_i \succeq C \}$ where $A_1,\ldots,A_d, C$ are $n\times n$ real symmetric matrices.
__label__machine_learning_for_healthcare We demonstrated the high accuracy of InstructMol on several real-world molecular datasets and out-of-distribution (OOD) benchmarks.
__label__deep_learning_architectures Although Transformers have dramatically advanced the landscape of forecasting, their effectiveness remains debated.
__label__natural_language_processing In the orthogonal subspace, the GD parameters diverge in norm and select the direction that maximizes a margin specific to NTP.
__label__graph_neural_networks In this paper, we aim to bridge this gap by rethinking the transferable patterns on graphs as computation trees -- i.e., tree structures derived from the message-passing process.
__label__learning_theory We also provide extensive numerical experiments for compressed sensing and rank-one matrix estimation demonstrating the advantages of our unrolled architecture \--- in addition to being able to obliviously adapt to general priors, it exhibits improvements over Bayes AMP in more general settings of low dimensions, non-Gaussian designs, and non-product priors.
__label__robotics Experiments show that LLaMAR achieves a 30\% higher success rate than other state-of-the-art LM-based multi-agent planners in MAP-THOR and Search \& Rescue tasks.
__label__machine_vision SlimSAM yields significant performance improvements while demanding over 10 times less training data than any other existing compression methods.
__label__machine_learning_for_healthcare Existing interpretable methods in this domain encounter several challenges, including dependency on specific models, difficulties in understanding and visualization, and issues related to efficiency.
"__label__reinforcement_learning Na\""{i}vely combining a standard offline RL approach with a standard federated learning approach to solve this problem can lead to poorly performing policies."
__label__natural_language_processing However, the processes of manually curating high-quality training data and utilizing online human evaluation platforms are both expensive and limited.
__label__generative_models Previous methods usually quantize the entire body pose into one code, which not only faces the difficulty in encoding all joints within one vector but also loses the spatial relationship between different joints.
__label__interpretability_and_explainability Adhering to the principle of maximum sample reuse and avoiding amplifying factors, we propose a one-sample-fits-all framework parameterized by a sampling vector to approximate  intermediate terms that can be converted to any probabilistic value.
__label__diffusion_based_models The optical transparent layers, which are trained with an online training approach, backpropagating the error to the analytical model of the system, are passive and kept the same across different steps of denoising.
__label__reinforcement_learning The convergence rate of our algorithm matches with the best convergence rate of policy-based algorithms for robust MDPs.
__label__active_learning In particular, we address two fundamental challenges associated with active 3D object detection: data imbalance and the need to cover the distribution of the data, including LiDAR-based point cloud data of varying difficulty levels.
__label__natural_language_processing In addition, we carry out an extensive analysis across different domains and architectures, showing that fractal parameters are robust.
__label__machine_vision However, existing diffusion models rely on extensive and unnecessary mapping to a Gaussian noise domain, which can be replaced by a more efficient and stable interpolation process.
__label__optimization_for_deep_networks Experimental results demonstrate the superiority of SalientLoRA, which outperforms state-of-the-art methods by 0.96\%-3.56\% on multiple datasets.
__label__natural_language_processing While effective for fine-tuning, low-rank structures are generally less suitable for pretraining because they restrict parameters to a low-dimensional subspace.
__label__diffusion_based_models Finally, we design an auxiliary prompting adapter, utilizing Vision-Language Models (VLMs) to enhance text prompts and improve the model's task generalization.
__label__learning_theory In this paper, we investigate the estimation error between the learner obtained by the SCLS method and the actual learner.
__label__robotics In contrast, MoEs effectively address the aforementioned issues while retaining the ability to represent complex distributions but are notoriously difficult to train.
__label__graph_neural_networks A notable application of Graph-AEs is graph-level anomaly detection (GLAD), whose objective is to identify graphs with anomalous topological structures and/or node features compared to the majority of the graph population.
__label__machine_vision Multi-Task Learning (MTL) for Vision Transformer aims at enhancing the model capability by tackling multiple tasks simultaneously.
__label__learning_theory Our methods further extend to semi-parametric settings and imply the first positive results for low-dimensional convex sets.
__label__deep_learning_architectures Numerous TTA studies in deep learning have aimed at minimizing entropy.
__label__learning_theory Through several examples, we demonstrate that even with the same set of eigenfunctions, the order of these functions significantly impacts regression outcomes.
__label__learning_theory Finally, we show how these theoretical findings shed new light on the behavior of the feature maps on the space of PDs that are used in ML-oriented applications of Topological Data Analysis.
__label__machine_learning_for_other_sciences_and_fields Experiments on several real-world data sets demonstrate the efficacy of our approach in terms of both performance prediction accuracy and knowledge estimation ability, when compared with existing student cognitive models.
__label__probabilistic_methods Differential equations are important mechanistic models that are integral to many scientific and engineering applications.
__label__machine_vision Although Vision Transformers (ViTs) have recently advanced computer vision tasks significantly, an important real-world problem was overlooked: adapting to variable input resolutions.
__label__reinforcement_learning Unfortunately, PPO requires multiple heuristics to enable stable convergence (e.g.
__label__machine_vision Code is available at https://github.com/ChelsieLei/EZ-HOI.
__label__optimization When predictions are precise, DISC leverages this information to achieve near-optimal performance.
__label__machine_vision Vision-language models, such as CLIP, have shown impressive generalization capacities when using appropriate text descriptions.
__label__neuroscience_and_cognitive_science Extensive experimental results have demonstrated that our model can outperform previous state-of-the-art works on various types of datasets, which promote SNNs to achieve a brand-new level of performance comparable to quantized ANNs.
__label__interpretability_and_explainability Achieving this in an unsupervised manner requires human users to understand the model's learned concepts and, if necessary, revise incorrect ones.
__label__machine_learning_for_other_sciences_and_fields In the theoretical part, we conduct a quantitative comparison of approximation error between ternary and binary labels to elucidate the superiority of ternary labels over binary labels.
__label__algorithmic_game_theory The dual nature of these systems naturally influences both users and creators: users' preferences are affected by the items they are recommended, while creators may be incentivized to alter their content to attract more users.
__label__natural_language_processing Our re-organization method involves initially extracting logical relationships from the contextual content, such as documents or paragraphs, and subsequently pruning redundant content to minimize noise.
__label__diffusion_based_models Customizing diffusion models to generate identity-preserving images from user-provided reference images is an intriguing new problem.
__label__machine_learning_for_healthcare The proposed knowledge augmentation approach uses large language models (LLM) to refine and enrich surgical concepts, thus providing comprehensive language supervision and reducing the risk of overfitting.
__label__learning_theory The spectacular results achieved in machine learning, including the recent advances in generative AI, rely on large data collections.
__label__reinforcement_learning Moreover, we propose using the lower confidence bound of Q-ensembles for pessimistic Q-value function estimation.
__label__diffusion_based_models We also introduce a cross-attention map regularization term to enhance the learning of the attention map.
__label__optimization Vehicle Routing Problems (VRPs) can model many real-world scenarios and often involve complex constraints.
__label__generative_models The use of deep generative models (DGMs) for synthetic data generation is known to induce considerable bias and imprecision into synthetic data analyses, compromising their inferential utility as opposed to original data analyses.
__label__reinforcement_learning However, these approaches have not significantly advanced in adapting these skills to unseen contexts, which may involve changing environmental conditions or different user requirements.
__label__infrastructure We propose three optimization techniques to mitigate sources of inefficiencies, namely (1) Dynamic gating, (2) Expert Buffering, and (3) Expert load balancing.
__label__deep_learning_architectures Additionally, the computation flow enables prefilling to early exit without changing the final output, thereby significantly speeding up the prefill stage.
__label__other The method is applied directly in the feature space at test time and does not intervene in training process.
__label__robotics Existing VLN models are optimized through expert demonstrations by supervised behavioural cloning or incorporating manual reward engineering.
__label__machine_learning_for_physical_sciences We further design to encode conditions into quantum circuits for property-specified generation.
__label__machine_learning_for_physical_sciences Solving parametric partial differential equations (PDEs) presents significant challenges for data-driven methods due to the sensitivity of spatio-temporal dynamics to variations in PDE parameters.
__label__generative_models Also, we introduce a Generation Trace Capturing Network (GTC) that can efficiently identify generation traces of input images, enhancing the understanding of generated images' provenances.
__label__natural_language_processing Specifically, InfLLM stores distant contexts into additional memory units and employs an efficient mechanism to lookup token-relevant units for attention computation.
__label__machine_vision Deep multiple clustering methods have achieved remarkable performance by exploiting complex patterns and relationships in data.
__label__generative_models In this work, we explore the generation of novel games in the comparatively expansive Ludii game description language, which encodes the rules of over 1000 board games in a variety of styles and modes of play.
__label__machine_vision Continual learning (CL) is designed to learn new tasks while preserving existing knowledge.
__label__reinforcement_learning This approach guarantees safety during and after training, with bounded recovery regret that decreases exponentially with planning horizon depth.
__label__graph_neural_networks Graph class incremental learning (GCIL) requires the model to classify emerging nodes of new classes while remembering old classes.
__label__machine_learning_for_other_sciences_and_fields Extensive evaluations on sixteen diverse datasets demonstrate that SICSM outperforms existing methods, particularly in scenarios characterized by irregular sampling and incomplete observations, which highlight its potential as a reliable tool for scientific discovery and system diagnostics in disciplines that demand precise modeling of complex interactions.
__label__other While the similarity between images is ambiguous, we argue that the spatial location of semantic objects does neither influence human perception nor deep learning classifiers.
__label__causal_inference This paper focuses on interventional CRL under unknown multi-node (UMN) interventional environments and establishes the first identifiability results for *general* latent causal models (parametric or nonparametric) under stochastic interventions (soft or hard) and linear transformation from the latent to observed space.
__label__natural_language_processing Benefiting from the powerful language expression and planning capabilities of Large Language Models (LLMs), LLM-based autonomous agents have achieved promising performance in various downstream tasks.
__label__learning_theory On the other hand, we show that additional assumptions on the volume of the data manifold alleviate these fundamental limitations and guarantee learnability via a simple interpolation argument.
__label__machine_learning_for_physical_sciences The integration of the PDE into a loss function endows PINNs with a distinctive feature to require computing derivatives of model up to the PDE order.
__label__privacy We complement our results with a lower bound and demonstrate the optimality of our polynomial-time algorithms in terms of sample complexity.
__label__reinforcement_learning Our approach outperforms traditional RL methods and existing GPT agents, demonstrating superior efficiency.
__label__machine_vision Additionally, we propose the first RID-oriented iterative mean-teacher framework, termed the Coherence-based Label Generator, to generate high-quality pseudo labels for network training.
__label__graph_neural_networks However, the optimal ratio that favors node classification is unknown, and the non-smooth features of deep GCN with ReLU or leaky ReLU activation function diminish.
__label__privacy Curvature of loss with respect to input (termed input loss curvature) is the trace of the Hessian of the loss with respect to the input.
__label__evaluation We illustrate our concepts on the benchmark suite PMLB and on the platform OpenML.
__label__safety_in_machine_learning This method bypasses the gradient dilemma through selective attacks on vulnerable purifications, incorporating $N$-evaluation into loops and using gradient grafting for comprehensive and efficient evaluations.
__label__learning_theory Extensive simulations confirm the effectiveness of the proposed generalization error estimates.
__label__reinforcement_learning We quantify the policy discrepancies between episodes to enhance exploration and between agents to heterogenize agents, termed intra-agent and inter-agent policy divergence.
__label__diffusion_based_models Through experimental analysis, we demonstrate that our method significantly outperforms existing approaches in complex indoor scene generation with diverse textures, consistent geometry, and realistic visual quality.
__label__speech_and_audio We comprehensively analyze why the boundaries learned by REBORN improve the unsupervised ASR performance.
__label__natural_language_processing Next, the open-ended text output is parsed into an executable action to interact with the environment to obtain goal-directed task rewards.
__label__natural_language_processing We find clear advantages for IRL-based imitation, in particular for retaining diversity while maximizing task performance, rendering IRL a strong alternative on fixed SFT datasets even without online data generation.
"__label__safety_in_machine_learning MEG is based on an adaptation of
the maximum causal entropy framework used in inverse reinforcement learning."
__label__causal_inference For this task, most approaches model count data using Bayesian networks or ordinal relations.
__label__speech_and_audio Exploiting this observation, we propose SILENCE, a lightweight system that selectively obscuring short-term details, without damaging the long-term dependent speech understanding performance.
__label__online_learning The code is available at: https://github.com/Mehrdad-Noori/WATT.
__label__robotics This representation is critical because it enables direct density calculation for diffusion models, making them compatible with existing LLM alignment theories.
__label__reinforcement_learning Exploring unknown environments efficiently is a fundamental challenge in unsupervised goal-conditioned reinforcement learning.
__label__machine_learning_for_physical_sciences It is then evaluated on a suite of 15 challenging downstream tasks that include a wide variety of PDE types and operators.
__label__infrastructure Longer KV cache requires larger memory, limiting the batch-size thus decreasing throughput.
__label__natural_language_processing Given a tokenizer's merge list along with data samples for each category of interest (e.g., different natural languages), we formulate a linear program that solves for the relative proportion of each category in the tokenizer's training set.
__label__probabilistic_methods We demonstrate the performance of our method across several tasks, showing that it can deliver informative designs and facilitate accurate decision-making.
__label__other Furthermore, inspired by the recent success of masked feature prediction, our framework incorporates a two-stage masked token prediction process in which the student model predicts both the global embeddings and token-wise local embeddings derived from the teacher models trained in the first stage.
__label__machine_vision We further propose an efficient surface alignment loss to facilitate training even in the absence of full ground-truth annotation, which is common in publicly available datasets.
__label__causal_inference Standard black-box approaches mapping sequences of categorical variables to outputs are applicable, but they rely on poorly understood assumptions on how reliable generalization can be obtained, and may underperform under sparse sequences, temporal variability, and large action spaces.
__label__human-AI_interaction We use these insights to propose a new interactive algorithm that uses the specified reward to infer potential user expectations about the system behavior.
__label__machine_vision Furthermore, we introduce a data annotation pipeline for generating text prompts based on natural language instructions for the DeepCAD dataset using Mistral and LLaVA-NeXT.
__label__safety_in_machine_learning The algorithm then continues with annotating inside the identified labeling region, reaping the full benefit of human feedback.
__label__learning_theory We also extend our multi-label logistic losses to more comprehensive multi-label comp-sum losses, adapting comp-sum losses from standard classification to the multi-label learning.
__label__optimization We also present experimental results with real datasets that, together with our theoretical analyses, suggest that average-link is a better choice than other related methods when both cohesion and separability are important goals.
__label__safety_in_machine_learning We show at least three scenarios of aligning LLMs with human preferences can benefit from unlearning: (1) removing harmful responses, (2) erasing copyright-protected content as requested, and (3) reducing hallucinations.
__label__machine_vision Using 3D Gaussian Splatting (3DGS) as the underlying scene representation simplifies the extraction of objects of interest which are considered to be a subset of the scene's Gaussians.
__label__generative_models We analyze, theoretically and empirically, how the number of LM calls affects the performance of Vote and Filter-Vote, two of the simplest compound system designs, which aggregate LM responses via majority voting, optionally applying LM filters.
__label__neuroscience_and_cognitive_science The model is integrated with a convolutional neural network (CNN) model of visual processing and evaluated using both artificial and natural image stimuli.
__label__graph_neural_networks Graph Auto-Encoders (GAEs) are powerful tools for graph representation learning.
__label__reinforcement_learning This new approach reduces the chaotic state dynamics, rendering the learnt policies more resilient to sensor noise or adversarial attacks and thereby improving the suitability of deep reinforcement learning for real-world applications.
__label__deep_learning_architectures Our experiments demonstrate that BiXT models outperform larger competitors by leveraging longer sequences more efficiently on vision tasks like classification and segmentation, and perform on par with full Transformer variants on sequence modeling and document retrieval -- but require 28\% fewer FLOPs and are up to $8.4\times$ faster.
__label__machine_vision Complex 3D scene understanding has gained increasing attention, with scene encoding strategies built on top of visual foundation models playing a crucial role in this success.
__label__deep_learning_architectures Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.
__label__machine_vision We use spatial clustering and self-supervised scene flow to obtain a set of static and dynamic object proposals from LiDAR.
__label__neuroscience_and_cognitive_science Furthermore, we analyzed the impacts of different time windows and brain regions on decoding and reconstruction.
__label__bandits A universal minimax lower bound is also established, which scales as $\Omega(d^{L-\frac{3}{2}}\sqrt{T})$.
__label__other With our curated ablation studies and theoretical analyses, we discover that (i) MP improves the CF performance primarily by additional representations passed from neighbors during the forward pass instead of additional gradient updates to neighbor representations during the model back-propagation and (ii) MP usually helps low-degree nodes more than high-degree nodes.
__label__privacy While differential privacy (DP) is a prominent method used to gauge the degree of security provided to large foundation models, its application in large foundation models has been met with limited success because there are often significant performance compromises when applying DP during the pre-training phase.
__label__diffusion_based_models Both qualitative and quantitative comparisons demonstrate the superiority of LucidDrag over previous methods.
__label__graph_neural_networks Then, a graph QFormer encoder adaptively encodes the graph nodes into an auxiliary set of graph prompts to guide the denoising process of diffusion.
__label__reinforcement_learning Unsupervised skill discovery is a learning paradigm that aims to acquire diverse behaviors without explicit rewards.
__label__deep_learning_architectures While extensively studied in traditional explicit neural networks, the $\mathcal{NC}$ phenomenon has not received substantial attention in the context of implicit neural networks.
__label__machine_vision It not only outperforms other unsupervised methods but is also compatible with various architectures of homography estimators.
__label__neuroscience_and_cognitive_science Examples of such abstract environments include auditory tone sequences in which the pitch is continuously varied or images in which abstract features are continuously deformed (e.g., a cartoon bird whose legs stretch and shrink).
__label__natural_language_processing We then present ContextCite, a simple and scalable method for context attribution that can be applied on top of any existing language model.
__label__machine_vision Then, for better aligning visual features of VLMs to our class concept representation, we propose context-guided visual representation that is in the same linear space as class concept representation.
__label__machine_vision While state space models (SSMs) have shown promise in the long-sequence modeling, they face challenges in combining local invariants and global context in visual data.
__label__machine_learning_for_physical_sciences It is based on a multiscale operator transformer, with time-conditioned layer norms that enable continuous-in-time evaluations.
__label__other Upon receiving a textual problem description, VAP automatically synthesizes an image from the visual and spatial clues by utilizing external drawing tools.
__label__neuroscience_and_cognitive_science We validate these findings with human behavioral experiments and hypothesize that the gap is due to insufficient representations of social/emotional and physical knowledge in LMs.
__label__other Our algorithm achieves an order of magnitude improvement in running time compared to the full batch algorithm, with only a minor negative effect on the quality of the solution.
__label__deep_learning_architectures The model is compressed by substituting each layer with its $\mathbf{Q} + \mathbf{L}\mathbf{R}$ decomposition, and the zero-shot performance of the compressed model is evaluated.
__label__learning_theory Thus, the NTK theory may not explain the superior performance of neural networks.
__label__optimization We illustrate how to combine this with Richardson-Romberg extrapolation to derive an iterative scheme with a bias of order $O(\alpha^2)$.
__label__online_learning We study the fundamental problem of sequential probability assignment, also known as online learning with logarithmic loss, with respect to an arbitrary, possibly nonparametric hypothesis class.
__label__diffusion_based_models Time series imputation is important for numerous real-world applications.
__label__deep_learning_architectures The practical utility of FuseMoE in the real world is validated by a diverse set of challenging prediction tasks.
__label__reinforcement_learning To capture the relationship among state-action-RTG triplets, a fine-grained SSM module is designed and integrated into the original coarse-grained SSM in mamba, resulting in a novel mamba architecture tailored for offline RL.
__label__bandits The problem is fairly understood in toy settings with linear target functions or over finite small domains that limits practical interest.
__label__neuroscience_and_cognitive_science Our findings suggest that PAM represents a significant step forward in the pursuit of biologically plausible and computationally efficient sequential memory models, with broad implications for cognitive science and artificial intelligence research.
__label__reinforcement_learning Upon on these findings, in this paper, we propose an Empirical MDP Iteration (EMIT) framework.
__label__infrastructure In practice, adversarial agents can provide false information to the server in order to cheat its way out of contributing to federated training.
__label__interpretability_and_explainability We further find that inferential (reasoning-based) solutions exhibit low complexity bias, which we hypothesize is a key factor enabling them to learn individual mappings for single anchors.
__label__online_learning Cooperating with a pre-trained frozen encoder with Feature Fusion, F-OAL only needs to update a linear classifier by recursive least square.
__label__optimization We show that noisy predictions about the optimal solution can be used to break classical hardness results for maximization problems such as the max-cut problem and more generally, maximization versions of constraint satisfaction problems (CSPs).
__label__machine_learning_for_healthcare To develop computer-aided diagnosis model for histopathology WSIs, previous methods typically employ Multi-Instance Learning to enable slide-level prediction given only slide-level labels.
"__label__other While promising, it raises significant challenges due to the heterogeneous resources and data distributions of clients.This study introduces FlexLoRA, a simple yet effective  aggregation scheme for LLM fine-tuning, which mitigates the ""buckets effect"" in traditional FL that restricts the potential of clients with ample resources by tying them to the capabilities of the least-resourced participants."
__label__safety_in_machine_learning Extensive experiments demonstrate the effectiveness of the proposed universal meta-initialized and gradient robust adversarial attack (UMI-GRAT) toward SAMs and their downstream models.
__label__machine_vision However, fine-tuning on task-specific datasets often leads to overfitting to seen classes and suboptimal performance on unseen classes, due to the absence of unseen class labels.
__label__other How similar are images to each other for neural networks?
__label__machine_vision We investigate this question and find that the features and representations learned during pre-training are not essential.
__label__optimization_for_deep_networks In this paper, we show through systematic experiments with SGD and Adam that the overwhelming benefit of warmup arises from allowing the network to tolerate larger $\eta_{\text{trgt}}$ by forcing the network to more well-conditioned areas of the loss landscape.
__label__deep_learning_architectures This shows how infinite-dimensional Hilbert spaces and finite-dimensional vector spaces fundamentally differ.
__label__optimization_for_deep_networks Drawing on tensor decomposition research, we tackle the main bottleneck of backpropagation, namely the memory footprint of activation map storage.
__label__natural_language_processing This approach results in an average improvement of $3.6\%$ for high-resource languages and $2.3\%$ for low-resource languages across all tasks with just $400$ documents.
__label__robotics Once the trajectory diffusion model is trained, it can generate a temporally coherent sequence of future trajectory for agent based on its current observations.
__label__optimization These global bounds are all valid for any starting point $x_0$ and any symmetric positive definite initial Hessian approximation matrix $B_0$, though the choice of $B_0$ impacts the number of iterations needed to achieve these rates.
__label__safety_in_machine_learning In this paper, we explore a new perspective on this problem, suggesting that it can be alleviated by leveraging innovations inspired in transfer-based attacks that were originally proposed for attacking *black-box* image classification models.
__label__privacy We also find that open-source models typically have lower aggregate memorization than similar models trained on a subset of the data.
__label__neuroscience_and_cognitive_science Natural behaviors, even stereotyped ones, exhibit variability.
__label__machine_vision The source code is available at https://github.com/gwenzhang/Voxel-Mamba.
__label__safety_in_machine_learning Deep State Space Models (SSMs) have proven effective in numerous task scenarios but face significant security challenges due to Adversarial Perturbations (APs) in real-world deployments.
__label__learning_theory Our experiments further validate our theoretical findings.
__label__optimization Under smoothness conditions, we establish strong duality of the proposed DRO problem.
__label__probabilistic_methods Additionally, by leveraging CKA-based feature kernels, we derive feature repulsive terms applied to synthetically generated outlier examples.
__label__machine_vision 2) Leveraging the M-SOG metric, we propose a novel optimization strategy to refine multi-LiDAR placements.
__label__machine_learning_for_physical_sciences Generative models trained at scale can now produce novel text, video, and more recently, scientific data such as crystal structures.
__label__deep_learning_architectures With the steady growth of sensing technology and wearable devices, sensor-based human activity recognition has become essential in widespread applications, such as healthcare monitoring and fitness tracking, where accurate and real-time systems are required.
__label__machine_vision Our extensive testing on multiple public datasets reveals E2E-MFD's superior capabilities, showcasing not only visually appealing image fusion but also impressive detection outcomes, such as a 3.9\% and  2.0\% $\text{mAP}_{50}$ increase on horizontal object detection dataset M3FD and oriented object detection dataset DroneVehicle, respectively, compared to state-of-the-art approaches.
__label__optimization Code for $\mathsf{AOT}$ is available in the Hugging Face  TRL library  \url{https://ibm.biz/AOT_TRL}.
__label__machine_vision Instead, we address the challenge by pruning the search space for point tracking and let the model process only the important regions of the frames without down-sampling.
__label__optimization The goal in this framework is for algorithms to achieve an improved performance when the predictions are accurate while maintaining acceptable guarantees when the predictions are erroneous.
__label__machine_learning_for_social_sciences Additionally, we introduce InformativeBench, the first benchmark tailored for evaluating LLM agents' task-solving ability under information asymmetry.
__label__causal_inference In particular we study the asymmetries that arise in a simple model where we merge the predictors using one binary variable as target and two continuous variables as predictors.
__label__machine_learning_for_other_sciences_and_fields To this end, we formulate a novel Evidential Neural Stochastic Differential Equation (*E-NSDE*) to seamlessly integrate NSDE and evidential learning for effective time-aware sequential recommendations.
__label__natural_language_processing Utilizing `DART`, we have created new datasets for mathematical problem-solving that focus more on difficult queries and are substantially smaller than previous ones.
__label__generative_models While small models struggled with composing even with $k=3$, larger models like GPT-4 performed reasonably well with $k=5$ and $6$.
__label__robotics Addressing this limitation, we present an end-to-end general-purpose multi-modal system named Any-to-Policy Embodied Agents.
__label__natural_language_processing The experimental results demonstrate that UrbanKGent family can not only significantly outperform 31 baselines in UrbanKGC tasks, but also surpass the state-of-the-art LLM, GPT-4, by more than 10% with approximately 20 times lower cost.
__label__machine_vision We make three contributions: First, we introduce the first open-world counting model, CountGD,  where the prompt can be specified by a text description or visual exemplars or both; Second, we show that the performance of the model significantly improves the state of the art on multiple counting benchmarks -- when using text only, CountGD outperforms all previous text-only works, and when using both text and visual exemplars, we outperform all previous models; Third, we carry out a preliminary study into different interactions between the text and visual exemplar prompts, including the cases where they reinforce each other and where one restricts the other.
__label__diffusion_based_models This simplification falls short of addressing the challenging nature of real-world problems, leading to amplified cumulative errors and biases.
__label__machine_vision To address these challenges, most existing methods for domain adaptation harness self-training schemes and attempt to bridge the gap by focusing on a single factor that causes the inter-domain gap, such as objects' sizes, shapes, and foreground density variation.
__label__generative_models To generate realistic noises, we leverage an unpaired noise translation via contrastive learning with a novel mask-guided sampling scheme.
__label__robotics Despite the progress, current VIL methods naively employ VLMs to learn high-level plans from human videos, relying on pre-defined motion primitives for executing physical interactions, which remains a major bottleneck.
"__label__generative_models This phenomenon, characterized by overfitting to specific views, is referred to as the ""Janus Problem""."
__label__online_learning We demonstrate our WRS approach on the Passive-Aggressive Classifier (PAC) and First-Order Sparse Online Learning (FSOL), where our method consistently and significantly outperforms the unmodified approach.
__label__generative_models Our method demonstrates competitive performance in generating natural and faithful motion, providing architecture-agnostic insights and applicability in a variety of downstream tasks.
__label__natural_language_processing Pretrained language models can encode a large amount of knowledge and utilize it for various reasoning tasks, yet they can still struggle to learn novel factual knowledge effectively from finetuning on limited textual demonstrations.
__label__bandits We also show that a variant of our approach, using Exp3 to dynamically select the confidence sets, can be used to improve the empirical performance of stochastic linear bandits while enjoying a regret bound with optimal dependence on the time horizon.
__label__machine_vision To address this challenge, we propose the MambaSCI method, which leverages the Mamba and UNet architectures for efficient reconstruction of quad-Bayer patterned color video SCI.
__label__probabilistic_methods Existing sampling-based UQ methods are unbiased but cannot guarantee convergence and are time-consuming on large-scale graphs.
__label__interpretability_and_explainability Although the SSN approach presents a compelling potential solution, its adaptation to functional data remains complex.
__label__generative_models As large language models (LLMs) become increasingly prevalent across many real-world applications, understanding and enhancing their robustness to adversarial attacks is of paramount importance.
__label__machine_vision We propose the consistent-pixel-downsample (CP-Down) and consistent-pixel-upsample (CP-Up) to maintain dimension consistent and facilitate the full-precision information transfer.
__label__graph_neural_networks We have developed a graph coarsening algorithm based on node representation proximities to efficiently reduce a graph while preserving essential topological information.
__label__speech_and_audio To obtain a material-aware and geometry-aware condition for audio synthesis, we learn an explicit point-based scene representation with audio-guidance parameters on locally initialized Gaussian points, taking into account the space relation from the listener and sound source.
__label__graph_neural_networks We, however, report non-trivial counter-examples, a phenomenon we call reconstruction flip, and highlight the limitations of the existing Graph-AE-based GLAD methods.
__label__machine_learning_for_other_sciences_and_fields Extensive experiments show that PocketFlow outperforms baselines on multiple benchmarks, e.g., achieving an average improvement of 1.29 in Vina Score and 0.05 in scRMSD.
__label__generative_models However, developing practical and efficient methods for applying these representations for general and flexible model editing remains challenging.
__label__robotics Unlike previous work, we introduce FACTORSIM that generates full simulations in code from language input that can be used to train agents.
__label__generative_models Recent research has made significant progress in optimizing diffusion models for downstream objectives, which is an important pursuit in fields such as graph generation for drug design.
__label__graph_neural_networks Graph neural networks (GNNs) have become the dominant solution for learning on graphs, the typical non-Euclidean structures.
__label__natural_language_processing This paper boldly quantizes the weight matrices of LLMs to 1-bit, paving the way for the extremely low bit-width deployment of LLMs.
__label__deep_learning_architectures For example, on the HW-NasBench dataset, CE-NAS reduces carbon emissions by up to 7.22X while maintaining a search efficiency comparable to vanilla NAS.
__label__online_learning classification tasks.
__label__reinforcement_learning The key insight is that the regularization term mitigates erroneous actions for sparse samples and unobserved states covered by domain knowledge.
__label__natural_language_processing This work introduces an instance-adaptive prompting algorithm as an alternative zero-shot CoT reasoning scheme by adaptively differentiating good and bad prompts.
__label__reinforcement_learning To deploy reinforcement learning (RL) systems in real-world scenarios we need to consider requirements such as safety and constraint compliance, rather than blindly maximizing for reward.
__label__natural_language_processing First, the model self-constructs a preference dataset for image descriptions using unlabeled images.
__label__safety_in_machine_learning Intensive noise is then applied to these localized areas, destroying the high-visibility backdoors while preserving global semantic information.
__label__optimization_for_deep_networks Specifically, based on the block diagonal approximation of the Fisher information matrix, we first propose the layer-wise sample method to compute each block matrix without performing a complete back-propagation.
__label__deep_learning_architectures We conduct a series of experiment to verify the effectiveness and generalizability of CompressTracker.
__label__bandits Contextual multinomial logit (MNL) bandits capture many real-world assortment recommendation problems such as online retailing/advertising.
__label__generative_models Our methodology begins with the development of a point-cloud-based network that effectively generates precise and meaningful latent tri-planes, laying the groundwork for accurate 3D mesh reconstruction.
__label__machine_vision Specifically, we propose the Feature-Geometry Coherence Mining module to dynamically adapt the teacher for each mini-batch of data during training and discover reliable pseudo-labels by considering both high-level feature representations and low-level geometric cues.
__label__privacy We provide a new algorithmic framework for differentially private estimation of general functions that adapts to the hardness of the underlying dataset.
__label__privacy Compared with the SOTA two-party inference, $\mathsf{Nimbus}$ improves the end-to-end performance of $BERT_{base}$ inference by $2.7\times \sim 4.7\times$ across different network settings.
__label__optimization_for_deep_networks However, previous STE-based 2:4 pre-training methods (\eg~STE with hard-thresholding, SR-STE) suffer from optimization difficulties because of discontinuous pruning function.
__label__optimization Moreover, for general smooth non-convex problems, we establish a convergence rate of  $O((\log T)/\sqrt{T})$ for KATE, matching the best-known ones for AdaGrad and Adam.
__label__machine_vision This paper introduces a novel probabilistic framework that unifies various recent proposals in long-tail learning.
__label__machine_learning_for_healthcare We then show that many important ECG downstream tasks can be formulated as conditional generation methods in a Bayesian inverse problem framework using $\texttt{BeatDiff}$ as priors.
__label__online_learning Specifically, each loss function consists of two parts: a fixed non-smooth and convex regularizer, and a time-varying function which can be chosen either stochastically, adversarially, or in a manner that interpolates between the two extremes.
__label__infrastructure However, deploying such models for inference is difficult due to their large model size and complex communication pattern.
__label__deep_learning_architectures Additionally, based on varying sensitivity across layers to reparameterization, we develop an automated bit allocation strategy to further reduce memory usage and latency.
__label__diffusion_based_models The CVD framework includes a novel cross-video synchronization module that promotes consistency between corresponding frames of the same video rendered from different camera poses using an epipolar attention mechanism.
__label__safety_in_machine_learning Our code is available at https://github.com/HongchaoZhang-HZ/SEEV.
__label__probabilistic_methods It solves distribution-valued supervised learning, where the output values of the training dataset are probability distributions.
__label__optimization_for_deep_networks We demonstrate that, for the task of pretraining LLaMA models ranging from 60M to 1B parameters on the C4 dataset, Online Subspace Descent achieves lower perplexity than state-of-the-art low-rank training methods across different settings and narrows the gap with full-rank baselines.
__label__machine_vision We explicitly align of initial latents using a Neural Radiance Field (NeRF) to establish a consistent foundational structure in the inpainted area,  complemented by an implicit alignment of intermediate latents through cross-view attention during the denoising phases, enhancing appearance consistency across views.
__label__generative_models In this paper, we undertake a comprehensive examination of DP safety concerns by introducing adversarial scenarios, encompassing offline and online attacks, global and patch-based attacks.
__label__robotics Code and models will be released.
__label__machine_learning_for_other_sciences_and_fields Experimental results on benchmark datasets showcase that CoupleNet outperforms state-of-the-art methods, exhibiting particularly superior performance in low-sequence similarities scenarios,  adeptly identifying infrequently encountered functions and effectively capturing remote homology relationships in proteins.
__label__graph_neural_networks We introduce the Distributed-order fRActional Graph Operating Network (DRAGON), a novel continuous Graph Neural Network (GNN) framework that incorporates distributed-order fractional calculus.
__label__reinforcement_learning However, the generated partners are overfit, reducing their usefulness as training partners.
__label__learning_theory Our tight result indicates that uniform stability has reached its limit in stability analysis for the UD-based algorithm.
__label__deep_learning_architectures Project page is available at \url{https://slotssms.github.io/}
__label__deep_learning_architectures AlphaPruning can be used in conjunction with multiple existing LLM pruning methods.
__label__generative_models This paper introduces a framework that is both theoretical and practical, aimed at assessing how effectively LLMs are able to replicate real-world reasoning mechanisms using these probabilistic measures.
__label__machine_vision Existing methods to remove uninformative tokens either have significant overhead, negating any speedup, or require tuning for different datasets and examples.
__label__natural_language_processing At the same time, training on model-generated positives can amplify various spurious  correlations, resulting in flat or even inverse scaling trends as the amount of data increases.
__label__deep_learning_architectures Numerical experiments validate the effectiveness of our approach.
__label__neuroscience_and_cognitive_science We demonstrate that representations encoded from transformers pre-trained on general chemical structures are highly aligned with human olfactory perception.
__label__generative_models To identify these hybrid-DGMs requires inferring parameters of the physics-based component along with their neural component.
__label__safety_in_machine_learning Several primary defense strategies have been proposed to protect LLMs from producing harmful information, mostly focusing on model fine-tuning or heuristical defense designs.
__label__graph_neural_networks Experiments show that TFE-GNN achieves high generalization and new state-of-the-art performance on various real-world datasets.
__label__machine_learning_for_other_sciences_and_fields The uncovered patterns enable a direct generalization of individual formulas to infinite families, unveiling rich mathematical structures.
__label__deep_learning_architectures Unlike prior adaptive networks, our approach does not train every target sub-network in an iterative manner.
__label__neuroscience_and_cognitive_science The information processing in the brain and embodied agents form a sensory-action loop to interact with the world.
__label__generative_models Inference-time alignment methods avoid the complex post-training step and instead bias the generation towards responses that are aligned with human preferences.
__label__deep_learning_architectures Extensive experiments on ImageNet-1K demonstrate that Des-Nets initialized via LeTs outperform those with 100-epoch from scratch training after only 1 epoch tuning.
__label__graph_neural_networks Our analysis reveals a direct relationship between initial weights, number of training epochs and the model’s vulnerability, offering new insights into adversarial robustness beyond conventional defense mechanisms.
__label__reinforcement_learning To remedy this issue, recent advantage-weighted methods prioritize samples with high advantage values for agent training while inevitably ignoring the diversity of behavior policy.
__label__interpretability_and_explainability By comparing the neuron's response to these generated data points and control data points, we can estimate the quality of the explanation.
__label__machine_vision Humans judge perceptual similarity according to diverse visual attributes, including scene layout, subject location, and camera pose.
__label__machine_vision Heatmap regression has dominated human pose estimation due to its superior performance and strong generalization.
__label__probabilistic_methods These approximations are flexible enough to model complex distributions (multimodal, asymmetric), but they are simple enough that one can calculate their low-order moments and draw samples from them.
__label__optimization In this work, we consider a multi-task setting, in which each agent has its own private reward function corresponding to different tasks, while sharing the same transition kernel of the environment.
__label__algorithmic_game_theory This shift causes the agent to “free-fall” through their action space, yielding non-zero rewards for the principal at zero cost.
__label__natural_language_processing Our MoGU framework transforms the base LLM into two variants: the usable LLM and the safe LLM, and further employs dynamic routing to balance their contribution.
__label__safety_in_machine_learning Each critic provides analysis against the given task and corresponding generated response, equipped with external knowledge queried through relevant code snippets and tools like web search and code interpreter.
__label__algorithmic_game_theory by allowing either i) asymmetric initial conditions, or ii) an asymmetric game or iii) no-external regret dynamics suffices to destroy this result and lead to complex non-equilibrating or even chaotic behavior.
__label__natural_language_processing We present a generative dialogue system capable of operating in a full-duplex manner, allowing for seamless interaction.
__label__reinforcement_learning Partial observability of the underlying states generally presents significant challenges for reinforcement learning (RL).
__label__evaluation In particular, neural network (NN) architectures can withstand minor amounts of dataset imperfection with traditional countermeasures such as regularization, data augmentation, and batch normalization.
__label__diffusion_based_models This paper introduces $\textit{Bifröst}$, a novel 3D-aware framework that is built upon diffusion models to perform instruction-based image composition.
__label__fairness Our investigation reveals that while algorithmic fairness approaches have adapted concepts from legal theory, they can conflict with legal standards, highlighting the importance of bridging the gap between automated decisions, fairness, and anti-discrimination doctrine.
__label__neuroscience_and_cognitive_science Notably, with comparable size to Spikformer (66.34 M, 74.81\%), QKFormer (64.96 M) achieves a groundbreaking top-1 accuracy of **85.65\%** on ImageNet-1k, substantially outperforming Spikformer by **10.84\%**.
__label__diffusion_based_models Our experimental results demonstrate the effectiveness of DoT in multi-digit multiplication, boolean logic, and grade school math problems.
__label__machine_vision We propose a one-prompt Meta-learning framework for Universal Anomaly Segmentation (MetaUAS) that is trained on this synthetic dataset and then generalizes well to segment any novel or unseen visual anomalies in the real world.
__label__generative_models By the second iteration, it exceeds the performance of RLHF-based methods across all metrics, achieving these results with less data.
__label__reinforcement_learning We focus on a class of reinforcement learning algorithms, Monte-Carlo Tree Search (MCTS), in stochastic settings.
__label__graph_neural_networks In this paper, by presenting the experimental evidence and analysis, we surprisingly discover that the graph self-supervised learning models are highly redundant at both of neuron and layer levels, e.g., even randomly removing 51.6\% of parameters, the performance of graph self-supervised learning models still retains at least 96.2\%.
__label__machine_learning_for_other_sciences_and_fields To foster interest in this important application of graph learning, we are releasing a large-scale publicly-licensed benchmark for semantic routing consisting of real-world multi-objective navigation problems---expressed via natural language queries---on the richly annotated road networks of US cities.
__label__machine_learning_for_healthcare However, current methods often neglect the fact that the contribution to prognosis differs with tissue types.
__label__machine_vision With the explosive growth of available training data, single-image 3D human modeling is ahead of a transition to a data-centric paradigm.
__label__optimization_for_deep_networks Our findings demonstrate that, with appropriate weight initialization, GD can train a Transformer model (with either kernel type) to achieve a global optimal solution, especially when the input embedding dimension is large.
__label__causal_inference In many social, behavioral, and biomedical sciences, treatment effect estimation is a crucial step in understanding the impact of an intervention, policy, or treatment.
__label__reinforcement_learning Consequently, there is an urgent need to expedite training and enable model compression in MARL.
__label__graph_neural_networks Based on HTMP and empirical analysis, we reveal that the success of message passing in existing HTGNNs is attributed to implicitly enhancing the compatibility matrix among classes.
__label__reinforcement_learning Offline Reinforcement Learning (Offline RL) presents challenges of learning effective decision-making policies from static datasets without any online interactions.
__label__natural_language_processing Large language models (LLMs) excel at few-shot in-context learning (ICL) -- learning from a few examples provided in context at inference, without any weight updates.
__label__machine_learning_for_other_sciences_and_fields Furthermore, we provide theoretical analysis to guarantee the accuracy and convergence speed of CMCD.
__label__safety_in_machine_learning In this paper, we exploit the structure of the neural network verification problem to generate efficient and scalable cutting planes ${\it specific}$ to this problem setting.
__label__deep_learning_architectures We further demonstrate that the use of derivative loss can be extended to enhance other neural operators, such as the Fourier neural operator (FNO).
__label__deep_learning_architectures Notably, our 3-bit quantized Vicuna-1.5 exhibits competitive performance compared to their 16-bit counterparts.
__label__safety_in_machine_learning It adaptively directs the optimization process between the two tasks during training on data with different prediction uncertainty to calibrate the influence of OOD regularization, which is compatible with many prompt tuning based OOD detection methods.
__label__generative_models Since its introduction in parametric design, knockoff techniques have evolved to handle arbitrary data distributions using deep learning-based generative models.
__label__machine_vision Our approach applies various transformations to extract semantic, structural, boundary, color, and frequency information from datasets, and assess how much each type of information reflects their bias.
__label__natural_language_processing The experiments demonstrate that a robust evaluation model, such as WizardLM-2, closely matches human judgements in both intermediate question-answering and final scenario accuracy, achieving over 80% agreement--similar to the agreement levels among humans.
__label__neuroscience_and_cognitive_science Given that reconstructing stimuli can be improved independently by either improving signal extraction from the brain or by building more powerful generative models, improving the latter may fool us into thinking we are improving the former.
__label__neuroscience_and_cognitive_science Our project is available at https://github.com/gongzix/NeuroClips.
__label__machine_learning_for_healthcare However, our approach parametrizes the solution directly on a dynamic discrete mesh, allowing for the effective modeling of complex biomechanical behaviors.
"__label__machine_learning_for_physical_sciences To reduce the need for training data with heavy simulation costs, we mine unlabeled PDE data without simulated solutions,
and we pretrain neural operators with physics-inspired reconstruction-based proxy tasks."
__label__optimization We also show that $\bar{\theta}_n$ converges with high probability around $\theta^*+\alpha V$.
__label__natural_language_processing Some researchers achieve it by judging whether the question is answerable or not.
__label__neuroscience_and_cognitive_science We apply NER and six other dimensionality reduction techniques to neurons in the primary motor cortex (M1), dorsal premotor cortex (PMd), and primary somatosensory cortex (S1) as monkeys perform reaching tasks.
__label__reinforcement_learning Evaluations on tasks like D4RL show that RGMDT significantly outperforms heuristic DT-based baselines and can achieve nearly optimal returns under given DT complexity constraints (e.g., maximum number of DT nodes).
__label__machine_vision This paper makes a step towards modeling the modality discrepancy in the cross-spectral re-identification task.
__label__reinforcement_learning Moreover, we also propose a practical variant of \SDEPO\ to deal with continuous action space and empirical results demonstrate the practical superiority of the proposed method.
__label__generative_models To assess the quality of generated sequences, we conduct extensive experiments on 15 species for conditional and unconditional DNA generation.
__label__probabilistic_methods Codes are publicly available at \url{https://github.com/KenCao2007/WSM_TPP}.
__label__machine_learning_for_physical_sciences CoDA-NO can learn representations of different PDE systems with a single model.
__label__reinforcement_learning Such observations are available in many applications, including transactions, navigation and more.
__label__interpretability_and_explainability Our results suggest that the utility of formal specifications for human interpretability is still unsupported but point to other avenues of development which may enable improvements in system validation.
__label__natural_language_processing We analyze challenges that LLMs face in these settings and propose a new solution that combines the power of LLMs in understanding questions with automated reasoning techniques to handle complex database constraints.
__label__machine_learning_for_physical_sciences For downstream tasks, we demonstrate that finetuning MPP-trained models results in more accurate predictions across multiple time-steps on systems with previously unseen physical components or higher dimensional systems compared to training from scratch or finetuning pretrained video foundation models.
__label__machine_vision To address these limitations, we introduce the anomaly personalization method, which performs a personalized one-to-normal transformation of query images using an anomaly-free customized generation model, ensuring close alignment with the normal manifold.
__label__natural_language_processing Importantly, to the extent to which tokenizer training data is representative of the pretraining data, we indirectly learn about the pretraining data.
__label__safety_in_machine_learning Additionally, we have observed that current methods struggle to identify classifiers trojaned using adversarial training.
__label__other However, merely using the 'makeshift' embedding will result in suboptimal recommendation performance due to the substantial gap between the content feature and the behavioral embeddings.
__label__natural_language_processing Combining both cascades, CS Drafting achieves greater speedup compared to the baselines in our experiments, while preserving the same output distribution as the target model.
__label__online_learning We investigate this question from an information-theoretic perspective by introducing a new framework, Oracle-Efficient Online Estimation (OEOE), where the learner can only interact with the data stream indirectly through a sequence of offline estimators produced by a black-box algorithm operating on the stream.
__label__optimization Within the nonparametric setting (NPIV), recent methods have been based on nonlinear generalizations of Two-Stage Least Squares and on minimax formulations derived from moment conditions or duality.
__label__interpretability_and_explainability Our analysis reveals that the primary cause is data-related: critical information for image classification is encoded in the VLM's latent space but can only be effectively decoded with enough training data.
__label__reinforcement_learning Our analysis also provides new theoretical perspectives on categorical approaches to distributional RL, as well as introducing a new distributional Bellman equation, the stochastic categorical CDF Bellman equation, which we expect to be of independent interest.
__label__bandits For the latter problem, contextual linear bandits, we provide an algorithm that achieves $O ( \sqrt{d T \log ( K \min\{ 1, \frac{S}{d} \} )} )$ together with a matching lower bound, where $d$ and $S$ represent the dimensionality of feature vectors and the size of the context space, respectively.
__label__diffusion_based_models CAT3D can create entire 3D scenes in as little as one minute, and outperforms existing methods for single image and few-view 3D scene creation.
__label__causal_inference Notably, recent models have disregarded the importance of invertible representation, compromising identification assumptions.
__label__diffusion_based_models Our comprehensive experiments confirm the effectiveness of both VideoPrefer and VideoRM, representing a significant step forward in the field.
__label__natural_language_processing This theoretical framework could help to more effectively predict and guide the evolution of LLMs in desired directions.
__label__robotics Moving to the experimental side, we demonstrate that CON reaches SoA performance when learning complex nonlinear dynamics of mechanical systems directly from images.
__label__machine_vision For each query, this operation uses key-aware attention weights to combine their corresponding deformable sampling positions to predict a new query position.
__label__machine_learning_for_other_sciences_and_fields Deep learning models like AlphaFold2 have revolutionized protein structure prediction, achieving unprecedented accuracy.
__label__natural_language_processing Notably, our approach surpasses the state-of-the-art models Wanda and SparseGPT, showcasing its ability to excel even under high sparsity levels.
__label__deep_learning_architectures CherryQ identifies and preserves the critical cherry parameters in high precision while aggressively quantizing the remaining parameters to low precision.
__label__probabilistic_methods This coefficient is able to capture arbitrary monotone relationships, both linear and nonlinear ones.
__label__privacy We propose a differentially private (DP) best subset selection method with strong statistical utility properties by adopting the well-known exponential mechanism for selecting the best model.
__label__machine_learning_for_other_sciences_and_fields When benchmarked on the MD17 and QH9 datasets, DEQHNet, an instantiation of the DEQH framework, has demonstrated a significant improvement in prediction accuracy.
__label__generative_models The new challenges posed by T2MVid generation lie in the lack of massive captioned multi-view videos and the complexity of modeling such multi-dimensional distribution.
__label__robotics Specifically, three kinds of top-down guidance factors (i.e., driver intention, semantic context, and traffic rule) are integrated into our model.
__label__other Experiments on synthetic and real-world datasets including images and tabular data validate the efficacy of our EvoRate.
__label__causal_inference Causal discovery is essential for understanding relationships among variables of interest in many scientific domains.
__label__natural_language_processing In this work, we argue that the SFT stage significantly benefits from learning a reward model as well.
__label__machine_learning_for_physical_sciences Additionally, we present a benchmark of design, control, and learning tasks on high-fidelity, high-resolution dynamic fluid environments that pose challenges for existing differentiable fluid simulators.
__label__generative_models By constructing visual masks for instruction-irrelevant regions, IVM-enhanced multimodal models can effectively focus on task-relevant image regions to better align with complex instructions.
__label__machine_vision Specifically, on one hand, we generate complementary 3D primitives based on both geometric and textural priors, which reduces the initial errors that accumulate in subsequent procedures.
__label__interpretability_and_explainability Moreover, MambaLRP facilitates a deeper inspection of Mamba architectures, uncovering various biases and evaluating their significance.
__label__robotics For robots to be more generalizable embodied agents, they should be capable of following instructions and perceiving the world with adaptation to diverse modalities.
__label__machine_vision Pre-training on this dataset outperforms using English-only or English-dominated datasets on ImageNet, ImageNet distribution shifts, image-English-text retrieval and on average across 38 tasks from the DataComp benchmark.
__label__reinforcement_learning See more details in our project page: [https://sites.google.com/view/be-cause](https://sites.google.com/view/be-cause).
__label__other We also show that the tradeoff between memory and number of passes of our algorithms is near-optimal.
__label__other As no public dataset exists for the studied problem, we collect two datasets with map levels of LoD3.0 and LoD2.0, along with real RGB queries and ground-truth pose annotations.
__label__generative_models Despite its promising capabilities, Lumina-T2X still encounters challenges including training instability, slow inference, and extrapolation artifacts.
__label__machine_vision In this paper, we explore the potential of a pure visual foundation model as an alternative to widely used vision-language models for universal visual anomaly segmentation.
__label__machine_vision To address this problem, we propose TopoFR, a novel FR model that leverages a topological structure alignment strategy called PTSA and a hard sample mining strategy named SDE.
__label__probabilistic_methods Second, we develop an efficient neural-network-based CDE solver.
__label__generative_models We conduct extensive experiments on various diffusion models including Stable Diffusion series and DiTs.
__label__learning_theory This paper proposes a theoretical framework based on probably approximately correct (PAC) learning theory to assess the instance-level learnability of deep multiple instance learning (Deep MIL) algorithms.
__label__diffusion_based_models Warning: This paper contains model outputs that may be offensive in nature.
__label__machine_vision To ensure that the restored images lie onto the data manifold, we propose a novel sampling technique on a pre-trained diffusion model.
__label__reinforcement_learning In addition, DiffuserLite can also serve as a flexible plugin to increase the decision-making frequency of other diffusion planning algorithms, providing a structural design reference for future works.
__label__diffusion_based_models For example, SFD achieves 4.53 FID (NFE=2) on CIFAR-10 with only **0.64 hours** of fine-tuning on a single NVIDIA A100 GPU.
__label__neuroscience_and_cognitive_science Our findings uncover a parallel between the computational mechanisms of LLMs and human memory, offering valuable insights into both research fields.
__label__deep_learning_architectures In response to these problems, we propose the Spiking Token Mixer (STMixer) architecture, which consists exclusively of operations supported by asynchronous scenarios, including convolutional, fully connected layers and residual paths.
__label__optimization We tackle these weaknesses.
__label__optimization_for_deep_networks Additionally, $($FG$)^2$U is inherently designed to support parallel computing, enabling it to effectively leverage large-scale distributed computing systems to achieve significant computational efficiency.
__label__machine_vision Furthermore, we propose an area regularization loss, which coarsely reduces irrelevant background predictions on a large scale.
__label__natural_language_processing The models were able to align with human preferences on issues of safety, factualness, and bias concurrently.
__label__graph_neural_networks Tensor decomposition-based (TDB) models have demonstrated strong performance in KGC.
__label__machine_vision The collaboration optimization between point and pixel representations jointly facilitates our model to grasp underlying 3D anomaly patterns, contributing to detecting and segmenting anomalies of unseen diverse 3D objects.
__label__robotics However, existing fine-tuning efforts for VFMs often overlook the crucial role of probing in effectively adapting these descriptors for improved image representation.
__label__machine_vision Our qualitative and quantitative analyses reveal that Yo'LLaVA can learn the concept more efficiently using fewer tokens and more effectively encode the visual attributes compared to strong prompting baselines (e.g., LLaVA).
__label__causal_inference This paper focuses on _general_ latent causal models, stochastic _soft_ interventions, and a linear transformation from the latent to the observation space.
__label__learning_theory More precisely, we first propose an improved minimax lower bound for the kernel regression problem in large dimensional settings and show that the gradient flow with early stopping strategy will result in an estimator achieving this lower bound (up to a logarithmic factor).
__label__graph_neural_networks For example, S²GNNs allow for free positional encodings that make them strictly more expressive than the 1-Weisfeiler-Leman (WL) test.
__label__bandits However, it may not be available in practice.
__label__deep_learning_architectures if input percept is a field.
__label__machine_vision However, existing methods struggle with the lack of sample diversity for minority classes and the limitation of suitable placement.
__label__bandits In this work, we consider the same problem and provide the first provably efficient algorithms with sublinear regret under realizability.
__label__machine_learning_for_other_sciences_and_fields AliDiff shifts the target-conditioned chemical distribution towards regions with higher binding affinity and structural rationality, specified by user-defined reward functions, via the preference optimization approach.
__label__other Both our quantitative and qualitative results on Wikipedia show that OLLM outperforms subtask composition methods, producing more semantically accurate ontologies while maintaining structural integrity.
__label__probabilistic_methods We demonstrate the effectiveness of our framework, especially at high sparsity levels, across a range of different neural network architectures and datasets.
__label__diffusion_based_models We introduce Equivariant Neural Diffusion (END), a novel diffusion model for molecule generation in 3D that is equivariant to Euclidean transformations.
__label__probabilistic_methods In this paper, we propose Treeffuser, an easy-to-use method for probabilistic prediction on tabular data.
__label__learning_theory For Markovian models, we show how to efficiently approximate 2sED from below through a layerwise iterative approach, which allows us to tackle deep learning models with a large number of parameters.
__label__natural_language_processing Furthermore, a two-step training scheme is introduced to first train to embeded the external capabilities into LLMs and then train the collaborative utilization of the external capabilities and the built-in capabilities in LLMs.
__label__learning_theory Even then, empirical evidence shows preference aligned LLMs can be enticed to harmful behaviour.
__label__robotics Robot videos are best viewed at https://dynamo-ssl.github.io.
__label__interpretability_and_explainability To foster their reliable use in real-world scenarios, it is crucial to augment their transparency.
__label__fairness Specifically, we first estimate all possible treatment effects of sensitive attributes on a given prediction model from all possible adjustment sets of sensitive attributes via an efficient local approach.
__label__machine_learning_for_other_sciences_and_fields Single-stage neural combinatorial optimization solvers have achieved near-optimal results on various small-scale combinatorial optimization (CO) problems without requiring expert knowledge.
__label__learning_theory By employing affine combinations, our method explores areas beyond the convex hull defined by anchors, thereby illuminating blind spots in the reconstruction of missing samples.
__label__interpretability_and_explainability In this work, we dramatically expand a notion of *decision sparsity* called the *Sparse Explanation Value* (SEV) so that its explanations are more meaningful.
__label__machine_learning_for_physical_sciences Our method is built upon minimal inductive biases, encompassing not only commonly utilized symmetries rooted in Lie groups but also extending to symmetries derived from nonlinear generators.
__label__reinforcement_learning We conduct an evaluation with real human teammates, and the results show that GAMMA consistently improves performance, whether the generative model is trained on simulated populations or human datasets.
__label__learning_theory Our proposed methodologies are computationally efficient and easy to implement.
__label__privacy For generalized linear models (GLM) with unlabeled public data, we show an efficient algorithm which, given $\tilde{O}({n_{\text{priv}}}\epsilon)$ unlabeled public samples, achieves the dimension independent rate $\tilde{O}\big(\frac{1}{\sqrt{{n_{\text{priv}}}}} + \frac{1}{\sqrt{{n_{\text{priv}}}\epsilon}}\big)$.
__label__probabilistic_methods We introduce the notions of weakly and strongly invariant  laws (WI and SI) on the parameter space of each single unit, corresponding, respectively, to $G$-invariant distributions, and to distributions supported on parameters fixed by the group action (which encode EA).
__label__graph_neural_networks Graph Neural Networks have achieved remarkable accuracy in semi-supervised node classification tasks.
__label__generative_models In this paper, we present Lumina-Next, an improved version of Lumina-T2X, showcasing stronger generation performance with increased training and inference efficiency.
__label__neuroscience_and_cognitive_science However, current AAD algorithms overlook the spatial distribution information within EEG signals and lack the ability to capture long-range latent dependencies, limiting the model's ability to decode brain activity.
"__label__privacy Yet, providing a measure that captures how much a given dataset is ""easy"" for this task turns out to be challenging, and was not properly addressed in prior works."
__label__algorithmic_game_theory A fundamental question about strategic dynamics is such: for a given game and initial strategy profile, is it always possible to construct a satisficing path that terminates at an equilibrium?
__label__fairness Experimental results on real-world networks validate that the proposed tools herein deliver effective structural bias mitigation for both real and synthetic graphs.
__label__machine_vision We design Language Hierarchical Self-training (LHST) that introduces language hierarchy into weakly-supervised detector training for learning more generalizable detectors.
__label__natural_language_processing Experimental results show that this architecture achieves state-of-the-art performance on MUSIC-AVQA-R, notably obtaining a significant improvement of 9.32\%.
__label__machine_vision Unlike the guide hair sampled from the scalp UV map grids which may lose capturing details of the hairstyle in existing methods, our method samples optimal sparse guide strands by utilizing $k$-medoids clustering centres from low-pass filtered dense strands, which more accurately retain the hairstyle's inherent characteristics.
__label__infrastructure In Large Language Model (LLM) inference, the output length of an LLM request is typically regarded as not known a priori.
__label__diffusion_based_models Image editing serves as a practical yet challenging task considering the diverse demands from users, where one of the hardest parts is to precisely describe how the edited image should look like.
"__label__optimization This paper introduces a decentralized algorithm that eliminates the need for specific parameter
tuning."
__label__machine_learning_for_physical_sciences The interplays of highly complex light-matter interaction, e.g., scattering and resonance, sensitivity to local structure details, non-uniform learning complexity for full-domain simulation, and rich frequency information, contribute to the failure of existing neural PDE solvers.
__label__diffusion_based_models Models and codes will be made publicly available.
__label__machine_vision Specifically, we initially employ pre-trained stable video diffusion models to adapt the event sequence dataset.
__label__online_learning We show experimentally that our method is the least affected by the label delay factor and in some cases successfully recovers the accuracy of the non-delayed counterpart.
__label__natural_language_processing Modern large language models (LLMs) have established state-of-the-art performance through architectural improvements, but still require significant computational cost for inference.
__label__natural_language_processing Importantly, our framework does not require extra data collection and human annotations, offering strong flexibility and practicality for real-world applications.
__label__reinforcement_learning To address this challenge, we propose a primal dual-based policy gradient algorithm that adeptly manages the constraints while ensuring a low regret guarantee toward achieving a global optimal policy.
__label__graph_neural_networks However, enumerating the automorphism groups of all subgraphs of interest and finding appropriate equivariant operations for each one of them separately is generally not feasible.
"__label__other Our theory shows that the predictor statistics are expressed as a sum of independent kernels, each one pairing different ""attention paths"", defined as information pathways through different attention heads across layers."
__label__graph_neural_networks For example, in comparison with full fine-tuning GraphMAE on Amazon-Computers dataset, even randomly reducing 40\% of parameters, we can still achieve the improvement of 0.24\% and 0.27\% for Micro-F1 and Macro-F1 scores respectively.
__label__optimization_for_deep_networks In this paper, to address the issue above on camera-LiDAR fusion models, we propose a novelty pruning framework Alternative Modality Masking Pruning (AlterMOMA), which employs alternative masking on each modality and identifies the redundant parameters.
__label__learning_theory Motivated by this connection, we undertake a comprehensive study of secluded partitions and establish near-optimal relationships between $k$ and $\varepsilon$.
__label__natural_language_processing The total memory usage (TMU) of StarCoder2-15B also decreases from 22.02 (Mb*s) to 2.03 (Mb*s), which decreases 90.8\% total memory consumption during the execution process.
__label__bandits Concretely, our combinatorial HierTS algorithm attains comparable Bayes regret bound $O(m\sqrt{n}\log{n})$ with respect to the latest one.
__label__optimization We propose adaptive versions of GD and ProxGD that are based on observed gradient differences and, thus, have no added computational costs.
__label__probabilistic_methods In this paper, we repurpose this method for probabilistic inference by incorporating Markovian sampling methods in evaluating the FM objective, and using the learned CNF to improve Monte Carlo sampling.
__label__neuroscience_and_cognitive_science Brain-JEPA incorporates two innovative techniques: **Brain Gradient Positioning** and **Spatiotemporal Masking**.
__label__causal_inference There is a provably sound cumulant-based causal discovery method that allows the identification of the causal structure under a branching structure.
__label__machine_learning_for_other_sciences_and_fields To address this, we propose a novel forecasting model with a structured matrix basis.
"__label__learning_theory We show that
the global minimizer of the regularized loss of DNNs can fit for example
the composition of two functions $f^{*}=h\circ g$ from a small number
of observations, assuming $g$ is smooth/regular and reduces the dimensionality
(e.g."
__label__generative_models To address this, we introduce physics-informed generative cryo-electron microscopy (CryoGEM), which for the first time integrates physics-based cryo-EM simulation with a generative unpaired noise translation to generate physically correct synthetic cryo-EM datasets with realistic noises.
__label__machine_learning_for_other_sciences_and_fields The framework was implemented in a formal synthetic domain, demonstrating that it is transparent and systematic.
__label__graph_neural_networks Soft labels can improve the generalization of a neural network classifier in many domains, such as image classification.
__label__optimization_for_deep_networks Finally, we show that the need for warmup can be significantly reduced or eliminated by modifying the optimizer to explicitly normalize $\mathbf{u}_t$ based on the aforementioned metrics.
"__label__graph_neural_networks We claim that sum-based aggregators fail to ""mix"" features belonging to distinct neighbors, preventing them from succeeding at downstream tasks."
__label__interpretability_and_explainability We track both the emergence and the precise working of these iteration heads down to the attention level, and measure the transferability of the CoT skills to which they give rise between tasks.
__label__machine_learning_for_other_sciences_and_fields Moreover, our synthesized circuits significantly outperform the state-of-the-art results from several competitive winners in IWLS 2022 and 2023 competitions.
__label__safety_in_machine_learning However, most MIA methods perform no better than random guessing when discriminating between members and non-members from the same distribution (e.g., in this case, the same period of time).
__label__diffusion_based_models The project page is https://mvig-rhos.com/pa_diffusion.
__label__natural_language_processing whether a solution is correct or helpful).
__label__natural_language_processing Motivated by the transformative capabilities of large language models (LLMs) across various natural language tasks, there has been a growing demand to deploy these models effectively across diverse real-world applications and platforms.
__label__graph_neural_networks This extends 1-WL, which can only count homomorphisms of trees and, in fact, is incomparable to $k$-WL for any fixed $k$.
__label__deep_learning_architectures For example, we show that a one-layer Transformer of logarithmic width can perform index lookup, whereas an RNN requires a hidden state of linear size.
__label__natural_language_processing In this paper, we address the problem of sampling a set of high-quality and diverse translations.
__label__reinforcement_learning Due to the exponential growth of agent interactions and the curse of dimensionality, learning efficient coordination from scratch is inherently challenging in large-scale multi-agent systems.
__label__learning_theory To this end, we propose a novel anchor-based strategy for KGE, i.e., a relational clustering-based anchor selection strategy (RecPiece), where two characteristics are leveraged, i.e., (1) representative ability of the cluster centroids and (2) descriptive ability of relation types in KGs.
__label__machine_learning_for_social_sciences Experimental results on CogMir subsets show that LLM Agents and humans exhibit high consistency in irrational and prosocial decision-making under uncertain conditions, underscoring the prosociality of LLM Agents as social entities, and highlighting the significance of hallucination properties.
__label__optimization_for_deep_networks Additionally, the importance measurement focuses only on parameters with minimal impact on the loss, neglecting the dominant role of singular values in SVD-based matrices and the fluctuations during training.
__label__deep_learning_architectures With CLA, we find that it is possible to reduce the size of the KV cache by another $2\times$ while maintaining nearly the same accuracy as unmodified MQA.
__label__machine_learning_for_physical_sciences Additionally, we introduce an efficient ViT-like mesh autoencoder to generate spatially coherent mesh embeddings for a large number of meshing cells.
__label__natural_language_processing We furthermore derive the *idealized ranking accuracy* that a preference-tuned LLM would achieve if it optimized the DPO or RLHF objective perfectly.
__label__optimization One-shot pruning techniques offer a way to alleviate these burdens by removing redundant weights without the need for retraining.
__label__fairness Theoretical analysis indicates that the upper bound on the loss incurred by this method is less than or equal to the loss of the Lagrangian approach, which involves adding a regularization term to the loss function.
__label__causal_inference We provide realistic conditions on shift properties and the estimation objectives that lead to identification even when only one off-support target sample is available, tackling the most challenging scenarios.
__label__optimization_for_deep_networks However, the generalization properties of second-order methods are still being debated.
__label__probabilistic_methods Furthermore, we show that the underlying tree structure can be automatically learned from training data.
__label__reinforcement_learning Additionally, it achieves SOTA performance across all designated MineDojo tasks.
__label__interpretability_and_explainability Detailed decompositions quantify the importance of each variable to each term in the aggregate decomposition, which can provide a deeper understanding and suggest more targeted interventions.
__label__algorithmic_game_theory By carefully bounding the performative effect in theoretical analysis, we prove that the proposed algorithm achieves sublinear convergence rates for both performative regrets and constraint violation and maintains the same order of convergence rate as the case without performativity.
__label__learning_theory We introduce \textbf{N}on-\textbf{Euc}lidean-\textbf{MDS} (Neuc-MDS), which extends Multidimensional Scaling (MDS) to generate outputs that can be non-Euclidean and non-metric.
__label__safety_in_machine_learning We propose a formal threat model for AI agents communicating steganographically and derive rigorous theoretical insights about the capacity and incentives of large language models (LLMs) to perform secret collusion, in addition to the limitations of threat mitigation measures.
__label__natural_language_processing The judge answers based on a detailed reference scenario or evaluates if the player's predictions align with the reference one.
__label__optimization Existing methods for such problems either only guarantee asymptotic convergence, have slow sublinear rates, or require strong assumptions.
__label__machine_learning_for_social_sciences Predictive modeling often faces challenges due to limited data availability and quality, especially in domains where collected features are weakly correlated with outcomes and where additional data collection is constrained by ethical or practical difficulties.
__label__neuroscience_and_cognitive_science Spiking neural networks (SNNs) have gained more and more interest as one of the energy-efficient alternatives of conventional artificial neural networks (ANNs).
__label__reinforcement_learning This prior enables the application of any Bayesian approach for online decision-making, such as posterior sampling.
__label__machine_vision Code is at https://github.com/PAN083/MambaSCI.
__label__graph_neural_networks Moreover, controlling layer-wise training speeds is linked to grokking-like phenomena, which may be of independent interest.
__label__bandits The remarkable instruction-following capability of large language models (LLMs) has sparked a growing interest in automatically finding good prompts, i.e., prompt optimization.
__label__optimization_for_deep_networks We investigate the implications of our approach for ReLU MLPs and ResNets in the large width-then-depth limit.
__label__safety_in_machine_learning To reduce harm and misuse, efforts have been made to align these LLMs to human values using advanced training techniques such as Reinforcement Learning from Human Feedback (RLHF).
__label__natural_language_processing We propose a fast adversarial training algorithm (C-AdvUL) composed of two losses: the first makes the model robust on continuous embedding attacks computed on an adversarial behaviour dataset; the second ensures the usefulness of the final model by fine-tuning on utility data.
__label__reinforcement_learning However, they overlook a direct and complementary way to exploit cross-task similarities: the control policies of tasks already proficient in some skills can provide explicit guidance for unmastered tasks to accelerate skills acquisition.
__label__optimization When both distributions are discrete, a simple combinatorial framework has been used to find the exact solution (see e.g.
__label__safety_in_machine_learning Still, the limited certified robustness that is currently achievable has been a bottleneck for their practical adoption.
__label__interpretability_and_explainability By analyzing the information flow and vector representations within the model, we reveal the distinct mechanisms underlying these solution types.
__label__reinforcement_learning Our theoretical results demonstrate the benefits of using the learned representation from the upstream offline task instead of directly learning the representation of the low-rank model.
__label__machine_learning_for_other_sciences_and_fields The advancements in large language models (LLMs) present a promising solution to these problems from a semantic perspective.
__label__robotics Additionally, LeapAD incorporates an innovative dual-process decision-making module, which consists of an Analytic Process (System-II) for thorough analysis and reasoning, along with a Heuristic Process (System-I) for swift and empirical processing.
__label__deep_learning_architectures Meta-learning empowers data-hungry deep neural networks to rapidly learn from merely a few samples, which is especially appealing to tasks with small datasets.
__label__deep_learning_architectures the input resolution for vision transformers.
__label__natural_language_processing However, this approach involves trade-offs, such as slower inference speed and increased space occupancy.
__label__other Notably, compared to prompting with clean rationales, base LLM drops by 1.4%-19.8% in accuracy with irrelevant thoughts and more drastically by 2.2%-40.4% with inaccurate thoughts.
__label__learning_theory A common approach assumes that the sources fall in one of several unknown subgroups, each with an unknown input distribution and input-output relationship.
__label__safety_in_machine_learning We empirically validate our insights on a range of vision and language tasks, demonstrating that risk control can produce substantial computational savings, all the while preserving user-specified performance goals.
__label__causal_inference Despite the progress in the past decades, traditional causal discovery approaches (CDs) mainly rely on high-quality measured variables, usually given by human experts, to find causal relations.
__label__machine_learning_for_physical_sciences However, existing learning-based methods require re-training from scratch given a different PDE type or boundary geometry, which limits their applicability, and also often suffer from robustness issues in the form of inverted elements.
__label__learning_theory In Phase 2, the attention matrices and the MLP evolve jointly to enlarge the classification margin and reduce the loss to a near minimum value.
__label__safety_in_machine_learning Our approach achieves state-of-the-art deterministic robustness certificates on CIFAR-10 for the $\ell_2$ ($\epsilon = 36/255$) and $\ell_{\infty}$ ($\epsilon = 8/255$) threat models, outperforming the previous results by $+3.95$ and $+1.39$ percentage points, respectively.
__label__generative_models Our paper addresses the complex task of transferring a hairstyle from a reference image to an input photo for virtual hair try-on.
__label__optimization While exact solvers have been proposed for mixed-integer *linear* bilevel optimization, they tend to scale poorly with problem size and are hard to generalize to the non-linear case.
__label__generative_models This dataset depicts 44K diverse objects with 110K animations rendered in 48 viewpoints, resulting in 12M videos with a total of 300M frames.
__label__diffusion_based_models In this work, we develop a novel weight quantization method that quantizes the UNet from Stable Diffusion v1.5 to $1.99$ bits, achieving a model with $7.9\times$ smaller size while exhibiting even better generation quality than the original one.
__label__optimization_for_deep_networks In this work, we propose a simple yet effective approach for relaxing the symmetric condition, namely **$\epsilon$-softmax**, which simply modifies the outputs of the softmax layer to approximate one-hot vectors with a controllable error $\epsilon$.
__label__reinforcement_learning Our main thesis that we illustrate via examples is that because the agent state does not satisfy the Markov property, non-stationary agent-state based policies can outperform stationary ones.
__label__optimization Following the recent literature on performative prediction \cite{perdomo2020performative}, we introduce the concept of a performatively stable control (PSC) solution.
__label__optimization This paper aims at developing novel shuffling gradient-based methods for tackling two classes of minimax problems: nonconvex-linear and nonconvex-strongly concave settings.
__label__machine_vision Lastly, we employ a geometry-guided Gaussian enhancement strategy to improve rendering details by incorporating additional geometry priors.
__label__machine_vision Our method provides a simple yet effective decoding strategy that can be integrated to existing LMM frameworks without additional training.
__label__machine_learning_for_physical_sciences Extensive experiments showcase the superior performance of our method on both synthetic data and real world data captured by different imaging setups.
__label__natural_language_processing Inspired by these advancements, researchers have extended these techniques to develop Large Multimodal Models (LMMs) with ICL capabilities.
__label__safety_in_machine_learning Extensive experiments across widely used benchmark datasets and various real-world applications show that TPA can craft more transferable adversarial examples compared to state-of-the-art baselines.
__label__machine_learning_for_other_sciences_and_fields In particular, we demonstrate an 8.1 times improvement in zero shot molecular retrieval of active molecules over the previous state-of-the-art, reaching 77.33% in top-1% accuracy.
__label__privacy For example, the déjà vu method shows that for certain representation learning models and training images, it is sometimes possible to correctly predict the foreground label given only the representation of he background – better than through dataset-level correlations.
__label__interpretability_and_explainability Experiments on real-world datasets corroborate these theoretical findings, and further demonstrate that the merit of adaptivity can extend to more complex scenarios such as classification tasks and non-linear neural networks.
__label__graph_neural_networks To figure out the missing part, in this paper, we disentangle the graph homophily into three aspects: label, structural, and feature homophily, which are derived from the three basic elements of graph data.
__label__other Further validations on the proposed interactive scenario benchmark showcase planning compliance in interactive cases.
__label__interpretability_and_explainability These methods can be computationally expensive for large ML models.
__label__neuroscience_and_cognitive_science Such types of signals render the SSM's decoder model non-invertible, a requirement for previous TF-based methods.
__label__interpretability_and_explainability An advantage of this model class is the user's ability to intervene on predicted concept values, affecting the downstream output.
__label__safety_in_machine_learning While task-adaptive proxies average features to reflect the unique characteristics of each dataset, the sample-adaptive proxies weight features based on their similarity to individual test samples, exploring detailed sample-level nuances.
__label__optimization Lie NAG-SC provides acceleration over the momentumless case, i.e.
__label__machine_vision Our code is publicly available at https://github.com/Necolizer/CHASE .
__label__deep_learning_architectures Vision Transformer (ViT) has become a fundamental cornerstone in this regard due to its high accuracy.
__label__generative_models In this work, we introduce Spatial Region GPT (SpatialRGPT) to enhance VLMs’ spatial perception and reasoning capabilities.
__label__machine_learning_for_social_sciences Link prediction is a critical problem in graph learning with broad applications such as recommender systems and knowledge graph completion.
__label__machine_vision Rather than resorting to input sampling or token dropping, which may result in information loss, token merging shows promising results when used in collaboration with transformers.
__label__diffusion_based_models Based on two key observations: a significant similarity in the model's outputs at time step size that is not excessively large during the denoising process of existing ODE solvers, and a high resemblance between the denoising process and SGD.
__label__machine_learning_for_other_sciences_and_fields However, previous models typically adopt the tokenization methods designed for natural language, which are unsuitable for DNA sequences due to their unique characteristics.
__label__machine_learning_for_other_sciences_and_fields Through evaluation across a test suite of 329 datasets, we find that TABULA-8B has zero-shot accuracy on unseen tables that is over 15 percentage points (pp) higher than random guessing, a feat that is not possible with existing state-of-the-art tabular prediction models (e.g.
__label__machine_learning_for_other_sciences_and_fields Moreover, the same condition is also sufficient and necessary in the agnostic setting, where any hypothesis class meeting this criterion enjoys an $\tilde{O}(\sqrt{T})$ regret bound for any time step $T$, while others require an arbitrarily slow rate of regret.
__label__machine_learning_for_physical_sciences Molecule generation ideally in its 3-D form has enjoyed wide applications in material, chemistry, life science, etc.
__label__diffusion_based_models Besides, we achieve a $4\times$ faster tuning speed than tuning-based baselines and, if desired, avoid increasing the inference time.
__label__active_learning We address these challenges with a randomized algorithm to adaptively select a minimal set of users for data collection in order to initialize a set of services.
__label__optimization_for_deep_networks This combination is of particular interest as this parametrization is common in attention layers, the workhorse of transformers.
__label__optimization_for_deep_networks Here, the loss function poses the bottleneck when training a deep neural network.
__label__machine_vision However, these methods are primarily designed to maximize the image quality, which are sub-optimal in the performance of high-level computer vision tasks such as detection, recognition, and tracking.
__label__machine_vision For optimization, we alternately iterate between generative-discriminative parts for progressive refinements.
__label__machine_vision 3D Gaussian splatting (3DGS) has shown promising results in image rendering and surface reconstruction.
__label__safety_in_machine_learning The intuition behind this idea is that the deep generation model cannot reject safe generation with normal text prompts, while the editing models focus on modifying the local regions of images and do not involve a safety strategy.
__label__safety_in_machine_learning Existing methods primarily focus on jailbreak and adversarial attacks, which mainly evaluate the model's safety under malicious prompts.
__label__other Large Language Models (LLMs) have become pivotal in addressing reasoning tasks across diverse domains, including arithmetic, commonsense, and symbolic reasoning.
__label__neuroscience_and_cognitive_science These results underscore the importance of choosing the right experimental designs and data splits in BCI decoding tasks to prevent inflated accuracies due to EEG temporal correlations.
__label__graph_neural_networks Self-supervised heterogeneous graph learning (SHGL) has shown promising potential in diverse scenarios.
__label__machine_vision To address the challenge of open-set category generalization, our method employs high-order correlations and fuzzy representation to mitigate distribution skew through the Structure Fuzzy Reconstruction (SFR) module, by constructing a leveraged hypergraph based on local certainty and global uncertainty correlations.
__label__safety_in_machine_learning The implementation will be publicly available upon the acceptance of this work.
"__label__learning_theory That work stressed the open challenge of theoretically analyzing the optimal test
error in the more interesting regime where the number of samples is quadratic in
the dimension."
__label__neuroscience_and_cognitive_science Meanwhile, tremendous efforts have been made in the realm of machine learning to establish the nonlinear mapping between neuroimaging data and phenotypic traits.
__label__algorithmic_game_theory We consider the challenge of AI value alignment with multiple individuals that have different reward functions and optimal policies in an underlying Markov decision process.
__label__graph_neural_networks However, in many real-world graphs, connected nodes may display contrasting behaviors, termed as heterophilous patterns, which has attracted increased interest in heterophilous GNNs (HTGNNs).
__label__machine_vision Additionally, the absence of depth information may lead to points projected onto the image plane sharing the same 2D position or similar sampling points in the feature map, resulting in depth ambiguity.
__label__bandits Under minor regularity assumptions, our algorithm achieves an optimal regret bound of $\tilde{\mathcal{O}}(T^{2/3})$, improving the existing results.
__label__machine_vision During the CL of VLMs, we need not only to prevent the catastrophic forgetting on incrementally learned knowledge but also to preserve the zero-shot ability of VLMs.
__label__optimization_for_deep_networks 2DQuant gains an increase in PSNR as high as 4.52dB on Set5 (x2) compared with SOTA when quantized to 2-bit and enjoys a 3.60x compression ratio and 5.08x speedup ratio.
__label__algorithmic_game_theory We design peer prediction mechanisms for eliciting comparison data using a bonus-penalty payment.
__label__machine_vision Extensive experimental results demonstrate that the proposed FedHP coordinates the pre-trained model to multiple hardware con- figurations, outperforming prevalent FL frameworks for 0.35dB under challenging heterogeneous settings.
__label__machine_vision The code is available at https://github.com/XLearning-SCU/2024-NeurIPS-AverNet.
__label__machine_vision Lastly, we introduce a Mixed-Density Student to learn density-invariant features, addressing challenges related to density variation and low overlap in the outdoor scenario.
__label__machine_vision By excluding background noise distractions, the model is encouraged to focus on character morphology and generalize the ability to recognize complex samples when trained with only simple synthetic data.
__label__machine_vision To this end, we propose the DeepKKT condition, an adaptation of the traditional Karush-Kuhn-Tucker (KKT) condition for deep learning models, and confirm that generated Deep Support Vectors (DSVs) using this condition exhibit properties similar to traditional support vectors.
__label__machine_vision By employing object identifiers, we transform diverse 3D scene-language tasks into a unified question-answering format, facilitating joint training without the need for additional task-specific heads.
__label__reinforcement_learning Our approach further allows the policy to be conditioned on mode-specific embeddings to explicitly control the learned modes.
__label__machine_learning_for_other_sciences_and_fields Recent advances in deep learning-based generative methods have shown promise but face the issue of sample efficiency due to the computational expense of evaluating the reward function.
__label__machine_learning_for_other_sciences_and_fields Despite these advancements, current methods are often tailored for specific docking settings, and limitations such as the neglect of protein side-chain structures, difficulties in handling large binding pockets, and challenges in predicting physically valid structures exist.
__label__machine_vision Specifically, we elaborately design three pretext tasks: 1) Text-guided Image Colorization, aims to establish the correspondence between the person-related image regions and the fine-grained color-part textual phrases.
__label__privacy We develop estimators that are appropriate for such signals---our estimators are $(\varepsilon,\delta)$-differentially private and have sample complexity that is dimension-independent for anisotropic subgaussian distributions.
__label__other Afterward, we solve the ``how-to-edit`` problem by simply fine-tuning the identified parameters using a variant of gradient descent to achieve successful edits.
__label__graph_neural_networks Specifically, we empirically and theoretically investigate when this assumption holds and when it fails.
__label__machine_vision Specifically, CLFD can increase the accuracy of the SOTA CL method by up to 6.83% and reduce the training time by 2.6×.
__label__natural_language_processing Compared to Sign2GPT, a state-of-the-art method based on large-scale pre-trained vision and language models, SignCLachieves better performance with only 35\% of its parameters.
__label__diffusion_based_models The project page: \url{https://cond-image-leak.github.io/}.
__label__diffusion_based_models To address this issue, we propose COrrespondence-guided Video Editing (COVE), leveraging the inherent diffusion feature correspondence to achieve high-quality and consistent video editing.
__label__reinforcement_learning We consider the distribution of states reached by a policy conditioned on each skill and leverage the successor state representation to maximize the difference between these skill distributions.
__label__diffusion_based_models Our method is capable of multi-subject generation and compatible with popular diffusion extensions.
__label__diffusion_based_models This excessive variance can lead to over-smoothing and unrealistic outputs.
__label__interpretability_and_explainability Within the model, MLP and attention layers use Fourier features in complementary ways: MLP layers primarily approximate the magnitude of the answer using low-frequency features, while attention layers primarily perform modular addition (e.g., computing whether the answer is even or odd) using high-frequency features.
"__label__machine_vision Novel-view synthesis aims to generate novel views of a scene from multiple input
images or videos, and recent advancements like 3D Gaussian splatting (3DGS)
have achieved notable success in producing photorealistic renderings with efficient
pipelines."
__label__evaluation Going beyond this study, we derive different implementations by analyzing layer-wise behaviors of CRATE, both theoretically and empirically.
__label__causal_inference It significantly enhances the quality of recovered structures while maintaining good efficiency, which learns better structures using 90\% fewer samples than the data-based method on a real-world dataset.
__label__reinforcement_learning Based on our analysis, we develop a method that directly trains on scenarios with high learnability.
__label__graph_neural_networks Rethink convolution-based graph neural networks (GNN)---they characteristically suffer from limited expressiveness, over-smoothing, and over-squashing, and require specialized sparse kernels for efficient computation.
__label__optimization This work considers the problem of sampling from a probability distribution known up to a normalization constant while satisfying a set of statistical constraints specified by the expected values of general nonlinear functions.
__label__machine_vision Video transformers are slow to train due to extremely large numbers of input tokens, even though many video tokens are repeated over time.
__label__diffusion_based_models CEP significantly improves the performance of various DMs in both pre-training and downstream tasks.
__label__bandits Our theoretical and empirical findings also shed light on an intriguing concept of optimal fidelity for each arm.
__label__generative_models Furthermore, iteratively adopting this self-play process can continuously promote LLMs' reasoning abilities.
__label__machine_vision 2) Unintentional data leakage exists in LLM and LVLM training.
__label__graph_neural_networks However, their utilization of these normal nodes is limited.
__label__diffusion_based_models This is achieved through a parallel attention mechanism that facilitates the feature injection of multiple garments from conditionally encoded branches into the main network.
__label__natural_language_processing Consequently, the employment of LLMs for Temporal Knowledge Graph Reasoning (TKGR) has sparked increasing interest among researchers.
__label__learning_theory Gaussian or uniform Boolean data distributions.
__label__neuroscience_and_cognitive_science In simulations, we demonstrate our algorithm's ability to dissociate and model the dynamics within two time-series sources while being agnostic to their respective observation distributions.
__label__other We show that our auction maximizes logarithmic social welfare, a new notion of welfare that balances allocation efficiency and fairness, and we characterize the associated incentive-compatible pricing rule.
__label__optimization For both settings, we design algorithms that combine a sampling strategy with a classic approximation algorithm for correlation clustering and study their theoretical guarantees.
__label__bandits With $\mathcal{A}$ actions and $N$ units, minimizing regret is combinatorially difficult since the action space grows as $\mathcal{A}^N$.
__label__natural_language_processing Then, we show that output distributions from an MoE model using different routing strategies substantially differ, indicating that different experts do not always act synergistically.
__label__machine_learning_for_physical_sciences We present NeuralFluid, a novel framework to explore neural control and design of complex fluidic systems with dynamic solid boundaries.
__label__learning_theory Furthermore, we complement our algorithmic results with Statistical Query lower bounds, providing evidence that our estimators are likely to have nearly optimal sample complexity.
__label__graph_neural_networks Since the associated graph convolution operation of the proposed HC-GAE is restricted in each individual separated subgraph and cannot propagate the node information between different subgraphs, the proposed HC-GAE can significantly reduce the over-smoothing problem arising in the classical convolution-based GAEs.
__label__graph_neural_networks Extensive experiments on four datasets across three GNN backbones, demonstrate that \ourmethod (I) achieves or surpasses the performance of the full dataset with $30\%\sim50\%$ fewer training samples, (II) attains up to a $2.81\times$ lossless training speedup, and (III) outperforms state-of-the-art pruning methods in imbalanced training and noisy training scenarios by $0.3\%\sim4.3\%$ and $3.6\%\sim7.8\%$, respectively.
__label__neuroscience_and_cognitive_science Moreover, the Hamiltonian framework requires SOM neurons to receive no direct feedforward connections, consistent with neuroanatomy.
__label__natural_language_processing We find that the accuracy of our logistic regression on a linear feature space is slightly (around 3 percentage points) lower than GPT-4, which is in turn around 5 percentage points below that of a human expert.
__label__interpretability_and_explainability Taking advantage of this redundancy results in 45\% computational savings when prompting with 5 examples, and task recognition achieved at layer 14 / 32 using an example with Machine Translation.
__label__interpretability_and_explainability Furthermore, models that are excessively customized and devoid of causal connections further undermine the generalizability and  interpretability.
__label__safety_in_machine_learning Adversarial Training (AT) is a mainstream approach to enhancing Adversarial Robustness (AR) and has been validated on various traditional DNN architectures.
__label__neuroscience_and_cognitive_science Our analyses of LLMs pre-trained on extensive text data show that CMR-like heads often emerge in the intermediate and late layers, qualitatively mirroring human memory biases.
__label__diffusion_based_models We demonstrate the performance of our algorithm on a variety of unpaired data translation tasks.
__label__machine_learning_for_social_sciences Intent learning, which aims to learn users' intents for user understanding and item recommendation, has become a hot research spot in recent years.
__label__diffusion_based_models We propose a simple extension that applies flow matching in the embedding space of data pairs, where the embeddings are learned jointly with the dynamic function to ensure the validity of the flow which is also easier to learn.
__label__natural_language_processing Data, code, and scripts are freely available at https://github.com/Blue-Raincoat/SelectIT.
__label__diffusion_based_models Moreover, by applying SIM to a leading transformer-based diffusion model, we distill a single-step generator for text-to-image (T2I) generation that attains an aesthetic score of 6.42 with no performance decline over the original multi-step counterpart, clearly outperforming the other one-step generators including SDXL-TURBO of 5.33, SDXL-LIGHTNING of 5.34 and HYPER-SDXL of 5.85.
__label__machine_vision On the data side, we construct the first object-centric occupancy dataset from scratch using an automated pipeline.
__label__neuroscience_and_cognitive_science However, task-optimized RNNs typically have a fixed weight matrix representing the synaptic connectivity between neurons.
__label__optimization In SFL, a global model is typically split into two parts, where clients train one part in a parallel federated manner, and a main server trains the other.
"__label__other We show that for preserving the ordering of the $k$-NN for every point in:
- $\ell_2$: $d = \Theta(k)$ is necessary and sufficient."
__label__safety_in_machine_learning However, existing FIT methods are dedicated to handling data heterogeneity across different clients (i.e., client-aware data heterogeneity), while ignoring the variation between data from different domains (i.e., domain-aware data heterogeneity).
__label__human-AI_interaction The code could be accessed at pengxi.me.
"__label__safety_in_machine_learning Experimental results show that 
the choice of pre-trained models significantly affects the white-box adversarial robustness of LM-VP, and standard AT even substantially degrades its performance."
__label__robotics Robo-Instruct further addresses this with InstAlign, an instruction-program alignment procedure that revises the task instruction to reflect the actual results of the generated program.
__label__optimization Bilevel Optimization has experienced significant advancements recently with the introduction of new efficient algorithms.
__label__other In $k$-NN, for each of the $n$ data points we are given an ordered set of the closest $k$ points.
__label__safety_in_machine_learning First, in existing methods, there is commonly an appearance gap between simulated distant adversarial patterns and their physical world counterparts, leading to incorrect optimization.
__label__speech_and_audio Without relying on speaker information, the weight-shared network in the decoder directly learns to discriminate features using a separation objective.
__label__generative_models The inherent challenge stems from their high-dimensional structures and complex interdependencies, which complicate effective modeling.
__label__machine_learning_for_other_sciences_and_fields Moreover, we observe a substantial increase in the maximum proof length found by POETRY, from 10 to 26.
__label__safety_in_machine_learning Recently, Anil et al.
__label__learning_theory Leveraging inspiration from portfolio optimization that combining two independent assets will maintain the income while reducing the risk, we introduce two prompts: global prompt and local prompt to construct a prompt portfolio to balance the generalization and personalization.
__label__natural_language_processing In this paper, we introduce SHED, an automated dataset refinement framework based on Shapley value for instruction fine-tuning.
__label__optimization_for_deep_networks Existing model merging methods usually suffer from (1) significant performance degradation or (2) requiring tuning by additional data or training.
__label__deep_learning_architectures Existing super-resolution (SR) methods optimize all model weights equally using $\mathcal{L}_1$ or $\mathcal{L}_2$ losses by uniformly sampling image patches without considering dataset imbalances or parameter redundancy, which limits their performance.
__label__bandits We also prove a lower bound to show that our regret bound is nearly optimal, both in scenarios with and without ($C=0$) adversarial feedback.
__label__reinforcement_learning In this work, we introduce STAR, a framework for OPE that encompasses a broad range of estimators -- which include existing OPE methods as special cases -- that achieve lower mean squared prediction errors.
__label__neuroscience_and_cognitive_science When synaptic turnover is introduced, the learned rule incorporates a form of homeostasis, better maintaining robust sequential dynamics relative to other previously proposed rules.
__label__generative_models This allows us to compare different player styles, as well as synthesize new (human-like) styles, e.g.
__label__probabilistic_methods Bayesian Optimization (BO) is widely used for optimising black-box functions but requires us to specify the length scale hyperparameter, which defines the smoothness of the functions the optimizer will consider.
__label__deep_learning_architectures Tabular data is a pervasive modality spanning a wide range of domains, and this inherent diversity poses a considerable challenge for deep learning.
__label__machine_vision In a similar manner, we aggregate reliable SSC predictions among multiple moments and leverage them as semantic pseudo-GT for adaptation.
__label__deep_learning_architectures To make the discretized quadtree partition end-to-end trainable, we further devise a sequence masking strategy based on Gumbel-Softmax and its straight-through gradient estimator.
__label__learning_theory Additional heuristic calculations suggest that the weak recovery threshold of $p$-step QAOA matches that of $p$-step tensor power iteration when $p$ is a fixed constant.
__label__machine_vision Experimental results on popular face benchmarks demonstrate the superiority of our TopoFR over the state-of-the-art methods.
__label__machine_vision Extensive evaluations demonstrate the versatility of DDR as an image descriptor, with strong correlations observed with key image attributes such as complexity, colorfulness, sharpness, and overall quality.
__label__machine_vision The effectiveness of this segmentation heavily depends on the precision of these derived prompts.
__label__fairness Furthermore, we propose a simple method consisting of a clustering-based adaptive margin loss with a blackbox feature encoder, with no knowledge of the bias attribute.
__label__online_learning We consider the classic problem of online convex optimisation.
__label__neuroscience_and_cognitive_science Our visual experience in daily life are dominated by dynamic change.
__label__machine_learning_for_other_sciences_and_fields Protein optimization is a fundamental biological task aimed at enhancing theperformance of proteins by modifying their sequences.
__label__learning_theory To solve the clustering problem, we consider a variant of Lloyd's algorithm, adapted to estimate and utilize covariance information iteratively.
__label__machine_vision The entire rasterization process is parallelized using CUDA, achieving optimization and rendering speeds 100 times faster than NeRF-based methods.
__label__natural_language_processing We observe that IM is especially beneficial when trained on datasets with lengthy instructions paired with brief outputs, or under the Superficial Alignment Hypothesis (SAH) where a small amount of training examples are used for instruction tuning.
__label__optimization_for_deep_networks We show that IRE can be practically incorporated with *generic base optimizers* without introducing significant computational overload.
__label__causal_inference In this paper, we develop graphical characterizations and estimation tools to bound the effect of policies given a causal graph and observational data collected in non-identifiable settings.
__label__machine_learning_for_physical_sciences By employing a diffusion-based formulation, we achieve greater stability and enable longer rollouts compared to conventional MSE training.
__label__reinforcement_learning One key idea behind SACPO, supported by theory, is that the optimal policy incorporating reward and safety can be directly obtained from a reward-aligned policy.
__label__machine_vision In this paper, we investigate this phenomenon and reveal that it leads to sparse representation spaces with reduced uniformity.
__label__optimization_for_deep_networks However, settings which violate this assumption are becoming more popular;  examples include supervised learning under distributional shifts, reinforcement learning, continual learning and non-stationary contextual bandits.
__label__algorithmic_game_theory Algorithmically, constant factor approximations exist for the problem for any number of agents.
__label__diffusion_based_models Unlike most existing methods, our approach does not require training a text encoder or optimizing text embeddings and achieves text-image alignment by fine-tuning only the U-Net component.
__label__machine_vision *Do we fully leverage the potential of visual encoder in Multimodal Large Language Models (MLLMs)?
__label__graph_neural_networks Unlike previous approaches, GCFormer develops a hybrid token generator to create two types of token sequences, positive and negative, to capture diverse graph information.
__label__machine_learning_for_physical_sciences The ability to perform fast and accurate atomistic simulations is crucial for advancing the chemical sciences.
__label__diffusion_based_models To further enhance multi-color hairstyle editing, we fine-tuned a CLIP model using a multi-color hairstyle dataset.
__label__generative_models This GPT-style method allows the model to learn the motion distribution in real driving scenarios.
__label__safety_in_machine_learning One principled approach to address this issue is to use samples from external datasets as outliers ($\textit{i.e.
__label__optimization In this paper, we focus on simple bilevel optimization problems, where we minimize a convex smooth objective function over the optimal solution set of another convex smooth constrained optimization problem.
__label__machine_vision However, both modalities have shortcomings in holistically capturing the appearance and geometry of objects.
__label__machine_vision In this paper, we target the realm of visual aesthetics and aim to align vision models with human aesthetic standards in a retrieval system.
__label__optimization_for_deep_networks We prove that, despite the non-convexity of the objective function, AGN achieves Q-linear convergence from random initialization to the global optimal solution.
__label__privacy In this work, we revisit the problem of DP ReLU regression in high-dimensional regimes.
__label__other The IMM principle is thus generally applicable in common scenarios where restricted data is cheaper to collect or restricted models are easier to learn.
__label__diffusion_based_models Extensive experiments show that with a single model, DiffPath is competitive with prior work using individual models on a variety of OOD tasks involving different distributions.
__label__natural_language_processing This paper introduces AutoSurvey, a speedy and well-organized methodology for automating the creation of comprehensive literature surveys in rapidly evolving fields like artificial intelligence.
__label__infrastructure The rapid development of Large Language Models (LLMs) has been pivotal in advancing AI, with pre-trained LLMs being adaptable to diverse downstream tasks through fine-tuning.
__label__machine_learning_for_physical_sciences Thus, it can explore globally and plan near-optimal control sequences.
__label__learning_theory Apart from the independence of $X^*$, we only require a small fraction entries of $\eta$ to have magnitude at most $1$.
__label__deep_learning_architectures Particularly, its state evolving process implies stronger modality fusion paradigm, making multi-modal fusion on SSMs an appealing direction.
__label__safety_in_machine_learning Our attack wins over the current state of the art AutoAttack on the standard benchmark datasets CIFAR-100 and Imagenet and for different robust models.
__label__natural_language_processing This enables learning from cheap, incomplete, and possibly incorrect label information, such as coarse logical rules or the generations of a language model.
__label__optimization_for_deep_networks This feature is present in SSMs, as well as in other architectures, such as LSTMs.
__label__natural_language_processing Specifically, by fitting the D-CPT Law, we can easily predict the general and downstream performance of arbitrary mixture ratios, model sizes, and dataset sizes using small-scale training costs on limited experiments.
__label__optimization First-order optimization (FOO) algorithms are pivotal in numerous computational domains, such as reinforcement learning and deep learning.
__label__machine_vision In this paper, we provide a new insight to reveal that noisy-to-noisy masked-reconstruction harms sufficient utilization of contextual information.
__label__graph_neural_networks Enhancing node-level Out-Of-Distribution (OOD) generalization on graphs remains a crucial area.
__label__natural_language_processing Besides, prior methods use textual responses as communication media, ignoring the valuable information in the internal representations.
__label__robotics Autonomous systems often encounter environments and scenarios beyond the scope of their training data, which underscores a critical challenge: the need to generalize and adapt to unseen scenarios in real time.
__label__reinforcement_learning Then, the humanoid learns to collaborate with others by considering the shared dynamics of the manipulated object using centralized training and decentralized execution (CTDE) multi-agent RL algorithms.
__label__learning_theory These approaches have demonstrated an enhancement in performance when the predictions are accurate, while also ensuring robustness by providing worst-case guarantees when predictions fail.
__label__optimization Given a neural VRP method, we adversarially train multiple models in a collaborative manner to synergistically promote robustness against attacks, while boosting standard generalization on clean instances.
__label__machine_vision To bridge this gap, we introduce the novel challenge of Semantic Keypoint Comprehension, which aims to comprehend keypoints across different task scenarios, including keypoint semantic understanding, visual prompt-based keypoint detection, and textual prompt-based keypoint detection.
__label__bandits Common examples include  human preferences in matchmaking problems.
__label__safety_in_machine_learning A prevailing belief in attack and defense community is that the higher flatness of adversarial examples enables their better cross-model transferability, leading to a growing interest in employing sharpness-aware minimization and its variants.
__label__machine_vision Humans naturally retain memories of permanent elements, while ephemeral moments often slip through the cracks of memory.
__label__other Based on the findings, we propose **A**dversarial **CO**-learning **N**etworks (**ACON**), to enhance transferable representation learning through a collaborative learning manner in three aspects: (1) Considering the multi-periodicity in time series, multi-period frequency feature learning is proposed to enhance the discriminability of frequency features; (2) Temporal-frequency domain mutual learning is proposed to enhance the discriminability of temporal features in the source domain and improve the transferability of frequency features in the target domain; (3) Domain adversarial learning is conducted in the correlation subspaces of temporal-frequency features instead of original feature spaces to further enhance the transferability of both features.
__label__other Our investigation leads us to propose **persistent TTA (PeTTA)**, which senses when the model is diverging towards collapse and adjusts the adaptation strategy, striking a balance between the dual objectives of adaptation and model collapse prevention.
__label__reinforcement_learning In this work, we introduce Evolutionary Adversarial Simulator Identification (EASI), a novel approach that combines Generative Adversarial Network (GAN) and Evolutionary Strategy (ES) to address sim-to-real challenges.
__label__machine_learning_for_physical_sciences Particularly, we demonstrate that the more accurate energy data can improve the accuracy of structure prediction.
__label__natural_language_processing Blockwise parallel decoding (BPD) was proposed by Stern et al.
__label__diffusion_based_models However, as the most extreme quantization form, 1-bit binarization causes the generation performance of DMs to face severe degradation or even collapse.
__label__probabilistic_methods We propose the Deep Contractive Drift Calculator (DCDC), the first general-purpose sample-based algorithm for bounding the convergence of Markov chains to stationarity in Wasserstein distance.
__label__machine_vision Image coding for multi-task applications, catering to both human perception and machine vision, has been extensively investigated.
__label__machine_vision However, a common issue is their susceptibility to domain shifts, hindering their ability in generalizing to diverse, unseen realistic domains.
__label__machine_learning_for_other_sciences_and_fields FPS is evaluated via two important but challenging applications, intelligent tutoring systems and a healthcare application for sepsis treatment and intervention.
__label__deep_learning_architectures We believe that this work is an important advancement towards a transformative NAS paradigm where search space expressivity and strategic search initialisation play key roles.
__label__evaluation To obtain the ability hierarchy among these models, we assign each LLM a learnable capability parameter to adjust the final ranking.
__label__machine_vision By using a combination of cross-attention and positional embeddings we disentangle the representation structure and image structure.
__label__other It still remains an open problem to address EDG in the domain-incremental setting, where source domains are non-static and arrive sequentially to mimic the evolution of training domains.
__label__learning_theory Prior to our work, no algorithm was known that achieves $\alpha = 3$ in near-linear time.
__label__generative_models Our findings demonstrate that beyond producing photo-realistic images from precise spatial inputs, controllable image diffusion can effectively produce a refined, clear sketch from a noisy input.
__label__machine_vision Achieving such a universal model inevitably requires incorporating multi-domain data for joint training to learn across multiple problem scenarios.
__label__diffusion_based_models Experiments demonstrate the versatility of our method in tackling cross-modal and multimodal interpolation tasks in the audiovisual space.
"__label__active_learning Code: https://github.com/davidbrandfonbrener/color-filter-olmo

Filtered data: https://huggingface.co/datasets/davidbrandfonbrener/color-filtered-c4"
__label__optimization In federated learning, it is common to assume that clients are always available to participate in training, which may not be feasible with user devices in practice.
__label__reinforcement_learning Our findings demonstrate that this new formalization promotes more robust and efficient exploration by combining mutual information maximization and exploration bonuses.
__label__learning_theory Autoregressively trained transformers have brought a profound revolution to the world, especially with their in-context learning (ICL) ability to address downstream tasks.
__label__active_learning Further testing on real-world datasets demonstrates improved performance compared to existing multi-label active learning methods.
__label__infrastructure Moreover, employing PaRO-CC independently for model parallel strategies, such as Megatron, can also boost the training speed by 17\%.
__label__online_learning Numerical experiments show that our model achieves outstanding performance associated with the side information while maintaining low tracking error, demonstrating marked improvements over traditional PA methods across various scenarios.
__label__robotics Despite significant progress in robotics and embodied AI in recent years, deploying robots for long-horizon tasks remains a great challenge.
__label__machine_vision To this end, we develop a cross-domain AED benchmark, consisting of 322 base and 153 novel environments.
__label__machine_learning_for_other_sciences_and_fields Since LLMs cannot directly interpret check-ins, we reprogram these sequences to help LLMs comprehensively understand the semantics of human visiting intentions and travel preferences.
__label__other We solve a simple multi-linear model in this skill-basis, finding analytic expressions for the emergence of new skills, as well as for scaling laws of the loss with training time, data size, model size, and optimal compute.
__label__machine_learning_for_other_sciences_and_fields To avoid the overfitting problem in common preference optimization objectives, we further develop an improved Exact Energy Preference Optimization method to yield an exact and efficient alignment of the diffusion models, and provide the closed-form expression for the converged distribution.
__label__online_learning Existing state-of-the-art methods represent classes as Gaussian distributions in the feature extractor's latent space, enabling Bayes classification or training the classifier by replaying pseudo features.
__label__fairness Extensive experiments conducted on $60$ culture-related datasets reveal that CultureLLM significantly surpasses various counterparts such as GPT-3.5 (by $8.1$\%) and Gemini Pro (by $9.5$\%), demonstrating performance comparable to or exceeding that of GPT-4.
__label__learning_theory For some $p$ and $q$, the QAOA has an effective recovery threshold that is a constant factor better than tensor power iteration.
__label__bandits We apply IGL to learning from image feedback and learning from text feedback, which are reward-free settings that arise in practice.
__label__optimization_for_deep_networks The paper introduces the concept of uncertainty-aware HPO and presents a novel approach called the UQ-guided scheme for quantifying uncertainty.
__label__machine_learning_for_physical_sciences EScAIP leverages a novel multi-head self-attention formulation within graph neural networks, applying attention at the neighbor-level representations.
__label__optimization This paper studies simple bilevel problems, where a convex upper-level function is minimized over the optimal solutions of a convex lower-level problem.
__label__fairness To this end, we propose a drop-in correction for label noise in last-layer retraining, and demonstrate that it achieves state-of-the-art worst-group accuracy for a broad range of symmetric label noise and across a wide variety of datasets exhibiting spurious correlations.
__label__natural_language_processing Pretraining data selection has the potential to improve language model pretraining efficiency by utilizing higher-quality data from massive web data corpora.
__label__machine_vision By leveraging robust DINO features and integrating an appearance modeling module within 3DGS, our method achieves state-of-the-art results.
__label__privacy Besides, users with large local sample sizes can make the sensitivity much larger, thus the method is not suitable for imbalanced users.
__label__privacy However, such datasets often contain sensitive, private and copyrighted material that requires formal protection.
__label__machine_learning_for_healthcare We demonstrate how to obtain improved MIL explanations using layer-wise relevance propagation (LRP) and conduct extensive evaluation experiments on three toy settings and four real-world histopathology datasets.
__label__optimization_for_deep_networks However, after a comprehensive evaluation of contextual sparsity methods on various complex generation tasks, we find that although CS succeeds in prompt-understanding tasks, it significantly degrades the model performance for reasoning, deduction, and knowledge-based tasks.
__label__machine_vision Recently, Gaussian Splatting, a method that represents a 3D scene as a collection of Gaussian distributions, has gained significant attention in addressing the task of novel view synthesis.
__label__optimization_for_deep_networks Furthermore, as the rank allocation space expands, our method ensures fine-tuning efficiency, achieving a speed improvement of 94.5\% compared to AdaLoRA.
__label__safety_in_machine_learning However, it suffers from high computational overhead, and we also find that it overly relies on prominent backdoor features that are highly distinguishable from benign features.
__label__machine_vision The manual prompt design process for LLMs at the reasoning phase is tedious and an automated prompt optimization method is desired.
"__label__learning_theory We find that the variance quantities depend on the
non-linearity w.r.t."
__label__machine_vision In this work, we enable LMMs to perform multimodal, many-shot in-context learning by leveraging Multimodal Task Vectors (MTV)---compact implicit representations of in-context examples compressed in the model's attention heads.
"__label__probabilistic_methods Our method contains three key components: (1) a cyclical step size schedule where large steps discover new modes and small steps exploit each mode; (2) a cyclical balancing schedule, ensuring ""balanced"" proposals for given step sizes and high efficiency of the Markov chain; and (3) an automatic tuning scheme for adjusting the hyperparameters in the cyclical schedules, allowing adaptability across diverse datasets with minimal tuning."
__label__fairness This induces the question: *what is the tradeoff between these objectives, and what are the characteristics of (multi-objective) optimal solutions?
__label__neuroscience_and_cognitive_science And the computational complexity of the feedforward circuit can be even lower than common signal processing algorithms in certain conditions.
__label__interpretability_and_explainability Concept Bottleneck Models (CBMs) provide interpretable prediction by introducing an intermediate Concept Bottleneck Layer (CBL), which encodes human-understandable concepts to explain models' decision.
__label__interpretability_and_explainability We propose a new model for dimensionality reduction, the PCA tree, which works like a regular autoencoder, having explicit projection and reconstruction mappings.
__label__machine_vision The intuition is that the probability of neighboring replication levels should be continuous and smooth.
__label__machine_vision Recent advancements, particularly with LLAMA 3, reveal that enlarging the codebook significantly enhances model performance.
__label__machine_learning_for_other_sciences_and_fields Leveraging the contextual priors of MVD at both the data and label levels, we propose a novel consistency learning framework Con4m, which effectively utilizes contextual information more conducive to discriminating consecutive segments in segmented TSC tasks, while harmonizing inconsistent boundary labels for training.
__label__machine_vision Our codes are available at https://github.com/Zoilsen/CLS_Token_CDFSL.
"__label__graph_neural_networks By using KL-divergence and additional constraints, \method~delivers 
an end-to-end solution for learning and predicting label distribution for nodes."
__label__natural_language_processing Notably, recent advancements in multi-round on-policy self-improving methods allow LLMs to generate new examples for training subsequent models.
__label__deep_learning_architectures Experiments demonstrate that DenseFormer is more data efficient, reaching the same perplexity of much deeper transformer models, and that for the same perplexity, these new models outperform transformer baselines in terms of memory efficiency and inference time.
__label__machine_vision We propose perception prior embedding to better integrate perception priors with image features.
__label__other To address these challenges, we propose Instance-wise LoRA (iLoRA).
__label__graph_neural_networks GraphMETRO achieves state-of-the-art results on four datasets from the GOOD benchmark, which is comprised of complex and natural real-world distribution shifts, improving by 67% and 4.2% on the WebKB and Twitch datasets.
__label__machine_learning_for_other_sciences_and_fields Therefore, in this study, we introduce frequency-domain information and design Frequency-aware Generative Models for Multivariate Time Series Imputation (FGTI).
__label__safety_in_machine_learning When adapting the output interface, label mapping methods transform the pretrained labels to downstream labels by establishing a gradient-free one-to-one correspondence between the two sets of labels.
__label__deep_learning_architectures We further prove that AOPU attains minimum variance estimation in NN, wherein the truncated gradient approximates the natural gradient.
__label__machine_learning_for_physical_sciences Statistical downscaling, or super-resolution, is a common workaround where a low-resolution prediction is improved using statistical approaches.
__label__diffusion_based_models In DKI, we adopt positive (real) domain finetuning and negative (rendered) domain embedding to inject knowledge into a pretrained Text-to-image (T2I) diffusion model.
__label__optimization Inverse optimization has been increasingly used to estimate unknown parameters in an optimization model based on decision data.
__label__machine_vision Specifically, the pseudo-label exploration can be formulated as a decision-making paradigm by adopting a conformal pseudo-label explorer and a multi-clue selection evaluator.
__label__other The key challenge lies in the objective function, as effective visualization algorithms like NE require computing loss functions among pairs of data.
__label__learning_theory Second, most approaches focus on analyzing indirect transferability metrics, which does not allow for accurate assessment of the final target loss and extent of transferability.
__label__machine_learning_for_healthcare The issue can lead to suboptimal performance when applying standard contrastive learning techniques to molecular datasets.
__label__robotics At the coarse level, we consider the positional information of 3D points to achieve location-based clustering, which is then refined at the fine level.
__label__deep_learning_architectures Previous visual object tracking methods employ image-feature regression models or coordinate autoregression models for bounding box prediction.
__label__other This gap arises from confirmation bias, which is more pronounced in FSSL due to multiple local training epochs and the separation of labeled and unlabeled data.
__label__human-AI_interaction We implement IA with a diffusion copilot (termed IDA) trained on expert demonstrations with goal masking.
__label__bandits Suppose parameters $\eta_{\min}, \eta_{\max} \in (0,1]$ respectively lower and upper bound the reward earned and the resources consumed in a time round.
__label__generative_models Unfortunately, Best-of-N requires vastly more resources at inference time than standard decoding strategies, which makes it computationally not viable.
__label__machine_vision The strategy of utilizing NODE to leverage continuous dynamics in iterative methods enhances unsupervised learning and aids in achieving better convergence compared to discrete-space approaches.
__label__machine_vision However, current Vid-LLMs struggle to simultaneously retain high-quality frame-level semantic information (i.e., a sufficient number of tokens per frame) and comprehensive video-level temporal information (i.e., an adequate number of sampled frames per video).
__label__neuroscience_and_cognitive_science This undermines evidence used to claim that the transformer architecture biases computations to be more brain-like.
__label__reinforcement_learning Our result builds off a new perspective on the role off-policy evaluation guarantees and coverage coefficient in LMDPs, a perspective, which has been overlooked in the context of exploration in partially observed environments.
__label__interpretability_and_explainability Our key findings show that these MLLMs rely on MLP and self-attention blocks in much earlier layers for information storage, compared to LLMs whose mid-layer MLPs are more important.
__label__learning_theory We evaluate the statistical performance of a reduced-rank estimator in reproducing kernel Hilbert spaces (RKHS) in the partial knowledge setting.
__label__machine_learning_for_other_sciences_and_fields In this work, we introduce \emph{Feedforward-tied Energy-based Models} (ff-EBMs), a hybrid model comprised of feedforward and energy-based blocks housed on digital and analog circuits.
__label__deep_learning_architectures Experiments on multiple datasets show that our method achieves superior performance in multimodal contrastive learning benchmarks.
__label__generative_models This paper aims to bridge the gap between Code LLMs' reliance on static text data and the need for semantic understanding for complex tasks like debugging and program repair.
__label__reinforcement_learning Accurate environment dynamics modeling is crucial for obtaining effective state representations in visual reinforcement learning (RL) applications.
__label__active_learning Three main data selection approaches are: (1) leveraging external non-CLIP models to aid data selection, (2) training new CLIP-style embedding models that are more effective at selecting high-quality data than the original OpenAI CLIP model, and (3) designing better metrics or strategies universally applicable to any CLIP embedding without requiring specific model properties (e.g., CLIPScore is one popular metric).
__label__machine_vision The major challenges for this setup is the modality gap between training images and testing point clouds, which prevents effective integration of 2D knowledge into OV-3Det.
__label__graph_neural_networks First, we design a positive knowledge transfer module that ensures privacy during inter-domain knowledge transmission.
__label__evaluation However, the focus of the study was primarily on the basic implementation, and whether this objective is optimized in practice and its causal relationship to generalization remain elusive.
__label__machine_vision The findings are validated in both supervised and self-supervised learning, enabling models trained on imbalanced data to achieve CLIP-level performance on diverse recognition tasks.
__label__machine_vision Visual place recognition (VPR) is an essential task for multiple applications such as augmented reality and robot localization.
__label__human-AI_interaction Additionally, we introduce a new model VOILA-A that integrate gaze information into VLMs while maintain pretrained knowledge from webscale dataset.
"__label__deep_learning_architectures In this paper, we propose a novel framework $\textit{Read-ME}$ that transforms pre-trained dense LLMs into smaller MoE models (in contrast to ``upcycling"" generalist MoEs), avoiding the high costs of ground-up training."
__label__human-AI_interaction In the absence of class priors, recent deep clustering methods resort to data augmentation and pseudo-labeling strategies to generate supervision signals.
__label__optimization Lim et.
__label__reinforcement_learning Motivated by this, we formulate a novel poisoning attack framework which interlinks the adversary's objectives with those of finding an optimal policy -- guaranteeing attack success in the limit.
__label__natural_language_processing First, models' responses are governed by ordering and labeling biases, for example, towards survey responses labeled with the letter “A”.
__label__safety_in_machine_learning Furthermore, we introduce an output-preserving regularization term to maintain the model's generative capabilities on safe content.
__label__machine_learning_for_healthcare We demonstrate the effectiveness of our method for both retrospectively simulated motion and prospectively collected real motion-corrupted data.
__label__other Previous studies have largely overlooked the significance of the NME in their analysis for various reasons.
__label__machine_learning_for_other_sciences_and_fields We augment the vanilla transformer with a simple module we call SAND (self-attention on derivatives), which naturally encourages smoothness by modeling the sub-derivative of the imputed curve.
__label__optimization Code is available at https://github.com/lisha-chen/FERERO/.
__label__natural_language_processing It is well known that a large batch is essential to stable and effective training with InfoNCE loss, which requires significant hardware resources.
__label__other 2021] generalizing [Forrow et al.
__label__optimization_for_deep_networks Moreover, overparameterization can markedly decelerate gradient descent methods, transitioning its convergence rate from linear to sub-linear.
__label__machine_vision The significant domain gap between the source and target datasets leads to a sharp decline in the performance of existing few-shot segmentation (FSS) methods in cross-domain scenarios.
__label__machine_learning_for_other_sciences_and_fields Our approach adapts models originally designed for able-bodied individuals to forecast joint motion in limb-impaired patients without altering model parameters.
__label__natural_language_processing Finaly, the above process evolves in a dynamic refinement phase, where more effective LLMs are prioritized, enhancing the overall data quality.
__label__machine_learning_for_physical_sciences *Project page*: https://geps-project.github.io
__label__optimization_for_deep_networks Experiments demonstrate the effectiveness of GRE on various model architectures and graph datasets in terms of multiple editing situations.
__label__generative_models Generative retrieval represents a novel approach to information retrieval, utilizing an encoder-decoder architecture to directly produce relevant document identifiers (docids) for queries.
__label__interpretability_and_explainability With the appropriate tokenization, we show that even the simplest unigram models (over tokens) learnt by transformers are able to model the probability of sequences drawn from $k^{\text{th}}$-order Markov sources near optimally.
__label__natural_language_processing Motivated by this, we propose a new approach to calibrate reasoning by up-weighting reasoning paths with high internal consistency, resulting in a significant boost in reasoning performance.
__label__machine_vision Based on this, we propose Geometry guided Neural LiDAR Fields (GeoNLF), a hybrid framework performing alternately global neural reconstruction and pure geometric pose optimization.
__label__machine_vision Extensive experiments demonstrate that our AICT achieves state-of-the-art performance with a lightweight architecture.
__label__reinforcement_learning Specifically, MAST incorporates the Soft Mellowmax Operator with a hybrid TD-($\lambda$) schema to establish dependable learning targets.
__label__neuroscience_and_cognitive_science Time-varying linear state-space models are powerful tools for obtaining mathematically interpretable representations of neural signals.
__label__machine_learning_for_other_sciences_and_fields By combining certificate-driven training and expert iteration, our model learns better representations than models trained for classification only, with a much higher data efficiency -- requiring orders of magnitude less training data.
__label__machine_learning_for_healthcare However, building a unified MLLM for multi-task learning in the medical field remains a thorny challenge.
__label__diffusion_based_models Generative modeling techniques like Generative Adversarial Networks (GANs) and Denoising Diffusion Models (DMMs) have been successfully adapted to solve such transport problems, resulting in CycleGAN and Bridge Matching respectively.
__label__other This framework not only enables interpolation of image content and style but also offers a generation-based approach to address the domain shift problems in unsupervised domain adaptation and domain generalization.
__label__probabilistic_methods Our case studies in linear preference learning and phylogenetic inference showcase the effectiveness of SB-GFlowNets in sampling from an unnormalized posterior in a streaming setting.
__label__diffusion_based_models The BELM formulation is derived from the variable-stepsize-variable-formula linear multi-step method via integrating a bidirectional explicit constraint.
__label__machine_learning_for_physical_sciences Inverse molecular design with diffusion models holds great potential for advancements in material and drug discovery.
__label__machine_learning_for_healthcare Numerical experiments show that the combination of $\texttt{BeatDiff}$ and $\texttt{EM-BeatDiff}$ outperforms SOTA methods for the problems considered in this work.
__label__other Most of the LM-based recommenders convert historical interactions into language prompts, pairing with a positive item as the target response and fine-tuning LM with a language modeling loss.
__label__reinforcement_learning Our CiL experiments with complex tasks in the Franka-Kitchen and Meta-World demonstrate the robust performance of IsCiL in both task adaptation and sample-efficiency.
__label__fairness We demonstrate the efficacy of our approaches on three publicly available conference reviewer assignment datasets.
__label__bandits Indeed, we prove that a learning algorithm leveraging the structure of this problem achieves a regret of $\tilde{O}(K^{4/3}T^{2/3})$ under bandit feedback, improving over the bound of $\tilde{O}(K^{7/4}T^{3/4})$ previously obtained in the literature.
__label__generative_models In this paper, we propose a mixture of multimodal experts (MoME) to mitigate task interference and obtain a generalist MLLM.
__label__safety_in_machine_learning We then leverage the influence of the training dynamics to select high-quality data from different private domains, with centralized model updates on the server side in a collaborative training fashion by either model merging or federated learning.
__label__optimization_for_deep_networks We evaluate our approach using real-world healthcare and vision-and-language datasets with state-of-the-art models, demonstrating superior performance over traditional methods focusing only on one type of modality dependency.
__label__deep_learning_architectures Our code is publicly available at http://github.com/icannotnamemyself/FAN.
__label__robotics Human decision-making in navigation is sequential, planning a most likely sequence of actions toward the goal.
__label__generative_models However, despite these significant advancements, current diffusion models still suffer from several limitations, including inferior visual quality, inadequate aesthetic appeal, and inefficient inference, without a comprehensive solution in sight.
__label__machine_learning_for_other_sciences_and_fields XGBoost, TabPFN).
__label__machine_vision Many prior methods either support only specific categories of images, such as portraits, or require special capture conditions, like using a flashlight.
__label__machine_vision Image relighting is the task of showing what a scene from a source image would look like if illuminated differently.
__label__diffusion_based_models In contrast, we introduce Self-Refining Diffusion Samplers (SRDS) that retain sample quality and can improve latency at the cost of additional parallel compute.
__label__machine_vision To tackle these issues, we revisit the classic sliding inference framework, upon which we propose a Surrounding Guided Segmentation  framework  (SGNet) for ultra image segmentation.
__label__graph_neural_networks Although the message-passing mechanism seems unsuitable for heterophilous graphs due to the propagation of class-irrelevant information, it is still widely used in many existing HTGNNs and consistently achieves notable success.
__label__natural_language_processing Despite the recent observation that large language models (LLMs) can store substantial factual knowledge, there is a limited understanding of the mechanisms of how they acquire factual knowledge through pretraining.
__label__deep_learning_architectures The quadratic complexity and weak length extrapolation of Transformers limits their ability to scale to long sequences, and while sub-quadratic solutions like linear attention and state space models exist, they empirically underperform Transformers in pretraining efficiency and downstream task accuracy.
__label__neuroscience_and_cognitive_science We found that while training dataset size was a core determinant of alignment with human choices, contrastive training with multi-modal data (text and imagery) was a common feature of currently publicly available models that predicted human generalisation.
__label__reinforcement_learning We hence introduce Model-Based Transfer Learning (MBTL), which layers on top of existing RL methods to effectively solve contextual RL problems.
__label__graph_neural_networks (3) We investigate the achievable ratio between smooth and non-smooth feature components for GCNs with the augmented message passing scheme.
__label__machine_vision Cross-domain few-shot segmentation (CD-FSS) is proposed to first pre-train the model on a large-scale source-domain dataset, and then transfer the model to data-scarce target-domain datasets for pixel-level segmentation.
__label__speech_and_audio In this light, we propose SongCreator, a song-generation system designed to tackle this challenge.
__label__reinforcement_learning The project page and code are in https://yingchengyang.github.io/ceurl.
__label__reinforcement_learning Specifically, TRACER first models all corruptions as the uncertainty in the action-value function.
__label__diffusion_based_models We demonstrate the effectiveness of our method using the Stable Diffusion model, showing that it outperforms state-of-the-art erasure methods in eliminating unwanted content while maintaining the integrity of other unrelated elements.
__label__other We present the first mini-batch kernel $k$-means algorithm.
__label__safety_in_machine_learning We also explore the influence of varying training data conditions on \textbf{CeTaD}'s performance.
__label__diffusion_based_models In each optimization, we allow the sample to take multiple steps along the gradient of the proxy constraint function until we can no longer trust the proxy, according to the variance at each diffusion level.
__label__learning_theory At the source stage, we transform a pre-trained deterministic model into a Bayesian Neural Network (BNN) via a variational warm-up strategy, injecting uncertainties into the model.
__label__machine_learning_for_other_sciences_and_fields We also conduct an information-theoretic analysis of the generated cover images, revealing that message hiding predominantly occurs in low-variance pixels, reflecting the waterfilling algorithm's principles in parallel Gaussian channels.
__label__generative_models These results suggest that SMART has initially emulated two important properties: scalability and zero-shot generalization, and preliminarily meets the needs of large-scale real-time simulation applications.
__label__machine_vision Code is available at https://github.com/alibaba/imood.
__label__machine_vision To address these issues, we propose Gaussian Voxel Kernel Functions (GVKF), which establish a continuous scene representation based on discrete 3DGS through kernel regression.
__label__machine_vision Comprehensive evaluations on image classification and object detection  have shown  (1) for logit distillation WKD-L outperforms very strong KL-Div variants; (2) for feature distillation WKD-F is superior to the KL-Div counterparts and state-of-the-art competitors.
__label__machine_vision For practical application, we tailor the implementation of our proposed HC constraint for two main paradigms of adapter tuning.
__label__infrastructure In this paper, we propose a heterogeneous FL framework DapperFL, to enhance model performance across multiple domains.
"__label__robotics In this paper, we hypothesize that learning temporal action abstractions using latent variable models (LVMs), which learn to map data to a compressed latent space and back, is a
promising direction towards low-level skills that can readily be used for new tasks."
__label__natural_language_processing JANUS achieves tie+win rate of 75.2%, 72.4%, and 66.4% against Mistral 7B Instruct v0.2, GPT-3.5 Turbo, and GPT-4, respectively.
__label__machine_vision Finally, we apply temporal weather degradation augmentation to consecutive frames to more accurately represent adverse weather degradations.
__label__optimization We propose an algorithm that leverages stochastic Riemannian gradients and a manifold projection operator to improve computational efficiency, uses local updates to improve communication efficiency, and avoids client drift.
__label__graph_neural_networks In this paper, we approach an overlooked yet critical task Graph2Image: generating images from multimodal attributed graphs (MMAGs).
__label__natural_language_processing RoPE has been further utilized to extend long context capability, which is roughly based on adjusting the \textit{base} parameter of RoPE to mitigate out-of-distribution (OOD) problems in position embedding.
__label__diffusion_based_models Extensive experiments are provided to illustrate the advanced manipulation capabilities of our method concerning state-of-the-art editing works.
__label__natural_language_processing Specifically, we study the structural properties of the solutions selected by gradient-based optimizers among the many possible minimizers of the NTP objective.
__label__natural_language_processing Finally, we show that a ZeTT hypernetwork trained for a base (L)LM can also be applied to fine-tuned variants without extra training.
__label__reinforcement_learning Zero-shot reinforcement learning (RL) promises to provide agents that can perform _any_ task in an environment after an offline, reward-free pre-training phase.
__label__safety_in_machine_learning Current reconstruction-based method provides a good alternative approach, by measuring the reconstruction error between the input and its corresponding generative counterpart in the pixel/feature space.
__label__neuroscience_and_cognitive_science The Bayesian inference-based Expectation-Maximization (EM) framework has proven effective for motion segmentation in event streams, allowing for decoupling without prior information about the motion or its source.
__label__probabilistic_methods LB intermittently adds smaller length scale candidate values while retaining longer scales, balancing exploration and exploitation.
__label__interpretability_and_explainability To guide progress in interpretable dictionary learning, we introduce a new SAE training technique, $p$-annealing, which demonstrates improved performance on our metric.
__label__causal_inference Furthermore, we study how the decision boundaries of these two solutions differ whenever we observe only some of the bivariate distributions implications for Out-Of-Variable (OOV) generalisation.
__label__machine_vision For instance, DyT achieves superior performance compared to existing PEFT methods while evoking only 71% of their FLOPs on the VTAB-1K benchmark.
__label__machine_vision To address this issue, we propose a simple yet effective method called Adaptive Text-Image Harmony (ATIH) to generate novel and surprising objects.
__label__learning_theory Gaussian covariates and shows that, with appropriately chosen reweighting policy, the algorithm can achieve favorable performance in only a handful of iterations.
__label__optimization For Online Matching under long-run Fairness (OM-LF) with a single offline agent, we show  that the first-come-first-serve (FCFS) policy is $1$-competitive, i.e., matching any optimal clairvoyant.
__label__machine_learning_for_physical_sciences Graph DiT has a condition encoder to learn the representation of numerical and categorical properties and utilizes a Transformer-based graph denoiser to achieve molecular graph denoising under conditions.
__label__machine_vision DSRL employs a novel information aggregation strategy to progressively learn event context in hyperbolic spaces, which selects aggregation nodes through layer-sensitive hyperbolic association degrees constrained by hyperbolic Dirichlet energy.
__label__reinforcement_learning Surprisingly, when the reward dimension is larger than $1$, we show that standard analysis of categorical TD learning fails, which we resolve with a novel projection onto the space of mass-$1$ signed measures.
__label__optimization We show that this concern manifests itself in the classical secretary problem in the learning-augmented setting---the state-of-the-art algorithm can have zero probability of accepting the best candidate, which we deem unfair, despite promising to accept a candidate whose expected value is at least $\max\{\Omega (1) , 1 - O(\varepsilon)\}$ times the optimal value, where $\varepsilon$ is the prediction error.
__label__optimization_for_deep_networks In this paper, we study the sharpness of deep linear networks for univariate regression.
__label__learning_theory is well-behaved except at certain sharp transition boundaries.
__label__natural_language_processing In xRAG, the only trainable component is the modality bridge, while both the retriever and the language model remain frozen.
__label__machine_vision The core of LRM-Zero is our procedural 3D dataset, Zeroverse, which is automatically synthesized from simple primitive shapes with random texturing and augmentations (e.g., height fields, boolean differences, and wireframes).
__label__diffusion_based_models To fill this blank, we begin by examining the intermediate statuses during the gradual denoising generation process in DPM.
__label__machine_vision We synchronize the spatio-temporal saliency cues in a single graph for joint optimization, which exhibits better dynamics compared to the previous stage-wise methods that prioritize spatial cues followed by temporal cues.
__label__deep_learning_architectures Our novel MoE mechanism allows SwitchHead to compute up to 8 times fewer attention matrices than the standard Transformer.
__label__learning_theory We study this question for synthetic datasets generated via a Probabilistic Context-Free Grammar (PCFG)---a hierarchical generative model that captures the tree-like structure of natural languages.
__label__machine_vision Image restoration has experienced significant advancements due to the development of deep learning.
__label__bandits Under our proposed LAC condition, we prove that the cumulative expected regret of the greedy algorithm for the linear contextual bandit is bounded by $\mathcal{O}(\operatorname{poly} \log T)$.
__label__machine_learning_for_healthcare We demonstrate that with the proposed set of reactions and building blocks, it is possible to obtain a search space of molecules orders of magnitude larger than existing screening libraries coupled with low cost of synthesis.
__label__optimization Stochastic smooth nonconvex minimax problems are prevalent in machine learning, e.g., GAN training, fair classification, and distributionally robust learning.
__label__deep_learning_architectures In this work we consider the problem of discretization of neural operators in a general setting.
__label__machine_learning_for_physical_sciences Machine learning approaches often struggle to capture this variability.
__label__machine_vision Object pose estimation plays a crucial role in robotic manipulation, however, its practical applicability still suffers from limited generalizability.
__label__neuroscience_and_cognitive_science This introduces a novel perspective of dynamic, subjective, and action-relevant interactions between spatial representations and environmental cues.
__label__machine_learning_for_healthcare Effective extraction of features for the disease areas is crucial for disease classification on radiographic images.
__label__machine_learning_for_other_sciences_and_fields Predicting molecular impact on cellular function is a core challenge in therapeutic design.
__label__probabilistic_methods Our empirical evaluation demonstrates that CMPE not only outperforms current state-of-the-art algorithms on hard low-dimensional benchmarks, but also achieves competitive performance with much faster sampling speed on two realistic estimation problems with high data and/or parameter dimensions.
__label__machine_vision Within such a unified framework, we further promote knowledge collaboration by disentangling the knowledge representations into the semantic and truthfulness spaces.
__label__bandits We also show that this advantage comes with a trade-off: the hypothesis space of the soft tree ensemble model is more constrained than that of a ReLU-based neural network.
__label__machine_vision The code is available at https://github.com/Adlith/MoE-Jetpack.
__label__interpretability_and_explainability However, exact computation of these values is often exponentially expensive, necessitating approximation techniques.
__label__optimization We introduce a convex relaxation of this first-order stochastic dominance and cast it as an optimal transport problem with a smooth and convex cost.
__label__safety_in_machine_learning Theoretically, we show that our algorithm provably converges to the first-order $\varepsilon$-equilibrium point in $O(\varepsilon^{-2})$ gradient iterations with $O(\varepsilon^{-4})$ samples per iteration.
__label__online_learning We introduce the Strategic Littlestone Dimension, a new combinatorial measure that captures the joint complexity of the hypothesis class and the manipulation graph.
__label__diffusion_based_models In a great number of tasks in science and engineering, the goal is to infer an unknown image from a small number of noisy measurements collected from a known forward model describing certain sensing or imaging modality.
__label__diffusion_based_models Link to project page: https://rbz-99.github.io/Diffusion-PID/.
__label__machine_vision Surface parameterization plays an essential role in numerous computer graphics and geometry processing applications.
__label__neuroscience_and_cognitive_science In these cases, it is clear that the priors of the methods' generative models are so powerful that the outputs they produce extrapolate far beyond the neural signal they decode.
__label__neuroscience_and_cognitive_science Through extensive exposure to image data, our network evolves from salt-and-peppers to pinwheel structures, with neurons becoming localized bandpass filters responsive to various orientations.
__label__learning_theory Finally, our simulation results verify the theoretical results.
__label__safety_in_machine_learning Comprehensive experiments demonstrate the effectiveness of SampDetox in defending against various state-of-the-art backdoor attacks.
__label__generative_models Two dominant approaches in this field are AutoRegressive (AR) models and Diffusion Models (DMs).
__label__natural_language_processing However, generating accurate PDDL files typically demands human inputs or correction, which can be time-consuming and costly.
__label__interpretability_and_explainability These channels can be clustered into recurring sets, corresponding to distinct brain regions, indicating the formation of visual concepts.
__label__graph_neural_networks Extensive experiments on ABIDE and ADNI datasets demonstrate that ALTER consistently outperforms generalized state-of-the-art graph learning methods (including SAN, Graphormer, GraphTrans, and LRGNN) and other graph learning based brain network analysis methods (including FBNETGEN, BrainNetGNN, BrainGNN, and BrainNETTF) in neurological disease diagnosis.
__label__machine_learning_for_healthcare This dataset includes well-designed perturbations that distinguish between significant modifications (e.g., removal of a diagnosis) and insignificant ones.
__label__reinforcement_learning We term this the edge-of-reach problem.
__label__diffusion_based_models Code is available at https://github.com/feifeiobama/RectifID.
__label__natural_language_processing The two agents are trained together.
__label__optimization_for_deep_networks EMR-Merging is tuning-free, thus requiring no data availability or any additional training while showing impressive performance.
__label__machine_learning_for_other_sciences_and_fields However, in real-world systems, most users interact with only a handful of items, while the majority of items are seldom consumed.
__label__other (c) Proof of concept with Llama-3-8b, showing 1.11× faster wall clock inference using projected SS1 layers without finetuning.
__label__privacy Training generative models with differential privacy (DP)  typically involves injecting noise into gradient updates or adapting the discriminator's training procedure.
__label__machine_vision We find that it significantly improves the base systems' pose accuracy while yielding high-quality 3D reconstructions that outperform the results from current multi-view reconstruction baselines.
__label__machine_vision Upon the reuse of shared features, as low as 1.89\% parameters are further augmented and fine-tuned for a specific task, which completely avoids extensive optimization of the entire model.
__label__privacy We show experimentally that our heuristic is predictive of the outcome of privacy auditing applied to various training procedures.
__label__machine_vision By extending the observation, we propose a Random Linear Enhancement (RLE) strategy which includes Moderate Random Linear Enhancement (MRLE)  and Radical Random Linear Enhancement (RRLE)  to push the boundaries of both types of transformation.
__label__machine_vision We present a novel diffusion-based approach for coherent 3D scene reconstruction from a single RGB image.
"__label__deep_learning_architectures Specifically, IPRM's ""iterative"" computation facilitates compositional step-by-step reasoning for scenarios wherein individual operations need to be computed, stored, and recalled dynamically (e.g."
__label__learning_theory We complement our theoretical results with experiments on standard RL tasks and autoregressive language generation to validate the practical relevance of our findings.
__label__machine_vision Extensive experiments on twelve object recognition datasets demonstrate that our model, termed Topology-Preserving Reservoir (TPR), outperforms strong baselines including both prompt learning and conventional generative-based zero-shot methods.
__label__causal_inference Empirical experiments demonstrate the effectiveness of our method.
__label__other Our study demonstrates, through both theoretical and empirical evidence, that decomposition is key to containing excessive model inflation while achieving uniformly superior and robust results across various datasets.
__label__interpretability_and_explainability Using eigendecomposition of the pullback of the distance metric defined on the output space through the Jacobian of the model, we are able to reconstruct equivalence classes in the input space and navigate across them.
__label__machine_learning_for_other_sciences_and_fields Traditional Markov Chain Monte Carlo methods face slow convergence and computational burdens.
__label__machine_learning_for_physical_sciences A polymer inverse design task for gas separation with feedback from domain experts further demonstrates its practical utility.
__label__machine_learning_for_healthcare Moreover, to mitigate the impact of cross-domain variants, an Expectation-Maximization (EM) based state recalibration mechanism is designed to map the patch embeddings into a more compact space.
__label__learning_theory Recent advances in algorithmic design show how to utilize predictions obtained by machine learning models from past and present data.
"__label__deep_learning_architectures when counting individual colors for the query: ""determine the maximum occurring color amongst all t-shirts'"")."
__label__deep_learning_architectures In our work, we derive self-attention from kernel principal component analysis (kernel PCA) and show that self-attention projects its query vectors onto the principal component axes of its key matrix in a feature space.
__label__optimization_for_deep_networks Dataset condensation (DC) is an emerging technique capable of creating compact synthetic datasets from large originals while maintaining considerable performance.
__label__diffusion_based_models The source code will be released.
__label__machine_learning_for_other_sciences_and_fields Inspired by recent advancements in generative models, we introduce a novel cover selection framework, which involves optimizing within the latent space of pretrained generative models to identify the most suitable cover images, distinguishing itself from traditional exhaustive search methods.
__label__machine_vision To better reuse the VLM resource and fully leverage its potential on different zero-shot image classification tasks, a promising strategy is selecting appropriate Pre-Trained VLMs from the VLM Zoo, relying solely on the text data of the target dataset without access to the dataset’s images.
"__label__machine_learning_for_healthcare To address the challenge of large search spaces and high sampling costs, 
we design a relaxation mechanism that uses an approximation strategy to efficiently explore optimal subgraph configurations."
__label__interpretability_and_explainability Furthermore, we demonstrate that for a challenging reasoning task with a large search space, GPT-4-Turbo and Gemini-1.5-Pro based on non-parametric memory fail badly regardless of prompting styles or retrieval augmentation, while a fully grokked transformer can achieve near-perfect accuracy, showcasing the power of parametric memory for complex reasoning.
__label__robotics Our dataset construction is cost-efficient, with the carefully-design hand-object interaction retargeting strategy, and the LLM-assisted language guidance annotation system.
__label__neuroscience_and_cognitive_science Whereas the standard usage of GLDMs is to model a single data source, certain applications require jointly modeling two generalized-linear time-series sources while also dissociating their shared and private dynamics.
__label__interpretability_and_explainability We validate these mechanisms by introducing MultEdit a model-editing algorithm that can correct errors and insert new long-tailed information into MLLMs by targeting these causal blocks.
__label__other The model is then trained end-to-end on videos depicting a diverse range of simulated and real-world physical systems, and outperforms its ounterparts —RNNs, autoregressive models, and variational approaches— in state estimation and missing data imputation tasks.
__label__deep_learning_architectures Our code is publicly available at https://github.com/mingcv/DSIT.
__label__other to comply with users' requests to delete their data, or remove mislabeled, poisoned or otherwise problematic data.
__label__causal_inference Therefore, it is natural to employ LLMs to assist with proposing useful high-level factors and crafting their measurements.
__label__learning_theory We study the problem of learning under arbitrary distribution shift, where the learner is trained on a labeled set from one distribution but evaluated on a different, potentially adversarially generated test distribution.
__label__generative_models By automatically generating missing position-related inputs and incorporating position information, the appropriate tool can be effectively employed to address each sub-problem.
__label__optimization We study federated learning in the presence of heterogeneous and non-stationary client availability, which may occur when the deployment environments are uncertain, or the clients are mobile.
__label__optimization We believe our approach can provide valuable insights into best training practices and novel scaling rules.
__label__reinforcement_learning Offline-to-online (O2O) reinforcement learning (RL) provides an effective means of leveraging an offline pre-trained policy as initialization to improve performance rapidly with limited online interactions.
__label__generative_models Zero-shot and in-context learning enable solving tasks without model fine-tuning, making them essential for developing generative model solutions.
__label__graph_neural_networks Rehearsal-based methods, which consolidate old knowledge with a replay memory buffer, are a de facto solution due to their straightforward workflow.
__label__machine_learning_for_healthcare Our method outperforms state-of-the-art techniques in image accuracy and inference speed, demonstrating its potential for intraoperative applications and inverse problems like pose registration.
__label__learning_theory Our novel approach updates the student model by combining priors from both the source and teacher models.
__label__learning_theory While PAC-Bayes allows construction of data-informed priors, the final confidence intervals depend only on the number of points that were not used for the construction of the prior, whereas confidence information in the prior, which is related to the number of points used to construct the prior, is lost.
__label__optimization_for_deep_networks The design of Artificial Neural Network (ANN) is inspired by the working patterns of the human brain.
__label__machine_vision However, we identify two key limitations within this paradigm: (1) under relatively severe domain shifts, most selected reliable pixels appear speckled and remain noisy.
__label__reinforcement_learning Nonetheless, existing methods that evaluate state discrepancy in the latent space under $L_1$ or $L_2$ norm often depend on count-based episodic terms as scaling factors for exploration bonuses, significantly limiting their scalability.
__label__natural_language_processing We also find areas where \textsf{\small RepNoise} still remains ineffective and highlight how those limitations can inform future research.
__label__machine_learning_for_healthcare Our approach initializes a midthickness surface, which is then deformed inward and outward to form the inner (white matter) and outer (pial) cortical surfaces, respectively, by jointly learning diffeomorphic flows by minimizing loss functions to optimize the surfaces towards the boundaries of the cortical ribbon segmentation maps.
__label__learning_theory Further, we develop scratchpad techniques and show that: (i) agnostic scratchpads cannot break the globality barrier, (ii) educated scratchpads can break the globality with intermediate steps, although not all such scratchpads can generalize out-of-distribution (OOD), (iii) a notion of 'inductive scratchpad', that composes the prior information more efficiently, can both break the globality barrier and improve the OOD generalization.
__label__safety_in_machine_learning This involves deriving the evidence lower bounds (ELBOs) for these distributions, approximating the likelihood using the ELBO, and calculating classification probabilities via Bayes' theorem.
__label__machine_vision Furthermore, we introduce depth-based mutual learning to enhance the rendering consistency among multiple sub-NeRFs and mitigate the depth ambiguity.
__label__generative_models In this work, we present Animate3D, a novel framework for animating any static 3D model.
__label__privacy Fixed size subsampling is appealing for its constant memory usage, unlike the variable sized minibatches in Poisson subsampling.
__label__generative_models Post-training Quantization (PTQ) has emerged as a fast and data-efficient solution that can significantly reduce computation and memory footprint by using low-bit weights and activations.
__label__causal_inference Overall, our study finds that amortized causal discovery still needs to obey identifiability theory, but it also differs from classical methods in how the assumptions are formulated, trading more reliance on assumptions on the noise type for fewer hypotheses on the mechanisms.
__label__robotics With the advancement of embodied AI, robots are increasingly capable of satisfying human demands.
__label__diffusion_based_models While previous studies suggest that blended text embeddings lead to improper attribute binding, few have explored this in depth.
__label__machine_learning_for_other_sciences_and_fields We achieve 75.6% accuracy on the labelling of financial meanings against human annotations.
__label__deep_learning_architectures Our work addresses these challenges by proposing the Calibrated Self-Rewarding (CSR) approach, which enables the model to self-improve by iteratively generating candidate responses, evaluating the reward for each response, and curating preference data for fine-tuning.
__label__learning_theory Our code is available at [https://github.com/marlo-z/reversal_curse_analysis/](https://github.com/marlo-z/reversal_curse_analysis/).
__label__causal_inference We see that our method is able to reveal heterogeneity of the effect across different quantiles.
__label__safety_in_machine_learning The commercial text-to-image deep generation models (e.g.
__label__other However, topological conjugacies have historically been challenging to compute.
__label__deep_learning_architectures Mixed time series (MiTS) comprising both continuous variables (CVs) and discrete variables (DVs) are frequently encountered yet under-explored in time series analysis.
__label__diffusion_based_models Recently, the strong latent Diffusion Probabilistic Model (DPM) has been applied to high-quality Text-to-Image (T2I) generation (e.g., Stable Diffusion), by injecting the encoded target text prompt into the gradually denoised diffusion image generator.
__label__fairness It generates high-quality cross-cultural dialogues encapsulating human beliefs, norms, and customs.
__label__causal_inference In this paper, we draw a clear connection between the two and their key assumptions, allowing us to apply identifiable methods developed in causal representation learning to dynamical systems.
__label__diffusion_based_models However, malicious fine-tuning can still make models prone to generating harmful or undesirable images even with these methods.
__label__online_learning The key hurdle is partial information due to MDP uncertainty.
__label__machine_learning_for_healthcare We also modulate protein language models (PLMs) with structural conditions to precisely approximate the Markov bridge process, thereby significantly enhancing generation performance while maintaining parameter-efficient training.
__label__optimization In particular, we investigate the convergence rate of MFLD applied to the bilevel reduction in the low-noise regime and obtain two results.
__label__interpretability_and_explainability Although some penalty-based methods have been developed to penalize the spurious features (e.g., invariance penalty, intervention penalty, etc) to help MMI work better, these are merely remedial measures.
__label__other Based on a data generation model consisting of signal and noise, our analysis is performed on a ReLU network trained with the InfoMax objective function.
__label__robotics More specifically, (i) we show analytically that CON is a Lagrangian system - i.e., it possesses well-defined potential and kinetic energy terms.
__label__diffusion_based_models Our algorithm only involves the evaluation of the estimated Stein score, making it scalable to existing pre-trained models at inference time and online during training.
__label__speech_and_audio By employing a non-autoregressive vector field estimator based on a feed-forward transformer and channel-level cross-modal feature fusion with strong temporal alignment, our model generates audio that is highly synchronized with the input video.
__label__generative_models Additionally, we have developed a trajectory interpolation algorithm and synthetic trajectory data to enhance model capacity and improve generalization.
__label__machine_vision To address this gap, we propose a more meticulous mask-level alignment between 3D features and the 2D-text embedding space through a cross-modal mask reasoning framework, XMask3D.
__label__deep_learning_architectures In this paper, we propose a novel approach named Diffusion-based Representation with Random Distance matching (D2R2) for tabular few-shot learning.
__label__learning_theory We present a non-asymptotic statistical bound that quantifies this variance reduction effect and relates it to the eigenvalue decay of Markov operators.
__label__machine_learning_for_other_sciences_and_fields We consider a long-standing open problem in mathematics: discovering a Lyapunov function that ensures the global stability of a dynamical system.
__label__safety_in_machine_learning To efficiently solve the optimization problem, a three-stage process \textit{Ask, Attend, Attack}, called \textit{AAA}, is proposed to coordinate with the solver.
__label__machine_learning_for_physical_sciences Yet, in numerous real-world scenarios, identifying the specific symmetry within a given data distribution often proves ambiguous.
__label__machine_vision We present DC-Gaussian, a new method for generating novel views from in-vehicle dash cam videos.
__label__machine_learning_for_social_sciences The key technical contribution is a novel LLM agent framework that accounts for individual activity patterns and motivations, including a self-consistency approach to align LLMs with real-world activity data and a retrieval-augmented strategy for interpretable activity generation.
__label__causal_inference Most studies focus on the linear latent variable model or impose strict constraints on latent structures, which fail to address cases in discrete data involving non-linear relationships or complex latent structures.
__label__other The proposed ST$_k$ Loss outperforms AT$_k$ Loss and achieves the best average performance on multiple benchmarks, with the lowest standard deviation.
__label__learning_theory Besides, to the best of our knowledge, our high probability results on the generalization gap measured by gradients for nonconvex problems are also the sharpest.
__label__interpretability_and_explainability We make the simple observation that the empirical smooth calibration linear program can be reformulated as an instance of minimum-cost flow on a highly-structured graph, and design an exact dynamic programming-based solver for it which runs in time $O(n\log^2(n))$, and solves the calibration testing problem information-theoretically optimally in the same time.
__label__natural_language_processing Our codes are released at https://github.com/danshi777/IRCAN.
__label__optimization_for_deep_networks The largest eigenvalue of the Hessian, or sharpness, of neural networks is a key quantity to understand their optimization dynamics.
__label__optimization_for_deep_networks However, finetuning LLMs using backpropagation requires excessive memory (especially from intermediate activations) for resource-constrained devices.
__label__interpretability_and_explainability To achieve this, we propose a two-phase scheme: First, we curate a new dataset that offers structured rationales for visual recognition tasks.
__label__optimization_for_deep_networks CoMERA also outperforms the recent GaLore in terms of both memory and computing efficiency.
__label__graph_neural_networks Empirical evaluation is conducted on the challenging CLRS Algorithmic Reasoning Benchmark, which consists of 30 diverse algorithmic tasks.
__label__generative_models These wavelet-based outputs are then processed and seamlessly fused with the original Mamba outputs through a cross-attention fusion layer, combining both spatial and frequency information to optimize the order awareness of state-space models which is essential for the details and overall quality of image generation.
__label__online_learning Let $n,T,\bar{d}$ denote the dimensionality, time horizon, and average delay, respectively.
__label__online_learning Moreover, ours is the first best-of-both-worlds algorithm providing bounds logarithmic in the number of constraints.
__label__machine_vision Extensive experiments demonstrate the effectiveness of the proposed AnA framework on learning plasticity and memory stability during continual learning.
__label__diffusion_based_models To address this shortcoming, we consider representations from pre-trained text-to-image diffusion models, which are explicitly optimized to generate images from text prompts and as such, contain text-conditioned representations that reflect highly fine-grained visuo-spatial information.
__label__generative_models In ICL, a conditional generative model (CGM) is prompted with a dataset and a prediction question and asked to generate a response.
__label__learning_theory Empirically, numerical results further validate the theoretical findings, showcasing the efficiency and accuracy of the proposed framework.
"__label__safety_in_machine_learning Focusing on 
the second-order Sobolev space, we then derive the optimal encoder and decoder."
__label__diffusion_based_models Besides its simplicity, RADD can reduce the number of function evaluations (NFEs) by caching the output of the time-independent network when the noisy sample remains unchanged in a sampling interval.
__label__reinforcement_learning To examine the achievability of ULI, we first provide two positive results for bandit problems with finite arms, showing that some elimination-based algorithms and high-probability adversarial algorithms with stronger analysis or additional designs, can attain near-optimal ULI guarantees.
__label__graph_neural_networks Prevailing OOD detection techniques developed in other domains like computer vision, do not cater to the interconnected nature of graphs.
__label__optimization A neural router is designed to adeptly distribute training instances among models, enhancing overall load balancing and collaborative efficacy.
__label__machine_vision This paper studies self-supervised image denoising, requiring only noisy images captured in a single shot.
__label__machine_vision The performance of 3D object detection in large outdoor point clouds deteriorates significantly in an unseen environment due to the inter-domain gap.
__label__natural_language_processing Second, LFMs outperform using LLMs as experts to directly predict actions, when controlling for the number of LLM output tokens.
__label__learning_theory Finally, we adapt our proof techniques to obtain a new ``neighborhood'' variant of the cubical KKM lemma (or cubical Sperner's lemma): For any coloring of $[0,1]^d$ in which no color is used on opposing faces, it holds for each $\epsilon\in(0,\frac12]$ that there is a point where the open $\epsilon$-radius $\ell_\infty$-ball intersects at least $(1+\frac23\epsilon)^d$ colors.
__label__safety_in_machine_learning Codes available at https://github.com/zhicheng2T0/Full-Distance-Attack.git
__label__probabilistic_methods We support our approach theoretically through the concept of variational inference and validate it empirically using different types of compressible signals.
__label__neuroscience_and_cognitive_science Here, we aim to bridge this gap by studying how a shared lexicon may emerge from local pragmatic interactions.
__label__machine_vision Existing Vision-Language Model (VLM)-based methods leverage VLM's rich knowledge to enhance additional explicit segmentation-specific networks, yielding competitive results, but at the cost of extensive training cost.
__label__deep_learning_architectures Vector Symbolic Architectures (VSAs) are one approach to developing Neuro-symbolic AI, where two vectors in $\mathbb{R}^d$ are 'bound' together to produce a new vector in the same space.
__label__bandits Specifically, we prove the following regret bounds: $\Theta(\ln T)$ in the deterministic setting, $\Omega(T)$ in the stochastic setting, and $\tilde{\Theta}(T^{2/3})$ in the stochastic setting when sellers' and buyers' valuations are independent of each other.
__label__learning_theory Yet there are fundamental limitations to this size reduction when we want to recover an accurate estimator for a task such as least square regression.
__label__optimization While LMC has been achieved for neural networks by considering the permutation invariance of neurons in each hidden layer, its attainment for other models remains an open question.
__label__natural_language_processing Our results reveal that LLMs can achieve fluency in KGL, drastically reducing errors compared to conventional KG embedding methods on KG completion.
__label__natural_language_processing By leveraging two teacher models possessing complementary knowledge, we introduce a LIghtweight kNowledge alignmEnt (LINE) module aimed at harmonizing their knowledge within a unified representation space.
__label__privacy Such a design enables a more unique and reliable fingerprinting with relatively low querying costs.
__label__graph_neural_networks Neural Algorithmic Reasoning (NAR) research has demonstrated that graph neural networks (GNNs) could learn to execute classical algorithms.
__label__neuroscience_and_cognitive_science In this work we explore the computational implications of synaptic gain scaling, a form of neuromodulation, using task-optimized low-rank RNNs.
__label__machine_vision DMesh considers both the geometry and connectivity information of a mesh.
__label__machine_vision Few-shot font generation (FFG) aims to learn the target style from a limited number of reference glyphs and generate the remaining glyphs in the target font.
__label__graph_neural_networks Finally, extensive experiments verify the effectiveness of the SpeAr, which can further alleviate the bias problem.
__label__optimization_for_deep_networks To maintain model calibration during optimization, we utilize a proportional-integral-derivative (PID) controller to dynamically adjust this gradient decay rate, where the adjustment relies on the proposed relative calibration error feedback in each epoch, thereby preventing the model from exhibiting over-confidence or under-confidence.
__label__causal_inference To address these challenges, we propose a dynamic subgroup identification covariate-adjusted response-adaptive randomization (CARA) design strategy with the following key features: (i) Our approach is an adaptive experimental strategy that allows the dynamic identification of the best subgroups and the revision of treatment allocation towards the goal of correctly identifying the best subgroups based on collected experimental data.
__label__interpretability_and_explainability The most widely used criterion for rationale extraction is the maximum mutual information (MMI) criterion.
__label__machine_learning_for_other_sciences_and_fields To address hallucinations for reliable results, we decompose the problem into several subtasks and introduce a series of novel strategies.
__label__interpretability_and_explainability In this paper, we propose selective explanations to (i) detect when amortized explainers generate inaccurate explanations and (ii) improve the approximation of the explanation using a technique we call explanations with initial guess.
__label__natural_language_processing Therefore, it has a high decoding speed but an unsatisfactory acceptance rate.
__label__machine_vision Despite advances in Gaussian pruning techniques that aim to remove individual 3D Gaussian primitives, the significant reduction in primitives often fails to translate into commensurate increases in rendering speed, impeding efficiency and practical deployment.
__label__privacy We provide an efficient GPU implementation for fully connected networks and show that it recovers high-dimensional ImageNet inputs in batches of up to $b \lesssim 25$ exactly while scaling to large networks.
__label__natural_language_processing We propose introspective planning, a systematic approach that guides LLMs to refine their own uncertainty in alignment with inherent task ambiguity.
__label__generative_models Inspired by these findings, we propose a simple self-correction strategy, Checking as Context (CaC), which finds novel applications in alleviating social bias and defending against LLM jailbreaks.
__label__machine_vision In this work, we introduce a new robust camera-insensitivity problem: how to overcome the issues caused by the failed camera perspectives, while stabilizing high collaborative performance with low calibration cost?
__label__machine_learning_for_other_sciences_and_fields The proliferation of abundant electricity time series (ETS) data presents numerous opportunities for various applications within power systems, including demand-side management, grid stability, and consumer behavior analysis.
__label__generative_models We verify Atlas3D's efficacy through extensive generation tasks and validate the resulting 3D models in both simulated and real-world environments.
__label__reinforcement_learning In the fine-tuning stage, we harness the imagined future videos to guide low-level action learning with a limited set of robot data.
__label__diffusion_based_models Our method allows us to deploy state-of-the-art latent diffusion models such as Stable Diffusion XL to solve video inverse problems.
__label__other Conventional outlier detection approaches typically use a two-stage procedure: first, outliers are detected and removed, and then estimation is performed on the cleaned data.
__label__machine_learning_for_healthcare Our code is at https://github.com/DeepGraphLearning/S3F.
__label__optimization This work presents BAdam, an optimization method that leverages the block coordinate descent (BCD) framework with Adam's update rule.
__label__graph_neural_networks We conduct an extensive set of experiments across diverse datasets and tasks, demonstrating a consistent and superior performance of DiGRAF compared to traditional and graph-specific activation functions, highlighting its effectiveness as an activation function for GNNs.
__label__probabilistic_methods We empirically validate both applications in centralized and federated learning settings, showing our theoretical results translate to lower inefficiency (average prediction set size) for popular CP methods.
__label__graph_neural_networks This ensures that downstream processes are more efficient and effective.
__label__interpretability_and_explainability Our work offers theoretical guarantees and practical insights for practitioners and policymakers on whether simple-yet-accurate machine learning models are likely to exist, based on knowledge of noise levels in the data generation process.
__label__machine_vision These samples are first roughly selected from current benchmarks with an automated pipeline, human review is then involved to ensure each curated sample exhibits visual dependency, minimal data leakage, and requires advanced multi-modal capabilities.
__label__natural_language_processing We conduct extensive experiments using Llama-2 and GPT-J in both zero-shot setting and few-shot setting.
__label__privacy We aim to minimize the average regret on $m$ clients working in parallel over time horizon $T$ with explicit differential privacy (DP) guarantees.
__label__graph_neural_networks However, current state-of-the-art learned simulators operate on meshes and scale poorly to scenes with many objects or detailed shapes.
__label__machine_learning_for_healthcare increasing secondary structures diversity, by introducing a Reinforced Finetuning (ReFT) objective.
__label__learning_theory We focus on this problem in the semi-random setting where each entry is independently revealed with probability at least $p = \frac{\textup{poly}(r, \mu, \log d)}{d}$.
__label__online_learning In this work, we provide a complete answer to this question in various settings.
__label__machine_vision Codes and checkpoints are available at Github.
__label__natural_language_processing Notably, the LLaMA-1|2|3 model pruned by our DSA reaches 4.73\%|6.18\%|10.65\% gain over the state-of-the-art techniques (e.g., Wanda and SparseGPT).
__label__reinforcement_learning For this reason, in this work, we focus on the reconstruction of the feasible reward set when, in addition to demonstrations from the optimal expert, we observe the behavior of multiple *sub-optimal experts*.
__label__machine_vision In brief, we incorporate the learnable graph kernels into the classic Discrete Element Analysis (DEA) framework to implement a novel mechanics-informed network architecture.
__label__optimization Numerical experiments are conducted on classification tasks using deep learning models to confirm the practical aspects of our analysis.
__label__privacy Previous private estimators on distributions over $\mathbb{R}^d$ suffer from a curse of dimensionality, as they require $\Omega(d^{1/2})$ samples to achieve non-trivial error, even in cases where $O(1)$ samples suffice without privacy.
__label__machine_vision Empirically, we find that TrAct (Training Activations) speeds up training by factors between 1.25x and 4x while requiring only a small computational overhead.
__label__probabilistic_methods In practice, however, this recipe entails i) approximating an intractable posterior at each time step; and ii) encapsulating results appropriately to allow for posterior propagation.
__label__natural_language_processing Our code is available at [github.com/zhxieml/internal-consistency](https://github.com/zhxieml/internal-consistency).
__label__bandits The best arm, or the arm with the largest mean amongst finitely many, is identified through an algorithm that at any sequential step independently pulls the empirical best arm, with a fixed probability $\beta$, and pulls the best challenger arm otherwise.
__label__natural_language_processing Human possess a remarkable ability to create mental images of unseen objects and actions through a process known as the Mind's Eye, enabling the imagination of the unseen world.
__label__optimization Further, existing algorithms have each only been shown to work for specific settings of $p$ and $q$ and under certain assumptions on the loss and the feasible set, whereas we provide a general algorithm for DP SSPs whenever $p,q\in[1,2]$.
__label__machine_vision We thoroughly evaluate our approach following the experimental protocol established in the literature and show that ZERO largely surpasses or compares favorably w.r.t.
__label__diffusion_based_models Our key contribution lies in how we parameterize the diffusion timestep in the forward diffusion process.
__label__fairness To further substantiate this claim, **as our major contribution**, we deep dive into the denoising subnetwork of the T2I model to track down the effect of these learned prompts by analyzing the cross-attention maps.
__label__machine_vision Our approach not only sets a new standard for state-of-the-art works but also significantly enhances attack performance, exceeding the baseline method by over 16\%.
__label__infrastructure We introduce a new approach called FLoRA that enables federated fine-tuning on heterogeneous LoRA adapters across clients through a novel stacking-based aggregation method.
__label__machine_vision Numerical experiments demonstrate that our method outperforms competitors in a wide range of ill-posed imaging inverse problems.
__label__human-AI_interaction While this type of systems have been proven to be effective at improving the average accuracy of the predictions made by humans, by restricting human agency, they may cause harm---a human who has succeeded at predicting the ground-truth label of an instance on their own may have failed had they used these systems.
__label__probabilistic_methods Machine learning practitioners often face significant challenges in formally integrating their prior knowledge and beliefs into predictive models, limiting the potential for nuanced and context-aware analyses.
__label__machine_learning_for_healthcare Medical imaging tasks require an understanding of subtle and localized visual features due to the inherently detailed and area-specific nature of pathological patterns, which are crucial for clinical diagnosis.
__label__diffusion_based_models However, most existing CDMs unreasonably assume that personalized concepts are fixed and cannot change over time.
__label__machine_vision In addition to the cache branch, we also construct a prior branch that includes learnable prompt vectors, using the text encoder of visual-language models for patch classification.
__label__privacy Given $n$ samples from a distribution with known covariance-proxy $\Sigma$ and unknown mean $\mu$, we present an estimator $\hat{\mu}$ that achieves error, $\|\hat{\mu}-\mu\|_2\leq \alpha$, as long as $n\gtrsim \text{tr}(\Sigma)/\alpha^2+ \text{tr}(\Sigma^{1/2})/(\alpha\varepsilon)$.
__label__machine_vision Instead, it employs open-vocabulary 2D models and pseudo-LiDAR to automatically label 3D objects in RGB images, fostering the learning of open-vocabulary monocular 3D detectors.
__label__reinforcement_learning Recent world models predominantly operate on sequences of discrete latent variables to model environment dynamics.
__label__neuroscience_and_cognitive_science Additionally, we introduce the last time step (LTS) approach to accelerate convergence in static tasks, and we propose a label smooth temporal efficient training (TET) loss to mitigate the conflicts between optimization objective and regularization term in the vanilla TET.
__label__learning_theory First, we rigorously confirm the so-called saturation effect for ridge regression with vector-valued output by deriving a novel lower bound on learning rates; this bound is shown to be suboptimal when the smoothness of the regression function exceeds a certain level.
__label__human-AI_interaction We lift the major limitation of prior work in this area—scalability to high-dimensional settings—with contrastive successor representations.
__label__machine_vision To this end, we propose to treat the video frames as  samples from an unknown distribution, enabling us to frame their distance calculation as an optimal transport (OT) problem.
__label__natural_language_processing Extensive experiments conducted across a variety of models and tasks demonstrate that IRCAN not only achieves remarkable improvements in handling knowledge conflicts but also offers a scalable, plug-and-play solution that can be integrated seamlessly with existing models.
__label__evaluation It assumes equal importance for all classes, hence equal severity for misclassifications.
__label__generative_models We present Diffusion-KTO, a novel approach for aligning text-to-image diffusion models by formulating the alignment objective as the maximization of expected human utility.
__label__natural_language_processing Iteratively improving and repairing source code with large language models (LLMs), known as refinement, has emerged as a popular way of generating programs that would be too complex to construct in one shot.
__label__machine_vision Project page: https://junshengzhou.github.io/DeepPriorAssembly.
__label__learning_theory We show that under some assumptions on the fourth and the eighth moments of $\mathcal{D}$, there is a polynomial-time algorithm that achieves error $o(\sqrt{\varepsilon})$ as long as $n \ge \tilde{O}(k^4 / \varepsilon^3)$.
__label__learning_theory We prove a square-root growth rate near zero for smooth margin-based surrogate losses in binary classification, providing both upper and lower bounds under mild assumptions.
__label__diffusion_based_models Additionally, the expanded dataset exhibits robustness across various architectural frameworks.
__label__optimization_for_deep_networks Importantly, with these findings we demonstrate that scaling experiments can be performed with significantly reduced compute and GPU hours by utilizing fewer but reusable training runs.
__label__graph_neural_networks In this work, we focus on the link prediction task and systematically analyze the impact of heterophily in node features on GNN performance.
__label__safety_in_machine_learning The code is available at https://github.com/LUMIA-Group/HuRef.
__label__diffusion_based_models In this paper, we introduce a new perspective and framework, demonstrating that diffusion models with cross-attention itself can serve as a powerful inductive bias to facilitate the learning of disentangled representations.
__label__machine_vision However, most diffusion models struggle with this task, \textit{i.e.
__label__reinforcement_learning Deep Reinforcement Learning (DRL) algorithms have achieved great success in solving many challenging tasks while their black-box nature hinders interpretability and real-world applicability, making it difficult for human experts to interpret and understand DRL policies.
__label__causal_inference Additionally, we present a case study showing how this algorithm could be modified to answer more general causal questions without learning the whole graph.
__label__machine_learning_for_other_sciences_and_fields For adders, our approach discovers designs of 128-bit adders that achieve Pareto optimality in theoretical metrics.
__label__causal_inference Moreover, incorporating a modest fraction of labeled data (5-10\%) substantially enhances DeepITE's performance, further solidifying its practical applicability.
__label__neuroscience_and_cognitive_science Existing coding schemes either cause huge delays and energy consumption or necessitate intricate neuron models and training techniques.
__label__fairness We evaluated these models across three downstream tasks: content moderation, cultural alignment, and cultural education.
__label__machine_learning_for_other_sciences_and_fields Our comprehensive experiments indicate that Time-FFM outperforms state-of-the-arts and promises effective few-shot and zero-shot forecaster.
__label__deep_learning_architectures Remote photoplethysmography (rPPG) enables non-invasive extraction of blood volume pulse signals through imaging, transforming spatial-temporal data into time series signals.
__label__natural_language_processing Further, by leveraging complexity theory, we support these findings with a theoretical analysis focused on the sample inefficiency of gradient descent in memorizing feedforward models.
__label__other Our code is publicly available at https://github.com/cuong-dm/IGNITE.
__label__natural_language_processing Current parameter-efficient fine-tuning (PEFT) methods build adapters widely agnostic of the context of downstream task to learn, or the context of important knowledge to maintain.
__label__safety_in_machine_learning Shadowcast demonstrates effectiveness in two attack types.
__label__safety_in_machine_learning Formally, GREAT Score carries the physical meaning of a global statistic capturing a mean certified attack-proof perturbation level over all samples drawn from a generative model.
__label__natural_language_processing Furthermore, it can be flexibly combined with other decoding methods to further enhance their performance.
__label__learning_theory In actual deployment, however, the data distribution may (and often does) vary, causing domain adaptation/generalization issues.
__label__diffusion_based_models Extensive user studies demonstrate that our model is preferred nearly twice as often compared to the popular SDXL model and is on par with the proprietary Stable Diffusion 3 with 8B parameters.
__label__safety_in_machine_learning Finally, we introduce the notion of undetectable backdoors to language models and extend our neural network backdoor attacks to such models based on the existence of steganographic functions.
__label__machine_learning_for_physical_sciences We further apply the pre-trained EUNet to animate various garments based on energy optimizations.
__label__neuroscience_and_cognitive_science We make testable predictions: a) rapidly changing sensory context will disrupt place fields, b) place fields will form even if recurrent connections are blocked, but reversion to previously learned representations upon remapping will be abolished, c) the dimension of temporally smooth experience sets the dimensionality of place fields, including during virtual navigation of abstract spaces.
__label__deep_learning_architectures We find that a simple yet heuristic integration strategy can significantly alleviate the modality imbalance phenomenon.
__label__privacy The result shows that the new method is much less sensitive to the imbalance of user-wise sample sizes and the tail of sample distributions.
__label__safety_in_machine_learning Extensive experiments demonstrate that DiffusionFake significantly improves cross-domain generalization of various detector architectures without introducing additional parameters during inference.
__label__machine_learning_for_other_sciences_and_fields Autoformalization is the task of translating natural language materials into machine-verifiable formalisations.
__label__algorithmic_game_theory In this work, we provide the first quantum algorithm for market equilibrium computation with sub-linear performance.
__label__graph_neural_networks The code is available at https://github.com/structlearning/GraphEdX.
__label__optimization_for_deep_networks Due to intractable bi-level optimization, the OBD objective is difficult to minimize to small values, which deteriorates PBC by its distillation performance guarantee with quadratic discount complexity $\mathcal{O}(1/(1-\gamma)^2)$.
__label__optimization_for_deep_networks In this work, we analyze SAM’s training dynamics using the maximum eigenvalue of the Hessian as a measure of sharpness and propose a third-order stochastic differential equation (SDE), which reveals that the dynamics are driven by a complex mixture of second- and third-order terms.
__label__machine_vision However, these prior methods fail to generalize across different camera views due to the lack of cross-view geometric modeling.
__label__neuroscience_and_cognitive_science The network solves these tasks by flexibly switching between stochastic and deterministic modes as needed and projecting noise onto a null space.
__label__machine_vision Beyond that, a novel fine-tuning technology to further boost the performance of AdaPKC-based RSS networks is presented.
__label__natural_language_processing Extensive experiments on $20$ datasets for both language and vision tasks demonstrate the effectiveness of our method, showing an average improvement of $28.34\%$ in absolute normalized score for discriminative tasks and even surpassing the fine-tuned upper bound on the generative tasks.
__label__optimization More specifically, we introduce a without-replacement sampling based algorithm which achieves a faster convergence rate compared to its counterparts that rely on independent sampling.
__label__evaluation However, it is not clear whether such studies shed light on the extent to which models show reasoning ability, and there is controversy about the significance and implications of such results.
__label__deep_learning_architectures Besides, we also utilize trajectory memory and visual memory to improve tracking stability.
__label__safety_in_machine_learning In every case, VRCP achieves above nominal coverage and yields significantly more efficient and informative prediction regions than the SotA.
__label__generative_models Finally, we provide bounds in the total variation distance between the data distribution and the resulting distribution of our model in the case where the base distribution is the standard $d$-dimensional Gaussian distribution.
__label__machine_learning_for_healthcare Such a crude approximation can degrade the quality of treatment, potentially causing unnecessary radiation exposure to healthy tissues—this may lead to significant radiation-induced side effects—or delivering inadequate radiation to the tumor, which is crucial for effective tumor treatment.
__label__diffusion_based_models However, most existing molecular diffusion models treat each atom as an independent entity, overlooking the dependency among atoms within the substructures.
__label__algorithmic_game_theory More specifically, no black-box optimisation algorithm for finding the unique Nash equilibrium in two-player zero-sum games can exceed logarithmic complexity relative to search space size.
__label__machine_learning_for_healthcare Previous approaches have focused on enhancing discrimination and marginal calibration.
__label__learning_theory As models get deployed in a variety of real-world scenarios, they inevitably face strategic environments.
__label__diffusion_based_models Our algorithm also comes with better or comparable computational efficiency than previous state-of-the-art methods.
__label__generative_models likes or dislikes, to align the model with human preferences.
__label__other From an information-theoretic perspective, we propose FedMDMI, a federated posterior inference framework based on model-data mutual information (MI).
__label__safety_in_machine_learning In addition, we conduct comprehensive and elaborate (e.g., making sure to use correct system prompts) evaluations against other aligned LLMs and advanced defenses, where our method consistently achieves nearly 100% ASRs.
__label__optimization However, addressing these problems is generally challenging due to their non-convex nature and the computational intensity of the constraints.
__label__reinforcement_learning Deep reinforcement learning (RL) is a powerful approach to complex decision-making.
__label__optimization_for_deep_networks Secondly, to achieve a balance between fine-tuning performance and efficiency, we propose an adaptive adjustment of time-series window, which adaptively controls the size of time-series for significance measurement and rank reduction during training, allowing for rapid rank allocation while maintaining training stability.
__label__interpretability_and_explainability Empirically, Source outperforms existing TDA techniques in counterfactual prediction, especially in settings where implicit-differentiation-based approaches fall short.
__label__graph_neural_networks Toward this goal, we first develop a Graph Question Answering (GraphQA) benchmark with data collected from different tasks.
__label__machine_vision In this paper, we analyze two inherent challenges in assessing the ability of a VLM in this Language-Only VLM selection: the “Modality Gap”—the disparity in VLM’s embeddings across two different modalities, making text a less reliable substitute for images; and the “Capability Gap”— the discrepancy between the VLM’s overall ranking and its ranking for target dataset, hindering direct prediction of a model’s dataset-specific performance from its general performance.
__label__neuroscience_and_cognitive_science First, inspired by some ANN-SNN methods that directly copy-paste the weight parameters from trained ANN with light modification to homogeneous SNN can obtain a well-performed SNN, we use rich information of the weight parameters from the trained ANN counterpart to guide the feature representation learning of the SNN.
__label__privacy For a stream of length $T$ and privacy *without* expiration continual counting is possible with maximum  (over all time steps) additive error $O(\log^2(T)/\varepsilon)$ and the best known lower bound is $\Omega(\log(T)/\varepsilon)$; closing this gap is a challenging open problem.
__label__neuroscience_and_cognitive_science Event-based cameras are attracting significant interest as they provide rich edge information, high dynamic range, and high temporal resolution.
__label__machine_learning_for_other_sciences_and_fields Experiments on small, medium, and large-scale satellite networks datasets demonstrate that Satformer outperforms mathematical and neural baseline methods notably.
__label__machine_vision To achieve efficient pruning and accurate vicinity characterization, we further propose a novel overlap-aware Sinkhorn Distance, which retains only the most likely overlapping points for local measurement and next level exploration.
__label__machine_vision Hence, maintaining the style-invariant property with varying domain styles becomes the key bottleneck in harnessing VFM for DGSS.
__label__generative_models Specifically, given an input textual query, our scheme consists of four stages: 1) we leverage the LLMs as the director to first decompose the complex query into several sub-queries, where each sub-query describes each element of the generated video; 2) to generate each element, pre-trained models are invoked by the LLMs to obtain the corresponding 3D representation; 3) to composite the generated 3D representations, we prompt multi-modal LLMs to produce coarse guidance on the scale, location, and trajectory of different objects; 4) to make the results adhere to natural distribution, we further leverage 2D diffusion priors and use score distillation sampling to refine the composition.
__label__natural_language_processing On the other hand, the impact of the *instruction*, another essential component in the prompt given to the LLM, is often overlooked in existing exemplar selection methods.
__label__other A branch of research enhances CF methods by message passing (MP) used in graph neural networks, due to its strong capabilities of extracting knowledge from graph-structured data, like user-item bipartite graphs that naturally exist in CF.
__label__infrastructure In this paper, we introduce Distributed Lion, an innovative adaptation of Lion for distributed training environments.
__label__machine_vision Remarkably, using only synthetic datasets for training, HODC achieves state-of-the-art generalization performance with various existing stereo matching network architectures, across multiple realistic datasets.
__label__safety_in_machine_learning Multiple samples from such distribution are classified by the same adversarially-trained model, and an aggregation of its outputs finally constitutes the *robust prediction* of interest.
__label__bandits Instead, one must consider a set of possibly optimal arms/interventions, each being a special subset of the ancestors of the reward node, making causal discovery beyond the parents of the reward node essential.
__label__diffusion_based_models Our code is released at https://github.com/yukang123/GGDMOptim.git.
__label__reinforcement_learning Motivated by this challenge, we introduce an innovative Multi-Agent Sparse Training (MAST) framework aimed at simultaneously enhancing the reliability of learning targets and the rationality of sample distribution to improve value learning in sparse models.
__label__machine_vision Through iterative optimization and merging, we reconstruct complete and accurate 3D planes with solid surfaces and clear boundaries.
__label__other Then the weighted combination of the self-supervised loss and the distillation loss can support the global model to encode all classes from clients into a unified space.
__label__learning_theory gave the first and only known result that achieves sublinear bounds in both the sampling complexity and the query time while preserving polynomial data structure space.
__label__generative_models Our work delineates the viability of an integrated approach to multimodal generation within the visual text domain, setting a foundation for subsequent inquiries.
__label__diffusion_based_models However, in this paper, we find that these models tend to generate videos with less motion than expected.
__label__generative_models We integrate a comprehensive range of existing models into the tool library and utilize the agent for tool selection and execution.
__label__causal_inference To yield context-specific insights, conditional independence is tested on context-specific data.
__label__natural_language_processing However, existing topic models generally struggle with either effectiveness, efficiency, or stability, highly impeding their practical applications.
__label__other The identifiability claims are thoroughly validated using synthetic and real-world data.
__label__interpretability_and_explainability This paper aims to develop a new criterion that treats spurious features as plain noise, allowing the model to work on datasets rich in spurious features as if it were working on clean datasets, thereby making rationale extraction easier.
__label__natural_language_processing To mitigate this limitation, we first propose a lightweight uncertainty quantification method that assesses the reliability of the proxy reward using only the last layer embeddings of the reward model.
__label__graph_neural_networks Rigid interactions are notoriously hard to model: small changes to the initial state or the simulation parameters can lead to large changes in the final state.
__label__machine_vision In this study, we aim to propose a strong vision-language connector that enables MLLM to simultaneously achieve high accuracy and low computation cost.
__label__machine_vision During training, our DoGaussian maintains one global 3DGS model on the master node and $K$ local 3DGS models on the slave nodes.
"__label__natural_language_processing A prime example is the recently debated ""reversal curse"", which surfaces when models, having been trained on the fact ""A is B"", struggle to generalize this knowledge to infer that ""B is A""."
__label__machine_vision To address these limitations, we propose a comprehensive design framework that includes specific, effective strategies like implementing soft category-aware matching and adjusting the learning rate schedule.
__label__reinforcement_learning In sequential decision-making problems, the *information structure* describes the causal dependencies between system variables, encompassing the dynamics of the environment and the agents' actions.
__label__machine_vision Our code, models, and data are available at: tripletclip.github.io.
__label__machine_vision Experimental results show that MPA achieves performance comparable to state-of-the-art methods in both task-specific and multi-objective optimization across human viewing and machine analysis tasks.
__label__machine_vision Due to their data scarcity, training large-scale RS models from scratch is unrealistic, and the alternative is to transfer pre-trained models by fine-tuning or a more data-efficient method LoRA.
__label__infrastructure However, the computational and memory demands of conventional attention mask computations, typically scaling with an $\mathcal{O}(N^2)$ complexity where $N$ is the sequence length, pose significant challenges.
__label__natural_language_processing Our work highlights the importance of jointly considering tokenization and model scaling for efficient pre-training.
__label__privacy Second, Poisson subsampling is sometimes assumed to have similar privacy guarantees compared to sampling without replacement.
__label__neuroscience_and_cognitive_science Our second result utilizes this link to derive analytical PID terms for two more classes of distributions: convolution-closed distributions and a sub-class of the exponential family.
__label__online_learning Our approach is based on an adaptive Online Mirror Descent (OMD) algorithm that applies OMD locally to each information set, using individually decreasing learning rates and a *regularized loss*.
__label__other Based on this, CoLA conducts two collaboration strategies for devices with different computational resources and latency demands.
__label__machine_vision Central to this design is an efficient transformation between SDF-based implicit representations and explicit surface points via our proposed Surface Points Marching Cubes (SP-MC), enabling differentiable learning with both rendering and physical losses.
__label__machine_vision Anchoring is a recent, architecture-agnostic principle for training deep neural networks that has been shown to significantly improve uncertainty estimation, calibration, and extrapolation capabilities.
__label__causal_inference Two major difficulties stem from (i) an inherent uncertainty about the skeletons of the component DAGs that constitute the mixture and (ii) possibly cyclic relationships across these component DAGs.
__label__online_learning In the case that $V_T$ is unknown, we run multiple FTPL-D with different restarting parameters as experts and use a meta-algorithm to track the best one on the fly.
"__label__safety_in_machine_learning Given a DNN $\mathcal{N}$
with parameters $\theta$, input polytope $P$, and output
polytope $Q$, PREPARED finds new parameters $\theta'$ such that $\forall
\mathrm{x} \in P
    ."
__label__other However, this assumption is often violated in practice due to {\it a positive distribution shift}, where the negative-conditional density does not change but the positive-conditional density can vary.
__label__machine_learning_for_other_sciences_and_fields As our main result, we prove that inductive inference is possible if and only if the hypothesis class is a countable union of online learnable classes, potentially with an uncountable size, no matter the observations are adaptively chosen or iid sampled.
__label__machine_learning_for_healthcare Consequently, these methods often overlook the long-term relationships and regional motion characteristic of cardiac.
__label__graph_neural_networks Firstly, GAs designed for specific scenarios may compromise the universality of models if mishandled.
__label__active_learning Extensive experiments provide  qualitative and quantitative evidence of our method's superior efficacy, consistently outperforming the existing baselines.
__label__robotics We also demonstrate its effectiveness in generating robotic tasks.
__label__machine_learning_for_physical_sciences This versatility eliminates the need for patching and allows efficient processing of diverse geometries.
__label__reinforcement_learning The approach scales to challenging partially observable domains, where the resulting agent frequently performs significantly better (and never performs worse) than a baseline recurrent agent with only a single value network.
__label__interpretability_and_explainability In this paper, we propose a feature factorization activation map (FFAM) to generate high-quality visual explanations for 3D detectors.
__label__causal_inference In particular, this is achieved by distributional covariate balancing weights, which are model-free alternatives to IPW.
__label__neuroscience_and_cognitive_science We conclude that over-reliance on brain scores can lead to over-interpretation of similarity between LLMs and brains, and emphasize the importance of deconstructing what LLMs are mapping to in neural signals.
__label__diffusion_based_models Furthermore, to accommodate a variety of makeup styles, hierarchical texture details are decomposed via a Laplacian pyramid and selectively introduced to the content representation.
__label__causal_inference Remarkably, the size of the interventions is optimal if the underlying mixture model does not contain cycles across its components.
__label__diffusion_based_models Extensive quantitative and qualitative analyses demonstrate the effectiveness of our method.
__label__neuroscience_and_cognitive_science Our framework is, to our knowledge, the first enabling gradient-based training of SSNNs with noise affecting both the spike timing and the network's dynamics.
__label__probabilistic_methods Hierarchical clustering has usually been addressed by discrete optimization using heuristics or continuous optimization of relaxed scores for hierarchies.
__label__robotics To minimize the interpolation errors associated with direct coordinate transformations, we also devise a spherical-based feature encoding followed by spherical-to-Cartesian feature aggregation.
__label__causal_inference We construct and curate several synthetic and real-world benchmarks including analysis of human reviews and diagnosis of neuropathic and brain tumors, to comprehensively evaluate COAT.
__label__safety_in_machine_learning Large language models (LLMs) have advanced to encompass extensive knowledge across diverse domains.
__label__natural_language_processing Based on our findings, we design two training-free context window extension methods, positional vector replacement and attention window extension.
__label__reinforcement_learning We then address the challenge of a large domain parameter space by modeling domain calibration as a cooperative multi-agent reinforcement learning (MARL) problem.
__label__machine_vision In an offline setting, the context is typically captured by the segmentation network after observing the entire sequence.
__label__learning_theory It is a crucial tool for forecasting when a precise uncertainty quantification is required.
__label__graph_neural_networks This offers a new and fundamentally different pipeline for learning on very large non-sparse graphs, whose applicability is demonstrated empirically on node classification tasks and spatio-temporal data processing.
__label__causal_inference In the machine learning literature, significant efforts have been put into developing machinery to predict the effectiveness of policies efficiently.
__label__machine_learning_for_healthcare We apply our method to identify digital twins of cardiac hemodynamics using noninvasive echocardiogram videos, and demonstrate its utility in unsupervised disease detection and in-silico clinical trials.
__label__interpretability_and_explainability We therefore explore training amortized models with noisy labels, and we find that this is inexpensive and surprisingly effective.
__label__graph_neural_networks The expressive power of graph learning architectures based on the $k$-dimensional Weisfeiler-Leman ($k$-WL) hierarchy is well understood.
__label__machine_learning_for_other_sciences_and_fields While there are precise simulators based on ray tracing, they do not lend themselves to solving inverse problems or the integration in an automated design loop.
__label__learning_theory they are consistent).
__label__reinforcement_learning Through simulations in an option-trading domain, we validate that proper modeling of the superiority distribution produces improved controllers at high decision frequencies.
"__label__learning_theory This demonstrates that a a minimalistic architecture with a theoretically grounded loss can achieve 
competitive results,  even in the face of more complex architectures."
__label__neuroscience_and_cognitive_science We show that the 'Counter-Hebb' mechanism can provide an exact backpropagation synaptic modification.
__label__diffusion_based_models We validate the effectiveness of our method across various tasks, including TS forecasting, refinement, and generation, on datasets from diverse domains.
__label__deep_learning_architectures Our CompressTracker-4 with 4 transformer layers, which is compressed from OSTrack, retains about $\mathbf{96\%}$ performance on LaSOT ($\mathbf{66.1\%}$ AUC) while achieves $\mathbf{2.17\times}$ speed up.
__label__interpretability_and_explainability Despite diverse modeling techniques, existing models are black boxes and fail to provide insights and explanations about their representations.
__label__optimization In this paper, we consider making a batch of neural network outputs satisfy bounded and general linear constraints.
__label__machine_learning_for_other_sciences_and_fields Unlike natural language processing and computer vision, the development of Foundation Models (FMs) for time series forecasting is blocked due to data scarcity.
__label__generative_models We find that training Transformers with the logical sequence of steps is necessary and without such training, they fail to learn Sudoku.
__label__evaluation Thus, we argue that accuracy and perplexity are necessary but not sufficient for evaluating compressed models, since these metrics hide large underlying changes that have not been observed by previous work.
__label__machine_learning_for_healthcare It has inspired various applications in molecular property prediction and drug design.
__label__robotics One of the roadblocks for training generalist robotic models today is heterogeneity.
__label__machine_vision This work proposes a novel diffusion-based methodology to formulate the tracking task.
__label__robotics While Self-Instruct is a promising solution by generating a diverse set of training data, it cannot verify the correctness of these programs.
__label__graph_neural_networks In the literature, message-passing GNN (MP-GNN), as the simplest GNN structure, is frequently used as a fast approximation of SB and we find that not all MILPs's SB can be represented with MP-GNN.
__label__deep_learning_architectures For the latter, recent works reported Mamba’s ICL rivals SOTA Transformer LLMs using non-standard benchmarks.
