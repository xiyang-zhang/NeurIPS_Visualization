__label__machine_learning_for_physical_sciences Depending on the types of symmetries, we develop a pipeline for incorporating symmetry constraints into various equation discovery algorithms, including sparse regression and genetic programming.
__label__reinforcement_learning Unsupervised skill discovery is a learning paradigm that aims to acquire diverse behaviors without explicit rewards.
__label__machine_learning_for_healthcare We validate ProtoSurv across five different cancer types from TCGA (i.e., BRCA, LGG, LUAD, COAD and PAAD), and demonstrate the superiority of our method over the state-of-the-art methods.
__label__deep_learning_architectures We explore its convergence, conduct extensively experimental benchmarking, and provide consistent complexity evaluation by considering chip architecture, memory hierarchy, dataflow, and arithmetic precision.
__label__algorithmic_game_theory However, this introduces a challenge of a feasibility set with coupled constraints.
__label__diffusion_based_models Diffusion models (DMs) have become the dominant paradigm of generative modeling in a variety of domains by learning stochastic processes from noise to data.
__label__learning_theory Our investigation encompasses both synthetic and real-world datasets, such as MNIST and Olivetti Faces, demonstrating the robustness and applicability of our findings to practical scenarios.
__label__machine_vision We also present a baseline architecture, a simple yet effective approach tailored to estimate scene depth across a wide range of sensors and environments using minimal labeled data.
__label__reinforcement_learning In this paper, we address the challenges of offline reinforcement learning (RL) under model mismatch, where the agent aims to optimize its performance through an offline dataset that may not accurately represent the deployment environment.
__label__machine_vision Recently, the emergence of 3D Gaussian Splatting has provided new insights into this problem.
__label__generative_models However, during the continued training, the MLLM catastrophically forgets the text-only instructions that the initial LLM masters.
__label__reinforcement_learning Cooperation between self-interested individuals is a widespread phenomenon in the natural world, but remains elusive in interactions between artificially intelligent agents.
__label__neuroscience_and_cognitive_science Training our autoencoder to accurately pattern-complete and reconstruct sensory experiences with a constraint on total activity causes spatially localized firing fields, i.e., place cells, to emerge in the encoding layer.
__label__diffusion_based_models However, the theoretical properties of these heuristic samplers remain unknown and they often exhibit mediocre sampling quality.
__label__generative_models Experiments on multiple tasks demonstrate the effectiveness of ARE in various model editing scenarios.
__label__machine_learning_for_physical_sciences The two loss terms work together to replace the traditional L2 losses such as MSE and weighted MSE for the spatiotemporal prediction problem on signal-based data.
__label__safety_in_machine_learning It is a fast, straightforward, and effective approach that reaches near-optimal worst group accuracy without needing group annotations, marking a new chapter in the robustness of trained models against spurious correlation.
__label__deep_learning_architectures We show that SNE outperforms the relevant baselines on standard benchmarks.
__label__other Tools like Persistent Homology or the Euler Transform give a single complex description of the global structure of the point cloud.
__label__natural_language_processing Despite the impressive accomplishments, large language models (LLMs), even with retrieval-augmented generation (RAG), still struggle to efficiently and effectively integrate a large amount of new experiences after pre-training.
__label__evaluation We propose SpatialEval, a novel benchmark that covers diverse aspects of spatial reasoning such as relationship understanding, navigation, and counting.
__label__safety_in_machine_learning We explore the effects of representational alignment between humans and AI agents on learning human values.
__label__neuroscience_and_cognitive_science Overall, this work suggests that PC inference makes the loss landscape more benign and robust to vanishing gradients, while also highlighting the fundamental challenge of scaling PC to deeper models.
__label__machine_learning_for_other_sciences_and_fields Snapshot compressive imaging (SCI) recovers high-dimensional (3D) data cubes from a single 2D measurement, enabling diverse applications like video and hyperspectral imaging to go beyond standard techniques in terms of acquisition speed and efficiency.
__label__diffusion_based_models Specifically, we propose a novel approach called PDM for Personalized Diffusion Features Matching, that leverages intermediate features of pre-trained text-to-image models for personalization tasks without any additional training.
__label__probabilistic_methods Conditional density estimation (CDE) goes beyond regression by modeling the full conditional distribution, providing a richer understanding of the data than just the conditional mean in regression.
__label__optimization_for_deep_networks Our method employs sensitivity-based analysis of low-rank matrix parameters to identify knowledge-specific parameters between sequential tasks, which are used to initialize the low-rank matrix parameters in new tasks.
__label__reinforcement_learning During decentralized executions, MADiff simultaneously performs teammate modeling, and the centralized controller can also be applied in multi-agent trajectory predictions.
__label__machine_vision Modality differences have led to the development of heterogeneous architectures for vision and language models.
__label__interpretability_and_explainability We track both the emergence and the precise working of these iteration heads down to the attention level, and measure the transferability of the CoT skills to which they give rise between tasks.
__label__machine_vision MM-Det utilizes the profound perceptual and comprehensive abilities of Large Multi-modal Models (LMMs) by generating a Multi-Modal Forgery Representation (MMFR) from LMM's multi-modal space, enhancing its ability to detect unseen forgery content.
__label__safety_in_machine_learning In the ordinal feedback setting, we show the necessity of both high- and low-level feedback, and develop a hierarchical experimental design algorithm that efficiently acquires both types of feedback for learning.
__label__machine_vision We introduce FlexCap, a vision-language model that generates region-specific descriptions of varying lengths.
__label__learning_theory We establish a necessary condition for SGD-learnability, involving both the characteristics of the target function and the expressiveness of the activation function.
__label__generative_models For these three particular instances, we show that the jump rates and kernels of the corresponding time reversals admit explicit expressions depending on some conditional densities of the PDMP under consideration before and after a jump.
__label__deep_learning_architectures Due to their high inference costs, there is renewed interest in exploring the potential of efficient recurrent architectures (RNNs).
__label__diffusion_based_models Due to semantic sharing, when a smaller patch is jointly denoised alongside a generatable-size image, the two become semantic clones.
__label__optimization_for_deep_networks Therefore,  $S^{2} - SAM$ not only exhibits the capacity to improve generalization but also aligns with the efficiency goal of sparse training.
__label__machine_learning_for_physical_sciences The SEA integrated transformer demonstrates the state-of-the-art rollout error compared to other competitive baselines.
__label__machine_learning_for_physical_sciences Our code is available at \url{https://github.com/lamda-bbo/macro-regulator}.
__label__deep_learning_architectures We find that none of the current linear models meet all three conditions, resulting in suboptimal performance.
__label__graph_neural_networks However, many important sources of data, such as biological and social networks, are naturally structured as graphs rather than arrays, making the design of graph neural network (GNN) architectures that retain the strengths of CNNs an active and exciting area of research.
__label__robotics Existing zero-shot object navigation methods prompt LLM with the text of spatially closed objects, which lacks enough scene context for in-depth reasoning.
__label__learning_theory Despite efficiency improvements, existing methods overlook the misguidance in anchors learning induced by partial missing samples, i.e., the absence of samples results in shift of learned anchors, further leading to sub-optimal clustering performance.
__label__machine_learning_for_physical_sciences Code is available at https://github.com/OpenEarthLab/FNP.
__label__machine_vision In the past two decades, many AUC optimization methods have been proposed to improve model performance under long-tail distributions.
__label__algorithmic_game_theory The problem of learning a reward function is one of preference aggregation that, we argue, largely falls within the scope of social choice theory.
__label__diffusion_based_models The recent progress in text-to-image models pretrained on large-scale datasets has enabled us to generate various images as long as we provide a text prompt describing what we want.
__label__neuroscience_and_cognitive_science Presenting these models with the same text as the participants allows to identify brain areas where there is a significant correlation between the functional magnetic resonance imaging (fMRI) time series and the ones predicted by the models' artificial neurons.
__label__reinforcement_learning Feint behaviors refer to a set of deceptive behaviors in a nuanced manner, which enable players to obtain temporal and spatial advantages over opponents in competitive games.
__label__machine_vision Procedural activities are sequences of key-steps aimed at achieving specific goals.
__label__machine_learning_for_healthcare Early MIL methods treated the instances in a bag independently.
__label__privacy We will release the code and benchmark in the near future.
__label__optimization_for_deep_networks Improving model generalization ability within each learning phase is one solution to help CL learning overcome the gap in the joint knowledge space.
__label__generative_models While state-space networks, including Mamba, a revolutionary advancement in recurrent neural networks, typically scan input sequences from left to right, they face difficulties in designing effective scanning strategies, especially in the processing of image data.
__label__machine_vision The first stage of our approach ensures data consistency by focusing on accurately reconstructing the low-frequency content with a spatially varying deconvolution method that adjusts to changes in the Point Spread Function (PSF) across the camera's field of view.
__label__natural_language_processing Then it is presented with randomly selected exemplar solved questions associated with that skill label.
"__label__infrastructure Inference on large language models (LLMs) can be expensive in terms of the
compute and memory costs involved, especially when long sequence lengths are
used."
__label__interpretability_and_explainability On the other hand, token frequency neurons, which we discover and describe here for the first time, boost or suppress each token’s logit proportionally to its log frequency, thereby shifting the output distribution towards or away from the unigram distribution.
__label__interpretability_and_explainability Taking the viewpoint of recent works showing that transformers learn in context by formulating an internal optimizer, we propose an influence function-based attribution technique, DETAIL, that addresses the specific characteristics of ICL.
__label__online_learning Finally, we continually refine the 3DGS with additional Gaussians, which are later merged into the initial 3DGS to maintain consistency with the evolving scene.
__label__neuroscience_and_cognitive_science The results showed the label can be successfully decoded as long as continuous EEG data with the same label were split into training and test sets.
"__label__machine_learning_for_physical_sciences Importantly, we show that even with datasets containing only a few relevant transitions due to sub-optimal biasing, our approach
recovers relevant information about the transition mechanism."
__label__infrastructure To address this, we introduce a family of memory efficient building blocks with controllable activation memory, which can reduce the peak activation memory to 1/2 of 1F1B without sacrificing efficiency, and even to 1/3 with comparable throughput.
__label__natural_language_processing Meanwhile, this robust pattern can be obtained from an `observation' window located at the end of the prompts.
__label__optimization_for_deep_networks Our empirical study further validate the theoretical findings.
__label__optimization_for_deep_networks We open-source our implementation of Sirius at https://github.com/Infini-AI-Lab/Sirius.git.
__label__deep_learning_architectures However, existing works have learned features that are implicitly aligned from the data, without considering the explicit relationships in the medical context.
__label__diffusion_based_models (not identically and independently distributed) data and ensuring model consistency across heterogeneous environments present significant challenges.
__label__causal_inference To address this problem, most existing methods attempt to find proper valid instrumental variables (IVs) for the target causal effect by expert knowledge or by assuming that the causal model is a one-directional MR model.
__label__natural_language_processing Direct Preference Optimization (DPO) has emerged as a compelling approach for training Large Language Models (LLMs) to adhere to human preferences.
__label__safety_in_machine_learning For training from scratch, \textbf{RAMP} achieves a union accuracy of $44.6\%$ and good clean accuracy of $81.2\%$ on ResNet-18 against AutoAttack on CIFAR-10.
__label__natural_language_processing Additionally, retaining the $\operatorname{softmax}$ operation is particularly beneficial in ``finetuning pretrained Transformers to RNNs'' (T2R) settings, reducing the need for extensive training from scratch.
__label__machine_vision To take advantage of multi-task unification, we enhance performance by leveraging inter-task connections.
__label__evaluation Qualitative evaluations on general-domain and medical-domain VLMs confirm our findings.
__label__neuroscience_and_cognitive_science Our method directly converts the well-trained transformer without modifying its attention architecture.
__label__machine_learning_for_other_sciences_and_fields We demonstrate the utility of $\texttt{DESP}$ in improving solve rates and reducing the number of search expansions by biasing synthesis planning towards expert goals on multiple new benchmarks.
__label__natural_language_processing These heads display sparse characteristics that allow for near-independent control over different tasks.
__label__interpretability_and_explainability Leveraging the theory of optimal prediction, we anticipate and then find that belief states are linearly represented in the residual stream of transformers, even in cases where the predicted belief state geometry has highly nontrivial fractal structure.
__label__optimization_for_deep_networks This limitation could be ameliorated if we use coordinate-wise learning rates, as designed in Adam.
__label__optimization_for_deep_networks To address these challenges, we introduce the Block-Level Adaptive STructured (BLAST) matrix, designed to learn and leverage efficient structures prevalent in the weight matrices of linear layers within deep learning models.
__label__optimization By synthesizing these results, we outline the first global complexity characterization of BFGS with the Armijo-Wolfe line search.
__label__graph_neural_networks We conduct thorough theoretical analyses of the stability and over-smoothing properties of CITRUS in response to domain-specific graph perturbations and graph spectra effects on the performance.
__label__deep_learning_architectures To respect symmetries inherent in network weight space, we utilize Logit Invariance to learn the required minimal invariance properties.
__label__reinforcement_learning Then, it is distilled into a vision-language-action model via behavior cloning.
__label__machine_learning_for_healthcare The project code is available at https://github.com/mrirecon/aid.
__label__diffusion_based_models One of the main drawback of diffusion models is the slow inference time for image generation.
__label__other Empirical evaluations performed on real-world datasets corroborate our theoretical results.
__label__deep_learning_architectures However,  constraints in either data and/or computational resources among participating clients introduce several challenges in learning, including the inability to train large model architectures, heightened risks of overfitting, and more.
__label__optimization We propose a new simple model, JKOnet*, which bypasses the complexity of existing architectures while presenting significantly enhanced representational capabilities: JKOnet* recovers the potential, interaction, and internal energy components of the underlying diffusion process.
__label__interpretability_and_explainability Unlike previous approaches that model the concept relations via an autoregressive structure, we introduce an explicit, distributional parameterization that allows SCBMs to retain the CBMs' efficient training and inference procedure.
__label__natural_language_processing Theoretically, we prove that Panacea recovers the entire Pareto front with common loss aggregation methods under mild conditions.
__label__causal_inference Despite the multitude of identifiability results under various interventional CRL settings, the existing guarantees apply exclusively to the _infinite-sample_ regime (i.e., infinite observed samples).
__label__optimization_for_deep_networks Our findings reveal that the dynamics of standard SAM effectively reduce to applying SAM solely in the last layer in wide neural networks, even with optimal hyperparameters.
__label__safety_in_machine_learning Extensive experiments demonstrate that PamaCF effectively defends against various types of poisoning attacks while significantly enhancing recommendation performance.
__label__generative_models The current state of the art hairstyle transfer methods use an optimization process for different parts of the approach, making them inexcusably slow.
__label__infrastructure elaborated an approach to speed up attention on GPUs through minimizing memory reads/writes.
__label__probabilistic_methods (2022) introduced a simple and inexpensive method for training CNFs in generative modeling, termed flow matching (FM).
"__label__reinforcement_learning RL-AR performs policy combination via a ""focus module,"" which determines the appropriate combination depending on the state—relying more on the safe policy regularizer for less-exploited states while allowing unbiased convergence for well-exploited states."
__label__machine_vision This performance surpasses previous video-text pre-training approaches and proves competitive with recent GPT-3.5 based video agents.
__label__learning_theory Despite this loss of structure, we significantly strengthen the results of Geshkovski et al in this context: While previous rigorous results focused on cases where all three matrices (key, query, and value) were scaled identities, we prove asymptotic convergence to a single cluster for arbitrary key-query matrices and value matrix equal to the identity.
__label__fairness Previous works have proposed methods that guarantee CF.
__label__optimization This allows us to derive exact expressions for the optimal regularizer in certain cases.
__label__online_learning Continual learning of partially similar tasks poses a challenge for artificial neural networks, as task similarity presents both an opportunity for knowledge transfer and a risk of interference and catastrophic forgetting.
__label__machine_learning_for_other_sciences_and_fields To address these issues, we propose a framework that automates the protocol translation process through a three-stage workflow, which incrementally constructs Protocol Dependence Graphs (PDGs) that approach structured on the syntax level, completed on the semantics level, and linked on the execution level.
__label__diffusion_based_models Theoretically, we show that minimizing this loss is equivalent to maximizing the lower bound of an adjusted conditional log-likelihood over ID-preserving data.
__label__reinforcement_learning We study generative modeling for planning with datasets repurposed from offline reinforcement learning.
__label__learning_theory In this paper, we take a step in this direction by providing a tight theoretical analysis of the emergence of semantic attention in a solvable model of dot-product attention.
__label__machine_vision Our model can also operate as a strong relighting prior for 3D tasks, such as relighting a radiance field.
__label__privacy The project is open in https://github.com/XeniaLLL/FOOGD-main.git.
__label__machine_learning_for_healthcare Extensive experiments show the consistent improvement on the downstream tasks as the model size grows up.
__label__machine_learning_for_physical_sciences Deep learning approaches have been widely adopted for precipitation nowcasting in recent years.
__label__reinforcement_learning In this study, we aim to first obtain a clear understanding of the generalization capability of world models by examining the impact of _latent representation error_, and then devise new methods to enhance its generalization.
__label__other We further explored the scaling behavior of SuperClass on model size, training length, or data size, and reported encouraging results and comparisons to CLIP.
__label__machine_learning_for_other_sciences_and_fields In addition to being intractable with existing approaches to semantic routing, our benchmark poses a significant scaling challenge for graph learning methods.
__label__diffusion_based_models However, most previous numerical and machine learning approaches that target forecasting cannot be applied out-of-the-box for data assimilation.
__label__diffusion_based_models This design offers two significant advantages: first, although the total model size is increased, the model produced by the mixing operation shares the same architecture as a plain model, making the overall model as efficient as a standard diffusion transformer.
__label__infrastructure To address this, we propose ArkVale, a page-based KV cache manager that can recognize and recall currently important tokens evicted before.
__label__other Our method attains state-of-the-art relighting quality after only ${\sim}1$ hour of training in a single NVIDIA A100 GPU.
__label__online_learning classification tasks.
__label__other We leverage this unique capability to propose NoMAD-Attention, an efficient attention algorithm that replaces MAD operations with in-register lookups.
__label__safety_in_machine_learning However, most MIA methods perform no better than random guessing when discriminating between members and non-members from the same distribution (e.g., in this case, the same period of time).
__label__safety_in_machine_learning Our theoretical analysis highlights the connection between the provided score and the model's uncertainty.
__label__machine_learning_for_social_sciences In these models, a central object of interest is the discrete origin-destination matrix which captures spatial interactions and agent trip counts between locations.
__label__diffusion_based_models We propose a novel method, \textbf{TwinAct}, to tackle the challenge of decoupling actions and actors in order to customize the text-guided diffusion models (TGDMs) for few-shot action image generation.
__label__generative_models Most prior approaches rely on multiple images or video diffusion models, utilizing score distillation sampling for optimization or generating pseudo novel views for direct supervision.
__label__causal_inference We additionally present augmented versions of TSCI that leverage the expressive power of latent variable models and deep learning.
__label__reinforcement_learning In this paper, we propose a novel adaptive preference loss, underpinned by distributionally robust optimization (DRO), designed to address this uncertainty in preference strength.
__label__speech_and_audio To address these challenges, we propose a novel approach named ContAV-Sep ($\textbf{Cont}$inual $\textbf{A}$udio-$\textbf{V}$isual Sound $\textbf{Sep}$aration).
__label__learning_theory A rigorous analysis of our transferable federated algorithm, termed FedGTST (Federated Global Transferability via Statistics Tuning), reveals that increasing the averaged Jacobian norm across clients and reducing its variance ensures tight control of the target loss.
__label__optimization_for_deep_networks Fine-tuning large-scale pretrained models is prohibitively expensive in terms of computational and memory costs.
__label__natural_language_processing Empirically, we align 1B and 7B models using proposed methods and evaluate them on a reward benchmark model and the HuggingFace Open LLM Leaderboard.
"__label__probabilistic_methods With the exception of spectral clustering methods, there is little theoretical understanding
for commonly used approaches to learning embeddings."
__label__generative_models Motivated by this, we explore the question of whether LLMs can be empowered to think (predict and score) backwards to provide unsupervised feedback that complements forward LLMs.
__label__machine_vision Finally, we employ an \textit{Auxiliary Quadruple Adversarial Loss} to amplify the differences between modalities, thereby improving the distinction and recognition of features between visible and infrared images, which causes the system to output incorrect rankings.
__label__machine_vision These approaches prioritize a well-designed training schedule over traditional methods that focus primarily on data augmentation and the enhancement of discriminative feature learning.
__label__machine_learning_for_healthcare Employing a novel SkeletonDijkstra algorithm, the Morph Module produces a centerline mask that aligns with the predicted graph.
__label__causal_inference After this powerful result, a key open problem faced by the community has been to relax these conditions: allow for coarser than perfect single-node interventions, and allow for fewer than $d$ of them, since the number of latent factors $d$ could be very large.
__label__optimization_for_deep_networks We introduce a novel Gaussian neighborhood loss, which provides a tight upper bound on the loss function of data distribution, facilitating a flattened loss landscape correlated to improved model generalization.
__label__natural_language_processing At the same time, it maintains comparable performance to baseline models across 16 long sequence datasets.
__label__optimization We provide a comprehensive convergence analysis for both algorithms under full and partial block participation, and show that their sample complexities match or outperform those of the same type of methods in standard bilevel optimization.
__label__safety_in_machine_learning As an alternative to refusal training and adversarial training, circuit-breaking directly controls the representations that are responsible for harmful outputs in the first place.
__label__safety_in_machine_learning In this work, we analyze the privacy protection and performance of the four most recent methods for private adaptation of closed LLMs.
__label__learning_theory Under our framework, we first show that pretrained LLMs will mimic harmful behaviour if present in the training corpus.
__label__diffusion_based_models We propose a novel inference technique based on a pretrained diffusion model for text-conditional video generation.
__label__deep_learning_architectures MEGALODON reaches a training loss of 1.70, landing mid-way between LLAMA2-7B (1.75) and LLAMA2-13B (1.67).
__label__online_learning Furthermore, we enhance the expert-loss to exploit the smoothness property, and demonstrate that our algorithm can attain small-loss regret for multiple types of convex and smooth functions.
__label__machine_learning_for_physical_sciences We explore the convergence guarantees of such methods in both linear and nonlinear cases, addressing challenges such as spectral bias and slow convergence.
__label__natural_language_processing The key idea of the proposed algorithm called aespa is to perform quantization layer-wise for efficiency while targeting attention-wise reconstruction to consider the cross-layer dependency.
__label__interpretability_and_explainability In this work, we address this question by investigating the relationship between the amount of noise and model simplicity across various hypothesis spaces, focusing on decision trees and linear models.
__label__learning_theory Specifically, our result implies the optimality in the minimax sense of many of the most-frequently used estimators (including the U-statistic, the V-statistic, and the Nyström-based one) on $\mathbb{R}^d$.
__label__machine_vision Comprehensive experiments on six diverse anomaly detection datasets and seven metrics demonstrate state-of-the-art performance, substantiating the method's effectiveness.
__label__reinforcement_learning In the fine-tuning stage, we harness the imagined future videos to guide low-level action learning with a limited set of robot data.
__label__online_learning Experimental results confirm that SER effectively enhances the performance (in some cases up to about twenty percent points) of state-of-the-art continual learning methods, both in class-incremental and task-incremental settings.
__label__machine_vision In this work, we propose a graph model for video salient object ranking.
__label__machine_vision We propose a one-prompt Meta-learning framework for Universal Anomaly Segmentation (MetaUAS) that is trained on this synthetic dataset and then generalizes well to segment any novel or unseen visual anomalies in the real world.
__label__diffusion_based_models Diffusion models have demonstrated empirical successes in various applications and can be adapted to task-specific needs via guidance.
__label__deep_learning_architectures Furthermore, we prove that targeting key memory buffers in Mamba’s customized CUDA kernels for low-rank adaptation regularizes SSM parameters, thus achieving parameter efficiency while retaining speedups.
__label__reinforcement_learning Finding the optimal elimination order that minimizes the number of necessary multiplications can be seen as a single player game which in our case is played by an RL agent.
__label__reinforcement_learning The source code can be accessed directly with this link: https://github.com/morning9393/ADRL.
__label__diffusion_based_models However, a significant flaw in these models is evident: they struggle to locate a desired instance when other instances within the same class are presented.
__label__generative_models We evaluate its effectiveness across a wide range of cases, including standard autoregressive models and generalized masked autoregressive (MAR) variants.
__label__machine_learning_for_healthcare This investigation underscores the efficacy of incorporating 2D convolutions within the framework of 3D CNNs to overcome the intrinsic limitations of volumetric segmentation, thereby potentially expanding the frontiers of medical image analysis.
__label__causal_inference Notably, it can seamlessly incorporate expert knowledge as constraints within the optimization process, which enhances the interpretability of the outcomes.
__label__natural_language_processing Towards this problem, our initial attempt is to relabel the data with long captions, however, directly learning with which may lead to performance degradation in understanding short text (e.g., in the image classification task).
__label__machine_vision To that end, knowledge dissemination, separation, and distillation are carried out successively.
__label__generative_models The resulting algorithm, which we coin as RefDrop, allows users to control the influence of reference context in a direct and precise manner.
__label__learning_theory setting, where the training data is sampled from a stationary mixing process.
__label__neuroscience_and_cognitive_science We apply the algorithm to novel recordings from body-selective regions in macaque IT cortex in order to understand why some images of objects excite these neurons.
__label__machine_learning_for_other_sciences_and_fields Experiments on real-wold dataset show that LM-Weather outperforms the state-of-the-art results by a large margin across various tasks (e.g., forecasting and imputation at different scales).
__label__causal_inference In this work, we consider precisely such a setting, where we allow a smaller than $d$ number of environments, and also allow for very coarse interventions that can very coarsely \textit{change the entire causal graph over the latent factors}.
__label__safety_in_machine_learning We demonstrate this threat using a three-staged attack framework: (i) first, we obtain a malicious LLM through fine-tuning on an adversarial task; (ii) next, we quantize the malicious model and calculate constraints that characterize all full-precision models that map to the same quantized model; (iii) finally, using projected gradient descent, we tune out the poisoned behavior from the full-precision model while ensuring that its weights satisfy the constraints computed in step (ii).
__label__reinforcement_learning The proposed approach demonstrates state-of-the-art performance across a range of tasks in the D4RL benchmarks, significantly improving upon existing diffusion-based policies.
__label__deep_learning_architectures We showcase the advantage of CosAE via extensive experiments on flexible-resolution super-resolution and blind image restoration, two highly challenging tasks that demand the restoration network to effectively generalize to complex and even unknown image degradations.
__label__learning_theory The first algorithm explores the geometric features of the problem and enables us to provide Lipschitz estimates that are comparable to existing methods by solving small semidefinite programs (SDPs) that are only as large as the size of each layer.
__label__reinforcement_learning Due to the exponential growth of agent interactions and the curse of dimensionality, learning efficient coordination from scratch is inherently challenging in large-scale multi-agent systems.
__label__machine_learning_for_other_sciences_and_fields Due to the abundant seasonal information they contain, timestamps possess the potential to offer robust global guidance for forecasting techniques.
__label__machine_vision Specifically, we incorporate a pre-trained VLM to extract textual features from texts generated by VLM and augmented by LLM, and incrementally fine-tune the text encoder to minimize the domain gap between generated texts and original visual modalities.
__label__machine_vision We propose VLM Selection With gAp Bridging (SWAB) to mitigate the negative impact of two gaps.
__label__online_learning The proposed algorithm is proven to achieve strongly adaptive regret over all intervals while maintaining valid coverage.
__label__generative_models To address these issues, we develop and theoretically justify the novel Optimal Flow Matching approach which allows recovering the straight OT displacement for the quadratic transport in just one FM step.
__label__machine_vision All instantiations of this paradigm were trained using strong and well-established hand-crafted data augmentations, leading to the general belief that they are required for the proper training and performance of such models.
__label__machine_vision However, we observe that extracting only the surface information of the extrudes and utilizing it results in suboptimal outcomes due to the challenges in the occlusion and surface segmentation.
__label__causal_inference This is an important problem when there is the possibility of a shift between historical and future environments, \emph{e.g.}
__label__machine_vision This paper proposes OMG-LLaVA, a new and elegant framework combining powerful pixel-level vision understanding with reasoning abilities.
__label__evaluation Nevertheless, these models are susceptible to adversarial examples.
"__label__probabilistic_methods In this
paper, we propose a novel method for SIVI called Particle Variational Inference
(PVI) which employs empirical measures to approximate the optimal mixing
distributions characterized as the minimizer of a free energy functional."
__label__optimization_for_deep_networks Neural Network Pruning has been established as driving force in the exploration of memory and energy efficient solutions with high throughput both during training and at test time.
__label__machine_learning_for_other_sciences_and_fields However, existing models primarily focus on learning the chemical distribution of all drug candidates, which lacks effective steerability on the chemical quality of model generations.
__label__natural_language_processing Notably, our experiments demonstrate that BAL-PM requires 33\% to 68\% fewer preference labels in two popular human preference datasets and exceeds previous stochastic Bayesian acquisition policies.
__label__neuroscience_and_cognitive_science By leveraging the inherent benefits of rate-coding, this work sets the stage for more scalable and efficient SNNs training within resource-constrained environments.
__label__optimization_for_deep_networks This strategy seeks to amplify the contrast between hallucinatory and targeted tokens relying on a fine-tuned theoretical preference model (i.e., Contrary Bradley-Terry Model), thereby facilitating efficient contrast decoding to alleviate hallucinations in LVLMs.
__label__machine_learning_for_physical_sciences In this work, we propose DiffusionPDE that can simultaneously fill in the missing information and solve a PDE by modeling the joint distribution of the solution and coefficient spaces.
__label__graph_neural_networks Graph self-supervised learning, as a powerful pre-training paradigm for Graph Neural Networks (GNNs) without labels, has received considerable attention.
__label__algorithmic_game_theory Motivated by the success of Nesterov's accelerated gradient algorithm for convex minimization problems, we examine whether it is possible to achieve similar performance gains in the context of online learning in games.
__label__other This paper points out two issues of the state-of-the-art GR models.
__label__fairness Our results display that in many cases, the balanced distribution does not correspond to selectively removing the undesired dependencies in a causal graph of the task, leading to multiple failure modes and even interference with other mitigation techniques such as regularization.
__label__reinforcement_learning To evaluate performance, we create a new environment, Education Dialogue, where a teacher agent  guides  a  student  in  learning  a  random  topic,  and  show  that  a  deep  RL variant of our algorithm outperforms RLHF baselines.
"__label__other Our analysis
determines how much we should scale down the level of collaboration, according
to data heterogeneity and the tolerable fraction of adversarial clients."
__label__optimization Our findings indicate the significance of accounting for architecture-specific invariances in achieving LMC.
__label__interpretability_and_explainability We show that it is possible to manipulate the internal model representations as well as edit model weights based on the mechanism we discover in order to significantly improve performance on our synthetic Laundry List task, which requires recall from a list, often improving task accuracy by over 20\%.
__label__optimization_for_deep_networks We discover a trade-off between sharpness and diversity: minimizing the sharpness in the loss landscape tends to diminish the diversity of individual members within the ensemble, adversely affecting the ensemble's improvement.
__label__machine_vision However, the learning pipeline in 3DGS inherently lacks the ability to quantify uncertainty, which is an important factor in applications like robotics mapping and navigation.
__label__reinforcement_learning The first term can be obtained from a diffusion model trained on the dataset and we propose an in-sample learning objective to learn the second term.
__label__causal_inference This paper focuses on _general_ latent causal models, stochastic _soft_ interventions, and a linear transformation from the latent to the observation space.
__label__natural_language_processing To ensure the reliability, a large number of critiques are annotated to serve as references, enabling GPT-4 to evaluate textual critiques reliably.
__label__probabilistic_methods In this paper, we show how these questions can have compact, closed form solutions in terms of learned representations.
__label__fairness Furthermore, while current methodologies are generally effective when the source model is identified, they falter in scenarios where the model version remains unknown, or the test set comprises outputs from various source models.
__label__safety_in_machine_learning Our results on open source mainstream LLMs (e.g., Llama2, Opt, Vicuna) demonstrate that Vaccine can boost the robustness of alignment against harmful prompts induced embedding drift while reserving reasoning ability towards benign prompts.
__label__optimization In this paper, we propose a novel SNN formulation for HMVC.
__label__deep_learning_architectures However, these methods typically exhibit a performance gap compared to full fine-tuning.
__label__diffusion_based_models Our PyTorch implementation is available at https://github.com/cxy1997/DiffuBox.
__label__machine_vision We initially train Ex4DGS using short timestamps and progressively extend timestamps, which makes it work well with a few point clouds.
__label__machine_learning_for_physical_sciences At the core of 3D PTV is the dual-frame fluid motion estimation algorithm, which tracks particles across two consecutive frames.
__label__reinforcement_learning Building on this key idea, SACPO aligns LLMs step-wise with each metric while leveraging simple yet powerful alignment algorithms such as direct preference optimization (DPO).
__label__graph_neural_networks Experimental results demonstrate that our model-agnostic framework more effectively captures topological graph knowledge, resulting in superior performance of the student models when compared to traditional KD methodologies.
__label__neuroscience_and_cognitive_science Brain stimulation has the potential to create desired neural population activity states.
__label__optimization_for_deep_networks This framework includes a lightweight-design diffusion model architecture, and a training-free Attention Modulation Matrix and its alternation arrangement in EDT inspired by human-like sketching.
__label__fairness Finally, we conduct experiments to evaluate the consistency of our theoretically derived egalitarian fairness bounds with the empirically achieved egalitarian fairness in fair FL settings.
__label__other Subsequently, VAP formulates a chain of thought in both modalities and iteratively refines the synthesized image.
__label__machine_vision RPN attains state-of-the-art performance with merely about ${\bf 3M}$ trainable parameters (2\% of total parameters).
__label__graph_neural_networks To address this, we present UniGAD, the first unified framework for detecting anomalies at node, edge, and graph levels jointly.
__label__diffusion_based_models (3) Teacher: Step-distilled Teacher allows T2I models to reduce the noising steps.
__label__learning_theory This in particular implies that the realizable boosting methodology has the potential to offer computational relief without compromising on sample efficiency.
__label__learning_theory One may instead augment a small set of $n$ data points from the target distribution with data from more accessible sources, e.g.
__label__neuroscience_and_cognitive_science Subsequently, we propose a robust SNN (FEEL-SNN) with Frequency Encoding (FE) and Evolutionary Leak factor (EL) to defend against different noises, mimicking the selective visual attention mechanism and non-fixed leak observed in biological systems.
__label__interpretability_and_explainability Is it possible to conduct dynamic image fusion with a clear theoretical justification?
__label__natural_language_processing While a long line of membership inference attacks aim to identify training examples on an instance level, they do not extend easily to *global* statistics about the corpus.
__label__robotics This paper addresses the problem of on-road object importance estimation, which utilizes video sequences captured from the driver's perspective as the input.
__label__neuroscience_and_cognitive_science We demonstrate that LDMs with redundancy reduction and prototype-based regularizations produce near-human-like drawings (regarding both samples' recognizability and originality) -- better mimicking human perception (as evaluated psychophysically).
__label__machine_learning_for_other_sciences_and_fields However, conventional Quantization-aware training (QAT) focuses on training DNNs with uniform bit-width.
__label__learning_theory We include a colab notebook https://tinyurl.com/2saj6bkj, nanoChinchilla, that reproduces some key results of the paper.
__label__natural_language_processing Our code is available at https://github.com/ZhengxiangShi/InstructionModelling.
__label__neuroscience_and_cognitive_science To validate SpGesture's performance, we collected a new sEMG gesture dataset which has different forearm postures, where SpGesture achieved the highest accuracy among the baselines ($89.26\%$).
__label__deep_learning_architectures Additionally, it employs multi-level modeling to capture channel correlations and infuse exogenous signals during fine-tuning.
__label__diffusion_based_models We use this formulation to develop CatFlow, a flow matching method for categorical data that is easy to implement, computationally efficient, and achieves strong results on graph generation tasks.
__label__machine_vision We validate our design choices through exhaustive ablations and observe improved performance of the resulting long-video (128 frames) encoders over short-video (32 frames) counterparts.
__label__safety_in_machine_learning We introduce Shadowcast, a stealthy data poisoning attack where poison samples are visually indistinguishable from benign images with matching texts.
"__label__safety_in_machine_learning \mathcal{N}(\mathrm{x}; \theta') \in Q$
while minimizing the changes $\lVert{\theta' - \theta}\rVert$."
__label__machine_learning_for_healthcare FuncMol performs all-atom generation of 3D molecules without assumptions on the molecular structure and scales well with the size of molecules, unlike most existing approaches.
__label__machine_learning_for_physical_sciences Higher depth increases expressivity, but also results in a detrimental accumulation of errors.
__label__generative_models Code, model and data are available at https://github.com/2toinf/IVM.
__label__other We aim to tackle a subclass of these ill-posed settings, characterized by a flexible separable observation probability assumption that can depend on the matrix measurements.
__label__deep_learning_architectures Unlike preselected prior pdfs with fixed shapes, the advocated NCoV model can effectively approximate a considerably wide range of pdfs.
__label__machine_vision We further apply *DeepStack* to vision transformer layers, which brings us a similar amount of improvements, 3.8 on average compared with LLaVA-1.5-7B.
__label__neuroscience_and_cognitive_science Despite participants engaging in single modality stimuli, such as watching images or silent videos, recent work has demonstrated that multi-modal Transformer models can predict visual brain activity impressively well, even with incongruent modality representations.
__label__reinforcement_learning However, existing diffusion planning methods suffer from low decision-making frequencies due to the expensive iterative sampling cost.
__label__optimization In the adversarial streaming model, the input is a sequence of adaptive updates that defines an underlying dataset and the goal is to approximate, collect, or compute some statistic while using space sublinear in the size of the dataset.
__label__machine_learning_for_other_sciences_and_fields At a high level, inductive inference asks whether one can make at most finite errors amidst an infinite sequence of observations, when deducing the correct hypothesis from a given hypothesis class.
__label__deep_learning_architectures Contrary to LLMs, humans acquire skills and complete tasks not merely through repeated practice but also by understanding and following instructional guidelines.
__label__machine_vision In this paper, we introduce a cross-modal ZSTAD baseline with mutual cross-attention, integrating both text and visual information throughout the detection process.
__label__privacy In this paper, we propose, \textit{HEPrune}, to construct a FHE data-pruning protocol and then design an FHE-friendly data-pruning algorithm under client-aided or non-client-aided settings, respectively.
__label__machine_vision Visual foundation models have achieved remarkable results in zero-shot image classification and segmentation, but zero-shot change detection remains an open problem.
__label__diffusion_based_models Our results show that PID is a potent tool for evaluating and diagnosing text-to-image diffusion models.
__label__machine_vision However, current video reasoning tasks are limited in scope, primarily executed in a question-answering paradigm and focusing on short videos containing only a single event and simple causal relationships, lacking comprehensive and structured causality analysis for videos with multiple events.
__label__optimization As a technical tool, we establish a restricted isometry property (RIP) restricted to a tangent space of the manifold of symmetric rank-$r$ matrices given random Euclidean distance  measurements, which might be of independent interest for the analysis of other non-convex approaches.
__label__safety_in_machine_learning We experimentally demonstrate the feasibility and severity of such an attack across three diverse scenarios: vulnerable code generation, content injection, and over-refusal attack.
__label__natural_language_processing Since emotions often influence human decisions, this paper examines LLM alignment in complex strategic and ethical environments, providing an in-depth analysis of the drawbacks of our psychology and the emotional impact on decision-making in humans and LLMs.
__label__fairness Our deep learning-based approach concurrently optimizes for both accuracy and fairness objectives, and under certain assumptions, achieving proven Pareto optimal solutions while mitigating bias in the trained model.
__label__machine_learning_for_social_sciences iAgents employs a novel agent reasoning mechanism, InfoNav, to navigate agents' communication towards effective information exchange.
__label__machine_vision By integrating our unsupervised pseudo masks into SA-1B’s ground-truth masks and training UnSAM with only 1% of SA-1B, a lightly semi-supervised UnSAM can often segment entities overlooked by supervised SAM, exceeding SAM’s AR by over 6.7% and AP by 3.9% on SA-1B.
__label__reinforcement_learning Instead, naïve reinforcement learning algorithms typically converge to Pareto-dominated outcomes in even the simplest of social dilemmas.
__label__learning_theory \emph{E-RLHF} brings no additional training cost, and is compatible with other methods.
__label__reinforcement_learning (2) For the components of DeMa, we identify the hidden attention mechanism as a critical factor in its success, which can also work well with other residual structures and does not require position embedding.
__label__machine_learning_for_healthcare Given the limited availability of functional annotations from wet-lab experiments, previous methods have primarily relied on self-supervised models trained on vast, unlabeled protein sequence or structure datasets.
__label__machine_learning_for_other_sciences_and_fields Through evaluation across a test suite of 329 datasets, we find that TABULA-8B has zero-shot accuracy on unseen tables that is over 15 percentage points (pp) higher than random guessing, a feat that is not possible with existing state-of-the-art tabular prediction models (e.g.
__label__machine_vision However, extant methods are highly dependent on the fixed hand-crafted causal context.
__label__generative_models Further, we also provide initial experiments on protein motif scaffolding and outperform reconstruction guidance methods.
__label__online_learning Following the recent line of research on algorithms endowed with additional predictive features, we revisit this problem by allowing the decision maker to acquire additional information on the actions to be selected.
__label__machine_vision In the meantime, to better leverage the sparse annotations and extra unlabeled RGB-Thermal videos, a semi-supervised learning baseline, SemiMV, is proposed to enforce consistency regularization through a dedicated Cross-collaborative Consistency Learning (C3L) module and a denoised temporal aggregation strategy.
__label__other Our source code and datasets are available at https://github.com/andylolu2/ollm.
__label__neuroscience_and_cognitive_science High activity variability can be induced while adhering to an energy constraint or while avoiding terminal states defined by specific neurons' activities, also in a context-dependent manner.
__label__machine_vision An INR is typically trained to capture one signal of interest, resulting in learned neural features that are highly attuned to that signal.
__label__reinforcement_learning We present empirical results to demonstrate the effectiveness of our algorithm and compare it to various baselines in different teaching environments.
__label__algorithmic_game_theory This result relies on unique properties fundamental to fair-division constraints that allow faster rates of learning, despite the restricted action space.
__label__machine_learning_for_social_sciences Moreover, we find that more advanced LLMs have a striking dependence on the domain that a real-world graph comes from --- by yielding the best recall accuracy when the graph is narrated in a language style consistent with its original domain.
__label__optimization We establish their oracle complexity bounds under standard assumptions, which, to our best knowledge, are the best-known for this specific setting.
__label__machine_learning_for_physical_sciences UM2N consists of a Graph Transformer (GT) encoder for extracting features and a Graph Attention Network (GAT) based decoder for moving the mesh.
__label__online_learning CONTRAST has two distinguishing features.
__label__safety_in_machine_learning However, existing LNL methods either require pertaining using the memorization effect to separate clean data from noisy ones or rely on dataset assumptions that cannot extend to various scenarios.
__label__generative_models Specifically, MVInpainter partially inpaints multi-view images with the reference guidance rather than intractably generating an entirely novel view from scratch, which largely simplifies the difficulty of in-the-wild NVS and leverages unmasked clues instead of explicit pose conditions.
__label__learning_theory Then, Kégl raises an open problem in COLT 2014 to look for a convergence result for the factorized AdaBoost.MH.
__label__privacy These measures allow us to place a variety of proposed privatization schemes---some differentially private, some not---on the same footing.
__label__optimization_for_deep_networks This measurement mitigates instability and randomness that may arise during importance assessment.
__label__machine_vision Notably, we propose a simple yet highly effective parameter-efficient Integrative spatiotemporal Coherence (ISC) modeling method, alongside a lightweight Global Temporal Regressor (GTR) to harness temporal cues.
__label__natural_language_processing To this end, we propose a CTC-based draft model which strengthens the correlations between draft tokens during the draft phase, thereby generating higher-quality draft candidate sequences.
__label__machine_learning_for_other_sciences_and_fields Traditional LS approaches rely on manually designed heuristics to tackle the LS task, while machine learning recently offers a promising approach towards next-generation logic synthesis by neural circuit generation and optimization.
__label__online_learning There are $m$ types of buyers in the market, where buyers of the same type $i$ have the same valuation curve $v_i:[N]\rightarrow [0,1]$, where $v_i(n)$ is the value for having $n$ data points.
__label__machine_vision However, serializing 3D voxels into 1D sequences will inevitably sacrifice the voxel spatial proximity.
__label__reinforcement_learning Previous work tackles this challenge by training on the source domain with modified rewards derived by matching distributions between the source and the target optimal trajectories.
__label__infrastructure Several methods have emerged to aggregate diverse client models; however, they either lack the ability of personalization, raise privacy and security concerns, need prior knowledge, or ignore the capability and functionality of personalized models.
__label__evaluation Large language models are often ranked according to their level of alignment with human preferences---a model is better than other models if its outputs are more frequently preferred by humans.
"__label__other The kernels are weighted according to a ""task-relevant kernel combination"" mechanism that aligns the total kernel with the task labels."
__label__machine_vision On one hand, AUC optimization in a pixel-level task involves complex coupling across loss terms, with structured inner-image and pairwise inter-image dependencies, complicating theoretical analysis.
__label__learning_theory Transfer learning is an attractive framework for problems where there is a paucity of data, or where data collection is costly.
__label__optimization Existing learning-based models that always formulate the solution process as sequential decisions suffer from high computational overload.
__label__bandits This assumption allows the low-rank representation to be learned before all tasks are revealed, which can be unrealistic in real-world applications.
__label__machine_vision Particularly, we design an Object-Event Slots module, i.e., OE-Slots, that adaptively aggregates the dense video tokens from the vision encoder to a set of representative slots.
__label__natural_language_processing Our knowledge-preserved adaptation not only achieves better performance than LoRA on fine-tuning tasks, but also mitigates the forgetting of world knowledge.
__label__machine_vision When learning vision-language models (VLM) for the fashion domain, most existing works design new architectures from vanilla BERT with additional objectives, or perform dense multi-task learning with fashion-specific tasks.
__label__optimization Given a source and a target probability measure, the Monge problem studies efficient ways to map the former onto the latter.
__label__algorithmic_game_theory We show that power mean functions are learnable with polynomial sample complexity in both cases, even if the social welfare information is noisy.
__label__machine_vision In order to address this issue, we present SINE, a simple image $\textbf{S}$egmentation framework utilizing $\textbf{in}$-context $\textbf{e}$xamples.
__label__machine_vision These complementary slots are combined to form the vision context, serving as the input to the LLM for effective video reasoning.
__label__machine_learning_for_healthcare Our code is at https://github.com/Franblueee/SmMIL.
__label__deep_learning_architectures Specifically, in our coupled scheme, we devise an inter-modal hidden states transition scheme, in which the current state is dependent on the states of its own chain and that of the neighbouring chains at the previous time-step.
__label__machine_vision Class-agnostic object detection (OD) can be a cornerstone or a bottleneck for many downstream vision tasks.
__label__algorithmic_game_theory However, this is unrealistic, as the reward function or distribution is often only partially known and may be subject to uncertainty.
__label__probabilistic_methods The Pandora's Box problem admits a Bayesian-optimal solution based on an expression called the Gittins index, which can be reinterpreted as an acquisition function.
__label__machine_vision These techniques effectively calibrate the 3D labels and enable RGB-only training for 3D detectors.
"__label__machine_learning_for_healthcare Optimal transport (OT)
has emerged as a potent solution, but traditional discrete solvers are hampered by
scalability, privacy, and out-of-sample estimation issues."
__label__reinforcement_learning To solve discounted infinite horizon Markov Decision Processes with discount factor $\gamma$, we show that $h$-PMD which generalizes the standard PMD enjoys a faster dimension-free $\gamma^h$-linear convergence rate, contingent on the computation of multi-step greedy policies.
__label__reinforcement_learning Prior work have studied the complexity of $\varepsilon$-optimal policy identification only when a generative model is available.
__label__natural_language_processing We reveal the theoretical foundations of techniques for editing large language models, and present new methods which can do so without requiring retraining.
__label__machine_vision An efficient decoder parameterizes a set of semantic anisotropic Gaussians, allowing supervised end-to-end learning.
__label__diffusion_based_models Our experimental results demonstrate the effectiveness of DoT in multi-digit multiplication, boolean logic, and grade school math problems.
__label__reinforcement_learning Experimental results demonstrate that GCPO is capable of effectively addressing both multi-goal MR and NMR problems.
__label__diffusion_based_models This allows us to flexibly integrate state-of-the-art LLMs into the text-to-image generation model.
__label__machine_vision The foundation of our approach is anchored in compositionality, alongside the use of task-specific 2D diffusion models as priors for optimization.
__label__probabilistic_methods We provide theoretical analyses of our method and demonstrate its effectiveness with both toy examples and practical applications.
__label__causal_inference Central to our approach is a novel bilinear attention mechanism (BAM) operating on covariance matrices of transformed data while respecting the geometry of the manifold of symmetric positive definite (SPD) matrices.
__label__diffusion_based_models Besides its simplicity, RADD can reduce the number of function evaluations (NFEs) by caching the output of the time-independent network when the noisy sample remains unchanged in a sampling interval.
__label__natural_language_processing Moreover, we present uncertainty-aware decoding techniques that leverage both the graph structure and uncertainty estimates to improve the factuality of LLM generations by preserving only the most reliable claims.
__label__deep_learning_architectures Specifically, the class of the sample should depend on the points from its “context distribution” for better generalisation across distributions.
__label__neuroscience_and_cognitive_science This raises the question of how accurately these multi-modal models can predict brain activity when participants are engaged in multi-modal stimuli.
__label__deep_learning_architectures Extensive experiments across various models, datasets, and scale factors demonstrate that our method achieves comparable or superior performance to existing approaches with about 34\% reduction in computational cost.
__label__natural_language_processing With the assistance of MACM, the accuracy of GPT-4 Turbo on the most challenging level five mathematical problems in the MATH dataset increase from $\mathbf{54.68\\%}  \text{ to } \mathbf{76.73\\%}$.
__label__machine_vision To this end, we propose a simple yet powerful paradigm for seamlessly unifying different human pose and shape-related tasks and datasets.
__label__machine_vision To address these issues, we propose a zero-shot framework that utilizes monocular depth estimation and stereo matching models pretrained on diverse image datasets.
__label__diffusion_based_models We introduce positive and negative binding vectors to enhance disentanglement, further with a neighbor strategy to increase accuracy.
"__label__natural_language_processing This method adaptively allocates weights among these models at each decoding step,
learning the weights through Kullback-Leibler divergence constrained optimization problems."
__label__probabilistic_methods To solve this problem, we propose in this paper a regularised variant that guarantees that divergence is well defined for all distributions.
__label__other Experimental results on various datasets validate the effectiveness of the proposed approach in dealing with the MLOSR problem.
__label__interpretability_and_explainability Going beyond this process, we utilize the back-door adjustment to specifically address the sub-regions identified as non-causal in the upstream phase.
__label__fairness $\texttt{FWC}$ uses an efficient majority minimization algorithm to minimize the Wasserstein distance between the original dataset and the weighted synthetic samples while enforcing demographic parity.
__label__learning_theory Anchor-based methods further establish sample-level similarities for representative anchor generation, effectively addressing scalability issues in large-scale scenarios.
__label__machine_vision However, while it is possible to collect data from new scenarios, the annotations are not always available.
__label__learning_theory The \emph{adaptive adversary} is allowed to arbitrarily corrupt an $\varepsilon$-fraction of the samples $(X_1^*, y_1^*),\ldots, (X_n^*, y_n^*)$.
__label__machine_vision This is because only through dedicated and comprehensive analysis can the category composition of a data domain be obtained, which contradicts the premise of unsupervised scenarios.
__label__learning_theory Prior to our work, no algorithm was known that achieves $\alpha = 3$ in near-linear time.
__label__reinforcement_learning For robust average cost MDPs, the goal is to optimize the worst-case average cost over an uncertainty set of transition kernels.
__label__diffusion_based_models However, these approaches suffer from severe performance degradation or domain shifts.
"__label__infrastructure Leveraging the sign operator in Lion, our Distributed Lion only requires to communicate binary or lower-precision vectors
between workers to the center server, significantly reducing the communication cost."
__label__causal_inference In the machine learning literature, significant efforts have been put into developing machinery to predict the effectiveness of policies efficiently.
__label__diffusion_based_models Existing watermarking methods face the challenge of balancing robustness and concealment.
__label__diffusion_based_models Multiple hypotheses generated by the probabilistic SMPL method are conditioned via continuous 3D shape representations.
__label__diffusion_based_models To address this problem, we leverage pre-trained large vision-language models (VLMs) to guide the evolution of the base model.
__label__machine_vision CuMo outperforms state-of-the-art multimodal LLMs across various VQA and visual-instruction-following benchmarks within each model size group, all while training exclusively on open-sourced datasets.
__label__graph_neural_networks While significant progress has been made in SGNNs research, two issues (i.e., graph sparsity and unbalanced triangles) persist in the current SGNN models.
__label__privacy We also present a second algorithm for pure $\epsilon$-DP representations with the same error using space at most $k \epsilon \cdot \log(e)$ bits, but requiring large decoding times.
__label__generative_models To address this limitation, we propose a novel Group Robust Preference Optimization (GRPO) method to align LLMs to individual groups' preferences robustly.
__label__natural_language_processing LLMs are computationally expensive to pre-train due to their large scale.
__label__evaluation We propose and study the implementation of Rockafellian Relaxation (RR), a new loss re-weighting, architecture-independent methodology, for neural network training.
__label__neuroscience_and_cognitive_science Perceptual learning refers to the practices through which participants learn to improve their performance in perceiving sensory stimuli.
__label__learning_theory A key technical innovation is a delocalisation result for the eigenvectors of the kernel matrix corresponding to small eigenvalues, which takes inspiration from the field of Random Matrix Theory.
__label__generative_models To validate the effectiveness of VisionPrefer, we train a reward model VP-Score over VisionPrefer to guide the training of text-to-image generative models and the preference prediction accuracy of VP-Score is comparable to human annotators.
__label__machine_vision The motion graph transforms patches of video frames into interconnected graph nodes, to comprehensively describe the spatial-temporal relationships among them.
__label__machine_vision In this paper, we propose an uncertainty estimation method built upon the Bayesian inference framework.
__label__safety_in_machine_learning Experimental results on transformer-based and CNN+RNN-based image-to-text models confirmed the effectiveness of our proposed \textit{AAA}.
__label__algorithmic_game_theory In this work, we provide the first quantum algorithm for market equilibrium computation with sub-linear performance.
__label__neuroscience_and_cognitive_science KalmanNet achieved comparable or better results than other deep learning models in offline and online modes, relying on the dynamical model for stopping while depending more on neural inputs for initiating movements.
__label__reinforcement_learning Offline reinforcement learning (RL) has progressed with return-conditioned supervised learning (RCSL), but its lack of stitching ability remains a limitation.
__label__machine_vision We validate our approach on standard image and video datasets - ImageNet-21K, Kinetics400, and Something-Something-v2.
__label__reinforcement_learning Further ablation studies highlight the necessity of applying a distinct learning rate for the context encoder.
__label__diffusion_based_models However, most existing CDMs unreasonably assume that personalized concepts are fixed and cannot change over time.
__label__machine_vision Our work paves the way for further exploration of natural language interfaces in visual pretraining.
__label__safety_in_machine_learning Additionally, we expand on previous work by showing that our universal backdoors, while not completely undetectable in white-box settings, can be harder to detect than some existing designs.
__label__generative_models In this paper, we identify three key flaws in the current design of Latent Consistency Models~(LCMs).
__label__causal_inference We develop an adaptive strategy to design indirect experiments that optimally inform a targeted query about the ground truth mechanism in terms of sequentially narrowing the gap between an upper and lower bound on the query.
__label__machine_learning_for_other_sciences_and_fields To tackle these challenges, we propose PocketFlow, a generative model that incorporates protein-ligand interaction priors based on flow matching.
"__label__safety_in_machine_learning It
can measure goal-directedness with respect to a known utility function, a hypothesis
class of utility functions, or a set of random variables."
__label__privacy We study the problem of multi-agent reinforcement learning (multi-agent RL) with differential privacy (DP) constraints.
__label__generative_models We present a new method for making diffusion models faster to sample.
__label__safety_in_machine_learning When adapting the output interface, label mapping methods transform the pretrained labels to downstream labels by establishing a gradient-free one-to-one correspondence between the two sets of labels.
__label__interpretability_and_explainability In this work, we show that the semantic structure of CLIP's latent space can be leveraged to provide interpretability, allowing for the decomposition of representations into semantic concepts.
__label__diffusion_based_models In this paper, we revisit the diffusion sampling process and identify a fundamental cause of sample quality degradation: the denoiser is poorly estimated in regions that are far Outside Of the training Distribution (OOD), and the sampling process inevitably evaluates in these OOD regions.
__label__privacy Hence, we propose an Adaptive Scale Critical Parameters (AdaSCP) attack to circumvent the defenses and seamlessly incorporate malicious updates into the global model.
__label__machine_vision Based on the two-stage framework, we introduce a 2D-to-3D feature lifting module and a shape-matching module, both of which leverage pre-trained vision foundation models to improve object representation and matching accuracy.
__label__graph_neural_networks This module enhances target domain training by expanding the graph of the target domain to generate reliable domain attentions and fine-tunes the target model for improved negative knowledge filtering and more accurate predictions.
__label__machine_vision Additionally, we illustrate the effectiveness of the proposed method through real-world demonstrations, showcasing its practical utility.
__label__machine_vision To reinforce motion attention, we construct an auxiliary consistency loss within the motion and video space, guiding the video branch to focus on the motion modality.
__label__machine_vision Extensive experiments on four benchmarks validate our rationale and state-of-the-art performance.
__label__reinforcement_learning In the Minecraft game, it rapidly obtains diamonds within a single day on an RTX3090.
__label__reinforcement_learning We conduct an evaluation with real human teammates, and the results show that GAMMA consistently improves performance, whether the generative model is trained on simulated populations or human datasets.
__label__evaluation Despite the rapid pace of algorithmic progress and the development of new architectures such as the transformer, our analysis reveals that the increase in compute made an even larger contribution to overall performance improvements over this time period.
__label__machine_vision Real-world noise removal is crucial in low-level computer vision.
__label__interpretability_and_explainability Based on these findings, we enhance a VLM by integrating classification-focused datasets into its training, and demonstrate that the enhanced classification performance of the VLM transfers to its general capabilities, resulting in an improvement of 11.8% on the newly collected ImageWikiQA dataset.
__label__probabilistic_methods Through this framework, it is possible to leverage tools in OT to build unbalanced losses to handle noisy views and customize the representation space by changing the constraints on alignment.
__label__graph_neural_networks Firstly, we revisit over-smoothing from the perspective of overlapping neighborhood subgraphs, and based on this, we explain how residual methods can alleviate over-smoothing by integrating multiple orders neighborhood subgraphs to avoid the indistinguishability of the single high-order neighborhood subgraphs.
__label__optimization Unlike previous pieces of advice, knowing an exact additive gap does not make the problem trivial.
__label__learning_theory Recently, some preliminary connections have also been made between IRLS and certain types of non-convex linear neural network architectures that are observed to exploit low-dimensional structure in high-dimensional linear models.
__label__safety_in_machine_learning In practice, the adversary could host the resulting full-precision model on an LLM community hub such as Hugging Face, exposing millions of users to the threat of deploying its malicious quantized version on their devices.
__label__other We solve a simple multi-linear model in this skill-basis, finding analytic expressions for the emergence of new skills, as well as for scaling laws of the loss with training time, data size, model size, and optimal compute.
__label__evaluation Accuracy is a commonly adopted performance metric in various classification tasks, which measures the proportion of correctly classified samples among all samples.
__label__reinforcement_learning Based on these analyses, we develop a novel algorithm Pre-trained Embodiment-Aware Control (PEAC) for handling CEURL, incorporating an intrinsic reward function specifically designed for cross-embodiment pre-training.
__label__reinforcement_learning To address these challenges, we harness the potential of imitation learning (IL) to comprehend and anticipate actions of the opponents, aiming to mitigate uncertainties with respect to the game dynamics.
__label__robotics In this work, we present DynaMo, a new in-domain, self-supervised method for learning visual representations.
__label__optimization However, despite its practical success, there is limited theoretical analysis on Adafactor's convergence.
__label__neuroscience_and_cognitive_science However, these techniques may not be able to capture the underlying nonlinear dynamics associated with neural propagation.
__label__reinforcement_learning In this paper, we show that partner specialization, in addition to diversity, is crucial for the robustness of a downstream generalist agent.
__label__probabilistic_methods To generate data from trained diffusion models, most inference algorithms, such as DDPM, DDIM, and other variants, rely on discretizing the reverse SDEs or their equivalent ODEs.
__label__probabilistic_methods Nonparametric Bayesian models naturally capture this phenomenon, but have significant practical barriers to widespread adoption, namely implementation complexity and computational inefficiency.
__label__optimization It applies a Depth Value Network (DVN) based on graph convolution that exploits the symmetry property in $Q$ to auto-grasp value features.
__label__privacy This approach first employs a generative adversarial network to learn a fixed distributional prior, which is then used to guide the inversion process during the attack.
__label__algorithmic_game_theory We answer this question in the affirmative for normal-form games.
__label__deep_learning_architectures These solutions use GS to represent the scene's structure and the neural network to model dynamics.
__label__optimization Linear Mode Connectivity (LMC) refers to the phenomenon that performance remains consistent for linearly interpolated models in the parameter space.
__label__fairness Neural networks trained on biased datasets tend to inadvertently learn spurious correlations, hindering generalization.
__label__machine_vision Besides the ordinary non-structure Gaussian primitives, our model also consists of ray-based Gaussian primitives that are bound to matching rays and whose optimization of their positions is restricted along the ray.
__label__machine_vision However, existing methods rely heavily on epipolar priors, which can be unreliable in complex real-world scenes, particularly in non-overlapping and occluded regions.
__label__generative_models Code is at https://github.com/yifeiwang77/Self-Correction.
__label__graph_neural_networks After the completion of one round of GNN, we use the last-layer embeddings to update the alignments, and proceed to the next round.
__label__machine_learning_for_other_sciences_and_fields This is due to random bit-flips in static random access memory (SRAM), where model parameters are stored.
__label__machine_learning_for_other_sciences_and_fields $\texttt{DESP}$ can make use of existing one-step retrosynthesis models, and we anticipate its performance to scale as these one-step model capabilities improve.
__label__learning_theory As this condition may be a strong requirement for many applications, the practical usefulness of pursuing bounded regret has been questioned.
__label__machine_vision Existing prototype-based methods learn prototypes from the 3D support set to guide the segmentation of query point clouds.
__label__bandits In this work, we close the fundamental gap of theory and practice by providing an improved regret bound for linear ensemble sampling.
__label__human-AI_interaction We then model the pricing problem of generative AI software as a game between two different companies who sequentially release their models before users choose their preferred model for each task.
__label__deep_learning_architectures Consequently, recent research efforts have focused on developing pre-trained TS forecasting models.
__label__machine_learning_for_other_sciences_and_fields To foster interest in this important application of graph learning, we are releasing a large-scale publicly-licensed benchmark for semantic routing consisting of real-world multi-objective navigation problems---expressed via natural language queries---on the richly annotated road networks of US cities.
__label__deep_learning_architectures Our Dit-CNNs compete favorably with state-of-the-art models across multiple classification benchmarks, e.g., ImageNet-1K, while retaining the simplicity and efficiency of traditional CNNs.
__label__machine_vision Given the scarcity of scene-language data, we model the scene embeddings as a sequence of explicit object-level embeddings, derived from semantic-rich 2D or 3D representations.
__label__neuroscience_and_cognitive_science Accumulating evidence suggests stochastic cortical circuits can perform sampling-based Bayesian inference to compute the latent stimulus posterior.
__label__optimization To obtain this result, we develop a modified version of recursive regularization.
__label__neuroscience_and_cognitive_science The framework employs Resonate-and-Fire (RF) neurons with a phase-locking coding (RF-PLC) method to achieve energy-efficient audio processing.
__label__generative_models Our code is available online: \url{https://github.com/shadyabh/UDPM/}
__label__diffusion_based_models Models and codes will be made publicly available.
__label__reinforcement_learning We develop a belief-weighted optimistic asymmetric actor-critic algorithm with polynomial sample and quasi-polynomial computational complexities, where one key component is a new provable oracle for learning belief states that preserve *filter stability* under a misspecified model, which may be of independent interest.
__label__diffusion_based_models Finally, we use our method to visually characterize word ambiguity and similarity from the model’s perspective and illustrate the efficacy of our method for prompt intervention.
__label__machine_learning_for_physical_sciences Finally, the Poseidon model as well as underlying pretraining and downstream datasets are open sourced, with code being available at https://github.com/camlab-ethz/poseidon and pretrained models and datasets at https://huggingface.co/camlab-ethz.
__label__learning_theory Our algorithm shows improvements in terms of constraint violation over a set of learn-to-defer baselines and can control multiple constraint violations at once.
__label__neuroscience_and_cognitive_science Here, we hypothesize that the brain generalizes how it maps spatial domains to mapping abstract spaces.
"__label__other In this paper, we make a critical observation that
 how well the current token’s output logits memorizes the closely preceding input
 tokens also provides strong evidence."
__label__speech_and_audio Consumer electronics used to follow the miniaturization trend described by Moore’s Law.
__label__reinforcement_learning An important finding is that, compared to the existing results for Scaffnew, the sample complexity scales with the inverse of the number of agents, a property referred to as linear speed-up.
__label__causal_inference The effectiveness of our method is demonstrated through experiments on both synthetic and real datasets.
__label__machine_vision Furthermore, we incorporate low-rank adaptation (LoRA) fine-tuning and introduce a noise and diffusion prior update scheduling technique that accelerates the training process by 14 times.
__label__machine_vision We propose using curvature as the input of neural network architectures for surface processing, and explore this proposition through experiments making use of the shape operator.
__label__optimization_for_deep_networks To tackle this issue, we conducted the theoretical analysis to promote the effectiveness of contrast decoding.
__label__optimization_for_deep_networks LoRA, as one of the most popular Parameter-Efficient Fine-Tuning (PEFT) methods, offers a cost-effective alternative by fine-tuning an auxiliary low-rank model that has significantly fewer parameters.
__label__machine_learning_for_physical_sciences We find that QDEQ is not only  competitive with comparable existing baseline models, but also achieves higher performance than a network with 5 times more layers.
__label__algorithmic_game_theory However, these algorithms often fail to take into consideration the additional information available to each player (e.g.
__label__deep_learning_architectures Conversely, in TS-forecasting, the effectiveness of synthetic data distillation is determined by the distance between predictions of the two models.
__label__evaluation Given the vast number of classifiers that have been (and continue to be) proposed, reliable methods for comparing them are becoming increasingly important.
__label__natural_language_processing Based on observed language ratio shifts among layers and the relationships between network structures and certain capabilities, we hypothesize the LLM's multilingual workflow ($\texttt{MWork}$): LLMs initially understand the query, converting multilingual inputs into English for task-solving.
__label__graph_neural_networks Crucially, controlling the coarsening function enables meaningful selection of any number of subgraphs.
__label__machine_learning_for_other_sciences_and_fields Consequently, formula discovery lacks a clear distance metric needed to guide automated discovery in this realm.
__label__interpretability_and_explainability Moreover, we qualitatively show that our method produces meaningful explanations that correlate well with different text prompts.
__label__machine_learning_for_physical_sciences This demonstrates that the QDEQ paradigm can be used to develop significantly more shallow quantum circuits for a given task, something which is essential for the utility of near-term quantum computers.
__label__interpretability_and_explainability Existing relevant works rely on the $\ell_p$ distance for stability assessment, which diverges from human perception.
"__label__learning_theory Besides, we conduct exploratory analyses beyond the first data condition 
and prove that generally, the trained transformer will not perform vanilla gradient descent for the OLS problem."
__label__probabilistic_methods This allows us to define symmetric models compatible with taking $N\to\infty$ and  give an interpretation of the asymptotic dynamics of DA, FA and EA in terms of Wasserstein Gradient Flows describing their MF limits.
__label__optimization_for_deep_networks Subsequently, we rewire the gradient of the loss on the target node to preserve performance on the training node using anchor gradient.
__label__optimization For general functions, we prove that the Polyak step-size and normalized GD obtain fast, path-dependent rates despite using no knowledge of the directional smoothness.
__label__machine_vision We hope our release will inspire and accelerate advancements in multimodal systems and visual representation learning.
__label__optimization We then utilize our heavy-hitter algorithm to reduce the problem to estimating the frequency moment of the tail vector.
__label__reinforcement_learning Given the variations in task content and complexity, formulating policies becomes a challenging endeavor, requiring careful parameter sharing and adept management of conflicting gradients to extract rich cross-task knowledge from multiple tasks and transfer it to unseen tasks.
__label__diffusion_based_models Experimental results show that L2C largely outperforms samplers such as DDIM and DPM-Solver, alongside prior cache-based methods at the same inference speed.
__label__bandits By proving a  lower bound, we show the expected sample complexity of PS$\varepsilon$BAI$^+$ is optimal up to a logarithmic factor.
__label__infrastructure Empirical results demonstrate its robustness across a range of tasks, worker counts, and batch sizes, on both vision and language problems.
__label__natural_language_processing Ms-PoE leverages the position indice rescaling to relieve the long-term decay effect introduced by RoPE, while meticulously assigning distinct scaling ratios to different attention heads to preserve essential knowledge learned during the pre-training step, forming a multi-scale context fusion from short to long distance.
__label__machine_learning_for_healthcare Deep neural networks, including Convolutional Neural Networks (CNNs) and Visual Transformers (ViT), have achieved stunning success in the medical image domain.
__label__learning_theory Leveraging inspiration from portfolio optimization that combining two independent assets will maintain the income while reducing the risk, we introduce two prompts: global prompt and local prompt to construct a prompt portfolio to balance the generalization and personalization.
__label__machine_learning_for_healthcare Given the absence of large-scale motion time series data, we derive and synthesize time series from existing motion skeleton data with all-joint coverage.
__label__natural_language_processing We validate MoD via experiments across a range of NLP datasets and tasks, demonstrating its state-of-the-art performance and shedding new light on the future design of retrieval methods for ICL.
__label__machine_vision Our evaluations show that the spurious features captured by CounterAnimal are generically learned by CLIP models with different backbones and pre-train data, yet have limited influence for ImageNet models.
__label__privacy Current FL researches typically address either covariate-shift data through OOD generalization or semantic-shift data via OOD detection, overlooking the simultaneous occurrence of various OOD shifts.
__label__safety_in_machine_learning In this paper, we circumvent this problem by leveraging the concept of textual entailment to evaluate the correctness of the generated sequence, and propose two selective generation algorithms which control the false discovery rate with respect to the textual entailment relation (FDR-E) with a theoretical guarantee: $\texttt{SGen}^{\texttt{Sup}}$ and $\texttt{SGen}^{\texttt{Semi}}$.
__label__graph_neural_networks Moreover, to obtain general-purpose S²GNNs, we propose spectrally parametrized filters for directed graphs.
__label__diffusion_based_models The assignment functions analogously to external forces to expel the diffuse-able areas of images, thus mitigating the inherent difficulties in diffusion training.
__label__active_learning Results on publicly available video captioning datasets with diverse video captioning models demonstrate that our algorithm outperforms SOTA active learning methods by a large margin, e.g.
__label__machine_vision To achieve efficient pruning and accurate vicinity characterization, we further propose a novel overlap-aware Sinkhorn Distance, which retains only the most likely overlapping points for local measurement and next level exploration.
__label__privacy When releasing outputs from confidential data, agencies need to balance the analytical usefulness of the released data with the obligation to protect data subjects' confidentiality.
__label__machine_learning_for_physical_sciences Molecule generation ideally in its 3-D form has enjoyed wide applications in material, chemistry, life science, etc.
__label__diffusion_based_models Modelling partial differential equations (PDEs) is of crucial importance in science and engineering, and it includes tasks ranging from forecasting to inverse problems, such as data assimilation.
__label__robotics We carry out the largest-scale BC evaluation of PVRs for robotic motor control to date, which encompasses 21 tasks within a real-world Franka robot arm and 5 simulated environments.
__label__machine_vision The second stage enhances photorealism by incorporating a generative prior from pre-trained diffusion models.
__label__fairness We present a general-purpose post-processing algorithm that, using accurate estimates of the regression function and a sensitive attribute predictor, generates predictions that meet the demographic parity constraint.
__label__learning_theory Data-driven algorithm design is a promising, learning-based approach for beyond worst-case analysis of algorithms with tunable parameters.
__label__bandits In bandit best-arm identification, an algorithm is tasked with finding the arm with highest mean reward with a specified accuracy as fast as possible.
__label__natural_language_processing This confidence metric effectively differentiates between CoT and non-CoT paths.
__label__learning_theory Learn-to-Defer is a paradigm that enables learning algorithms to work not in isolation but as a team with human experts.
__label__learning_theory In contrast, our work establishes the information-theoretic and algorithmic limits of matrix denoising with doubly heteroscedastic noise.
__label__probabilistic_methods Permutations, trees, partitions, and binary sequences frequently appear as building blocks in Bayesian nonparametric models, and these models have been studied and developed independently.
__label__learning_theory Meanwhile, the remaining datapoints are guaranteed to have no significant influence on the algorithm's output.
__label__optimization_for_deep_networks To address the above issues, we propose a retraction-free and penalty parameter-free algorithm, which lands on the manifold.
__label__graph_neural_networks Our code and data are released at: https://github.com/CGCL-codes/NIFA.
__label__fairness Prompted by these insights, we conduct a review of over 1.5 million scientific papers to understand the origin of this invalid claim, finding that it is often made without citation, misattributed to papers that do not argue this point, and aggressively over-generalized from source arguments.
__label__generative_models The codes and pretrained models are released at https://github.com/VinAIResearch/DiMSUM.git.
__label__optimization_for_deep_networks * We analyze different metrics for the update size including the $\ell_2$-norm, resulting directional change, and impact on the representations of the network, providing a new perspective on warmup.
__label__probabilistic_methods Causal disentanglement aims to learn about latent causal factors behind data, hold- ing the promise to augment existing representation learning methods in terms of interpretability and extrapolation.
__label__graph_neural_networks By applying CNA modules, GNNs search and form super nodes in each layer, which are normalized and activated individually.
__label__probabilistic_methods We empirically demonstrate that *one and the same* (pretrained) recognition model can infer, *in a zero-shot fashion*, hidden MJPs evolving in state spaces of different dimensionalities.
__label__human-AI_interaction This introduces the possibility of algorithmically-informed teaching in these domains through more relatable AI partners and deeper insights into human decision-making.
__label__diffusion_based_models Drawing from the extensive potential unveiled by the Diffusion Model in both semantic correspondence and open vocabulary segmentation, our work initiates an investigation into employing the Latent Diffusion Model for Few-shot Semantic Segmentation.
__label__machine_vision Synthetic Aperture Radar (SAR) object detection has gained significant attention recently due to its irreplaceable all-weather imaging capabilities.
__label__generative_models Subsequently, based on these specialized modules, we employ a self-supervised pattern completion approach for controllable generation training.
__label__machine_vision To address these issues, we introduce a debiased learning framework, namely **Happy**, characterized by **H**ardness-**a**ware **p**rototype sampling and soft entro**py** regularization.
__label__infrastructure These improvements translate into up to 104% improvement in inference and 39% improvement in training existing models based on neighborhood attention, and additionally extend its applicability to image and video perception, as well as other modalities.
__label__machine_vision Specifically, the pseudo-label exploration can be formulated as a decision-making paradigm by adopting a conformal pseudo-label explorer and a multi-clue selection evaluator.
__label__safety_in_machine_learning Extensive theoretical and empirical results on multiple OoD data sets and network structures verify the superiority of our KPCA detector in  efficiency and efficacy with state-of-the-art detection performance.
__label__safety_in_machine_learning This interpretable rank-one weight edit results in an effective jailbreak technique that is simpler and more efficient than fine-tuning.
__label__diffusion_based_models Masked (or absorbing) diffusion is actively explored as an alternative to autoregressive models for generative modeling of discrete data.
__label__safety_in_machine_learning The code is available at https://github.com/EhanW/AUE-AAP.
__label__reinforcement_learning Existing Goal-Conditioned Reinforcement Learning (GCRL) algorithms are built upon Hindsight Experience Replay (HER), which densifies rewards through hindsight replay and leverages historical goal-achieving information to construct a learning curriculum.
__label__generative_models We introduce PARD, a Permutation-invariant Auto Regressive Diffusion model that integrates diffusion models with autoregressive methods.
__label__natural_language_processing Furthermore, we validate that DeAR is an efficient method that achieves a superior trade-off between accuracy and reasoning time compared to ToT and GoT.
__label__speech_and_audio The model features two novel designs: a meticulously designed dual-sequence language model (DSLM) to capture the information of vocals and accompaniment for song generation, and a series of attention mask strategies for DSLM, which allows our model to understand, generate and edit songs, making it suitable for various songrelated generation tasks by utilizing specific attention masks.
__label__machine_vision We believe VisionLLM v2 will offer a new perspective on the generalization of MLLMs.
__label__learning_theory Our results are presented for proximal samplers that are based on Gaussian versus stable oracles.
__label__probabilistic_methods Code is available at [this link](https://github.com/GFNOrg/diffusion-finetuning).
__label__machine_vision Under the same low-shot setting, it outperforms the popular CoOp with around 10\% on average, and 20\% on EuroSAT which contains large domain shifts.
__label__optimization_for_deep_networks We show empirically that our approach performs well in non-stationary supervised and off-policy reinforcement learning settings.
__label__reinforcement_learning Our PaMoRL framework is hardware-efficient and stable, and it can be applied to various tasks with discrete or continuous action spaces using a single set of hyperparameters.
__label__generative_models Recent advancements in visualizing deep neural networks provide insights into their structures and mesh extraction from Continuous Piecewise Affine (CPWA) functions.
__label__learning_theory However, existing results require distributional assumptions on the data and are limited to a high-dimensional setting, where the input dimension $d_0$ scales at least logarithmically in the number of samples $n$.
__label__deep_learning_architectures We show how our method improves the overall compression performance in terms of the R-D trade-off, compared to its predecessors.
__label__machine_vision First, we introduce a novel Cross-view Geometric Constraint on Unpaired Data to model structural changes in images and segmentation masks across cameras.
__label__reinforcement_learning We consider the problem of learning an $\varepsilon$-optimal policy in controlled dynamical systems with low-rank latent structure.
__label__machine_vision A proper scene representation is central to the pursuit of spatial intelligence where agents can robustly reconstruct and efficiently understand 3D scenes.
__label__deep_learning_architectures In response to these problems, we propose the Spiking Token Mixer (STMixer) architecture, which consists exclusively of operations supported by asynchronous scenarios including convolutional, fully connected layers, and residual paths.
__label__neuroscience_and_cognitive_science Using this approach, we find that the number of contexts storable by the hippocampus grows exponentially with the number of place cells, and calculate this exponent for environments of different sizes.
__label__fairness We show that surprisingly, even constant-factor approximation to fair low-rank approximation requires exponential time under certain standard complexity hypotheses.
__label__interpretability_and_explainability Many tasks in explainable machine learning, such as data valuation and feature attribution, perform expensive computation for each data point and are intractable for large datasets.
__label__infrastructure FM-Delta maps fine-tuned and pre-trained model parameters into integers with the same bits, and entropy codes their integer delta.
__label__machine_learning_for_other_sciences_and_fields In response,  we propose to endow road network representation with the principles of the recent Third Law of Geography.
__label__natural_language_processing In this paper, we consider another dimension of scaling: the amount of data available at inference time.
__label__learning_theory Technically, we prove a novel property of the gradient flow, termed \textit{automatic balancing of gradients}, which enables the loss values of different samples to decrease almost at the same rate and further facilitates the proof of near minimum training loss.
__label__deep_learning_architectures Empirical studies on nuclear norm regularization, latent-space principal component analysis, and graphs adversarial learning demonstrate significant improvements in training efficiency while producing nearly identical outcomes to conventional approaches.
__label__evaluation (2) Instruction-tuned models are more robust to editing, showing less performance drop on general knowledge after editing.
__label__machine_vision However, their rigid combination hampers both the optimization of MoE and the effectiveness of reparameterization of LoRA, leading to sub-optimal performance and low inference speed.
"__label__natural_language_processing An *uncertainty-aware simulation approach* which enables the model to simulate possible future scenarios and how likely they are to occur,
2."
__label__causal_inference Scientific hypotheses typically concern specific aspects of complex, imperfectly understood or entirely unknown mechanisms, such as the effect of gene expression levels on phenotypes or how microbial communities influence environmental health.
__label__natural_language_processing This computational invariance is applied to the hidden state (residual) of the LLM, as well as to the activations of the feed-forward components, aspects of the attention mechanism, and to the KV cache.
__label__machine_vision Inspired by the successful applications of Mixture-of-Experts (MoE) in LLMs, which improves model scalability during training while keeping inference costs similar to those of smaller models, we propose CuMo, which incorporates Co-upcycled Top-K sparsely-gated Mixture-of-experts blocks into both the vision encoder and the MLP connector, thereby enhancing the multimodal LLMs with neglectable additional activated parameters during inference.
__label__machine_learning_for_physical_sciences L-GATr represents high-energy data in a geometric algebra over four-dimensional space-time and is equivariant under Lorentz transformations, the symmetry group of relativistic kinematics.
__label__natural_language_processing On both tasks, CIPHER outperforms baselines by achieving the lowest edit distance cost.
"__label__reinforcement_learning The distributional temporal difference learning has been accordingly proposed, which
is an extension of the temporal difference learning (TD) in the classic RL area."
__label__algorithmic_game_theory As a first result, we show that the continuous-time dynamics of FTRL are Poincaré recurrent, i.e., they return arbitrarily close to their starting point infinitely often, and hence fail to converge.
__label__optimization We then illustrate the usefulness of our approach on black-box autoencoder (AE) regularization, where we aim at applying some topological priors on the latent spaces associated to fixed, black-box AE models without modifying their (unknown) architectures and parameters.
__label__machine_vision Empirical evaluations confirm the capacity of the model to generate convincing guide hair and dense strands, complete with nuanced high-frequency details.
__label__interpretability_and_explainability We reveal novel pathologies in existing unsupervised methods seeking to discover latent knowledge from large language model (LLM) activations---instead of knowledge they seem to discover whatever feature of the activations is most prominent.
__label__interpretability_and_explainability We conclude our analysis by emphasizing the inherent trade-off between performance and computational efficiency, questioning the use of additive metrics such as the linear datamodeling score, and offering a range of discussions.
__label__natural_language_processing Experiments on three multilingual reasoning datasets and a language understanding dataset demonstrate that MergeMinds consistently outperforms all baselines, especially in low-resource languages.
__label__algorithmic_game_theory This paper introduces a payoff perturbation technique, introducing a strong convexity to players' payoff functions in games.
__label__graph_neural_networks Their results reveal an intricate empirical correlation between node classification accuracy and the ratio of smooth to non-smooth feature components.
__label__active_learning Our numerical experiments demonstrate the efficiency of AWS on various datasets by using either exact or estimated loss values.
__label__graph_neural_networks Our comprehensive experiments confirm the strong robustness of our proposed model under various scenarios, and the ablation study provides a deep understanding of its advantages.
__label__diffusion_based_models Graph is a prevalent discrete data structure, whose generation has wide applications such as drug discovery and circuit design.
__label__natural_language_processing Our data synthesis framework prioritizes both breadth and specificity.
__label__reinforcement_learning To deal with this problem, we disclose that there are evaluation and improvement mismatches between the offline dataset and the online environment, which hinders the direct application of pre-trained policies to online fine-tuning.
__label__machine_learning_for_social_sciences Forecasting future events is important for policy and decision making.
__label__machine_vision In this paper, we propose to improve AV joint representations from a data-centric perspective by aligning audio signals to visual data.
__label__deep_learning_architectures To address this issue, we propose the MambaTree network, which first dynamically generates a tree topology based on spatial relationships and input features.
__label__graph_neural_networks Fully connected Graph Transformers (GT) have rapidly become prominent in the static graph community as an alternative to Message-Passing models, which suffer from a lack of expressivity, oversquashing, and under-reaching.
__label__reinforcement_learning Focusing on the classical two-player zero-sum games, theoretical guarantees are provided to demonstrate that pre-trained transformers can provably learn to approximate Nash equilibrium in an in-context manner for both decentralized and centralized learning settings.
__label__optimization These evaluations demonstrate the efficacy of our approach.
__label__natural_language_processing Unexpectedly, on three benchmarks focused on response helpfulness (AlpacaEval 2.0, MT-Bench, Arena Hard Auto v0.1), JANUS also outperforms LLaMA 3 8B Instruct by a +4.0%p, +0.1%p, +3.0%p margin, underscoring that training with a vast array of system messages could also enhance alignment to the general public’s preference as well.
__label__other It's insightful to note that frequency features are more discriminative within a specific domain, while temporal features show better transferability across domains.
__label__machine_vision To this end, we propose AdaptIR, a Mixture-of-Experts (MoE) with orthogonal multi-branch design to capture local spatial, global spatial, and channel representation bases, followed by adaptive base combination to obtain heterogeneous representation for different degradations.
__label__machine_vision However, one crucial issue is completely ignored: the text descriptions given by users may be noisy, e.g., misspellings and typos, limiting the real-world practicality.
__label__other The key innovation of Terra lies in its construction of a continuous parameter manifold through a time variable, with its expressive power analyzed theoretically.
__label__deep_learning_architectures Then, feature propagation is performed based on this graph, thereby breaking the original sequence constraints to achieve stronger representation capabilities.
__label__causal_inference Recent years have seen a growing focus on causal approaches to addressing this problem, emphasizing the importance of learning a Granger causal graph from topological event sequences.
__label__machine_learning_for_other_sciences_and_fields Moreover, our method also demonstrates zero-shot transferability on unseen circuits.
__label__natural_language_processing In contrast, quality evaluation metrics (such as COMET or BLEURT) exhibit high correlations with human judgments, which has motivated their use as rerankers (such as quality-aware and minimum Bayes risk decoding).
__label__safety_in_machine_learning There has been significant research on designing approaches to construct such examples for tree ensembles.
__label__learning_theory We establish learning rules and algorithmic reductions to Empirical Risk Minimization (ERM), accompanied with  learning guarantees.
__label__neuroscience_and_cognitive_science Such models range broadly in both complexity and biological plausibility.
__label__machine_learning_for_other_sciences_and_fields We cast the design tasks as single-player tree generation games, leveraging reinforcement learning techniques to optimize their arithmetic tree structures.
__label__privacy These carefully crafted perturbations encode a customized message that is revealed when TGS attempts 3D reconstructions of the cloaked image.
__label__machine_learning_for_other_sciences_and_fields In this paper, we present NeuRes, a neuro-symbolic approach to address both challenges for propositional satisfiability, being the quintessential NP-complete problem.
__label__natural_language_processing Current hallucination detection and mitigation datasets are limited in domain and size, which struggle to scale due to prohibitive labor costs and insufficient reliability of existing hallucination annotators.
__label__algorithmic_game_theory Potentially, rejectesd items can be revisited and a fraction of their value can be recovered.
__label__generative_models Building on this foundation, we propose a strategy to migrate the temporal consistency in video diffusion models to the spatial-temporal consistency required for 4D generation.
__label__reinforcement_learning *Constrained Reinforcement Learning* (CRL) tackles sequential decision-making problems where agents are required to achieve goals by maximizing the expected return while meeting domain-specific constraints, which are often formulated on expected costs.
__label__safety_in_machine_learning We develop new conformal inference methods for obtaining validity guarantees on the output of large language models (LLMs).
__label__reinforcement_learning Reinforcement Learning (RL) problem with general utility is a powerful decision making framework that covers standard RL with cumulative cost, exploration problems, and demonstration learning.
__label__deep_learning_architectures This paper studies a unifying *matrix mixer* view of sequence mixers that can be conceptualized as a linear map on the input sequence.
__label__learning_theory Bounds are derived for the case of finite hypotheses spaces (both assuming realizability or not), as well as infinite model spaces, which directly generalize classical results.
__label__other In this paper, we present RankUp, a simple yet effective approach that adapts existing semi-supervised classification techniques to enhance the performance of regression tasks.
__label__causal_inference To this end, we introduce a new causal model, where individual events of the cause trigger events of the effect with dynamic delays.
__label__machine_vision Existing Domain Adaptive Object Detection (DAOD) works usually report their performance by selecting the best model on the validation set or even the test set of the target domain, which is highly impractical in real-world applications.
__label__interpretability_and_explainability To access the effectiveness of our approach, we introduce new benchmarks and conduct rigorous evaluations, demonstrating its plausibility, faithfulness, and stability.
__label__generative_models We illustrate the success of the training method on several highly structured datasets.
__label__interpretability_and_explainability We explain this phenomenon through both theoretical proofs and experiments on real-world data.
__label__deep_learning_architectures Differing from existing approaches, FBM (i) embeds the discrete Fourier transform with basis functions, and then (ii) can enable plug-and-play in various types of neural networks for better performance.
__label__natural_language_processing This is especially problematic for commercial cloud applications where the LoRA modules and the base models are hosted by service providers who may not be allowed to host proprietary client task data.
__label__robotics LLaMAR employs a plan-act-correct-verify framework, allowing self-correction from action execution feedback without relying on oracles or simulators.
__label__safety_in_machine_learning What happens when human feedback is based only on partial observations?
__label__machine_vision The code and an app to test the model are available at https://www.robots.ox.ac.uk/vgg/research/countgd/.
__label__generative_models We exemplify our proposal through a simulation study on toy data and two case studies on real-world data, highlighting the importance of tailoring DGMs for targeted data analysis.
__label__safety_in_machine_learning The VLN agent, closely integrated into daily lives, poses a substantial threat to the security of privacy and property upon the occurrence of malicious behavior.
__label__optimization_for_deep_networks The results show that CPR can outperform traditional weight decay and increase performance in pre-training and fine-tuning.
__label__machine_vision To overcome it, we establish a convolutional neural network (CNN) image compression model and adopt the unevenly channel-wise grouped strategy for high efficiency.
__label__natural_language_processing This is due to the overly long token sequences and the interleaved text-image data format, which limit performance.
__label__privacy Up to constant factors, these lower bounds show that the simple strategy of either treating all data as private or discarding the private data, is optimal.
__label__reinforcement_learning We prove the effectiveness of our algorithms by achieving a non-asymptotic problem-dependent upper bound on simple regret of order $O(n^{-1})$, where $n$ is the number of trajectories.
__label__diffusion_based_models The training loss, which is closely connected to the cross-entropy loss, is optimized with respect to both the control function and a family of reparameterization matrices which appear in the matching vector field.
__label__machine_learning_for_other_sciences_and_fields On simulated data from a conventional video, we achieve 34.35 mean PSNR with extremely photon-sparse binary input (<0.06 photons per pixel per frame).
__label__optimization We show for the first time that this leads both to improved convergence bounds in well-behaved settings and to stronger practical convergence.
__label__active_learning Our framework is based on a novel formulation of submodular optimization, specifically tailored to the problem of active 3D object detection.
__label__machine_learning_for_physical_sciences Inspired by the success of splat operation in high-dimensional filtering and random fields, we propose a splat-based implementation for this loss which is both efficient and effective.
__label__machine_vision Specifically, we regularize the latent space such that maps between encodings preserve a learned inner product and commute with a learned functional operator, in the same manner as rigid-body transformations commute with the Laplacian.
__label__natural_language_processing Building a single scaling law from multiple model families is challenging due to large variations in their training compute efficiencies and capabilities.
__label__safety_in_machine_learning To alleviate this problem, we propose \emph{ZeroMark} to conduct ownership verification without disclosing dataset-specified watermarks.
__label__online_learning To prove this result, we give another combinatorial dimension, termed the Level-constrained Branching dimension, and show that its finiteness characterizes constant minimax expected mistake-bounds.
__label__interpretability_and_explainability Specifically, we prove that transformer parameters trained on next-token prediction loss can either converge to global or local minima, contingent on the initialization and the Markovian data properties, and we characterize the precise conditions under which this occurs.
__label__robotics Generalization capabilities, or rather a lack thereof, is one of the most important unsolved problems in the field of robot learning, and while several large scale efforts have set out to tackle this problem, unsolved it remains.
__label__machine_vision Video-Language Models (VLMs), powered by the advancements in Large Language Models (LLMs), are charting new frontiers in video understanding.
__label__machine_vision However, training MoE models from scratch requires extensive data and computational resources, a challenge that limits their widespread adoption.
__label__learning_theory Many empirical studies have provided evidence for the emergence of algorithmic mechanisms (abilities) in the learning of language models, that lead to qualitative improvements of the model capabilities.
__label__learning_theory We show an *exponential improvement* in the dependence on Wasserstein error and depth, along with improved dependencies on other relevant parameters.
__label__machine_learning_for_other_sciences_and_fields Meanwhile, we introduce a Scoring Function to guide the model towards a more stable gradient descent.
__label__learning_theory Moreover, because the learner only has access to faulty sources of information, they require an error-tolerant algorithm for this task: i.e.
__label__machine_learning_for_healthcare Next, we show that this high-fidelity feature learning does not translate to invariance to domain shift, and learning-based methods are sensitive to such changes in the data distribution.
__label__privacy Our research primarily addresses the theoretical underpinnings of similarity-based edge reconstruction attacks (SERA), furnishing a non-asymptotic analysis of their reconstruction capacities.
__label__natural_language_processing The number of large language models (LLMs) with varying parameter scales and vocabularies is increasing.
__label__optimization_for_deep_networks Noisy labels pose a common challenge for training accurate deep neural networks.
__label__reinforcement_learning In this work we explore a particularly stealthy form of training-time attacks against RL -- backdoor poisoning.
__label__diffusion_based_models To address this issue, we develop constrained diffusion models by imposing diffusion constraints based on desired distributions that are informed by requirements.
__label__safety_in_machine_learning We find that these embedding-corrupted prompts not only lead to desirable outputs that satisfy the unlearning objective but also closely approximate the output from a model that has never been trained on the data intended for forgetting.
__label__safety_in_machine_learning A prevailing belief in attack and defense community is that the higher flatness of adversarial examples enables their better cross-model transferability, leading to a growing interest in employing sharpness-aware minimization and its variants.
__label__reinforcement_learning Preference-based Reinforcement Learning ( PbRL ) studies the problem where agents receive only preferences over pairs of trajectories in each episode.
__label__diffusion_based_models While these diffusion models have achieved remarkable success, the underlying mechanisms driving their performance are not yet fully accounted for, with many unanswered questions surrounding what they learn, how they represent visual-semantic relationships, and why they sometimes fail to generalize.
__label__interpretability_and_explainability If these assumptions fail, both noise transition matrices and clean labels cannot be accurately estimated.
__label__optimization We empirically show that, while vanilla topological optimization has to be re-run every time that new data comes out of the black-box models, learning a diffeomorphic flow can be done once and then re-applied to new data in linear time.
__label__machine_vision Based on the Lambertain model, we observe that the non-linear modality discrepancy mainly comes from diverse linear transformations acting on the surface of different materials.
__label__reinforcement_learning Learning a generalist embodied agent capable of completing multiple tasks poses challenges, primarily stemming from the scarcity of action-labeled robotic datasets.
__label__machine_vision NeRFs can adapt to such conditions easily through per-image embedding vectors, but 3DGS struggles due to its explicit representation and lack of shared parameters.
__label__machine_vision 3DGM converts multitraverse RGB videos from the same region into a Gaussian-based environmental map while concurrently performing 2D ephemeral object segmentation.
__label__machine_vision more accuracy with the same throughput on ImageNet-1K compared to the baselines, making it an effective solution for scenarios where hardware availability is diverse or varies over time.
__label__probabilistic_methods The objective automatically embodies an Occam's Razor effect that avoids collapse of conversation laws to the trivial constant, without the need to manually add and tune additional regularisers.
__label__other A graph is navigable if we can successfully move from any starting node to any target node using a greedy routing strategy where we always move to the neighbor that is closest to the destination according to the given distance function.
__label__machine_learning_for_other_sciences_and_fields Further, we propose a hybrid input embedding to handle coefficient tokens with continuity bias and avoid the growth of the vocabulary set.
__label__diffusion_based_models Furthermore, the method is also applicable to unconditional diffusion models, drastically improving their quality.
__label__natural_language_processing Without any training, InfLLM enables LLMs that are pre-trained on sequences consisting of a few thousand tokens to achieve comparable performance with competitive baselines that continually train these LLMs on long sequences.
__label__fairness Last-layer retraining methods have emerged as an efficient framework for correcting existing base models.
__label__diffusion_based_models Additionally, our method can enhance frame quality in video diffusion models, producing frames with high aesthetic value or specific styles.
__label__neuroscience_and_cognitive_science Therefore, they cannot be applied to networks whose activation functions do not have useful derivatives, such as binary and discrete-time spiking neural networks.
__label__machine_vision Visual In-Context Learning (VICL) is a prevailing way to transfer visual foundation models to new tasks by leveraging contextual information contained in in-context examples to enhance learning and prediction of query sample.
__label__privacy While Differential Privacy (DP) is the gold standard for protecting user privacy, standard DP mechanisms typically significantly impair performance.
__label__generative_models We validate our approach for various linear inverse problems, such as super-resolution, deblurring, inpainting, and compressed sensing, and demonstrate that we can outperform other methods based on flow matching.
__label__diffusion_based_models Owing to advancements in image synthesis techniques, stylization methodologies for large models have garnered remarkable outcomes.
__label__learning_theory These theoretical claims have been further supported by empirical experiments.
__label__optimization_for_deep_networks Supervised multi-modal learning involves mapping multiple modalities to a target label.
__label__machine_vision We propose a simple approach to improve OOD generalisation for cancer detection by focusing on nuclear morphology and organisation, as these are domain-invariant features critical in cancer detection.
__label__diffusion_based_models Extensive experiments demonstrate that our ReVideo has promising performance on several accurate video editing applications, i.e., (1) locally changing video content while keeping the motion constant, (2) keeping content unchanged and customizing new motion trajectories, (3) modifying both content and motion trajectories.
__label__deep_learning_architectures Importantly, when tested on multi-GPU systems using TensorRT-LLM engines, Kraken speeds up Time To First Token by a mean of 35.6% across a range of model sizes, context lengths, and degrees of tensor parallelism.
__label__safety_in_machine_learning Through extensive experiments on unlearning, we demonstrate the superiority of our method in achieving promising unlearning at \textit{nearly zero side effects} in general domains and domains closely related to the unlearned ones.
__label__diffusion_based_models In this work, we study the generalizability of diffusion models by looking into the hidden properties of the learned score functions, which are essentially a series of deep denoisers trained on various noise levels.
__label__optimization_for_deep_networks Experiments on standard downstream vision tasks demonstrate that our method achieves promising fine-tuning performance.
__label__reinforcement_learning While theoretical understanding has been obtained for ICL in reinforcement learning (RL), the previous results are largely confined to the single-agent setting.
__label__machine_vision To handle geometrical variations between prompt and query images, we propose a soft feature alignment module that bridges paired-image change perception and single-image semantic segmentation.
__label__safety_in_machine_learning Unlike most existing methods that primarily detect and remove/unlearn suspicious samples to mitigate malicious backdoor attacks, we propose a novel defense approach called PDB (Proactive Defensive Backdoor).
__label__generative_models From the optimization perspective, we consider a latent Gaussian special case and prove that the optimization problem has a closed-form minimizer.
__label__neuroscience_and_cognitive_science Grid-like neurons naturally emerge from the training of piRNNs, which allows us to investigate how the two aspects of the task, space and reward, are integrated in their firing patterns.
__label__generative_models Equipped with a new design of learnable group-wise deformable convolution block, our FlowDCN yields higher flexibility and capability to handle different resolutions with a single model.
__label__causal_inference We propose a greedy search-and-score algorithm for ancestral graphs, which include directed as well as bidirected edges, originating from unobserved latent variables.
__label__machine_vision In response to this challenge, we introduce a federated data augmentation algorithm named FedAvP that shares only the augmentation policies, not the data-related information.
__label__deep_learning_architectures In this formulation the number of network's parameters remains fixed.
__label__optimization_for_deep_networks Specifically, based on the block diagonal approximation of the Fisher information matrix, we first propose the layer-wise sample method to compute each block matrix without performing a complete back-propagation.
__label__machine_vision Importantly, the image pair (as well as the caption pair) contains minimal changes, i.e., between the two images (as well as between the two captions), only one aspect changes at a time from among the following possible types of changes: object, attribute, count, and spatial relation.
__label__online_learning Most existing procedures suffer from high variance due to the use of importance sampling over sequences of actions.
__label__diffusion_based_models Finally, we propose to apply this observation to accelerate the process of T2I generation by properly removing text guidance, which finally accelerates the sampling up to 25\%+.
__label__machine_vision GeCo robustly generalizes the prototypes across objects appearances through a novel dense object query formulation.
__label__safety_in_machine_learning Experimental results on four datasets for SAM and its two variant models demonstrate the powerful attack capability and transferability of DarkSAM.
__label__privacy In this paper, we provide a novel *signal processing perspective* to the design and analysis of DP optimizers.
__label__optimization In federated learning (FL), accommodating clients' varied computational capacities poses a challenge, often limiting the participation of those with constrained resources in global model training.
__label__safety_in_machine_learning Due to the increasing popularity of Artificial Intelligence (AI), more and more backdoor attacks are designed to mislead Deep Neural Network (DNN) predictions by manipulating training samples or processes.
__label__algorithmic_game_theory Remarkably, relaxing any of these three constraints, i.e.
__label__diffusion_based_models Yet their stochastic nature hinders artists from creating consistent images of the same subject.
__label__deep_learning_architectures However, they still have limitations, such as relying on manually designed augmentation strategies, having a limited search space, and using fixed augmentation strategies.
__label__learning_theory Our results build on a connection between testable learning and *fooling*.
__label__natural_language_processing Language models (LMs) are bound to their tokenizer, which maps raw text to a sequence of vocabulary items (tokens).
__label__machine_vision Even in new classes, the distribution of normal residual features would not remarkably shift from the learned distribution.
__label__machine_vision Large pretrained vision-language models like CLIP have shown promising generalization capability, but may struggle in specialized domains (e.g., satellite imagery) or fine-grained classification (e.g., car models) where the visual concepts are unseen or under-represented during pretraining.
__label__probabilistic_methods Deep neural networks (DNNs) lack the precise semantics and definitive probabilistic interpretation of probabilistic graphical models (PGMs).
__label__generative_models We also propose Diffusion by Dynamic Programming (DxDP), a novel reinforcement learning algorithm for diffusion models, as a subroutine in DxMI.
__label__probabilistic_methods The potential-aware trust region anchor selection considers the potential capability of the trust region for better local optimization.
__label__deep_learning_architectures This module adeptly aggregates both local and global information from individual views and interacting groups, enabling precise modeling of close physical interactions through dense point retrieval in small areas, supported by the implicit fields.
__label__learning_theory data collected under different circumstances or synthesized by generative models.
__label__causal_inference In this paper, we aim to learn the optimal policy capable of effectively balancing multiple short-term and long-term rewards, especially in scenarios where the long-term outcomes are often missing due to data collection challenges over extended periods.
__label__reinforcement_learning We discover that the recurrent update of these models resembles a monoid, leading us to reformulate existing models using a novel monoid-based framework that we call memoroids.
__label__reinforcement_learning Based on this, we propose a novel approach, Diffusion-DICE, that directly performs this transformation using diffusion models.
__label__privacy Moreover, PPR achieves a compression size within a logarithmic gap from the theoretical lower bound.
__label__optimization Experimentally, ZO-GDEGA can generate more effective poisoning attack data with an average accuracy reduction of 5\%.
__label__deep_learning_architectures Experimentally, TimeXer achieves consistent state-of-the-art performance on twelve real-world forecasting benchmarks and exhibits notable generality and scalability.
__label__reinforcement_learning However, the presence of reality gap often leads to poor performance when deploying policies trained in simulation directly onto real robots.
__label__diffusion_based_models Motion-to-music and music-to-motion have been studied separately, each attracting substantial research interest within their respective domains.
__label__generative_models ENAT improves the performance of NATs notably with significantly reduced computational cost.
__label__learning_theory Our analysis reveals that they conspire to influence the learning regime through a set of conserved quantities that constrain and modify the geometry of learning trajectories in parameter and function space.
__label__learning_theory We consider interpolation using both the smallest NN (having the minimal number of weights) and a random interpolating NN.
__label__safety_in_machine_learning To tackle these shortcomings, this paper improves backdoor feature inversion for backdoor detection by incorporating extra neuron activation information.
__label__graph_neural_networks GraphTrail is unique in automatically mining the discriminative subgraph-level concepts using Shapley values.
__label__diffusion_based_models Existing single image-to-3D creation methods typically involve a two-stage process, first generating multi-view images, and then using these images for 3D reconstruction.
__label__natural_language_processing MetaAligner models multi-objective alignment into three stages: (1) dynamic objectives reformulation algorithm reorganizes traditional alignment datasets to supervise the model on performing flexible alignment across different objectives; (2) conditional weak-to-strong correction paradigm aligns the weak outputs of fixed policy models to approach strong outputs with higher preferences in the corresponding alignment objectives, enabling plug-and-play inferences on any policy models, which significantly reduces training costs and facilitates alignment on close-source policy models; (3) generalizable inference method flexibly adjusts target objectives by updating their text descriptions in the prompts, facilitating generalizable alignment to unseen objectives.
__label__bandits Furthermore, we extend the aforementioned HierTS and HierBayesUCB algorithms to the multi-task combinatorial semi-bandit setting.
__label__machine_vision To address this, we propose FedGMKD, a novel framework that combines knowledge distillation and differential aggregation for efficient prototype-based personalized FL without the need for public datasets or server-side generative models.
__label__reinforcement_learning Finally, we propose Reach-Aware Value Learning (RAVL), a simple and robust method that directly addresses the edge-of-reach problem and hence - unlike existing methods - does not fail as the dynamics model is improved.
__label__safety_in_machine_learning We find that current safety purification methods are vulnerable to the rapid re-learning of backdoor behavior, even when further fine-tuning of purified models is performed using a very small number of poisoned samples.
__label__optimization_for_deep_networks Overall, this work explores the optimization and risk landscape of ICL in practically meaningful settings and contributes to a more thorough understanding of its mechanics.
__label__safety_in_machine_learning Consequently, with a single UAP, DarkSAM renders SAM incapable of segmenting objects across diverse images with varying prompts.
__label__privacy The goal is to compute for all users a personalized estimate of the user's distribution with error measured in KL divergence.
"__label__machine_vision This paper presents
3DGS-Enhancer, a novel pipeline for enhancing the representation quality of
3DGS representations."
__label__learning_theory We further conduct experiments to validate our theorems, and the results are in excellent agreement with our theoretical predictions.
__label__machine_vision We provide an extensive empirical study to address these issues.
__label__reinforcement_learning Recent studies have revealed that focusing the optimization of Bellman equations solely on in-sample actions tends to result in more stable optimization, especially in the presence of function approximation.
__label__machine_learning_for_physical_sciences We further apply the pre-trained EUNet to animate various garments based on energy optimizations.
__label__machine_learning_for_other_sciences_and_fields We attribute this issue to the imbalance between the abundance of tunable parameters and the scarcity of labeled molecules, and the lack of contextual perceptiveness in the encoders.
__label__natural_language_processing The advent of Large Language Models (LLMs) presents new avenues for automating EA with annotations, inspired by their comprehensive capability to process semantic information.
__label__diffusion_based_models Recent advancements in diffusion models and diffusion bridges primarily focus on finite-dimensional spaces, yet many real-world problems necessitate operations in infinite-dimensional function spaces for more natural and interpretable formulations.
__label__machine_learning_for_other_sciences_and_fields This approach leverages distributions learned from latent representations to avoid the alignment of individual samples.
__label__deep_learning_architectures Our innovative approach uniquely remodels the stem layer (i.e., the first layer) to emphasize minimizing a new learning criterion, namely, uncertainty.
__label__diffusion_based_models More precisely, it conducts a non-asymptotic convergence analysis of denoising score matching under gradient descent.
__label__other Extensive empirical evaluations demonstrate that NoMAD-Attention maintains the quality of the original LLMs well and speeds up the 4-bit quantized LLaMA-7B-based model by up to $2 \times$ at 16k context length.
__label__privacy We show that these updates expose individuals to high-accuracy reconstruction attacks which allow the attacker to recover their data in its entirety, even when the original models are so simple that privacy risk might not otherwise have been a concern.
__label__optimization_for_deep_networks Comparison with related works on compressing Llama2-7B via matrix factorization shows that ESPACE is a first step in advancing the state-of-the-art in tensor decomposition compression of LLMs.
__label__generative_models MOFT provides a distinct set of benefits, including the ability to encode comprehensive motion information with clear interpretability, extraction without the need for training, and generalizability across diverse architectures.
__label__privacy We show experimentally that our heuristic is predictive of the outcome of privacy auditing applied to various training procedures.
__label__machine_vision Our approach integrates original images with nuclear segmentation masks during training, encouraging the model to prioritise nuclei and their spatial arrangement.
__label__interpretability_and_explainability However, despite much real-world data being collected as time series, there is a lack of studies on transparent time series models.
__label__diffusion_based_models Notably, we directly optimize rewards associated with single-step generations that arise naturally from computing the CD loss, effectively bypassing the memory constraints imposed by backpropagating gradients through an iterative sampling process.
__label__machine_learning_for_other_sciences_and_fields We describe an agent that jointly learns to pose challenging problems for itself (conjecturing) and solve them (theorem proving).
__label__machine_vision We introduce Learning from Offline Foundation Features with Tensor Augmentations (LOFF-TA), an efficient training scheme designed to harness the capabilities of foundation models in limited resource settings where their direct development is not feasible.
__label__diffusion_based_models The creation of complex 3D scenes tailored to user specifications has been a tedious and challenging task with traditional 3D modeling tools.
__label__interpretability_and_explainability Additionally, in zero-shot learning, VQShape and its codebook can generalize to previously unseen datasets and domains that are not included in the pre-training process.
"__label__machine_learning_for_physical_sciences Existing state-
of-the-art approaches either resort to large scale transformer-based models that
diffuse over conformer fields, or use computationally expensive methods to gen-
erate initial structures and diffuse over torsion angles."
__label__graph_neural_networks EGonc has nice theoretical properties that guarantee an overall distinguishable margin between the detection scores for IND and OOD samples.
__label__machine_vision Consequently, DGNet achieves state-of-the-art performance under multiple datasets and various weakly supervised settings.
__label__learning_theory Motivated by the problem of compressing point sets into as few bits as possible while maintaining information about approximate distances between points, we construct random nonlinear maps $\varphi_\ell$ that compress point sets in the following way.
__label__machine_vision PartCLIPSeg integrates competitive part relationships and attention control, alleviating ambiguous boundaries and underrepresented parts.
__label__natural_language_processing In this paper, we conduct a comprehensive investigation into the nature of hallucinations within LLMs and furthermore explore effective techniques for detecting such inaccuracies in various real-world settings.
__label__safety_in_machine_learning However, the dynamics of agency become significantly more complex when autonomous agents interact with other agents and humans, necessitating engagement in theory-of-mind, the ability to reason about the beliefs and intentions of others.
__label__online_learning However, a critical aspect often overlooked is the label delay, where new data may not be labeled due to slow and costly annotation processes.
__label__reinforcement_learning The key idea of PPTB is to trim the policy learning path by canceling the policy updates in minor parameter directions, and boost the learning path by encouraging the advance in major directions.
__label__machine_learning_for_other_sciences_and_fields In this work, we propose a method that clusters data belonging to a union of nonlinear manifolds.
__label__machine_learning_for_other_sciences_and_fields Experimental results indicate that IWBVT can serve as a universal post-processing approach to significantly improving the model quality of existing state-of-the-art label integration algorithms and noise correction algorithms.
__label__robotics The Analytic Process leverages its logical reasoning to accumulate linguistic driving experience, which is then transferred to the Heuristic Process by supervised fine-tuning.
__label__optimization To our best knowledge, D-AdaST is the *first* distributed adaptive method achieving near-optimal convergence without knowing any problem-dependent parameters for nonconvex minimax problems.
__label__machine_learning_for_other_sciences_and_fields Estimating them typically requires reconstructed trees for massive amounts of aligned proteins, which poses a major computational bottleneck.
__label__algorithmic_game_theory Tax mechanisms are a common method to alleviate this issue and induce socially optimal behavior.
__label__graph_neural_networks Extensive experiments demonstrate the superiority of SNAPS, improving the efficiency of prediction sets and singleton hit ratio while maintaining valid coverage.
__label__machine_learning_for_other_sciences_and_fields In this work, we bridge the gap between encoder-placer and grouper-placer techniques and propose a novel framework for the task of device placement, relying on smaller computation graphs extracted from the OpenVINO toolkit.
__label__generative_models Experiments and visualizations on multiple datasets demonstrate the effectiveness of our approach, particularly in complicated prediction tasks.
__label__evaluation Our localization methods are independent of the downstream task, do not require any label information, and can be performed in a forward pass.
__label__neuroscience_and_cognitive_science Reconstruction of static visual stimuli from non-invasion brain activity fMRI achieves great success, owning to advanced deep learning models such as CLIP and Stable Diffusion.
__label__machine_vision Our approach outperforms existing best performing augmentation policy search methods and federated data augmentation methods, in the benchmarks for heterogeneous FL.
__label__learning_theory We hope our work will inspire further research on this topic, unleashing the full potential of ILL in wider scenarios where precise labels are expensive and complicated to obtain.
__label__optimization In developing efficient optimization algorithms, it is crucial to account for communication constraints&mdash;a significant challenge in modern Federated Learning.
"__label__natural_language_processing However, relying on a single translation with high estimated quality increases the chances of ""gaming the metric''."
__label__machine_vision We propose \textit{BlackFed} - a black-box adaptation of neural networks that utilizes zero order optimization (ZOO) to update the client model weights and first order optimization (FOO) to update the server weights.
__label__reinforcement_learning Moreover, benefiting from its OOD state correction feature, SCAS demonstrates enhanced robustness against environmental perturbations.
__label__deep_learning_architectures However, TPR-based neural networks often struggle to decompose unseen data into structured TPR representations, undermining their symbolic operations.
__label__diffusion_based_models The *Sensitive Transformer* produces the sensitive constraints, which guide the stereotyped image distribution to align with the stereotype-free probability distribution.
__label__machine_learning_for_other_sciences_and_fields While current state-of-the-art methods on large-scale datasets such as CIFAR provide good empirical performance, they do not have any proof of theoretical correctness.
__label__causal_inference Upon trial completion, identifying best-performing subgroups–those with the most beneficial treatment effects–is crucial for optimizing resource allocation or mitigating adverse treatment effects.
__label__machine_learning_for_physical_sciences This finding provides a practical prerequisite for the stable training of the FNO.
__label__reinforcement_learning This sample complexity bound is minimax optimal (up to logarithmic factors) in the case of the $1$-Wasserstein distance.
__label__machine_learning_for_other_sciences_and_fields However, learning a generic representation of ETS data for various applications is challenging due to the inherently complex hierarchical structure of ETS data.
__label__machine_vision Additionally, we integrate a Convolutional Feed-Forward Network (ConvFFN) to address the lack of channel mixing.
__label__neuroscience_and_cognitive_science Here, we developed the Poisson VAE (P-VAE), a novel architecture that combines principles of predictive coding with a VAE that encodes inputs into discrete spike counts.
__label__learning_theory Our bound follows from a new generalization bound that depends on both the dimension as well as the learning rate and number of iterations.
__label__machine_vision In this paper, we give an in-depth analysis of the potential factors and argue that the smoothness degree of samples' soft labels for different classes (i.e., hard class or easy class) will affect the robust fairness of DNNs from both empirical observation and theoretical analysis.
__label__probabilistic_methods Not only does our research complement existing studies that describe neural networks as kernel machines or infinite-sized Gaussian processes, it also elucidates a more direct approximation that DNNs make to exact inference in PGMs.
__label__probabilistic_methods However, existing methods typically assume that the global domain indices are sampled from a vanilla gaussian prior, overlooking the inherent structures among different domains.
__label__natural_language_processing We discover that, on the contrary, it is possible to reliably determine if a language model was trained on synthetic data if that data is output by a watermarked LLM.
__label__machine_vision The first is depth-aware masking that selectively masks certain parts of non-occluded object queries in the feature space for simulating occluded object queries for network training.
"__label__machine_vision Extensive experiments demonstrate that our approach produces high-quality reconstructions of both deformable and non-deformable objects in complex interactions, with improved
training efficiency compared to existing methods."
__label__diffusion_based_models Our approach presents subject region loss and video preservation loss to enhance the subject's learning performance, along with a subject token cross-attention loss to integrate the customized subject with motion control signals.
__label__natural_language_processing Our experimental study with various LLMs demonstrated that emotions can significantly alter the ethical decision-making landscape of LLMs, highlighting the need for robust mechanisms to ensure consistent ethical standards.
__label__machine_learning_for_social_sciences Surrogate models can address these computational limitations, but to do so they must behave consistently with the simulator under interventions of interest.
__label__bandits By deriving several non-trivial properties of soft trees, we extend the existing analytical techniques used for neural bandit algorithms to our soft tree-based algorithm.
__label__generative_models We do so by pre-optimizing a smooth and convex dual function that has a closed form.
__label__deep_learning_architectures Previous efforts to develop alternatives have focused on a small number of hand-crafted structured matrices, and have neglected to investigate whether these structures can surpass dense layers in terms of compute-optimal scaling laws when both the model size and training examples are optimally allocated.
__label__fairness DPR leverages the disagreement between the target label and the prediction of a biased model to identify bias-conflicting samples—those without spurious correlations—and upsamples them according to the disagreement probability.
__label__optimization_for_deep_networks In this study, we aim to bridge this understanding gap by answering the following two core questions: (1) Which types of Transformer architectures allow Gradient Descent (GD) to achieve guaranteed convergence?
__label__safety_in_machine_learning Based on SeTAR, we further propose SeTAR+FT, a fine-tuning extension optimizing model performance for OOD detection tasks.
__label__evaluation Contrary to earlier work, our experiments show this is not a universal phenomenon.
__label__machine_vision To bridge the domain gap, additional parametric modules are added to capture the temporal information.
__label__machine_vision After decompressing the features and network parameters, we can reconstruct the TSDF-Def volumes, where the 3D surfaces can be extracted through the deformable marching cubes.
__label__machine_vision A mainstream of Multi-modal Large Language Models (MLLMs) have two essential functions, i.e., visual recognition (e.g., grounding) and understanding (e.g., visual question answering).
__label__other Our simple model captures, using a single fit parameter,  the sigmoidal emergence of multiple new skills as training time, data size or model size increases in the neural network.
__label__other Specifically, AlphaRec is comprised of three main components: a multilayer perceptron (MLP), graph convolution, and contrastive learning (CL) loss function, making it extremely easy to implement and train.
__label__machine_learning_for_healthcare From the phenotype view, phenotype co-occurrence can be modeled to reveal patterns across patients.
__label__reinforcement_learning Through extensive experiments, we show that CurrMask exhibits superior zero-shot performance on skill prompting tasks, goal-conditioned planning tasks, and competitive finetuning performance on offline RL tasks.
__label__graph_neural_networks Experimentally, we apply our mechanism to a collection of graph pooling methods and observe consistent and substantial performance gain over several popular datasets, demonstrating its wide applicability and flexibility.
__label__diffusion_based_models It is trained to estimate the motion conditions between two provided images in the semantic spaces.
__label__deep_learning_architectures This score identifies large clusters of similar tokens as high-energy, indicating potential candidates for merging, while smaller (unique and isolated) clusters are considered as low-energy and preserved.
__label__causal_inference When A represents a qualitative Bayesian network, QIM-compatibility with $\mathcal A$ reduces to satisfying the appropriate conditional independencies.
__label__machine_learning_for_other_sciences_and_fields In this study, we introduce a unified neural network architecture, the Deep Equilibrium Density Functional Theory Hamiltonian (DEQH) model, which incorporates Deep Equilibrium Models (DEQs) for predicting Density Functional Theory (DFT) Hamiltonians.
__label__speech_and_audio While volume rendering has been successful in modeling radiance fields for images and neural scene representations, IRs present unique challenges as time-series signals.
__label__infrastructure Furthermore, we introduce a specific aggregation algorithm for aggregating heterogeneous local models with tailored architectures and weights.
__label__optimization_for_deep_networks Recent work suggests an additional asymmetry of the valley beyond the flat and sharp ones, yet without thoroughly examining its causes or implications.
__label__learning_theory In particular, we consider the use of stochastic gradient descent (SGD) on a linear model initialized with pretrained weights and using a small training data set from the target distribution.
__label__other We verify our theory experimentally and show that our algorithm outperforms previous work such as KIP while being significantly more efficient, e.g.
__label__reinforcement_learning However, the nonlinearity of risk measures makes it challenging to achieve convergence and optimality.
__label__generative_models In this work, we propose InteractTraj, the first language-driven traffic trajectory generator that can generate interactive traffic trajectories.
__label__machine_learning_for_other_sciences_and_fields Recent advancements in large language models (LLMs) have unveiled their promising capabilities to formalize even competition-level math problems.
__label__machine_learning_for_other_sciences_and_fields To further refine these segments,  the Gaussian Boundary Delineation (GBD) algorithm is employed to define boundaries within each segmented distribution, effectively delineating normal from anomalous data points.
__label__other One-shot Federated learning (FL) is a powerful technology facilitating collaborative training of machine learning models in a single round of communication.
__label__diffusion_based_models We first present a hardness result indicating that a generic method leveraging the prior denoising oracle for posterior sampling becomes infeasible as soon as the measurement operator is mildly ill-conditioned.
__label__bandits The service rate of each job-server assignment is unknown and modeled by a feature-based Multi-nomial Logit (MNL) function.
__label__machine_vision In this paper, we analyze two inherent challenges in assessing the ability of a VLM in this Language-Only VLM selection: the “Modality Gap”—the disparity in VLM’s embeddings across two different modalities, making text a less reliable substitute for images; and the “Capability Gap”— the discrepancy between the VLM’s overall ranking and its ranking for target dataset, hindering direct prediction of a model’s dataset-specific performance from its general performance.
__label__optimization Specifically, we developed a clustering-based stochastic dueling bandits algorithm that strategically scales well to high-dimensional dueling bandits, and achieves a regret of $\mathcal{O}(K^2\log T)$, where $K$ is the number of clusters and $T$ is the number of rounds.
__label__deep_learning_architectures Specifically, we firstly derive the Retinex theory and we train a Retinex estimator capable of mapping inputs into two intermediary spaces, each approximating the target reflectance and illumination map, respectively.
__label__graph_neural_networks For floats, the formalism matching recurrent GNNs is a rule-based modal logic with counting, while for reals we use a suitable infinitary modal logic, also with counting.
__label__algorithmic_game_theory We further extend the bonus-penalty payment concept to eliciting networked data, designing a symmetrically strongly truthful mechanism when agents’ private signals are sampled according to the Ising models.
__label__natural_language_processing Recently, many methods have been developed to extend the context length of pre-trained large language models (LLMs), but they often require fine-tuning at the target length ($\gg4K$) and struggle to effectively utilize information from the middle part of the context.
__label__active_learning While the first two approaches have been extensively studied, the third remains under-explored.
__label__probabilistic_methods However, permutation inference using traditional maximum likelihood approaches becomes prohibitive due to combinatoric explosion, severely limiting model dimensionality and utility.
__label__machine_vision Zero-shot (ZS) 3D anomaly detection is a crucial yet unexplored field that addresses scenarios where target 3D training samples are unavailable due to practical concerns like privacy protection.
__label__privacy Additionally, we introduce the concept of watermark distribution to establish a verification mechanism for copyright ownership of hybrid or partial infringements, addressing deficiencies in the traditional mechanism of dataset copyright ownership for AI mimicry.
__label__natural_language_processing Specifically, we first construct the knowledgeable instruction set for UrbanKGC tasks (such as relational triplet extraction and knowledge graph completion) via heterogeneity-aware and geospatial-infused instruction generation.
__label__natural_language_processing Extensive experiments show that HaloScope can achieve superior hallucination detection performance, outperforming the competitive rivals by a significant margin.
__label__machine_vision We leverage large language models to generate corrective texts and utilize existing motion generation and editing frameworks to compile datasets of triplets (source motion, target motion, and corrective text).
__label__interpretability_and_explainability We thus propose to measure progress in interpretable dictionary learning by working in the setting of LMs trained on Chess and Othello transcripts.
__label__machine_vision The current methods follow the paradigm of adapting the visual task outputs to the format of the language model, which is the main component of a LMM.
__label__optimization_for_deep_networks Focusing on small-scale GPT training with AdamW/Lion, we explore the following question: *Why and by which criteria are early updates $\mathbf{u}_t$ too large?
__label__online_learning This assumption, however, does not hold for many applications with indivisible bids.
__label__optimization We develop novel algorithms for computing the set of Pareto-optimal solutions (approximately) for various combinations of two objectives.
__label__machine_learning_for_other_sciences_and_fields Despite specified models have been proposed for different small- or macro-molecules, few have attempted to unify the learning process, resulting in redundant efforts.
__label__machine_vision Instead, we address the challenge by pruning the search space for point tracking and let the model process only the important regions of the frames without down-sampling.
"__label__fairness We find that our algorithm increases fairness
with only a minor decrease (and at times, even an increase) in efficiency."
__label__machine_learning_for_healthcare Our model is learned by a stochastic expectation-maximization process, which optimizes the model by iteratively refining the probability estimates of sample weights and updating the model parameters.
__label__interpretability_and_explainability Maintaining the interpretability of machine learning models in the presence of such missing data is challenging.
__label__graph_neural_networks To this end, we introduce a Node Injection-based Fairness Attack (NIFA), exploring the vulnerabilities of GNN fairness in such a more realistic setting.
__label__neuroscience_and_cognitive_science The improved model retains its prior generalization capabilities and, since there is a fully neural path through the network, avoids the pitfalls of other neurosymbolic techniques that elevate symbolic computation over neural computation.
__label__optimization This results in a sequence of policy-dependent system state data with policy-dependent temporal correlations.
__label__generative_models Moreover, the full-image or dense multiview attention they employ leads to a dramatic explosion of computational complexity as image resolution increases, resulting in prohibitively expensive training costs.
__label__natural_language_processing The framework adopts a three-pronged strategy.
__label__graph_neural_networks We delve into this direction via the open-book framework.
__label__interpretability_and_explainability Within this framework, concept information does not solely rely on the similarity between the whole image and general unstructured concepts; instead, we introduce the notion of concept hierarchy to uncover and exploit more granular concept information residing in patch-specific regions of the image scene.
__label__optimization Unlike standard Langevin dynamics, the nonlinearity of the objective functional induces particle interactions, necessitating multiple particles to approximate the dynamics in a finite-particle setting.
__label__natural_language_processing Large language models are usually fine-tuned to align with human preferences.
__label__machine_learning_for_other_sciences_and_fields Co-crystallization is an accessible way to control physicochemical characteristics of organic crystals, which finds many biomedical applications.
__label__reinforcement_learning The resulting agent learning framework, GenRL, allows one to specify tasks through vision and/or language prompts, ground them in the embodied domain’s dynamics, and learn the corresponding behaviors in imagination.
__label__machine_vision To address this issue, we propose to embed $SE(3)$ equivariance into the Perceiver IO architecture.
__label__generative_models Our unconditional results are situated in the same tier as the leading class-conditional ones.
__label__natural_language_processing Training on the resulting synthetic dataset transfers LoRA modules to new models.
__label__natural_language_processing Notably, it also beats LLama models by 5.7 accuracy points on the LM Harness Evaluation.
__label__machine_vision In this paper, we propose Gaussian Graph Network (GGN) to generate efficient and generalizable Gaussian representations.
__label__machine_learning_for_other_sciences_and_fields The concept of self-driving laboratories promises to automate and thus boost the experimental process following AI-driven discoveries.
__label__generative_models However, such solution often shows unsatisfactory editability on the source image.
__label__machine_vision https://xzr52.github.io/ATIH/
__label__natural_language_processing Through empirical evaluation, we demonstrate that our dynamic $\beta$ adjustment technique significantly improves DPO’s performance across a range of models and datasets, offering a more robust and adaptable training paradigm for aligning LLMs with human feedback.
__label__natural_language_processing While Large Language Models (LLMs) have become immensely popular due to their outstanding performance on a broad range of tasks, these models are prone to producing hallucinations— outputs that are fallacious or fabricated yet often appear plausible or tenable at a glance.
__label__learning_theory The results are illustrated through several examples, including Huber regression, pseudo-Huber regression, and their penalized variants with non-smooth regularizer.
__label__generative_models To achieve realistic 3D modeling of the input image, we introduce a dual encoder system tailored for high-fidelity reconstruction and realistic generation from different viewpoints.
__label__machine_learning_for_other_sciences_and_fields We propose a generative model based on the well-known Pointer Network and train it with SLIM.
__label__optimization_for_deep_networks (3) We derive the optimal risk for low-rank parameterized attention weights in terms of covariance spectrum.
__label__causal_inference Unlike hard interventions, which directly force changes in a causal variable, soft interventions exert influence indirectly by affecting the causal mechanism.
__label__natural_language_processing Our evaluation reveals that the masked and causal language models behave very differently, as they clearly outperform each other on different categories of tasks.
__label__optimization Finally, we provide experimental results using Variational Autoenconders (VAE) and applications to several learning frameworks that illustrate our convergence results and show how the effect of bias can be reduced by appropriate hyperparameter tuning.
__label__neuroscience_and_cognitive_science Existing coding schemes either cause huge delays and energy consumption or necessitate intricate neuron models and training techniques.
__label__graph_neural_networks For the first time, we observe that GNNs benefit tremendously from the increasing scale of depth, width, number of molecules and associated labels.
__label__safety_in_machine_learning Backdoor defenses are mainly based on backdoor inversion, which has been shown to be generic, model-agnostic, and applicable to practical threat scenarios.
"__label__machine_learning_for_physical_sciences A number of recent works gave provably efficient machine learning (ML) algorithms
for learning ground states."
__label__machine_learning_for_healthcare Several approaches aim to reduce the heterogeneity of multi-omic data while maintaining the discriminability of cell types with extensive annotated data.
__label__safety_in_machine_learning Our code and trained models are publicly available at https://github.com/xiaoyunxxy/ban.
__label__machine_vision We introduce a doubly hierarchical generative representation for strand-based hair geometry that progresses from coarse, low-pass filtered guide hair to densely populated hair strands rich in high-frequency details.
__label__generative_models We theoretically study the feasibility of GRPO and analyze its convergence for the log-linear policy class.
__label__privacy Conversely, we establish that sparsity is a critical factor for SERA's effectiveness, as demonstrated through analysis and experiments on (dense) stochastic block models.
__label__infrastructure However, managing long contexts brings substantial challenges due to the expansion of key-value cache (KV cache).
__label__optimization_for_deep_networks In modern deep learning, it is common to warm up the learning rate $\eta$, often by a linear schedule between $\eta_{\text{init}} = 0$ and a predetermined target $\eta_{\text{trgt}}$.
__label__human-AI_interaction Most importantly, we find that if the different tasks are sufficiently similar, the first-to-market model may become cost-ineffective on all tasks regardless of how this technology is priced.
__label__generative_models Our experiments demonstrate that by obtaining insights from errors in inference trees, TP-LLaMA significantly outperforms the baselines across almost all test scenarios by a large margin and exhibits better generalization capabilities with unseen APIs.
__label__machine_learning_for_physical_sciences equilibrium structure, demand more cost to compute than others, e.g.
"__label__robotics Crucially, our approach discards the traditional separation between ""history"" and ""future"" by modeling each time step as the ""current"" one for motion generation, leading to a simpler, more parameter- and data-efficient agent simulator."
__label__deep_learning_architectures Nevertheless, the inherent autoregressive property and decoder-only architecture of LLMs have not been fully considered, resulting in insufficient utilization of LLM abilities.
__label__generative_models In offline model-based optimization (MBO), we aim to find a design that maximizes the target function using only a pre-existing offline dataset.
__label__natural_language_processing In this work, we present AutoMix, an approach that strategically routes queries to larger LMs, based on the approximate correctness of outputs from a smaller LM.
__label__reinforcement_learning However, the inferred reward functions often fail to capture the underlying task objectives.
__label__natural_language_processing Such an annotator can not only evaluate the hallucination levels of various LLMs on the large-scale dataset but also help to mitigate the hallucination of LLMs generations, with the Natural Language Inference metric increasing from 25% to 37% on HaluEval.
__label__natural_language_processing This cross-modal methodology not only markedly enhances performance on standard textual QA  but also shows improved zero-shot VQA performance by utilizing synthetic graph images to augment the data for VQA tasks.
__label__machine_vision These strategies are grounded in empirical evidence and theoretical backing.
__label__optimization One of the common objectives in the design and analysis of such algorithms is to attain (Pareto) optimal tradeoffs between the {\em consistency} of the algorithm, i.e., its performance assuming perfect predictions, and its {\em robustness}, i.e., the performance of the algorithm under adversarial predictions.
__label__reinforcement_learning Despite using slow neurons, the brain achieves precise and low-latency control through a combination of predictive and sequence learning.
__label__machine_learning_for_other_sciences_and_fields Autoformalization, the task of automatically translating natural language descriptions into a formal language, poses a significant challenge across various domains, especially in mathematics.
__label__robotics Imitation learning has proven to be a powerful tool for training complex visuo-motor policies.
__label__generative_models In this work, we show that a better latent representation can be obtained by replacing these hard constraints with a soft constraint.
__label__machine_vision PLIP not only significantly improves existing methods on all these tasks, but also shows great ability in the zero-shot and domain generalization settings.
__label__natural_language_processing One way of demystifying transformer predictions would be to describe how they depend on their context in terms of simple template functions.
__label__probabilistic_methods It is obtained by removing all edges $(u,v)$ that are not the shortest path between $u$ and $v$.
__label__probabilistic_methods We then introduce a nonparametric constraint-based algorithm that prunes spurious edges by searching for local conditioning sets, achieving greater accuracy than current methods.
__label__optimization Furthermore, we show that a restarted version of the proposed methods can effectively identify the optimal solution's sparsity pattern within a finite number of steps, a result that appears to have independent significance.
__label__diffusion_based_models This has led to the emergence of pre-trained vision-language models as a tool for transferring representations learned from internet-scale data to downstream tasks and new domains.
__label__natural_language_processing Finally, we apply our framework to a wide range of problems: taxonomizing user chat dialogues, characterizing how they evolve across time, finding categories where one language model is better than the other, clustering math problems based on subareas, and explaining visual features in memorable images.
__label__causal_inference Existing methods in this setting are not scalable due to their high computational complexity.
__label__other This method follows a principle of exploration and exploitation: (1) rephrasing and selecting rationales in the input space to achieve explicit denoising and (2) exploring diverse reasoning paths and voting on answers in the output space.
__label__machine_learning_for_healthcare Extensive experiments on many benchmark datasets suggest the superiority of our DANCE over a series of state-of-the-art methods.
__label__infrastructure However, efficient systems are lacking for programming and executing these applications.
__label__natural_language_processing It is also laborious for model selection since different models excel in diverse knowledge.
__label__machine_vision Extensive experiments and analyses on diverse real-world datasets demonstrate the effectiveness of our method.
__label__graph_neural_networks We then train a linear projector that transforms the GNN's representations into a fixed number of graph token embeddings without tuning the LLM.
__label__natural_language_processing We find that the accuracy of our logistic regression on a linear feature space is slightly (around 3 percentage points) lower than GPT-4, which is in turn around 5 percentage points below that of a human expert.
__label__interpretability_and_explainability We propose *Stochastic Concept Bottleneck Models* (SCBMs), a novel approach that models concept dependencies.
__label__generative_models In this paper, we propose Sparse High Rank Adapters (SHiRA), a new paradigm which incurs no inference overhead, enables rapid switching, and significantly reduces concept-loss.
__label__learning_theory We then provide evidence that this mechanism holds for neural networks more generally.
__label__generative_models Our approach achieves state-of-the-art high-fidelity 3D mesh reconstruction from a single image in just 6 seconds, and experiments on various datasets demonstrate its effectiveness.
__label__reinforcement_learning Unfortunately, PPO requires multiple heuristics to enable stable convergence (e.g.
__label__causal_inference Our goal is to test the hypothesis that models trained on causal features generalize better across domains.
__label__deep_learning_architectures To learn models more powerful for defending against SC-GI, we propose a {\bf Correlation-Oriented Disentanglement and Augmentation (CODA)} modeling scheme, which includes two unique developments: (1) correlation-oriented disentanglement and (2) strategic sample augmentation with reweighted consistency (RWC) loss.
__label__machine_learning_for_healthcare However, due to architectural design and lack of supervision, PLMs model mutations implicitly with evolutionary plausibility, which is not satisfactory to serve as explainable and engineerable tools in real-world studies.
__label__robotics Additionally, we conduct a simple real-world validation in tabletop robot pick-and-place tasks.
__label__reinforcement_learning the number of episodes.
__label__safety_in_machine_learning Federated graph learning (FedGL) is an emerging learning paradigm to collaboratively train graph data from various clients.
__label__reinforcement_learning These additional behavior tokens will be augmented to the vocabulary of pretrained Multimodal Language Models.
__label__machine_vision Presently, pseudo-labeling stands as a prevailing approach in cross-domain semantic segmentation, enhancing model efficacy by training with pixels assigned with reliable pseudo-labels.
__label__machine_vision Experimental results on seven synthetic and real-world noisy datasets validate the effectiveness of DeFT in both noisy label detection and image classification.
__label__optimization In this work, we offer a theoretical analysis of two modern optimization techniques for training large and complex models: (i) adaptive optimization algorithms, such as Adam, and (ii) the model exponential moving average (EMA).
__label__optimization Current methods for differentiating optimization problems typically rely on implicit differentiation, which necessitates costly computations on the Jacobian matrices, resulting in low efficiency.
__label__diffusion_based_models This approach ensures stable erasure with minimal impact on the other concepts.
__label__optimization We present a nontrivial and novel analysis of the entries of the unnormalized Oja vector, which involves the projection of a product of independent random matrices on a random initial vector.
__label__natural_language_processing As a result, our guidelines facilitate the provision of relevant knowledge for the agent's current decision-making process, overcoming the limitations of the conventional demonstration-based learning paradigm.
__label__reinforcement_learning This makes the world model softly state-invariant.
__label__optimization Yet, current PBEMO approaches are prone to be inefficient and misaligned with the DM’s true aspirations, especially when inadvertently exploiting mis-calibrated reward models.
__label__machine_vision However, prevailing driving datasets predominantly utilize single-LiDAR systems and collect data devoid of adverse conditions, failing to capture the complexities of real-world environments accurately.
__label__generative_models At the core of our approach is casting the model pruning process into a SubNet search process.
__label__reinforcement_learning In this framework, the objective is to find a robust policy that achieves good performance under the worst-case scenario among all environments within a pre-specified uncertainty set centered around the training environment.
"__label__machine_learning_for_healthcare Subgraph-based methods have proven to be effective and interpretable in predicting drug-drug interactions (DDIs),
which are essential for medical practice and drug development."
__label__deep_learning_architectures In this paper, we first investigate the discretization gap and propose a novel structural differentiable mask pruning framework named S2HPruner to bridge the discretization gap in a one-stage manner.
__label__optimization We introduce **CoBo**, a *scalable* and *elastic*, SGD-type alternating optimization algorithm  that efficiently addresses these problem with theoretical convergence guarantees.
__label__machine_vision Non-exemplar class-incremental learning (NECIL) is a challenging task that requires recognizing both old and new classes without retaining any old class samples.
__label__safety_in_machine_learning To improve the corruption robustness, various data augmentation methods have been studied, but they are mainly limited to the spatial domain.
__label__other While reshuffling leads to test performances that are competitive with using fixed splits, it drastically improves results for a single train-validation holdout protocol and can often make holdout become competitive with standard CV while being computationally cheaper.
__label__other Rigorous derivation also shows that asymptotically this induced design will converge to the uniform measure over the Pareto front.
__label__active_learning Colander achieves up to 60% improvement on coverage over the baselines while maintaining error level below 5% and using the same amount of labeled data.
__label__learning_theory Existing works are either unable to pinpoint the exact asymptotic estimation error or, when they do so, the resulting approaches (e.g., based on whitening or singular value shrinkage) remain vastly suboptimal.
__label__bandits Online experimentation with interference is a common challenge in modern applications such as e-commerce and adaptive clinical trials in medicine.
__label__interpretability_and_explainability We also develop algorithms for tolerant variants of our testing problem improving upon black-box linear program solvers, and give sample complexity lower bounds for alternative calibration measures to the one considered in this work.
__label__privacy We also show for the first time that the widely-used Poisson subsampling and FSwoR with replace-one adjacency have the same privacy to leading order in the sampling probability.
__label__machine_vision We empirically evaluate our proposed approach across datasets and architectures of varying scales and complexities, demonstrating substantial performance gains in generalization and safety metrics compared to the standard training protocol.
__label__interpretability_and_explainability Surprisingly, with the addition of layer normalization, we show that a transformer with a constant number of layers can represent the in-context conditional empirical distribution, concurring with our empirical observations.
__label__machine_learning_for_other_sciences_and_fields Unlike previous step-by-step methods, POETRY searches for a verifiable sketch of the proof at each level and focuses on solving the current level's theorem or conjecture.
__label__algorithmic_game_theory To do this, we provide a black-box transformation from any truthful auction $A$ to an auction $A'$ such that: i) all mean-based no-regret learners that participate in $A'$ converge to bidding truthfully, ii) the distance between the allocation rule and the payment rule between $A, A'$ is negligible.
__label__online_learning In multi-agent settings, the decisions of each agent can affect the utilities/losses of the other agents.
__label__machine_vision State-of-the-art LiDAR-camera 3D object detectors usually focus on feature fusion.
__label__diffusion_based_models We conduct a comprehensive study of the UNet encoder and empirically analyze the encoder features.
__label__machine_vision By treating the overall representation as a *string-like concatenation* of the inferred FoVs, however, disentanglement provides a fundamentally *symbolic* treatment of compositional structure, one inherently at odds with the underlying *continuity* of deep learning vector spaces.
__label__evaluation We comprehensively investigate the impact of DNN architecture, training data, image resolution, and augmentations on transferability.
__label__generative_models Experiments on the existing benchmark RealEstate10K also confirm the effectiveness of our model.
__label__generative_models Additionally, we show that SelfCodeAlign is effective across LLMs of various sizes, from 3B to 33B, and that the base models can benefit more from alignment with their own data distribution.
__label__machine_vision Remarkably, using only synthetic datasets for training, HODC achieves state-of-the-art generalization performance with various existing stereo matching network architectures, across multiple realistic datasets.
__label__algorithmic_game_theory In this paper, we initiate the study of repeated contracts with learning agents, focusing on those achieving no-regret outcomes.
__label__generative_models In contrast, the compared approaches require hundreds of thousands of calls and achieve significantly lower accuracy.
__label__learning_theory Joint Embedding Predictive Architectures (JEPAs) is a class of architectures in which semantically similar inputs are encoded into representations that are predictive of each other.
__label__machine_learning_for_physical_sciences Quadratic Assignment Problem.
__label__deep_learning_architectures However, their performance significantly drops when dealing with complex textual expressions.
__label__deep_learning_architectures Our method, called MOHAWK, is able to distill a Mamba-2 variant based on the Phi-1.5 architecture (Phi-Mamba) using only 3B tokens.
__label__safety_in_machine_learning Existing backdoor attacks require either retraining the classifier with some clean data or modifying the model's architecture.
__label__machine_vision The recent advances in 3D Gaussian Splatting (3DGS) show promising results on the novel view synthesis (NVS) task.
__label__diffusion_based_models Remarkably, the 4-step generations from our T2V-Turbo achieve the highest total score on VBench, even surpassing Gen-2 and Pika.
__label__machine_vision We further decompose their semantic bias with object-level queries, and leverage natural language methods to generate detailed, open-ended descriptions of each dataset's characteristics.
__label__natural_language_processing Sign language translation (SLT) addresses the problem of translating information from a sign language in video to a spoken language in text.
__label__diffusion_based_models Recently, diffusion models have emerged as a powerful class of generative models.
__label__safety_in_machine_learning To our best knowledge, this work is the first principled OOD detection method that achieves state-of-the-art OOD detection performance without sacrificing OOD generalization ability.
__label__machine_vision However, existing diffusion priors-based methods either consider simple noise types or rely on approximate posterior estimation, limiting their effectiveness in addressing structured and signal-dependent noise commonly found in real-world images.
__label__machine_learning_for_other_sciences_and_fields Cryo-electron microscopy (cryo-EM) is an experimental technique for protein structure determination that images an ensemble of macromolecules in near-physiological contexts.
__label__machine_vision Natural images captured by mobile devices often suffer from multiple types of degradation, such as noise, blur, and low light.
__label__machine_vision Moreover, we implicitly apply window partition under the group-free framework by positional encoding, which further enhances spatial proximity by encoding voxel positional information.
__label__reinforcement_learning One reason for this dichotomy is because the learnt policies are not robust to observation noise or adversarial attacks.
__label__machine_vision This is achieved through a novel channel sampling strategy that encourages the selection of more distinct channel sets for training.
__label__probabilistic_methods A random object-level unitary transformation then captures size-and-shape preserving deviations of $\mu$ from an individual function, while a random linear term and measurement error capture size-and-shape altering deviations.
__label__neuroscience_and_cognitive_science Current neural network models of primate vision focus on replicating overall levels of behavioral accuracy, often neglecting perceptual decisions' rich, dynamic nature.
__label__generative_models In a compute-normalized setting, user evaluations demonstrably favor text generated by Superposed Decoding over Nucleus Sampling.
__label__machine_learning_for_social_sciences We provide theoretical results showing that our proposed approach induces surrogates to behave consistently with high probability with respect to the simulator across interventions of interest, facilitating rapid experimentation with policy interventions in complex systems.
__label__natural_language_processing Bayesian Active Learning provides a principled framework for addressing this challenge and has demonstrated remarkable success in diverse settings.
__label__bandits In online bilateral trade, a platform posts prices to incoming pairs of buyers and sellers that have private valuations for a certain good.
__label__optimization Finally, we prove the convergence results for the proposed methods and outline several settings in which they improve upon existing algorithms.
__label__learning_theory However, it also depends on the boundedness of the future-dependent value function and other related quantities, which we show could be exponential-in-length and thus erasing the advantage of the method.
__label__generative_models While FouRA is motivated for vision tasks, we also demonstrate its merits for language tasks on commonsense reasoning and GLUE benchmarks.
__label__natural_language_processing However, data contamination can lead to inflated performance, rendering them unreliable for model comparison.
__label__diffusion_based_models Many existing methods are based on StyleGAN to address this task.
__label__active_learning Motivated by this, we design a novel active learning algorithm that takes three complementary aspects, namely learnability, diversity, and uncertainty, into account.
__label__optimization_for_deep_networks Additionally, we demonstrate that our method scales effectively to large language models and facilitates automated per-task optimization for instruction fine-tuning datasets.
__label__speech_and_audio Our samples are available at https://thuhcsi.github.io/SongCreator/.
__label__other A critical issue with these algorithms is that the highest prediction probability of the classifier may appear on a non-candidate label.
"__label__optimization_for_deep_networks Then, we show
that S2L subsets have a bounded gradient error w.r.t."
__label__causal_inference This result, together with a comprehensive evaluation on synthetic cases, demonstrates our method's ability to effectively improve differentiable structure learning with partial orders.
__label__fairness Federated learning (FL) offers a machine learning paradigm that protects privacy, allowing multiple clients to collaboratively train a global model while only accessing their local data.
__label__speech_and_audio Our approach is simple and zero-shot, and it leverages pre-trained text-to-image and text-to-spectrogram diffusion models that operate in a shared latent space.
__label__safety_in_machine_learning For finite-sample evaluation, we also derive a probabilistic guarantee on the sample complexity and the difference between the sample mean and the true mean.
__label__machine_learning_for_other_sciences_and_fields These findings have significant implications for advancing assistive technology and mobility for patients with amputations, stroke, or aging.
__label__evaluation Then, we improved the honesty and helpfulness of LLMs in both training-free and fine-tuning settings.
__label__fairness We introduce Multi-Group Proportional Representation (MPR), a novel metric that measures representation across intersectional groups.
__label__bandits We consider two different practical settings: LDP-then-Corruption (LTC) where each user's locally private response might be further corrupted during the data collection process, and Corruption-then-LDP (CTL) where each user's raw data may be corrupted such that the LDP mechanism will only be applied to the corrupted data.
__label__diffusion_based_models Furthermore, our proposed approach scales to modern network architectures such as Attention U-Net and yields more accurate uncertainty estimates compared to existing methods.
__label__deep_learning_architectures Transformers have become one of the foundational architectures in point cloud analysis tasks due to their excellent global modeling ability.
__label__deep_learning_architectures We present a comprehensive theoretical study of the capacity of such SSMs as it compares to that of transformers and traditional RNNs.
__label__interpretability_and_explainability Recent work has studied these mechanisms for Large Language Models (LLMs), revealing insights on how information is stored in a model's parameters and how information flows to and from these parameters in response to specific prompts.
__label__generative_models The effectiveness of our approach is demonstrated on realism metrics after random hairstyle transfer and reconstruction when the original hairstyle is transferred.
__label__reinforcement_learning This recovers known tractability results and gives a novel perspective on reinforcement learning in general sequential decision-making problems, providing a systematic way of identifying new tractable classes of problems.
__label__fairness Existing definitions of fairness in image restoration are highly restrictive.
__label__natural_language_processing Moreover, we propose a tool-augmented iterative trajectory refinement module to enhance and refine the trajectories distilled from GPT-4.
__label__machine_vision Experimental results show that with a modest degree of similarity, training on the generated dataset can produce competitive performance compared to previous generation methods.
__label__diffusion_based_models In this study, we make an interesting and somehow surprising observation: the computation of a large proportion of layers in the diffusion transformer, through introducing a caching mechanism, can be readily removed even without updating the model parameters.
__label__probabilistic_methods In this paper, we design a Wasserstein distance-based criterion able to quantify the relevancy of an observation with respect to future predictions.
__label__optimization In particular, given a representative sample of instances, we learn a neural network that maps an instance of the problem to the most appropriate algorithm *for that instance*.
__label__deep_learning_architectures In this paper, we propose Adaptor Neural Cellular Automata (AdaNCA) for Vision Transformers that uses NCA as plug-and-play adaptors between ViT layers, thus enhancing ViT's performance and robustness against adversarial samples as well as out-of-distribution inputs.
__label__natural_language_processing Despite their shared objective, these have evolved rather independently, with IO receiving more research attention recently.
__label__safety_in_machine_learning However, implementing such a solution is non-trivial, and we need to overcome several challenges: how to automatically confirm the normal prompt to replace the unsafe prompts, and how to effectively perform editable replacement and naturally generate unsafe content.
__label__optimization As a by-product, ZO-GDEGA has advantages on the condition number for the NC-strongly concave case.
__label__diffusion_based_models In this paper, we take another approach to diffusion model acceleration.
__label__machine_vision We introduce a novel, generic, and efficient architecture, named RegionSpot, designed to integrate position-aware localization knowledge from a localization foundation model (e.g., SAM) with semantic information from a ViL model (e.g., CLIP).
__label__machine_vision This work presents the first online framework for temporal action segmentation.
__label__deep_learning_architectures These limitations have motivated researchers to explore more biologically plausible learning algorithms that could potentially shed light on how biological neural systems adapt and learn.
__label__learning_theory Can Transformers predict new syllogisms by composing established ones?
__label__online_learning The crux of our approach lies in a uniquely designed expert-loss for strongly convex functions, stemming from an innovative decomposition of the regret into the meta-regret and the expert-regret.
__label__safety_in_machine_learning To understand these phenomena, we empirically and theoretically  analyze the output error of SSMs under AP.
__label__machine_vision Such estimates are easy for a human annotator compared to pixel-accurate labeling.
__label__other Despite these advancements, the application of LLMs to Combinatorial Problems (CPs), known for their NP-hardness and critical roles in logistics and resource management remains underexplored.
__label__optimization We derive the competitive ratio of PFSUM as a function of the prediction error and conduct extensive experiments to show that PFSUM outperforms the primal-dual-based algorithm.
__label__other One effective way to accelerate inference is Speculative Decoding, which employs a small model to sample a sequence of draft tokens and a large model to validate.
__label__bandits We propose a novel piecewise stationary linear bandit (PSLB) model, where the environment randomly samples a context from an unknown probability distribution at each changepoint, and the quality of an arm is measured by its return averaged over all contexts.
__label__learning_theory Notably, an enhanced anchor dictionary learning mechanism has been utilized to recover the low-rank anchor structure, resulting in reduced computational complexity and increased resilience, especially in scenarios with inadequate dictionaries.
__label__graph_neural_networks Sequence models such as RNNs or Transformers have long been the predominant backbone networks for modeling such temporal graphs.
__label__diffusion_based_models As a result, they could even fail on simple tasks, and applying them to a new problem becomes unavoidably difficult.
__label__natural_language_processing Strikingly, it also surpasses an instruction-finetuned baseline (mistral-7b-instruct) with win rates between 55% and 57% on single-turn dialogue.
"__label__learning_theory Recent work [Cui et al., 2023]
established that linear regression provides Bayes-optimal test error to learn such
a function when the number of available samples is only linear in the dimension."
__label__diffusion_based_models Stylus outlines a three-stage approach that first summarizes adapters with improved descriptions and embeddings, retrieves relevant adapters, and then further assembles adapters based on prompts' keywords by checking how well they fit the prompt.
__label__machine_vision Using 3D Gaussian Splatting (3DGS) as the underlying scene representation simplifies the extraction of objects of interest which are considered to be a subset of the scene's Gaussians.
__label__optimization Specifically, we demonstrate that a clipped version of Adam with model EMA achieves the optimal convergence rates in various nonconvex optimization settings, both smooth and nonsmooth.
__label__machine_learning_for_other_sciences_and_fields Under the hood, gRNAde is a multi-state Graph Neural Network that generates candidate RNA sequences conditioned on one or more 3D backbone structures where the identities of the bases are unknown.
__label__other They should in-principle yield the same performance and share the same optimal learning rate.
__label__graph_neural_networks Moreover, a graphcode is simply an embedded graph and can therefore be readily integrated in machine learning pipelines using graph neural networks.
__label__natural_language_processing Specifically, we design a novel active learning policy to significantly reduce the annotation space by prioritizing the most valuable entities based on the entire inter-KG and intra-KG structure.
__label__graph_neural_networks The implementation codes are available at https://github.com/ArefEinizade2/CITRUS.
__label__natural_language_processing We propose a novel inference-time editing method, namely spectral editing of activations (SEA), to project the input representations into directions with maximal covariance with the positive demonstrations (e.g., truthful) while minimising covariance with the negative demonstrations (e.g., hallucinated).
__label__machine_learning_for_healthcare Although they have yielded excellent results in classification, their performance in terms of localization is comparatively limited.
__label__safety_in_machine_learning Notably, on the large-scale ImageNet benchmark, our AdaNeg significantly outperforms existing methods, with a 2.45\% increase in AUROC and a 6.48\% reduction in FPR95.
__label__privacy For a stream of length $T$ and privacy *without* expiration continual counting is possible with maximum  (over all time steps) additive error $O(\log^2(T)/\varepsilon)$ and the best known lower bound is $\Omega(\log(T)/\varepsilon)$; closing this gap is a challenging open problem.
__label__machine_learning_for_healthcare Extensive experiments on multiple public surgical scene understanding and cross-modal retrieval datasets show that our proposed method significantly improves zero-shot transferring performance and offers a generalist visual repre- sentation for further advancements in surgical scene understanding.
__label__learning_theory In particular, we give a sparse sketching method running in optimal space and current matrix multiplication time, which recovers a nearly-unbiased least squares estimator using two passes over the data.
__label__interpretability_and_explainability On one hand,  the LLM is harnessed to delineate hierarchical visual attributes, while concurrently,  a text-to-image API retrieves  images that are most aligned with these textual concepts.
__label__diffusion_based_models In SRDS, a quick but rough estimate of a sample is first created and then iteratively refined in parallel through Parareal iterations.
__label__machine_learning_for_other_sciences_and_fields In response, we introduce Education Network Restless Multi-armed Bandits (EdNetRMABs), utilizing a network to represent the relationships between interdependent arms.
__label__neuroscience_and_cognitive_science In this work, we propose SpikeSlicer, a novel-designed event processing framework capable of splitting events stream adaptively.
__label__diffusion_based_models Second, the ID-aware QR ReShuffle (IDRS) effectively rectifies the conflicts between face IDs and QR patterns, rearranging QR modules to maintain the integrity of facial features without compromising scannability.
__label__bandits Prod is a seminal algorithm in full-information online learning, which has been conjectured to be fundamentally sub-optimal for multi-armed bandits.
__label__safety_in_machine_learning Our code is available at https://github.com/zhyblue424/TGA-ZSR.
__label__diffusion_based_models By combining the Bradley-Terry preference model, the $\lambda$-Harmonic reward function also provides preference labels for subject-driven generation tasks.
__label__speech_and_audio To construct USDM, we fine-tune our speech-text model on spoken dialog data using a multi-step spoken dialog template that stimulates the chain-of-reasoning capabilities exhibited by the underlying LLM.
__label__natural_language_processing To address this, we introduce GraphVis, which conserves the intricate graph structure through the visual modality to enhance the comprehension of KGs with the aid of Large Vision Language Models (LVLMs).
__label__reinforcement_learning We propose the Experts-as-Priors algorithm (ExPerior), an empirical Bayes approach that utilizes expert data to establish an informative prior distribution over the learner's decision-making problem.
__label__learning_theory Our result refutes the previous hypothesis that LayerNorm plays no role in the rank collapse of self-attention and suggests that self-attention with LayerNorm constitutes a much more expressive, versatile nonlinear dynamical system than what was originally thought.
__label__machine_learning_for_other_sciences_and_fields The primary objective in biological-sequence editing is to determine the optimal modifications to a sequence which augment certain biological properties while adhering to a minimal number of alterations to ensure predictability and potentially support safety.
__label__machine_vision While significant advancements have been made in compressed representations for text embeddings in large language models (LLMs), the compression of visual tokens in multi-modal LLMs (MLLMs) has remained a largely overlooked area.
__label__graph_neural_networks It seamlessly integrates widely adopted contrastive losses and an introduced independence loss to fulfill the common requirements of consistency and diversity of augmentation across diverse scenarios.
__label__online_learning Extensive experiments conducted on two widely used datasets show that our framework improves learning efficiency of the state-of-the-art methods by about 20% and reduces the data storage by 85%, achieving competitive free-viewpoint video synthesis quality but with higher robustness and stability.
__label__generative_models We arrive at these lessons via a thorough study of seven action grounding approaches on five different environments, encompassing over 114 embodied tasks.
__label__machine_vision The potential to cause serious damage to surrounding areas limits the application of FSI policies in real-world scenarios.
__label__probabilistic_methods Finally, we provide both theoretical global exponential convergence guarantees and improved empirical simulation results for applying the IFT gradient flows to the sampling task of MMD-minimization.
__label__causal_inference Specifically, we introduce graphical criteria that allow for disentanglement under various conditions.
__label__interpretability_and_explainability For functions where all interactions involve at most $t$ inputs, we use group testing results to compute the Möbius transform with $O(Kt\log n)$ sample complexity and $O(K\mathrm{poly}(n))$ time.
__label__diffusion_based_models The proposed StoryDiffusion encompasses pioneering explorations in visual story generation with the presentation of images and videos, which we hope could inspire more research from the aspect of architectural modifications.
__label__optimization_for_deep_networks Additionally, our method consistently outperforms SAM across both vision and language tasks.
__label__other Topological conjugacy, a notion from dynamical systems theory, provides a precise definition of dynamical equivalence, offering a possible route to address this need.
__label__reinforcement_learning Extensive experiments on the Meta-world benchmark tasks validate the efficacy of the proposed method.
__label__evaluation In this paper, we propose such a formalization and develop a quantifiable notion of generalizability.
__label__machine_vision We name our method Mixed Gaussian Flow (MGF).
__label__algorithmic_game_theory However, their success has been mostly confined to the low-precision regime since the number of iterations grows polynomially in $1/\epsilon$, where $\epsilon > 0$ is the duality gap.
__label__machine_vision Prompt learning has proven effective in adapting vision language models for downstream tasks.
__label__reinforcement_learning This work enables learning from diverse populations of users with divergent preferences, an important challenge that naturally occurs in problems from robot learning to foundation model alignment.
__label__machine_learning_for_healthcare We illustrate the application of DLM in collaboration with ARMMAN, an India-based non-profit promoting preventative care for pregnant mothers, that currently relies on RMAB policies to optimally allocate health worker calls to low-resource populations.
__label__natural_language_processing However, deep learning-based TKGRs often lack interpretability, whereas rule-based TKGRs struggle to effectively learn temporal rules that capture temporal patterns.
__label__generative_models We argue for a responsible use of compute resources; urging research community to investigate sound and complete LLM-based approaches that uphold efficiency.
__label__probabilistic_methods In this paper, we answer this question in the affirmative, demonstrating that \emph{DPPs can provably outperform independently drawn coresets}.
__label__reinforcement_learning Safe reinforcement learning (RL) requires the agent to finish a given task while obeying specific constraints.
__label__machine_learning_for_physical_sciences This paper studies the problem of rigid dynamics modeling, which has a wide range of applications in robotics, graphics, and mechanical design.
__label__deep_learning_architectures This comprehensive benchmark includes 3,000 ViT architectures of varying model computational budgets evaluated on common large-scale OoD datasets.
__label__interpretability_and_explainability SEV considers movement along a hypercube towards a reference point.
__label__other From an information-theoretic perspective, we propose FedMDMI, a federated posterior inference framework based on model-data mutual information (MI).
__label__machine_vision These datasets are designed with the goal of training a model to do well on standard computer vision benchmarks, many of which, however, have been shown to be English-centric (e.g., ImageNet).
__label__deep_learning_architectures Empirically, we show that performance changes in Mamba’s inference and fine-tuning due to mixed-precision align with Transformer LLMs.
__label__safety_in_machine_learning In the second stage, based on observation 2), we design an Activeness-Aware Fine-Tuning to replace the vanilla fine-tuning.
__label__other We further identify the common behavior among successful PLL methods as a progressive transition from uniform to one-hot pseudo-labels, highlighting the critical role of mini-batch PL purification in achieving top performance.
__label__machine_learning_for_physical_sciences To tackle these two issues, we propose a hybrid approach combining physics-informed neural networks (PINNs) with the *cylindrical approximation*.
__label__reinforcement_learning It uses on-policy Reinforcement Learning (RL) to optimize a reward signal derived from an adversarial discriminator.
__label__bandits Finally, we support our theoretical findings by experimental evaluation against graph bandit multi-task learning and online clustering of bandits algorithms.
__label__deep_learning_architectures Extensive experimental results across different parameter scales (300M to 7B) and three pre-training tasks—English-focused language modeling, multi-lingual language modeling and masked multi-modality modeling—along with multiple downstream validation tasks, demonstrate the effectiveness of MH-MoE.
__label__probabilistic_methods These new variants combine the benefits of their predecessors and address key weaknesses.
__label__fairness Fairness in clustering has been considered extensively in the past; however, the trade-off between the two objectives --- e.g., can we sacrifice just a little in the quality of the clustering to significantly increase fairness, or vice-versa?
__label__other We address this gap by introducing OLLM, a general and scalable method for building the taxonomic backbone of an ontology from scratch.
__label__machine_learning_for_physical_sciences On complex downstream tasks with limited data, such as fluid flow simulations, fluid-structure interactions, and Rayleigh-Bénard convection, we found CoDA-NO to outperform existing methods by over 36%.
__label__machine_vision Despite their effectiveness, they inherently suffer from time-consuming volume rendering, and thus are impractical for many real-time applications.
__label__machine_vision SimGen achieves superior generation quality and diversity while preserving controllability based on the text prompt and the layout pulled from a simulator.
__label__natural_language_processing whether a solution is correct or helpful).
__label__reinforcement_learning In particular, we show that the sample complexity of our method scales optimally with the desired accuracy level and depends on a weak notion of coverage that only requires the empirical feature covariance matrix to cover a single direction in the feature space (as opposed to covering a full subspace).
__label__machine_vision Semantic Scene Completion (SSC) aims to perform geometric completion and semantic segmentation simultaneously.
__label__machine_learning_for_physical_sciences Scaling has been a critical factor in improving model performance and generalization across various fields of machine learning.
__label__optimization_for_deep_networks We show that quantizing the eigenvector matrix of the preconditioner in 4-bit Shampoo is remarkably better than quantizing the preconditioner itself both theoretically and experimentally.
__label__natural_language_processing In this work, we propose Representation Noising (\textsf{\small RepNoise}), a defence mechanism that operates even when attackers have access to the weights.
__label__reinforcement_learning Most opponent modeling methods deal with the non-stationarity caused by unknown opponent policies via predicting the opponent’s actions.
__label__machine_learning_for_healthcare Our method employs weighted sampling within a contrastive learning framework, assigning lower penalties to samples with similar survival outcomes.
__label__optimization Compared to DANE, this method uses an auxiliary sequence of prox-centers while maintaining the same deterministic communication complexity.
__label__learning_theory Our extensive empirical evaluations demonstrate the superior prediction set size performance of CPL compared to state-of-the-art methods across diverse real-world and synthetic datasets in classification, regression, and large language model-based multiple choice question answering.
"__label__optimization Fenchel-Young losses are a family of convex loss functions,
encompassing the squared, logistic and sparsemax losses, among others."
__label__interpretability_and_explainability We focus on the input saliency maps, as the input gradient field is decisive to the models' mathematical essence.
__label__neuroscience_and_cognitive_science The cortico-spinal neural pathway is fundamental for motor control and movement execution, and in humans it is typically studied using concurrent electroencephalography (EEG) and electromyography (EMG) recordings.
__label__reinforcement_learning However, optimizing GAIL is difficult in practise, with the training loss oscillating during training, slowing convergence.
__label__neuroscience_and_cognitive_science And the computational complexity of the feedforward circuit can be even lower than common signal processing algorithms in certain conditions.
__label__diffusion_based_models Furthermore, we also provide an effective manner to fuse multiple LLMs into our framework.
__label__optimization Here, we provide the first analysis which obtains a bound on the strong VI-gap of $\tilde{O}\big(\frac{1}{\sqrt{n}} + \frac{\sqrt{d}}{n\epsilon}\big)$.
__label__graph_neural_networks Then, an additional energy logit representing the virtual OOD class is learned from the residual of the feature against the principal space, and matched with the original logits by a constant scaling.
__label__machine_vision Traditional Anomaly Detection (AD) methods have predominantly relied on unsupervised learning from extensive normal data.
__label__diffusion_based_models In this work, we propose Smoothed Energy Guidance (SEG), a novel training- and condition-free approach that leverages the energy-based perspective of the self-attention mechanism to enhance image generation.
__label__machine_vision Then, these annotated textual videos are used to pre-align a language-only LLM with the video modality.
__label__graph_neural_networks Besides, to avoid extensive parameter fine-tuning and forgetting, we introduce a Memory Representation Adaptive Module called MRaM to separate the learning of prototypes and class representations and use Graph Knowledge Interchange Module (GKIM) to injects past knowledge information into GNN.
__label__deep_learning_architectures UniTS employs a modified transformer block to capture universal time series representations, enabling transferability from a heterogeneous, multi-domain pre-training dataset—characterized by diverse dynamic patterns, sampling rates, and temporal scales—to a wide range of downstream datasets with varied task specifications and data domains.
__label__machine_vision The basic idea is to enrich the representation of the infrared modality with textual descriptions automatically generated by VLMs.
__label__reinforcement_learning To address this, this paper introduces the *Coverage-based Evaluation of Novelty In Environment* (CENIE) framework.
__label__deep_learning_architectures A key technique is the use of Boolean RASP as a convenient intermediate language between transformers and LTL.
__label__learning_theory To derive such a scheme, we follow an approach introduced by Christ & Gunn (2024), which proceeds via first constructing pseudorandom codes satisfying undetectability and robustness properties analogous to those above; our codes have the additional benefit of relying on weaker computational assumptions than used in previous work.
__label__machine_learning_for_physical_sciences Unlike previous graph diffusion models that add noise separately on the atoms and bonds in the forward diffusion process, we propose a graph-dependent noise model for training Graph DiT, designed to accurately estimate graph-related noise in molecules.
__label__generative_models Under the hood, language models support this by running an autoregressive inference pass to provide a draft.
__label__algorithmic_game_theory When adding incentives to the mix, a jarring result by Amanatidis, Birmpas, Christodoulou, and  Markakis [EC 2017] shows that the best possible approximation for two agents and $m$ items is $\lfloor \frac{m}{2} \rfloor$.
__label__safety_in_machine_learning Instead, we map them to a Gaussian vector using an encoder, then convert it into a natural image using StyleGAN2, and finally publish the image.
__label__neuroscience_and_cognitive_science However, applying SNNs to sequential tasks, such as text classification and time-series forecasting, has been hindered by the challenge of creating an effective and hardware-friendly spike-form positional encoding (PE) strategy.
__label__robotics Extensive experiments on the GraspNet-1Billion benchmark demonstrate significant performance improvements compared to previous works.
"__label__machine_vision Specifically, blendfake and deepfake can be explicitly delineated as the oriented pivot anchors between ""real-to-fake"" transitions."
__label__reinforcement_learning In this paper, we introduce SPO: Sequential Monte Carlo Policy Optimisation, a model-based reinforcement learning algorithm grounded within the Expectation Maximisation (EM) framework.
__label__generative_models Unconditional generation -- the problem of modeling data distribution without relying on human-annotated labels -- is a long-standing and fundamental challenge in generative models, creating a potential of learning from large-scale unlabeled data.
__label__machine_vision In certain scenarios, we find that training with LOFF-TA yields better results than directly fine-tuning the foundation model.
__label__reinforcement_learning Predictions from ARPs estimated from off-policy data are provably consistent (asymptotically correct).
__label__natural_language_processing In contrast, MAGNET offers a customizable architecture where byte-level sequences are routed through language-script-specific predictors, each optimized for its respective language script.
__label__deep_learning_architectures We finally show that, when coupled with activation renormalization, the approach yields the best results in the task.
__label__natural_language_processing In this paper, we argue for the importance of alignment for \emph{honesty}, ensuring that LLMs proactively refuse to answer questions when they lack knowledge, while still not being overly conservative.
__label__machine_vision These two points make it difficult for the pixel-level selection mechanism to identify and correct these speckled close- and open-set noises.
__label__generative_models Our objective has a simple form&mdash;it is a mixture of classical masked language modeling losses&mdash;and can be used to train encoder-only language models that admit efficient samplers, including ones that can generate arbitrary lengths of text semi-autoregressively like a traditional language model.
__label__interpretability_and_explainability These results not only highlight the limitations of current approaches but also offer new insights into data debugging.
__label__machine_vision Our method also innovates the blurring diffusion process with the ResShift mechanism, finely balancing between sharpness and diffusion effects.
__label__neuroscience_and_cognitive_science Finally, we train recurrent neural networks on analog memory tasks to support the appearance of these systems as solutions and their generalization capabilities.
__label__interpretability_and_explainability However, while neuron attribution has made significant progress in deciphering text-only LLMs, its application to Multimodal LLMs (MLLMs) remains less explored.
__label__generative_models Our work provides a rigorous theoretical foundation for understanding the inherent trade-off between watermark strength and sampling efficiency in accelerating the generation of watermarked tokens for large language models.
__label__evaluation To mitigate this, we develop an algorithm for learning robust representations in which (a) synthetic data environments are constructed via hierarchical sampling, and (b) environment balancing penalization, inspired by out-of-distribution problems, is applied.
__label__optimization_for_deep_networks Our method can train student backbones that span across a variety of convolutional neural network (CNN), multi-layer perceptron (MLP), and ViT architectures on image classification datasets, achieving state-of-the-art knowledge distillation performance.
__label__reinforcement_learning Towards expanding the class of scenarios that cooperative learning methods may optimally address, we introduce $N$*-agent ad hoc teamwork* (NAHT), where a set of autonomous agents must interact and cooperate with dynamically varying numbers and types of teammates.
__label__machine_vision In this work, we show that frozen foundation models can be a versatile feature enhancer, even though they are not pre-trained for object detection.
__label__machine_learning_for_other_sciences_and_fields As new associations form and old ones dissolve, SARAD applies subseries division to capture their changes over time.
__label__diffusion_based_models Extensive user studies demonstrate that our model is preferred nearly twice as often compared to the popular SDXL model and is on par with the proprietary Stable Diffusion 3 with 8B parameters.
__label__learning_theory We study the \emph{in-context learning} (ICL) ability of a \emph{Linear Transformer Block} (LTB) that combines a linear attention component and a linear multi-layer perceptron (MLP) component.
__label__reinforcement_learning We pretrain a context-aware world model to learn a compact task representation, and inject it as a contextual condition to the causal transformer to guide task-oriented sequence generation.
__label__machine_vision Inferring the 3D structure underlying a set of multi-view images typically requires solving two co-dependent tasks -- accurate 3D reconstruction requires precise camera poses, and predicting camera poses relies on (implicitly or explicitly) modeling the underlying 3D.
__label__probabilistic_methods It has been shown that PCs with general directed acyclic graph (DAG) structure can be understood as a mixture of exponentially (in its height) many components, each of which is a product distributions over univariate marginals.
__label__causal_inference We believe that this text-based design of proxies allows for the use of proximal causal inference in a wider range of scenarios, particularly those for which obtaining suitable proxies from structured data is difficult.
__label__reinforcement_learning We leverage memoroids to propose a batching method that improves sample efficiency, increases the return, and simplifies the implementation of recurrent loss functions in reinforcement learning.
__label__machine_vision These designs enable LE3D to perform real-time novel view synthesis, HDR rendering, refocusing, and tone-mapping changes.
__label__generative_models Extensive experiments validate that DreamSteerer can significantly improve the editability of several T2I personalization baselines while being computationally efficient.
__label__other Specifically, we frame the user sequence imagination as a reinforcement learning problem and develop a recommendation-focused reward function to evaluate to what extent a user can help recommend the OOV items.
"__label__machine_vision We leverage 2D video diffusion priors to address the
challenging 3D view consistency problem, reformulating it as achieving temporal
consistency within a video generation process."
"__label__optimization_for_deep_networks Despite the effectiveness of data selection for pretraining and instruction fine-tuning
large language models (LLMs), improving data efficiency in supervised fine-tuning
(SFT) for specialized domains poses significant challenges due to the complexity
of fine-tuning data."
__label__natural_language_processing Nevertheless, they fall short in alignment with the emotional behaviors of human beings and cannot establish connections between similar situations.
__label__machine_vision Recent DETR-based methods have advanced the development of Video Instance Segmentation (VIS) through transformers' efficiency and capability in modeling spatial and temporal information.
__label__privacy We consider the problem of model selection in a high-dimensional sparse linear regression model under privacy constraints.
__label__neuroscience_and_cognitive_science Decoding such dynamic information from brain activity can enhance the understanding of the brain’s visual processing system.
__label__machine_learning_for_social_sciences However, existing MHeteroFL methods rely on training loss to transfer knowledge between the client model and the server model, resulting in limited knowledge exchange.
__label__machine_vision To address this, we introduce WildGaussians, a novel approach to handle occlusions and appearance changes with 3DGS.
__label__reinforcement_learning This paper formalizes the problem, and proposes the *Policy Optimization with Agent Modelling* (POAM) algorithm.
__label__neuroscience_and_cognitive_science Spiking neural networks (SNNs), capable of performing Bayesian computation with greater physiological interpretability, offer a novel approach to distributed information processing in the cortex.
__label__optimization_for_deep_networks For instance, taking a well pre-trained Swin-L as the teacher model, our method gets 75.15\%|82.03\%|84.16\%|78.63\%|81.96\%|83.93\%|83.80\%|85.53\%  top-1 accuracies for MobileNet-V1|ResNet-50|ConvNeXt-T|Mixer-S/16|Mixer-B/16|ViT-S/16|Swin-T|ViT-B/16 models trained on ImageNet-1K dataset from scratch, showing 3.05\%|3.39\%|2.02\%|4.61\%|5.52\%|4.03\%|2.62\%|3.73\% absolute gains to the individually trained counterparts.
__label__machine_vision In this paper, we propose eFreeSplat, an efficient feed-forward 3DGS-based model for generalizable novel view synthesis that operates independently of epipolar line constraints.
__label__learning_theory Additional heuristic calculations suggest that the weak recovery threshold of $p$-step QAOA matches that of $p$-step tensor power iteration when $p$ is a fixed constant.
__label__natural_language_processing Transformer based large-language models (LLMs) display extreme proficiency with language yet a precise understanding of how they work remains elusive.
__label__safety_in_machine_learning Past analyses of reinforcement learning from human feedback (RLHF) assume that the human evaluators fully observe the environment.
__label__machine_vision Vision models excel in image classification but struggle to generalize to unseen data, such as classifying images from unseen domains or discovering novel categories.
__label__machine_vision However, without sufficient 3D inductive bias, existing methods typically entail expensive training costs and struggle to extract high-quality 3D meshes.
__label__machine_vision However, the transfer suffers when the teacher model is significantly larger than the student.
__label__machine_vision This trend persists even when controlling for sample-level similarity between pretraining and downstream datasets, and testing on purely synthetic data distributions.
__label__privacy ARS extends the analysis of randomized smoothing using $f$-Differential Privacy to certify the adaptive composition of multiple steps.
__label__deep_learning_architectures Finally, we conduct extensive experiments on eight time series forecasting benchmarks, and experimental results have demonstrated our superior performance in terms of both effectiveness and efficiency compared with state-of-the-art methods.
__label__causal_inference By developing a compact and exact closed-form solution for the PGF of PB-SCM, we demonstrate that each component in this closed-form solution uniquely encodes a specific local structure, enabling the identification of the local structures by testing their corresponding component appearances in the PGF.
__label__deep_learning_architectures Furthermore, we find test-time HMR objectives are different from training-time objectives, which reduces the effectiveness of the learning of the meta-model.
__label__optimization The effectiveness of DLL is demonstrated on linear and nonlinear conic optimization problems.
__label__machine_learning_for_physical_sciences Conventional time series generation methods often struggle to capture extreme values adequately, diminishing their value in critical applications such as scenario planning and management for healthcare, finance, climate change adaptation, and beyond.
__label__machine_learning_for_other_sciences_and_fields However, just improving the accuracy of ability estimation is far from satisfactory in the real-world scenarios, since an accurate ranking of students is usually more important (e.g., in high-stakes exams).
__label__machine_vision Multi-object 3D Grounding involves locating 3D boxes based on a given query phrase from a point cloud.
__label__machine_vision At the same time, we discover the nature of general action completeness within large datasets, indicated by the per-frame diversity over time.
__label__machine_learning_for_physical_sciences Moreover, we show that LASR can be used to discover a new and powerful scaling law for LLMs.
__label__speech_and_audio Demo is available at \url{https://berkeley-speech-group.github.io/SSDM/}.
__label__generative_models https://github.com/360CVGroup/HiCo_T2I.
__label__diffusion_based_models We also relate VFM to score-based models, in which the dynamics are stochastic rather than deterministic, and derive a bound on the model likelihood based on a reweighted VFM objective.
__label__machine_learning_for_healthcare Customizing the shape and intensity of radiation beams for each patient leads to solving large-scale constrained optimization problems that need to be solved within tight clinical time-frame.
__label__machine_learning_for_other_sciences_and_fields Inspired by this, we propose G$^2$-Reasoner, a LLM causal reasoning method that incorporates general knowledge and goal-oriented prompts into LLMs' causal reasoning processes.
__label__natural_language_processing Instead, MLLM-based agents, which enhance capabilities through tool invocation, are gradually being applied to this scenario.
__label__deep_learning_architectures MOHAWK allows models like SSMs to leverage computational resources invested in training Transformer-based architectures, highlighting a new avenue for building such models.
__label__active_learning We demonstrate favorable scaling both as we subselect more aggressively and using small auxiliary models to select data for large target models.
__label__deep_learning_architectures Thus, for the already converged dimensions, the computations can be skipped.
__label__safety_in_machine_learning We formally define two failure cases: deceptive inflation and overjustification.
__label__safety_in_machine_learning The code is available at https://github.com/OakleyTan/FedSSP.
__label__speech_and_audio Moreover, the black-box nature of existing models limits their use in real-world scenarios, where explanations are required for model decisions.
__label__reinforcement_learning The empirical success of distributional reinforcement learning (RL) highly relies on the choice of distribution divergence equipped with an appropriate distribution representation.
__label__graph_neural_networks Our theoretical findings and empirical results potentially open up new possibilities for the research of equivariant GNNs.
__label__other We further validate the effectiveness and robustness of PSL through empirical experiments.
__label__causal_inference But giving semantics to hypergraphs using QIM-compatibility lets us do much more.
__label__optimization Moreover, based on our PGW metric, we introduce the analogous concept of barycenters for mm-spaces.
__label__other To address this gap, we formally investigate why MP helps CF from multiple perspectives and show that many assumptions made by previous works are not entirely accurate.
__label__generative_models To achieve this, GRPO adaptively and sequentially weights the importance of different groups, prioritizing groups with worse cumulative loss.
__label__generative_models Extensive experiments demonstrate that ZipCache achieves superior compression ratios, fast generation speed and minimal performance losses compared with previous KV cache compression methods.
__label__probabilistic_methods *, a *dynamic* function) $f : \mathcal{S} \times \mathcal{T} \to \mathbb{R}$ remains a challenge, since a dynamic Bayesian Optimization (DBO) algorithm has to keep track of the optimum over time.
__label__other We leverage these advantages to derive a new algorithm _Factor Relaxation with Latent Coupling_ (FRLC), which uses _coordinate_ mirror descent to compute the LC factorization.
__label__machine_vision Experiments on self-captured and public dash cam videos show that our method not only achieves state-of-the-art performance in novel view synthesis, but also accurately reconstructing captured scenes getting rid of obstructions.
__label__natural_language_processing To address this, we introduce a novel approach to re-train transformer encoder-based LMs as Hierarchy Transformer encoders (HiTs), harnessing the expansive nature of hyperbolic space.
__label__machine_vision Furthermore, the best-performing methods train object localization by a surrogate loss, that predicts a unit Gaussian at each object center.
__label__natural_language_processing Our findings highlight (1) the necessity of a multi-modal retriever for demonstration retrieval, (2) the importance of intra-demonstration ordering over inter-demonstration ordering, and (3) the enhancement of task comprehension through introductory instructions in prompts.
__label__other In this paper, we first discover that the high within-class similarity in condensed datasets necessitates the use of large-scale soft labels.
__label__deep_learning_architectures Based on label-specific features and confounders, we employ a cross-attention module to implement causal intervention, quantifying the causal correlations from all object categories to each predicted object category.
__label__natural_language_processing ReST-MCTS* circumvents the per-step manual annotation typically used to train process rewards by tree-search-based reinforcement learning: Given oracle final correct answers, ReST-MCTS* is able to infer the correct process rewards by estimating the probability this step can help lead to the correct answer.
__label__neuroscience_and_cognitive_science This impressive performance showcases SpGesture's potential to enhance the applicability of sEMG in real-world scenarios.
__label__diffusion_based_models Our framework provides an end-to-end, simulation-free optimization objective, effectively minimizing a variational upper bound on the negative log-likelihood.
__label__graph_neural_networks Empirically, we validate our approach by showcasing its ability to mitigate under-reaching and over-squashing effects, achieving state-of-the-art performance across multiple graph datasets.
__label__machine_learning_for_other_sciences_and_fields Although various methods have been proposed to generate high-quality MSA under these conditions, they fall short in comprehensively capturing the intricate co-evolutionary patterns within MSA or require guidance from external oracle models.
__label__machine_vision Our approach begins with the formulation of a spike-guided deblurring model that explores the theoretical relationships among spike streams, blurry images, and their corresponding sharp sequences.
__label__diffusion_based_models Then, the image-compositing model is uniquely designed to process multiple types of input features, enabling it to perform high-fidelity image compositions that consider occlusion, depth blur, and image harmonization.
__label__graph_neural_networks Graph homophily refers to the phenomenon that connected nodes tend to share similar characteristics.
__label__natural_language_processing We provide empirical evidence supporting our claims on contemporary LLMs.
__label__reinforcement_learning Meta-RL is a potential solution, as unlike standard RL, meta-RL can *learn* to explore, and potentially learn highly complex strategies far beyond those of standard RL, strategies such as experimenting in early episodes to learn new skills, or conducting experiments to learn about the current environment.
__label__learning_theory While the convergence behavior of Langevin Dynamics under unimodal distributions has been extensively studied in the literature, in practice the data distribution could consist of multiple distinct modes.
__label__online_learning We consider the problem of online multi-agent Nash social welfare (NSW) maximization.
__label__diffusion_based_models We propose a transformer-based audiovisual latent diffusion model and show that it can be trained in a task-agnostic fashion using our approach to enable a variety of audiovisual generation tasks at inference time.
__label__graph_neural_networks In the era of big data, graphs have emerged as a natural representation of intricate relationships.
__label__learning_theory However, through construction of nontrivial counterexamples, we then establish that with proper choice of value matrices, a general class of sequences may not converge to a rank one subspace, and the self-attention dynamics with LayerNorm can simultaneously possess a rich set of equilibria with any possible rank between one and full.
__label__probabilistic_methods This paper introduces an innovative approach to classification called Credal Deep Ensembles (CreDEs), namely, ensembles of novel Credal-Set Neural Networks (CreNets).
__label__optimization For constrained, not necessarily monotone submodular maximization, all known approximation algorithms with ratio greater than $1/e$ require continuous ideas, such as queries to the multilinear extension of a submodular function and its gradient, which are typically expensive to simulate with the original set function.
__label__machine_vision We conduct experiments on the public RetargetMe benchmark and demonstrate through objective experimental results and subjective user studies that our method outperforms previous approaches in terms of preserving semantics and aesthetics, as well as better generalization across diverse aspect ratios.
__label__reinforcement_learning To improve exploration and reward discovery, popular algorithms rely on optimism.
__label__privacy Moreover, our theoretical analysis extends beyond Gaussian-like data distributions to settings with eigenvalue decay, showing how data distribution impacts learning in high dimensions.
__label__safety_in_machine_learning However, the labor-intensive nature of these benchmarks limits their test scope, hindering their ability to generalize to the extensive variety of open-world use cases and identify rare but crucial long-tail risks.
__label__machine_vision We release the model (base, instructed, and chat) along with the datasets created for its training.
__label__infrastructure We further propose Expert Buffering, a new caching mechanism that only keeps hot, active experts in GPU memory while buffering the rest in CPU memory.
__label__optimization When the neural network's output is a single scalar, we prove that these quadrics can have multiple connected components, limiting the set of reachable parameters during training.
__label__other Our code is available at https://github.com/bigdata-ustc/Coral.
__label__machine_learning_for_physical_sciences Our model integrates the dynamics-informed diffusion framework (DYffusion) with the Spherical Fourier Neural Operator (SFNO) architecture, enabling stable 100-year simulations at 6-hourly timesteps while maintaining low computational overhead compared to single-step deterministic baselines.
__label__natural_language_processing Editing their internal representations has been shown to be effective in mitigating such behaviours on top of the existing alignment methods.
__label__graph_neural_networks Our second contribution explicitly model nodes' pairwise relationships with a cross-attention mechanism, providing an accurate edge representation for dynamic link prediction.
__label__probabilistic_methods Recently, generative graph models have shown promising results in learning graph representations through self-supervised methods.
__label__safety_in_machine_learning Based on that, we formalize a new scoring method, namely, *Confidence aVerage* (CoVer), which can capture the dynamic differences by simply averaging the scores obtained from different corrupted inputs and the original ones, making the OOD and ID distributions more separable in detection tasks.
__label__generative_models Although existing methods have shown superior performance, most methods prefer to learn a single-modal codebook (\emph{e.g.
__label__privacy Overall, our auditing procedure can offer valuable insight into how the privacy analysis of DP-SGD could be improved and detect bugs and DP violations in real-world implementations.
__label__natural_language_processing High-quality preference data leads to improvements of up to 8% in instruction following and truthfulness.
__label__infrastructure Experimental results demonstrate that TinyTTA improves TTA accuracy by up to 57.6\%, reduces memory usage by up to six times, and achieves faster and more energy-efficient TTA.
__label__neuroscience_and_cognitive_science To this end, we extend a recent information-theoretic framework for emergent communication in artificial agents, which integrates utility maximization, associated with pragmatics, with general communicative constraints that are believed to shape human semantic systems.
__label__robotics We tackle (iii) by approximating the forcing-to-input mapping with a decoder that is trained to reconstruct the input based on the encoded latent space force.
__label__online_learning In addition, our results imply that in some cases the optimal randomized mistake bound is approximately the square-root of its deterministic parallel.
__label__causal_inference Extensive simulations across multiple domains, using synthetic and real network data, demonstrate the efficacy of our approach in estimating total treatment effect dynamics, even in cases where interference exhibits non-monotonic behavior in the probability of treatment.
__label__interpretability_and_explainability Large Language Models (LLMs) have the capacity to store and recall facts.
__label__probabilistic_methods However, standard GP models assume homoskedastic Gaussian noise, while many real-world applications are subject to non-Gaussian corruptions.
__label__machine_vision Multimodal models trained on modality-complete data are plagued with severe performance degradation when encountering modality-missing data.
__label__machine_learning_for_physical_sciences In experiments, we highlight the advantages of our method over transfer operator approaches  and recent developments based on generator learning, demonstrating its effectiveness in estimating eigenfunctions and eigenvalues.
__label__bandits We designed a novel concentration guarantee for estimating the score parameters of the PL model using `\emph{Pairwise Rank-Breaking}', which builds the foundation of our proposed algorithms.
__label__graph_neural_networks Consequently, task planning is a decision-making problem that involves selecting a connected path or subgraph within the corresponding graph and invoking it.
__label__machine_vision Code will be available soon.
__label__safety_in_machine_learning Federated Instruction Tuning (FIT) advances collaborative training on decentralized data, crucially enhancing model's capability and safeguarding data privacy.
__label__machine_learning_for_other_sciences_and_fields Therefore, in this study, we introduce frequency-domain information and design Frequency-aware Generative Models for Multivariate Time Series Imputation (FGTI).
__label__causal_inference We empirically verified the robustness of estimators through simulations.
__label__diffusion_based_models TwinAct addresses the limitations of existing methods that struggle to decouple actions from other semantics (e.g., the actor's appearance) due to the lack of an effective inductive bias with few exemplar images.
__label__safety_in_machine_learning The internal dialogue is a dual cooperative system between a safety-driven critic and a helpfulness-driven critic.
__label__natural_language_processing Positional encoding plays a crucial role in transformers, significantly impact- ing model performance and length generalization.
__label__optimization This protocol ensures the consistency among stepsizes of nodes, eliminating the steady-state error due to the lack of coordination of stepsizes among nodes that commonly exists in vanilla distributed adaptive methods, and thus guarantees exact convergence.
__label__interpretability_and_explainability However, counterfactual explanations can also be leveraged to reconstruct the model by strategically training a surrogate model to give similar predictions as the original (target) model.
__label__machine_learning_for_healthcare We demonstrate how to obtain improved MIL explanations using layer-wise relevance propagation (LRP) and conduct extensive evaluation experiments on three toy settings and four real-world histopathology datasets.
__label__reinforcement_learning Planning in real-world settings often entails addressing partial observability while aligning with users' requirements.
__label__optimization We also extend the proposed technique to stochastic compositional optimization, obtaining the same optimal rate of $\mathcal{O}(T^{-1/3})$.
__label__optimization_for_deep_networks Data heterogeneity is one of the key challenges in federated learning, and many efforts have been devoted to tackling this problem.
__label__natural_language_processing In-Context Learning (ICL) empowers Large Language Models (LLMs) to tackle various tasks by providing input-output examples as additional inputs, referred to as demonstrations.
__label__learning_theory Furthermore, we prove that our algorithm maintains a near-optimal $\widetilde{O}(\sqrt{T})$ regret even in the worst cases and extend these results to the setting where the request and external factor are continuous.
__label__interpretability_and_explainability Overall, SCoBots thus result in more human-aligned RL agents.
__label__machine_learning_for_healthcare Our implementation is available at https://github.com/IMOP-lab/U-Shaped-Connection.
__label__safety_in_machine_learning Code is publicly available at \url{https://github.com/arekavandi/Certified_adv_RRegression/}.
__label__machine_vision The recent advancements in 3D Gaussian splatting (3D-GS) have not only facilitated real-time rendering through modern GPU rasterization pipelines but have also attained state-of-the-art rendering quality.
__label__learning_theory For both learning rules, we prove overfitting is tempered.
"__label__neuroscience_and_cognitive_science One approach towards quantifying this similarity is by measuring how well a model predicts neural signals, also called ""brain score""."
__label__algorithmic_game_theory Building upon this setup, we extend the Coase theorem [Coase, 2013], which suggests that the optimal approach for maximizing the social welfare in the presence of externality is to establish property rights, i.e., enabling transfers and bargaining between the players.
__label__machine_learning_for_physical_sciences Our flexible encoder-decoder architecture can obtain smooth latent representations of spatial physical fields from a variety of data types, including irregular-grid inputs and point clouds.
__label__algorithmic_game_theory Non-concave games introduce significant game-theoretic and optimization challenges: (i) Nash equilibria may not exist; (ii) local Nash equilibria, though they exist, are intractable; and (iii) mixed Nash, correlated, and coarse correlated equilibria generally have infinite support and are intractable.
__label__machine_vision Although subsequent efforts rapidly extend static 3D Gaussian to dynamic scenes, they often lack explicit constraints on object motion, leading to optimization difficulties and performance degradation.
__label__learning_theory This disagrees with the general form of neural scaling laws, which predict that increasing model size monotonically improves performance.
__label__optimization_for_deep_networks Due to intractable bi-level optimization, the OBD objective is difficult to minimize to small values, which deteriorates PBC by its distillation performance guarantee with quadratic discount complexity $\mathcal{O}(1/(1-\gamma)^2)$.
__label__machine_vision This is implemented by representing the image and kernel with implicit neural representations (INRs), whose resolution-free property enables consistent yet efficient computation for network training across multiple scales.
__label__robotics Then, we propose a two-stage codebook to discretize these features from coarse to fine levels.
__label__optimization Finally, we demonstrate this flexibility of our framework by presenting how it naturally addresses the important case of binary outcomes, which has received far less attention by recent developments in the NPIV literature.
__label__optimization_for_deep_networks It achieves a notable performance improvement of over 50\% in terms of accuracy regret and exploration time.
__label__machine_vision The LLM is responsible for understanding the user's text instructions and providing text responses and pixel-level segmentation results based on the visual information.
__label__deep_learning_architectures Multimodal Large Language Models (MLLMs) have demonstrated remarkable comprehension and reasoning capabilities with complex language and visual data.
__label__deep_learning_architectures Dense Associative Memories are high storage capacity variants of the Hopfield networks that are capable of storing a large number of memory patterns in the weights of the network of a given size.
__label__fairness Next, we empirically support our theory using experiments on both semi-synthetic and real-world fairness datasets.
__label__other For these kernels, taking $\epsilon$ to be a constant and $b=\Theta(\log n)$, our algorithm terminates within $O(1)$ iterations where each iteration takes time $O(n(\log n+k))$.
__label__graph_neural_networks Some models use polynomials with better approximation for approximating filters, yet perform worse on real-world graphs.
__label__neuroscience_and_cognitive_science Latent dynamical systems have been widely used to characterize the dynamics of neural population activity in the brain.
__label__learning_theory For the special case of subsampling without replacement, our results apply to independently sampled random features and kernel features and confirm recent conjectures (Conjectures 7 and 8) of the authors on the existence of such paths in Patil and Du (2023).
__label__optimization Split federated learning (SFL) is a recent distributed approach for collaborative model training among multiple clients.
__label__neuroscience_and_cognitive_science To address these issues, we propose a novel Stepwise Weighted Spike (SWS) coding scheme to enhance the encoding of information in spikes.
__label__generative_models }, Semantic Alignment Module, and Relationship Alignment Module) to transfer such prior knowledge into codes for achieving codebook text alignment.
__label__machine_vision Complex 3D scene understanding has gained increasing attention, with scene encoding strategies built on top of visual foundation models playing a crucial role in this success.
__label__other BeTop is derived from braid theory to distill compliant interactive topology from multi-agent future trajectories.
__label__optimization We provide competitive ratio bounds for DISC under both linear mixing of latent variables as well as a broader class of mixing functions.
__label__machine_vision Concretely, to estimate the instance’s trajectory, we track a group of points on the instance and aggregate their motion trajectories.
__label__machine_vision Recent advances in generative AI, e.g., diffusion models, have enabled more sophisticated augmentation techniques that produce data resembling natural images.
__label__neuroscience_and_cognitive_science The vertebrate hippocampus is thought to use recurrent connectivity in area CA3 to support episodic memory recall from partial cues.
__label__deep_learning_architectures However, modifying such objects over time is challenging.
__label__safety_in_machine_learning To address this problem, we propose Kernel Language Entropy (KLE), a novel method for uncertainty estimation in white- and black-box LLMs.
__label__optimization_for_deep_networks Observing the tensor contractions required to compute them, we propose a method with minimal FLOPs in 3D or greater tensor regimes by simultaneously computing the norms while computing the parameter gradients.
__label__online_learning We discuss implications on various aspects of online decision making such as: distributed systems, advice complexity and transaction costs, suggesting broad applicability.
__label__fairness To navigate this issue, we frame the resolution of hypergradient conflicts as a multi-player cooperative bargaining game.
__label__machine_vision Our framework derives the class-balanced contrastive loss through Gaussian kernel density estimation.
__label__natural_language_processing However, in this paper, we find that LLMs may obtain a superficial long-context ability based on the OOD theory.
__label__deep_learning_architectures The code of the experiments is available at https://github.com/DaShenZi721/HRA, and the method has been merged into the [PEFT](https://github.com/huggingface/peft) package.
__label__machine_vision However, their application on object detection is largely overlooked, especially without fine-tuning them.
__label__graph_neural_networks To address this question, this paper presents a theoretical analysis of the sample complexity of such GNNs from a statistical learning perspective, employing Vapnik–Chervonenkis (VC) dimension and covering number bounds.
__label__graph_neural_networks To this end, during the encoding process, we commence by utilizing the hard node assignment to decompose a sample graph into a family of separated subgraphs.
__label__natural_language_processing Next, we devise a search-based inverse transformation to transform the aggregated result back to the probability space of one of the ensembling LLMs (main model), in order to determine the next token.
__label__interpretability_and_explainability However, other researchers showed that these classifiers may fail to generalise, for example to negated statements.
__label__generative_models With Monarch matrices, Kronecker factorizations, and post-training quantization, we achieve non-vacuous generalization bounds for LLMs as large as LLaMA2-70B.
__label__diffusion_based_models However, most existing approaches are limited to linear inverse problems represented as Stochastic Differential Equations (SDEs).
__label__neuroscience_and_cognitive_science During natural evolution, the primary visual cortex (V1) of lower mammals typically forms salt-and-pepper organizations, while higher mammals and primates develop pinwheel structures with distinct topological properties.
__label__machine_vision We derive differentiable rasterization on top of AlphaTablets to efficiently render 3D planes into images, and propose a novel bottom-up pipeline for 3D planar reconstruction from monocular videos.
__label__machine_learning_for_physical_sciences Our results provide theoretical justification for why regression methods for generators of dynamical systems (Neural ODEs) fail to generalize, and why their statistical accuracy improves upon adding Jacobian information during training.
__label__other Our experiments highlighted the trade-offs between MDP rewards and receiver accuracy across various compression rates, showcasing the efficacy of our method compared to conventional compression baseline.
__label__natural_language_processing A conventional LLM unlearning task typically involves two goals: (1) The target LLM should forget the knowledge in the specified forget documents; and (2) it should retain the other knowledge that the LLM possesses, for which we assume access to a small number of retain documents.
__label__safety_in_machine_learning It is worth noting that $\textsf{Safe LoRA}$ is a training-free and data-free approach, as it only requires the knowledge of the weights from the base and aligned LLMs.
__label__natural_language_processing Specifically, SignCLachieves a significant improvement in BLEU score for the Sign Language Transformer and GFSLT-VLP on the CSL-Daily dataset by 39\% and 46\%, respectively, without any increase of model parameters.
__label__reinforcement_learning These interactions, influenced by individual-specific factors ranging from personal preferences to physiological differences, can causally affect state transitions, such as the health conditions in healthcare or learning progress in education.
__label__machine_learning_for_other_sciences_and_fields An implementation of LMI is available at *latentmi.readthedocs.io.
__label__safety_in_machine_learning Password-locked models enable a novel method of evaluating capabilities elicitation methods, by testing whether these password-locked capabilities can be elicited without using the password.
__label__interpretability_and_explainability To circumvent this problem, we propose new optimization methods by constructing and minimizing surrogate functions that exploit hidden mathematical structures of the CPH model.
__label__generative_models Additionally, we leverage the static 3D model’s multi-view renderings as conditions to preserve its identity.
__label__deep_learning_architectures The transformer architecture by Vaswani et al.
__label__learning_theory Specifically, there is a regime where it is possible to construct in polynomial time an accurate SSL classifier.
__label__evaluation In particular, we demonstrate a phenomenon that we call post-hoc reversal, where performance trends are reversed after applying post-hoc transforms.
__label__bandits We derive a bound on the maximum information gain of these invariant kernels, and provide novel upper and lower bounds on the number of observations required for invariance-aware BO algorithms to achieve $\epsilon$-optimality.
__label__natural_language_processing Constrained decoding approaches mitigate this problem by greedily restricting what tokens an LLM can output at each step to guarantee that the output matches a given constraint.
__label__diffusion_based_models Our comprehensive experiments confirm the effectiveness of both VideoPrefer and VideoRM, representing a significant step forward in the field.
__label__natural_language_processing However, these methods fail to allocate adaptive layer-wise sparsities, leading to performance degradation in challenging tasks.
__label__learning_theory In such scenarios, the operators map between Banach or Hilbert spaces.
__label__bandits As estimating the coefficients of a covariance matrix can be manageable in practice, leveraging them should improve the regret.
__label__optimization Bilevel Optimization has experienced significant advancements recently with the introduction of new efficient algorithms.
__label__machine_vision Image fusion aims to integrate complementary information from multiple input images acquired through various sources to synthesize a new fused image.
__label__reinforcement_learning Our result builds off a new perspective on the role off-policy evaluation guarantees and coverage coefficient in LMDPs, a perspective, which has been overlooked in the context of exploration in partially observed environments.
__label__machine_learning_for_other_sciences_and_fields To fulfill this purpose, we propose ATProt, an adversarial training framework for protein representations to robustly defend against the attack of protein flexibility.
__label__machine_vision The key challenge is to regularize the training to prevent over-fitting due to the absence of GT images.
__label__neuroscience_and_cognitive_science In this work, we systematically explore the divergences between human and machine language processing by examining the differences between LM representations and human brain responses to language as measured by Magnetoencephalography (MEG) across two datasets in which subjects read and listened to narrative stories.
__label__diffusion_based_models In a great number of tasks in science and engineering, the goal is to infer an unknown image from a small number of noisy measurements collected from a known forward model describing certain sensing or imaging modality.
__label__probabilistic_methods However, deploying these learning algorithms in safety-critical applications raises concerns, particularly when it comes to ensuring confidence calibration for both in-distribution and out-of-distribution data points.
__label__natural_language_processing First, it has a few-shot self-verification mechanism, which estimates the reliability of its own outputs without requiring extensive training.
__label__privacy In this paper, we propose ADV-TRA, a more robust fingerprinting scheme that utilizes adversarial trajectories to verify the ownership of DNN models.
__label__optimization However, as the number of objectives increases, the cost of learning and surrogation, as well as the difficulty of maintaining solution diversity, increases rapidly.
__label__optimization Moreover, the accuracy condition for solving the subproblem is milder, leading to enhanced local computation efficiency.
__label__natural_language_processing Our code is publicly available at https://github.com/bigai-nlco/cream.
__label__evaluation Based on MixEval, we further build MixEval-Hard, which offers more room for model improvement.
__label__machine_learning_for_social_sciences Our novel approach transforms the latent feature mining task to a text-to-text propositional reasoning task.
__label__generative_models Our method couples an arbitrary source distribution to a specified target distribution through a triangular COT plan, and a conditional generative model is obtained by approximating the geodesic path of measures induced by this COT plan.
__label__learning_theory Thus, the NTK theory may not explain the superior performance of neural networks.
__label__natural_language_processing The task we use to illustrate our method is predicting the characters in TV series, based on their lines in the show.
__label__robotics To minimize the interpolation errors associated with direct coordinate transformations, we also devise a spherical-based feature encoding followed by spherical-to-Cartesian feature aggregation.
__label__optimization Specifically, we show that when the stochastic gradients are light-tailed, the smoothed alternating GDA method can compute an $\varepsilon$-stationary point within $\mathcal{O}(\frac{\ell \kappa^2 \delta^2}{\varepsilon^4} + \frac{\kappa}{\varepsilon^2}(\ell+\delta^2\log({1}/{\bar{q}})))$ stochastic gradient calls with probability at least $1-\bar{q}$ for any $\bar{q}\in(0,1)$, where $\mu$ is the PL constant, $\ell$ is the Lipschitz constant of the gradient, $\kappa=\ell/\mu$ is the condition number, and $\delta^2$ denotes a bound on the variance of stochastic gradients.
__label__machine_learning_for_other_sciences_and_fields Finally, extensive experiments on real-world datasets showcase the efficacy of our framework in addressing the data sparsity issue with accurate and fair CD results.
__label__learning_theory We provide theoretical guarantees that ensure both optimization consistency and statistical accuracy.
__label__other Motivated by the success of collaborative modeling in various domains, such as recommender systems, we aim to investigate how collaborative signals among learners contribute to the diagnosis of human cognitive states (i.e., knowledge proficiency) in the context of intelligent education.
__label__bandits In this study, we consider multi-class multi-server asymmetric queueing systems consisting of $N$ queues on one side and $K$ servers on the other side, where jobs randomly arrive in queues at each time.
__label__reinforcement_learning We show theoretically that the method exhibits sublinear regret in the number of training tasks and discuss conditions to further tighten regret bounds.
__label__interpretability_and_explainability Our numerical experiments demonstrate that this approach accurately recovers underlying signals, enhances predictive performance, and performs favorably compared to competing methods.
__label__generative_models Our main insight is that under realistic settings, a single iteration of the Reflow algorithm for training rectified flows is sufficient to learn nearly straight trajectories; hence, the current practice of using multiple Reflow iterations is unnecessary.
__label__natural_language_processing Our method, Rule Based Rewards (RBR), uses a collection of rules for desired or undesired behaviors (e.g.
__label__machine_vision In this paper, we propose debLoRA, a generic training approach that works with any LoRA variants to yield debiased features.
"__label__privacy This privacy budget is in turn interpreted in terms of operational attack risks, such as accuracy, sensitivity, and specificity of inference attacks aimed to recover
information about the training data records."
__label__optimization Experimental results on constrained traveling salesman problems, partial graph matching with outliers, predictive portfolio allocation and power system unit commitment demonstrate the advantages of GLinSAT over existing satisfiability layers.
__label__machine_vision To this end, we make the following contributions: (i) We introduce a general and lightweight protocol to evaluate whether features of an off-the-shelf large vision model encode a number of physical ‘properties’ of the 3D scene, by training discriminative classifiers on the features for these properties.
__label__other The trade-off between cost and performance has been a longstanding and critical issue for deep neural networks.
__label__graph_neural_networks To bridge this gap, we posit that, by harnessing the orthogonality of input vectors, pure message-passing can indeed capture joint structural features.
__label__privacy However, prior work has shown that the data can actually be recovered by the server using so-called gradient inversion attacks.
__label__graph_neural_networks For heterophilic graphs, achieving balance based on relative gradients leads to faster training and better generalization.
__label__reinforcement_learning Finally, we devise a unifying framework for IRL and RFE that may be of independent interest.
__label__machine_learning_for_healthcare In this work, we present an innovative molecular pretraining model that leverages a two-track transformer to effectively integrate features at the atomic level, graph level, and geometry structure level.
__label__natural_language_processing This frequently leads to degenerate policies, sometimes causing even the probability of the preferred output to go to zero.
__label__machine_vision We demonstrate that our LRM-Zero, trained with our fully synthesized Zeroverse, can achieve high visual quality in the reconstruction of real-world objects, competitive with models trained on Objaverse.
__label__reinforcement_learning In embodied tasks, high-level planning is amenable to direct coding, while low-level actions often necessitate task-specific refinement, such as Reinforcement Learning (RL).
__label__speech_and_audio Finally, we develop a self-supervised pre-training method within our framework, proving its effectiveness alongside our semi-supervised approach.
__label__safety_in_machine_learning However, the vulnerability of these networks to adversarial attacks, primarily evasion-based, poses a concerning threat to their functionality.
__label__interpretability_and_explainability Using vector quantization, we show that time-series from different domains can be described using a unified set of low-dimensional codes, where each code can be represented as an abstracted shape in the time domain.
__label__neuroscience_and_cognitive_science Second, we replace the last Leaky Integrate-and-Fire (LIF) activation layer as the ReLU activation layer to generate the output feature, thus a more powerful SNN with full-precision feature representation can be achieved but with only a little extra computation.
__label__reinforcement_learning One commonly used idea to avoid the overestimation of Q-value is to make a pessimistic adjustment.
__label__probabilistic_methods We design a walk that mixes in $\widetilde O((nd+dL^2R^2)\log(w/\delta))$ steps with a per iteration cost of $\widetilde O(n^\omega+n^2d^{3\omega-5})$.
__label__learning_theory AdaBoost is a well-known algorithm in boosting.
__label__reinforcement_learning However, it remains a challenge due to the domain gap between humans and robots.
__label__neuroscience_and_cognitive_science For example, switching and decomposed models describe complex systems using latent variables that evolve according to simple locally linear dynamics.
__label__reinforcement_learning We present Multi-Agent Divergence Policy Optimization (MADPO), equipped with mutual policy divergence maximization framework.
__label__reinforcement_learning However, outside of specific applications, existing approaches for interacting with unknown environments are not well-adapted to these observations.
__label__speech_and_audio This problem is crucial for practical visually guided auditory perception as it can significantly enhance the adaptability and robustness of audio-visual sound separation models, making them more applicable for real-world scenarios where encountering new sound sources is commonplace.
__label__optimization_for_deep_networks This exploration is beneficial for finding good loss basins when training from scratch.
__label__machine_learning_for_physical_sciences Our approach also maintains continuous access to fine-grained modes and eliminates the need for force-matching, enhancing both the efficiency and accuracy of energy minimization.
__label__learning_theory We explore these problems in a setting where a predicted data distribution, possibly derived from historical data or predictive machine learning models, is available.
__label__optimization We develop a theoretically-grounded learning method for the Generalized Linear Programming Value Function (GVF), which models the optimal value of a linear programming (LP) problem as its objective and constraint bounds vary.
__label__graph_neural_networks In this paper, we revisit these frameworks and reveal a common mechanism—representation scattering—that significantly enhances their performance.
__label__natural_language_processing We present AMOR, an agent framework based on open-source LLMs, which reasons with external knowledge bases and adapts to specific domains through human supervision to the reasoning process.
__label__safety_in_machine_learning This technique effectively bridges the gap between discrete and continuous space optimization.
__label__learning_theory However, when labels are noisy, such queries are too fragile and lead to high query complexity even under the simple random classification noise model.
__label__machine_vision However, these models require predefined object categories as inputs during inference, which are not available in real-world scenarios.
__label__machine_learning_for_other_sciences_and_fields In real-world applications, this assumption often fails due to sensor malfunctions or data loss, making TSC with missing data a critical challenge.
__label__probabilistic_methods This paper presents a new gradient flow dissipation geometry over non-negative and probability measures.
__label__machine_vision For each input AV data pair, our AVAgent uses a multi-modal LLM to convert audio and visual data into language descriptions separately (i.e., tool use).
__label__machine_vision A simple idea is to scan on support features to selectively compress them into the hidden state, which is then used as the initial hidden state to sequentially scan query features.
__label__generative_models Through analysis using Principal Component Analysis (PCA), our work discloses that robust motion-aware feature already exists in video diffusion models.
__label__fairness In many large-scale settings, utilities are not known in advance, but are instead observed after realizing the allocation.
__label__machine_vision The code is released at https://github.com/Franpin/TopoLogic.
__label__deep_learning_architectures A framework to construct synthetic data with different common and complementary information is also developed to compare MVRL methods comprehensively.
__label__machine_vision State-of-the-art image restoration methods currently face challenges in terms of computational requirements and performance, making them impractical for deployment on edge devices such as phones and resource-limited devices.
__label__neuroscience_and_cognitive_science A common source of anxiety for the computational neuroscience student is the question “will my recurrent neural network (RNN) model finally learn that task?”.
__label__bandits To solve RMAB-G, we develop the Linear-Whittle and Shapley-Whittle indices, which extend Whittle indices from RMABs to RMAB-Gs.
__label__reinforcement_learning The *character* of an agent is defined as a different weight combination on reward components, representing distinct behavioral preferences.
__label__safety_in_machine_learning To enable dynamic weight learning in the semantically rich latent space, we introduce a novel approach called Conformalized Time Series with Semantic Features (CT-SSF).
__label__other Besides, the global knowledge anchored alignment module can make the local representation spaces close to the global spaces, which further improves the representation abilities of local ones.
__label__deep_learning_architectures This paper reveals the phenomenon of parameter heterogeneity in large language models (LLMs).
__label__deep_learning_architectures MambaNOs are evaluated on a diverse set of benchmarks with possibly multi-scale solutions and set new state-of-the-art scores, yet with fewer parameters and better efficiency.
__label__learning_theory We analyze mathematically this method under several classical statistical models, and validate our findings empirically on datasets from different domains.
__label__machine_vision Through extensive experiments, our model achieves superior performance and higher efficiency on long video benchmarks, showcasing precise temporal comprehension for detailed question answering.
__label__learning_theory Multi-objective reinforcement learning (MORL) is an excellent framework for multi-objective sequential decision-making.
__label__optimization_for_deep_networks Motivated by the gradient inconsistency observation, we propose a simple yet effective \underline{G}radient \underline{R}ewiring method for \underline{E}ditable graph neural network training, named \textbf{GRE}.
__label__evaluation Detailed error profiles are provided for a better understanding of LLMs' behavior.
__label__natural_language_processing Furthermore, it eliminates the recurring costs of paraphrase generation for each knowledge update.
__label__infrastructure Extensive experiments have demonstrated that FM-Delta efficiently reduces cloud storage consumption for massive fine-tuned models by an average of around 50% with only negligible additional time in most end-to-end cases.
"__label__natural_language_processing Using this dataset, we train a 7B LLM called Janus and test it on 921 prompts from 5 benchmarks (AlpacaEval 2.0, FLASK, Koala, MT-Bench, and Self-Instruct)
by adding system messages that reflect unseen user values."
__label__graph_neural_networks Our experiments demonstrate that even with only query access to molecular pre-trained models, there is a considerable risk of extracting training data, challenging the assumption that model sharing alone provides adequate protection against data extraction attacks.
__label__natural_language_processing Most instruction tuning data come from human crowd-sourcing or GPT-4 distillation.
__label__probabilistic_methods Compared to Deep Ensemble baselines, CreDEs demonstrate higher test accuracy, lower expected calibration error, and significantly improved epistemic uncertainty estimation.
__label__reinforcement_learning By analyzing $Q$-function over-generalization, which impairs stable stitching, QCS adaptively integrates $Q$-aid into RCSL's loss function based on trajectory return.
__label__natural_language_processing Through extensive experiments, we demonstrate that in addition to reducing segmentation disparities, MAGNET also enables faster language modeling and improves downstream utility.
__label__machine_vision We hypothesise that this symbolic-continuous mismatch produces broadly suboptimal performance in deep learning models that learn or use such representations.
__label__machine_vision However, the emergence of open-world SSL (OwSSL) introduces a more practical challenge, wherein unlabeled data may encompass samples from unseen classes.
__label__machine_learning_for_healthcare Multimodal representation learning techniques typically require paired samples to learn shared representations, but collecting paired samples can be challenging in fields like biology, where measurement devices often destroy the samples.
__label__safety_in_machine_learning However, the consistency model is designed not for purification thus it does not inherently ensure semantic alignment between purified and original images.
__label__deep_learning_architectures Besides, it's worth noting that our MH-MoE is straightforward to implement and decouples from other SMoE frameworks, making it easy to integrate with these frameworks for enhanced performance.
__label__other We present Sketch Structured Transform (SS1), an expressive and GPU-friendly operator that accelerates inference.
__label__machine_vision Existing human rendering methods require every part of the human to be fully visible throughout the input video.
__label__reinforcement_learning Typically, in offline reinforcement learning, the data is provided by various experts and some of them can be sub-optimal.
__label__diffusion_based_models Our experiments show that our RGBA diffusion model is capable of generating diverse and high quality instances with precise control over object attributes.
__label__machine_vision On top of the group-equivariant spatial feature that selectively detects objects similar to the input group, we additionally introduce an explorative group update strategy that reduces the false negative detection in the target domain, further reducing the inter-domain gap.
__label__reinforcement_learning These guarantees imply that LoRa-PI learns an $\varepsilon$-optimal policy using $\tilde{\cal O}({(S+A)\over \mathrm{poly}(1-\gamma)\varepsilon^2})$ samples where $S$ (resp.
__label__privacy Numerical studies further confirm the effectiveness of our algorithm.
__label__natural_language_processing Our method is conceptually simple and computationally lightweight, as it incurs minimal latency compared to greedy decoding.
__label__machine_vision Unlike previous approaches, our tuning framework does not rely on any proxy and treats the photo finishing pipeline as a black box.
__label__optimization The current methods for estimating the nadir objective vector perform effectively only on specific MOPs.
__label__other Recent work has shown that methods that regularize second order information like SAM can improve generalization in deep learning.
__label__machine_learning_for_physical_sciences We present comparisons against different baselines to showcase significantly increased accuracy in both the inverse and the surrogate tasks.
__label__other Ontologies are useful for automatic machine processing of domain knowledge as they represent it in a structured format.
__label__optimization For instance, although the gradient clipping threshold is a crucial hyperparameter in addition to the stepsize for preventing gradient explosion issues, none of the existing studies have investigated parameter-free methods for clipped gradient descent.
__label__interpretability_and_explainability In fact, it has been shown that noise in data generation processes leads practitioners to find simpler models.
__label__diffusion_based_models Lastly, the ID-preserved Scannability Enhancement (IDSE) markedly boosts scanning robustness through latent code optimization, striking a delicate balance between face ID, aesthetic quality and QR functionality.
__label__machine_vision To this end, we present a novel perspective on the occupancy prediction task: formulating it as a streamlined set prediction paradigm without the need for explicit space modeling or complex sparsification procedures.
__label__natural_language_processing Our experiments using seed models ranging from 590 million to 2 billion parameters show that our approach outperforms state-of-the-art approaches under the same data and compute budget in both perplexity and downstream tasks evaluations, confirming the effectiveness of BAM.
__label__graph_neural_networks Processing multidomain data defined on multiple graphs holds significant potential in various practical applications in computer science.
"__label__optimization We demonstrate that AdaGrad stepsizes work in a variety of situations
by proving, in a unified manner, three types of new results."
__label__online_learning In this paper, inspired by the black-box reduction of Cutkosky and Orabona [2018], we employ a surrogate loss defined over simpler domains to develop universal OCO algorithms that only require $1$ projection.
__label__natural_language_processing Our estimators use LLM conditional distributions (over variables of interest, given the text data) to assist in the computation of classical estimators of causal effect.
__label__safety_in_machine_learning Personalized Federated Graph Learning (pFGL) facilitates the decentralized training of Graph Neural Networks (GNNs) without compromising privacy while accommodating personalized requirements for non-IID participants.
__label__machine_learning_for_other_sciences_and_fields In this work, we create mma, a large, flexible, multi-language, and multi-domain dataset of informal-formal pairs, by using a language model to translate in the reverse direction, that is, from formal mathematical statements into corresponding informal ones.
__label__machine_vision Shape from polarization (SfP) benefits from advancements like polarization cameras for single-shot normal estimation, but its performance heavily relies on light conditions.
__label__reinforcement_learning The ability to approach the same problem from different angles is a cornerstone of human intelligence that leads to robust solutions and effective adaptation to problem variations.
__label__neuroscience_and_cognitive_science Unlike in machine learning where any architectural modification of an RNN (e.g.
__label__machine_vision However, the task is fundamentally hindered by the computation demands for brute-force correspondence matching across the frames.
__label__machine_learning_for_physical_sciences Our method is built upon minimal inductive biases, encompassing not only commonly utilized symmetries rooted in Lie groups but also extending to symmetries derived from nonlinear generators.
__label__human-AI_interaction These agents themselves are a single policy network, which is $\textbf{insufficient for handling non-stationary human dynamics}$.
__label__safety_in_machine_learning Before deploying outputs from foundation models in high-stakes tasks, it is imperative to ensure that they align with human values.
__label__diffusion_based_models BIR-D is able to fulfill multi-guidance blind image restoration.
__label__machine_learning_for_other_sciences_and_fields To address this, we propose a novel forecasting model with a structured matrix basis.
__label__machine_vision Crucially, it delivers high-quality results in 4 minutes, which is 12$\times$ faster than NeRF-based methods and on par with traditional algorithms.
__label__machine_learning_for_physical_sciences We propose the first quantum parametric circuit for 3-D molecule generation for its potential quantum advantage especially considering the arrival of Noisy Intermediate-Scale Quantum (NISQ) era.
__label__causal_inference Augmenting the celebrated GAIL method (Ho \& Ermon, 2016), our analysis leads to two novel causal imitation algorithms that can obtain effective policies guaranteed to achieve expert performance.
__label__optimization_for_deep_networks Scale has become a main ingredient in obtaining strong machine learning models.
__label__generative_models Recent works address the problem and generate better 3D results either by finetuning a multi-view diffusion model or training a fast feed-forward model.
__label__graph_neural_networks However, existing research overlooks a key factor: the reliability of the graph structure.
__label__deep_learning_architectures Following this, we assign confidence levels to the initial imputations by correlating missing data with valid data.
__label__machine_learning_for_healthcare Multimodal learning has gained increasing importance across various fields, offering the ability to integrate data from diverse sources such as images, text, and personalized records, which are frequently observed in medical domains.
__label__machine_vision Moreover, the comparison results further demonstrate the effectiveness of the proposed physics-constrained ONN learning framework over state-of-the-art ONN approaches.
__label__natural_language_processing While existing methods typically focus either on training KGE models solely based on graph structure or fine-tuning pre-trained language models with classification data in KG, KG-FIT leverages LLM-guided refinement to construct a semantically coherent hierarchical structure of entity clusters.
__label__reinforcement_learning Consequently, different individuals may exhibit different state-transition processes.
__label__bandits We further explicate this geometry for Gaussian distributions of rewards, and provide a convex reformulation of the lower bound solvable with linear programming.
__label__fairness We also provide some theoretical analysis of the generalization error bound, and based on this bound we give a strategy to set the hyper-parameter, which makes the proposed methods easy to use.
__label__learning_theory In particular, we identify a critical explaining-away effect in E-SSL that creates a synergy between the equivariant and classification tasks.
__label__bandits minimizing the overall regret) while paying little attention to the game-theoretic properties such as the stability of the final matching.
__label__online_learning While recent algorithms have tried to address scenarios with non-small or general bids, they often rely on the Fractional Last Matching (FLM) assumption, which allows for accepting partial bids when the remaining budget is insufficient.
__label__reinforcement_learning This feedback can be system generated or elicited from a human observing the training process.
__label__probabilistic_methods Theoretically, we prove the consistency of the estimator we propose.
__label__machine_vision The denoised text descriptions help OVAR models classify visual samples more accurately; in return, assigned visual samples help better denoising.
__label__generative_models The predominant generative modeling paradigm for discrete data is still autoregressive, with more recent alternatives based on diffusion or flow-matching falling short of their impressive performance in continuous data settings, such as image or video generation.
__label__reinforcement_learning Our framework leverages a joint-level input-output representation to unify the state and action spaces of heterogeneous embodiments and employs a novel structure-motion state encoder that is parameterized to capture both shared knowledge across all embodiments and embodiment-specific knowledge.
__label__safety_in_machine_learning For example, in our evaluation of seven open-source LLMs, we observe an average attack success rate of 99.14%, based on the classic keyword-matching criterion.
__label__safety_in_machine_learning Addressing these limitations, we exploit a novel backdoor attack called ManiBCI, where the adversary can arbitrarily manipulate which target class the EEG BCI will misclassify without engaging the training stage.
__label__machine_vision Spike cameras, sampling at rates up to 40,000 Hz, have proven effective in capturing motion features and beneficial for solving this ill-posed problem.
__label__natural_language_processing Language models (LMs) only pretrained on a general and massive corpus usually cannot attain satisfying performance on domain-specific downstream tasks, and hence, applying domain-specific pretraining to LMs is a common and indispensable practice.
__label__causal_inference Existing methods are largely limited to point estimates of potential outcomes with no uncertain quantification; thus, the full information about the distributions of potential outcomes is typically ignored.
__label__optimization This setting has one of the simplest algorithms with a point prediction, but what happens if the prediction is a distribution?
__label__bandits Our algorithm achieves a provably near-optimal competitive ratio of $O(\log(\eta_{\max}/\eta_{\min}))$, with a matching lower bound provided.
__label__diffusion_based_models The proposed method enables the estimation of instance-dependent transition matrix and extends the application of transition matrix method to a broader range of noisy label data.
__label__machine_learning_for_physical_sciences Specifically, we establish that the NTK yields a random matrix at initialization that is not constant during training, contrary to conventional belief.
__label__generative_models OmniTokenizer, for the first time, handles both image and video inputs within a unified framework and proves the possibility of realizing their synergy.
__label__other However, these methods still struggle to capture distribution variations, due to the complex time patterns of time series in the time domain.
__label__generative_models We present two air drawing datasets to study this problem.
__label__reinforcement_learning For our first algorithm, $\texttt{RRL-MNL}$, we adapt optimistic sampling to ensure the optimism of the estimated value function with sufficient frequency and establish that $\texttt{RRL-MNL}$ is both _statistically_ and _computationally_ efficient, achieving a $\tilde{\mathcal{O}}(\kappa^{-1} d^{\frac{3}{2}} H^{\frac{3}{2}} \sqrt{T})$ frequentist regret bound with constant-time computational cost per episode.
__label__machine_vision Therefore, we further introduce a reflection-aware surface model to initialize the geometry and refine it during inverse rendering.
__label__fairness (KDD 2022).
__label__deep_learning_architectures In the coarse-grained stage, we design a context-aware expert routing strategy to dynamically select the most suitable vision experts according to the user instruction, input image, and expertise of vision experts.
__label__machine_vision Inspired by the recent success of compression learning in natural language processing, we propose a novel vision model pre-training method called Latent Compression Learning (LCL) for interleaved image-text data.
__label__optimization_for_deep_networks The code can be found at: https://github.com/Zekun-Cai/Koodos.
__label__diffusion_based_models The proposed method leverages the (conditional) optimal transport theory to learn the probability flow in a simulation-free manner, in which the initial noise, missing data, and observations are treated as the source distribution, target distribution, and conditional information, respectively.
__label__machine_learning_for_other_sciences_and_fields Moreover, we demonstrate leveraging the feedback from AlphaFold2 (AF2) can further enhance the model’s capacity via Rejective Fine-tuning (RFT) and Reinforcement Learning from AF2 Feedback (RLAF).
__label__natural_language_processing Through extensive experiments on the Anthropic HH and TL;DR summarization datasets, we verify the effectiveness of AdvPO in mitigating the overoptimization problem, resulting in enhanced RLHF performance as evaluated through human-assisted evaluation.
__label__machine_vision In this paper, we propose SCGaussian, a Structure Consistent Gaussian Splatting method using matching priors to learn 3D consistent scene structure.
__label__diffusion_based_models In this work, we propose a novel framework that enables learning the 3D image prior through position-aware 3D-patch diffusion score blending for reconstructing large-scale 3D medical images.
__label__reinforcement_learning Empirical studies verify the effectiveness of the proposed framework.
"__label__optimization_for_deep_networks Because sparse matrix multiplication is much less efficient than dense matrix multiplication on GPUs, most
implementations simulate sparsity by masking weights."
__label__optimization_for_deep_networks Existing attempts for the GCIL either have poor performance or invade data privacy by saving exemplars.
__label__generative_models The ability to invent novel and interesting problems is a remarkable feature of human intelligence that drives innovation, art, and science.
__label__machine_learning_for_other_sciences_and_fields Anomaly detection in time series data is fundamental to the design, deployment, and evaluation of industrial control systems.
__label__machine_vision Evaluated on two indoor radar perception datasets, our approach outperforms existing state-of-the-art methods by a margin of 15.38+ AP for object detection and 11.77+ IoU for instance segmentation, respectively.
__label__reinforcement_learning Utilizing a local-access model, we propose a novel primal-dual algorithm that, after $\tilde{O}(\text{poly}(d) \epsilon^{-3})$ iterations, outputs with high probability a policy that strictly satisfies the constraints while nearly optimizing the value with respect to a reward function.
__label__natural_language_processing In the classical RLHF framework, a reward model is first trained to represent human preferences, which is in turn used by an online reinforcement learning (RL) algorithm to optimized the LLM.
__label__natural_language_processing Our theoretical insights show that a single metric (a measure of the intrinsic dimension of the model's features) can be used to assess a model's editability and reveals its previously unrecognised susceptibility to malicious *stealth attacks*.
__label__other As part of our evaluation, we propose the Deictic Visual Genome (DeiVG) dataset, containing paired visual input and complex, deictic textual prompts.
__label__optimization_for_deep_networks It is not necessarily ideal when resuming from a powerful foundation model because it can lead to large deviations from the pre-trained initialization and, consequently, worse robustness and generalization.
__label__machine_learning_for_physical_sciences We introduce multiple physics pretraining (MPP), an autoregressive task-agnostic pretraining approach for physical surrogate modeling of spatiotemporal systems with transformers.
__label__other We conduct a comprehensive open-world evaluation on 31 test sets, covering 7 Generative Adversarial Networks, 18 (variants of) Diffusion Models, and another 6 CNN-based generative models.
__label__generative_models This approach not only enhances the utilization of original expert data but also broadens the learning space of the model.
"__label__interpretability_and_explainability We evaluate
our proposed method on three benchmark datasets and a real-life dataset introduced
by us, and extensive experiments demonstrate its effectiveness in interpreting GNN
models in regression tasks."
__label__deep_learning_architectures We first show that strongly monotone diffeomorphism operators always admit finite-dimensional strongly monotone diffeomorphisms.
__label__other Based on the findings, we propose **A**dversarial **CO**-learning **N**etworks (**ACON**), to enhance transferable representation learning through a collaborative learning manner in three aspects: (1) Considering the multi-periodicity in time series, multi-period frequency feature learning is proposed to enhance the discriminability of frequency features; (2) Temporal-frequency domain mutual learning is proposed to enhance the discriminability of temporal features in the source domain and improve the transferability of frequency features in the target domain; (3) Domain adversarial learning is conducted in the correlation subspaces of temporal-frequency features instead of original feature spaces to further enhance the transferability of both features.
__label__diffusion_based_models To our knowledge, this is the first work to systematically explore robust DM unlearning through AT, setting it apart from existing methods that overlook robustness in concept erasing.
__label__machine_vision In particular, we show that (1) learned anisotropic scaling allows task vectors to be more disentangled, causing less interference in composition; (2) task vector composition excels with scarce or no labelled data and is less prone to domain shift, thus leading to better generalisability; (3) mixing the most informative parameter blocks across different task vectors prior to training can reduce the memory footprint and improve the flexibility of knowledge transfer.
__label__algorithmic_game_theory The online bipartite matching problem, extensively studied in the literature, deals with the allocation of online arriving vertices (items) to a predetermined set of offline vertices (agents).
__label__reinforcement_learning The learning objective of this formulation is to find a policy so that it is consistently preferred by the KL-regularized preference oracle over any competing LLMs.
__label__machine_vision CVPT calculates cross-attention between the prompt tokens and the embedded tokens, which allows us to compute the semantic relationship between them and conduct the fine-tuning of models exactly to adapt visual tasks better.
"__label__generative_models The successful integration of the retrieval system with Gorilla
demonstrates the potential for LLMs to use tools more accurately, keep up with
frequently updated documentation, and consequently increase the reliability and
applicability of their outputs."
__label__interpretability_and_explainability This paper shows that pre-trained LLMs add numbers using Fourier features---dimensions in the hidden state that represent numbers via a set of features sparse in the frequency domain.
__label__machine_vision Additionally, we propose a One-Token-Seg-All approach using a specially designed <TRK> token, enabling the model to segment and track objects across multiple frames.
__label__machine_vision However, in complex visual environments, these methods often fall short due to the heterogeneous nature of visual features within an object.
__label__neuroscience_and_cognitive_science Such heterogeneity in electrode number/placement poses a significant challenge for data integration, since there is no clear correspondence of the neural activity recorded at distinct sites between individuals.
__label__graph_neural_networks In practice, temporal graphs are formalized as an ordered sequence of static graph snapshots observed at discrete time points.
__label__algorithmic_game_theory Finally, we study revenue maximization in the non-asymptotic regime.
__label__graph_neural_networks MPNNs iteratively update each node’s representation in an input graph by aggregating messages from the node’s neighbors, which necessitates a memory complexity of the order of the __number of graph edges__.
__label__natural_language_processing We use our framework to obtain reranking laws which we validate empirically on two real-world tasks using LLMs: text-to-code generation with DeepSeek-Coder 7B and machine translation of medical data with TowerInstruct 13B.
__label__deep_learning_architectures Additionally, we substantiate these new insights with empirical validations and mathematical arguments.
__label__machine_vision Second, the Coefficient Learning Block provides a lightweight parameterization of the mapping from skeleton sequences to their specific coefficients in convex combinations.
__label__machine_learning_for_other_sciences_and_fields We propose a new method for generating synthetic training samples from random solutions, and show that sequence-to-sequence transformers trained on such datasets perform better than algorithmic solvers and humans on polynomial systems, and can discover new Lyapunov functions for non-polynomial systems.
__label__graph_neural_networks To enable effective information transfer among these node sets, we propose the Node-to-Cluster Attention (N2C-Attn) mechanism.
__label__diffusion_based_models In this paper, we propose a framework called FlowTurbo to accelerate the sampling of flow-based models while still enhancing the sampling quality.
__label__reinforcement_learning We show that the structure offered by Linear MDPs is not sufficient for efficiently estimating the feasible set when the state space is large.
__label__learning_theory For example, iteratively querying the empirical risk might prove computationally expensive.
__label__interpretability_and_explainability Specifically, we decompose the fused image into multiple components corresponding to its source data.
__label__machine_vision Logic gate networks are faster than conventional neural network approaches because their inference only requires logic gate operators such as NAND, OR, and XOR, which are the underlying building blocks of current hardware and can be efficiently executed.
__label__interpretability_and_explainability Prior efforts to interpret ViTs tend to focus on characterizing relevant low-level visual features.
__label__neuroscience_and_cognitive_science In this scenario, creating a model that maps the configuration of stimulation parameters to the brain’s response can be beneficial.
__label__generative_models Compared to works that bake shading in the 3D object’s appearance, AssetGen outputs physically-based rendering (PBR) materials, supporting realistic relighting.
__label__generative_models However, extending such success to 3D dynamic human-object interaction (HOI) generation faces notable challenges, primarily due to the lack of large-scale interaction data and comprehensive descriptions that align with these interactions.
__label__graph_neural_networks We provide theoretical results that support our design choices as well as an extensive empirical evaluation demonstrating the superior performance of GRANOLA over existing normalization techniques.
__label__machine_learning_for_physical_sciences Another significant difference from the linear regime is that, even in the idealistic infinite-width limit, the Hessian does not vanish and hence it cannot be disregarded during training.
__label__reinforcement_learning This decomposition effectively focuses each agent on specific tasks, proving highly efficient within our pipeline.
__label__generative_models While they are capable of generating human-like responses, they can also produce undesirable output including potentially harmful information, racist or sexist language, and hallucinations.
__label__deep_learning_architectures Previous attempts at extending MoE to the self-attention layer fail to match the performance of the parameter-matched baseline.
__label__safety_in_machine_learning A given string from the training data is considered memorized if it can be elicited by a prompt (much) shorter than the string itself---in other words, if these strings can be ``compressed'' with the model by computing adversarial prompts of fewer tokens.
__label__optimization Our SDEs accurately track the behavior of the respective optimizers, especially when compared to previous SDEs derived for Adam and RMSprop.
__label__algorithmic_game_theory It is a well-known fact that correlated equilibria can be computed in polynomial time in a large class of concisely represented games using the celebrated Ellipsoid Against Hope algorithm \citep{Papadimitriou2008:Computing, Jiang2015:Polynomial}.
__label__probabilistic_methods The sample-based version of this ideal procedure involves biased gradient estimators, thus hindering any theoretical study.
__label__deep_learning_architectures Despite these advancements, LVLMs often exhibit the hallucination phenomenon, where generated text responses appear linguistically plausible but contradict the input image, indicating a misalignment between image and text pairs.
__label__deep_learning_architectures Despite the recent strides in crafting specific architectures for time-series forecasting and developing pre-trained universal models, a comprehensive examination of their capability in accommodating varied-horizon forecasting during inference is still lacking.
"__label__evaluation The *magnitude* of a metric space is a novel
invariant that provides a measure of the 'effective size' of a space across
multiple scales, while also capturing numerous geometrical properties, such as curvature, density, or entropy."
__label__machine_vision Thanks to the rapid progress in RGB & thermal imaging, also known as multispectral imaging, the task of multispectral video semantic segmentation, or MVSS in short, has recently drawn significant attentions.
__label__other We show that, although a remarkably good approximation of Stockfish’s search-based algorithm can be distilled into large-scale transformers via supervised learning, perfect distillation is still beyond reach, thus making ChessBench well-suited for future research.
__label__learning_theory Our aim is to determine the minimum number of queries needed for exactly recovering an arbitrary $k$-clustering.
__label__machine_vision Code will be available.
__label__human-AI_interaction However, they require high-quality exemplar demonstrations to be included in their context window.
__label__machine_learning_for_other_sciences_and_fields Nevertheless, the effectiveness of prior arithmetic design techniques proves inadequate, as they do not sufficiently optimize speed and area, resulting in increased latency and larger module size.
__label__optimization In this paper, we propose an unrestricted method, FUGAL, which finds a permutation matrix that maps one graph to another by directly operating on their adjacency matrices with judicious constraint relaxation.
"__label__safety_in_machine_learning Measuring goal-directedness is important, as it is a critical
element of many concerns about harm from AI."
__label__reinforcement_learning We explicitly analyze the approximation error in policy evaluation, and show that \SDEPO\  achieves an $\tilde{O}(\frac{1}{(1-\gamma)^3\epsilon})$ last-iterate convergence to the $\epsilon-$optimal Nash equilibrium, which is independent of the cardinality of the state space.
__label__diffusion_based_models We show that LCSS surpasses existing methods in sample generation performance and matches the performance of denoising score matching, widely adopted by most SDMs, in evaluations such as FID, Inception score, and bits per dimension.
__label__natural_language_processing Building upon SaulLM-7B, this study refines the approach to produce an LLM better equipped for legal tasks and domains.
__label__privacy We use this check to exactly recover full batches in the honest-but-curious setting without any prior on the data for both encoder and decoder-based architectures using exhaustive heuristic search and a greedy approach, respectively.
__label__privacy As large language models (LLMs) become ubiquitous in our daily tasks and digital interactions, associated privacy risks are increasingly in focus.
__label__machine_vision Disturbed Taylor pruning is also proposed to address the misalignment between the pruning objective and training target, thereby boosting the post-distillation after pruning.
__label__evaluation We also show how to quantitatively identify cases in which exam results are not reliable  measurements of an LLM's ability.
__label__machine_learning_for_other_sciences_and_fields Specifically, NeurKItt employs a neural operator to predict the invariant subspace of the linear system and then leverages the predicted subspace to accelerate linear system solving.
__label__causal_inference Detecting dependencies among variables is a fundamental task across scientific disciplines.
__label__machine_vision Considering this, we present OccFusion, an approach that utilizes efficient 3D Gaussian splatting supervised by pretrained 2D diffusion models for efficient and high-fidelity human rendering.
__label__natural_language_processing Based on this insight, we propose a novel approach to scalable alignment, which firstly trains the (process-supervised) reward models on easy problems (e.g., level 1-3), and then uses them to evaluate the performance of policy models on hard problems.
__label__machine_vision A comprehensive performance study involving various datasets and evaluation scenarios is conducted, demonstrating the superior performance of our proposed color-aware DD compared to existing DD methods.
__label__safety_in_machine_learning Hopfield Boosting encourages the model to focus on hard-to-distinguish auxiliary outlier examples that lie close to the decision boundary between in-distribution and auxiliary outlier data.
__label__probabilistic_methods We demonstrate the efficacy and scalability of our approach on various datasets and a broad class of probabilistic models, showcasing its practical effectiveness.
__label__natural_language_processing Existing LLMs do not have an inherent functionality to provide the users with an uncertainty/confidence metric for each response it generates, making it difficult to evaluate trustworthiness.
__label__machine_learning_for_physical_sciences To take measures to reduce this noise, vibrations need to be simulated with expensive numerical computations.
__label__machine_learning_for_other_sciences_and_fields The device placement problem aims to identify optimal allocations of those nodes to a set of (potentially heterogeneous) devices.
__label__evaluation Each batch of new labels incurs a “state transition” to sharper beliefs, and we choose batches to minimize uncertainty on model performance at the end of the label collection process.
__label__other Surprisingly, our findings demonstrate that item representations, when linearly mapped from advanced LM representations, yield superior recommendation performance.
__label__machine_learning_for_physical_sciences By employing a diffusion-based formulation, we achieve greater stability and enable longer rollouts compared to conventional MSE training.
__label__safety_in_machine_learning It builds upon the work of Podkopaev and Ramdas [2022], who address scenarios where labels are available for tracking model errors over time.
__label__generative_models To address this issue, we devise a collection of provably variance-reducing control variates for gradient estimation based on the REINFORCE leave-one-out estimator.
__label__machine_vision Furthermore, a lightweight fine matching module for both sparse keypoints and dense features can estimate the transformation accurately.
__label__machine_vision Through statistical observations, we have identified two common challenges faced by different OOD detectors: misidentifying tail class ID samples as OOD, while erroneously predicting OOD samples as head class from ID.
__label__optimization The inherent delays in such settings not only introduce additional bias to the system but also obscure the disruptions caused by Byzantine faults.
__label__natural_language_processing However, this fine-tuning paradigm may not be able to efficiently learn optimal decision-making agents in multi-step goal-directed tasks from interactive environments.
__label__learning_theory Our algorithm, based on a policy gradient method, incorporates a novel quantum subroutine for solving the matrix Lyapunov equation.
__label__natural_language_processing Given a tokenizer's merge list along with data samples for each category of interest (e.g., different natural languages), we formulate a linear program that solves for the relative proportion of each category in the tokenizer's training set.
__label__probabilistic_methods We state and prove the asymptotic correctness of a data-free learning objective, *relative trajectory balance*, for training a diffusion model that samples from this posterior, a problem that existing methods solve only approximately or in restricted cases.
__label__neuroscience_and_cognitive_science These efforts improve both the retrieval capability of KHMs and the representation learning of corresponding transformers.
__label__bandits In this paper, we propose a new algorithm and show that the regret can be upper bounded by $O(N^2\log T/\Delta^2 + K \log T/\Delta)$.
__label__probabilistic_methods We then show that such HRPCFD admits many favourable analytic properties which allows us to design an efficient algorithm for training HRPCFD from data and construct the HRPCF-GAN by using HRPCFD as the discriminator for conditional time series generation.
__label__machine_learning_for_physical_sciences We also minimize the mutual information between prompt embeddings and observation embeddings to enhance the robustness of our model to different distributions.
__label__safety_in_machine_learning Safe reinforcement learning (RL) is crucial for deploying RL agents in real-world applications, as it aims to maximize long-term rewards while satisfying safety constraints.
__label__graph_neural_networks A thorough evaluation involving 10 benchmark datasets and comparative analysis against 13 well-established baselines highlights the superior performance of the HTMP mechanism and CMGNN method.
__label__diffusion_based_models To address this issue, we introduce CLIPAway, a novel approach leveraging CLIP embeddings to focus on background regions while excluding foreground elements.
__label__generative_models By studying the phase behavior over data sets of increasing dimension, we show that these phase transitions are genuine in the thermodynamic sense.
__label__machine_vision All data and source code will be made public.
__label__safety_in_machine_learning Extensive experiments on real-world datasets have been conducted to elucidate the effectiveness of the proposed method.
__label__machine_vision By conditioning on the low-frequency content retrieved in the first stage, the diffusion model effectively reconstructs the high-frequency details that are typically lost in the lensless imaging process, while also maintaining image fidelity.
__label__natural_language_processing In this paper, we provide a preliminary exploration of this question.
__label__natural_language_processing Could an LLM infer the censored knowledge by piecing together these implicit hints?
__label__deep_learning_architectures In addition, it applies this strategy to the original imbalanced data to create an augmented dataset and fine-tune the underlying long-tailed learning model.
__label__natural_language_processing However, an important gap remains unaddressed: the roles of a paper's citations vary significantly, ranging from foundational knowledge basis to superficial contexts.
__label__machine_learning_for_other_sciences_and_fields Extensive experiments on two popular tasks, the Traveling Salesman Problem (TSP) and Maximal Independent Set (MIS), demonstrate the superiority of Fast T2T regarding both solution quality and efficiency, even outperforming LKH given limited time budgets.
__label__bandits We also provide instance-dependent minimax regret bounds under uniform rewards.
__label__deep_learning_architectures Vision Transformer (ViT) has become a fundamental cornerstone in this regard due to its high accuracy.
__label__machine_vision Notably, when applying our method on a large-scale vision foundation model InternViT-6B, we improve its performance by 1\%-2\% on detection and segmentation with only 40\%-60\% of the original computation.
__label__speech_and_audio speech emotion classification, audio classification, text-to-speech generation, speech enhancement, etc.
__label__safety_in_machine_learning Building on these stronger linear trends, we demonstrate that combining TTA and AGL-based methods can predict the OOD performance with high precision for a broader set of distribution shifts.
__label__machine_learning_for_physical_sciences Experimentally, our model could generate plausible 3-D molecules and achieve competitive quantitative performance with significantly reduced circuit parameters compared with their classic counterparts.
__label__probabilistic_methods We evaluate our method on probabilistic models built on RNNs and Transformer architectures, and the results confirm the effectiveness of our approach in improving predictive accuracy and uncertainty quantification without significantly increasing the parameter size.
__label__algorithmic_game_theory However, the landscape of efficiently computable equilibria in sequential (extensive-form) games remains unknown.
__label__causal_inference In particular we study the asymmetries that arise in a simple model where we merge the predictors using one binary variable as target and two continuous variables as predictors.
__label__robotics Specifically, considering the fact that long trajectories, containing richer temporal information but potentially additional interference, may perform better or worse than short trajectories, we devise a dynamic length-agnostic knowledge distillation mechanism for exchanging information among trajectories of arbitrary lengths, dynamically determining the transfer direction based on prediction performance.
__label__machine_vision Starting with 2D superpixels and geometric cues from pre-trained models, we initialize 3D planes as AlphaTablets and optimize them via differentiable rendering.
__label__reinforcement_learning We theoretically prove that DCRL mitigates the learning variance while maintaining unbiasedness.
__label__learning_theory Our theory not only aligns well with existing E-SSL methods but also sheds light on new directions by exploring the benefits of model equivariance.
__label__natural_language_processing User intentions are typically formalized as evaluation rewards to be maximized when fine-tuning language models (LMs).
__label__machine_vision Firstly, to avoid expensive fine-grained annotation, we collect a very small number of WSIs at the slide level, and annotate an extremely small number of patches.
__label__other the model-data MI, which, at different levels of regularization, represents a federated version of the bias-variance trade-off.
__label__probabilistic_methods Our convergence result relies on the neural tangent kernel (NTK) to characterize the gradient dynamics that arise from considering the variational objective in function space.
__label__machine_vision On smaller datasets, it achieves up to 8-fold faster convergence and over 30% accuracy gains, highlighting its efficiency.
__label__deep_learning_architectures We introduce InfiNet, a model architecture that enables feature interaction within an infinite-dimensional space created by RBF kernel.
__label__probabilistic_methods Large Language Models (LLMs) often suffer from overconfidence during inference, particularly when adapted to downstream domain-specific tasks with limited data.
__label__optimization_for_deep_networks However, existing theoretical results are restricted to either linear models, the last two layers or binary classification.
__label__causal_inference Investigating the marginal causal effect of an intervention on an outcome from complex data remains challenging due to the inflexibility of employed models and the lack of complexity in causal benchmark datasets, which often fail to reproduce intricate real-world data patterns.
__label__learning_theory Motivated by these examples, we then propose a new paradigm for model selection in games wherein an agent seeks to choose amongst different model classes to use as their action set in a game.
__label__online_learning We show that information-theoretically, there exist algorithms that achieve near-optimal online estimation error via black-box offline estimation oracles, and give a nearly-tight characterization for minimax rates in the OEOE framework.
__label__reinforcement_learning To this end, we suggest sample-based entropy regularization to stabilize the policy training, and propose a consistency policy with prioritized proximal experience regularization (CP3ER) to improve sample efficiency.
__label__safety_in_machine_learning Edge-cloud collaborative inference empowers resource-limited IoT devices to support deep learning applications without disclosing their raw data to the cloud server, thus protecting user's data.
__label__machine_vision The core of LRM-Zero is our procedural 3D dataset, Zeroverse, which is automatically synthesized from simple primitive shapes with random texturing and augmentations (e.g., height fields, boolean differences, and wireframes).
__label__safety_in_machine_learning We theoretically prove that KLE generalizes the previous state-of-the-art method called semantic entropy and empirically demonstrate that it improves uncertainty quantification performance across multiple natural language generation datasets and LLM architectures.
__label__online_learning We model the set of feasible manipulations by a directed graph over the feature space, and assume the learner only observes the manipulated features instead of the original ones.
__label__generative_models Code Large Language Models (Code LLMs) have excelled at tasks like code completion but often miss deeper semantics such as execution effects and dynamic states.
__label__bandits The problem is framed as an $N$-armed structured bandit, each number of player sent being an arm $n$, with expected reward $r(n)$ fully characterized by $F$ and $p+n$.
__label__natural_language_processing This paper answers this question in the context of tackling hard reasoning tasks (e.g., level 4-5 MATH problems) via learning from human annotations on easier tasks (e.g., level 1-3 MATH problems), which we term as easy-to-hard generalization.
__label__machine_vision Novel view synthesis from raw images provides superior high dynamic range (HDR) information compared to reconstructions from low dynamic range RGB images.
__label__safety_in_machine_learning In our evaluation we demonstrate the versatility of VerSAILLE and Mosaic: We prove infinite-time safety on the classical Vertical Airborne Collision Avoidance NNCS verification benchmark for some scenarios while (exhaustively) enumerating counterexample regions in unsafe scenarios.
__label__machine_learning_for_other_sciences_and_fields The moments of the distributions of the associated geometries are then extracted for efficient diversity computing.
__label__learning_theory Given Hermitian $H,S\in\mathbb{C}^{n\times n}$, where $S$ is positive-definite,  let $\Pi_k$ be the true spectral projector on the invariant subspace that is associated with the $k$ smallest (or largest) eigenvalues of the GEP $HC=SC\Lambda$, for some $k\in[n]$.
__label__generative_models Then, at inference time, we decompose the LLM activations along the concept dictionary via sparse coding, to accurately represent the activations as linear combinations of benign and undesirable components.
__label__evaluation However, the focus of the study was primarily on the basic implementation, and whether this objective is optimized in practice and its causal relationship to generalization remain elusive.
__label__machine_vision To this end, we introduce a new Diffusion-Inspired Truncated Sampler (DITS) that jointly performs progressive alignment and modality gap modeling in the joint embedding space.
__label__learning_theory In this paper, we show that high probability excess risk bounds of order up to $O(1/n^2)$ are possible.
__label__reinforcement_learning [2018] and Rowland et al.
__label__human-AI_interaction The field of Multimodal Sentiment Analysis (MSA) has recently witnessed an emerging direction seeking to tackle the issue of data incompleteness.
__label__machine_learning_for_other_sciences_and_fields Foundation models in computer vision have demonstrated exceptional performance in zero-shot and few-shot tasks by extracting multi-purpose features from large-scale datasets through self-supervised pre-training methods.
__label__other However, this assumption is often violated in practice due to {\it a positive distribution shift}, where the negative-conditional density does not change but the positive-conditional density can vary.
__label__safety_in_machine_learning Despite their ease of implementation and computational efficiency, current logit-based methods are vulnerable to overconfidence issues, leading to prediction bias, especially under the natural shift.
__label__neuroscience_and_cognitive_science Despite recent advances in understanding visual and auditory perception, olfactory perception remains an under-explored topic in the machine learning community due to the lack of large-scale datasets annotated with labels of human olfactory perception.
__label__safety_in_machine_learning Gowal et al.
__label__safety_in_machine_learning We hope that these results can recalibrate preconceived impressions within the community and facilitate the development of stronger adversarial attack and defense mechanisms.
__label__safety_in_machine_learning The average consistency score of 12 LLMs ranges from $60.7\%$ in high-ambiguity moral scenarios to $84.8\%$ in low-ambiguity moral scenarios.
__label__diffusion_based_models Despite Flow Matching and diffusion models having emerged as powerful generative paradigms for continuous variables such as images and videos, their application to high-dimensional discrete data, such as language, is still limited.
__label__active_learning Modern machine learning models are becoming increasingly expensive to train for real-world image and text classification tasks, where massive web-scale data is collected in a streaming fashion.
__label__optimization_for_deep_networks By employing an intermediate layer or adding an auxiliary training objective, we recover most of the generalisation performance of the dense model.
__label__safety_in_machine_learning Out-of-distribution (OOD) detection aims to identify OOD inputs from unknown classes, which is important for the reliable deployment of machine learning models in the open world.
__label__learning_theory However, existing optimal subsampling probabilities depends on data scales, and some scaling transformations may result in inefficient subsamples.
__label__probabilistic_methods These empirical findings highlight the potential of our proposed methodology for optimizing SDEs in contemporary applications.
__label__diffusion_based_models Our method demonstrates significant improvements in identity preservation and text alignment compared to the baseline methods.
__label__deep_learning_architectures In this paper, we introduce CRT-Fusion, a novel framework that integrates temporal information into radar-camera fusion to address this challenge.
__label__natural_language_processing The paper gives evidence that they also  have metacognitive knowledge, including ability to name skills and procedures to apply given a task.
__label__diffusion_based_models We also apply FLIPD to natural images where the true LID is unknown.
__label__generative_models Vision Language Models (VLMs) have demonstrated remarkable performance in 2D vision and language tasks.
__label__safety_in_machine_learning We propose a novel training framework \textbf{RAMP}, to boost the robustness against multiple $l_p$ perturbations.
__label__optimization We analytically compute the number of these components and discuss the possibility of mapping one to the other through neuron rescaling and permutation.
"__label__bandits The second model removes the linearity assumption, requiring only that the expected buyer valuation is $\beta$-H\""older in the context."
__label__algorithmic_game_theory Recently, many improvements have been focused on enhancing the convergence speed of the CFR algorithm.
__label__interpretability_and_explainability We investigate this challenge and focus on model sensitivity to domain shifts, such as data sampled from different hospitals or data confounded by demographic variables such as sex, race, etc, in the context of chest X-rays and skin lesion images.
__label__machine_vision The key challenge of multi-view indoor 3D object detection is to infer accurate geometry information from images for precise 3D detection.
__label__machine_vision Experimental results show that our proposed ChatTracker achieves a performance comparable to existing methods.
__label__machine_vision Also, non-synchronization may appear between audio and video streams.
__label__other Our work suggests that the discovery of effective data-pruning metrics could provide a viable path to both enhanced efficiency and superior generalization in transfer learning.
__label__speech_and_audio Exploiting this observation, we propose SILENCE, a lightweight system that selectively obscuring short-term details, without damaging the long-term dependent speech understanding performance.
__label__online_learning The code is available at: https://github.com/Mehrdad-Noori/WATT.
__label__reinforcement_learning Here, we propose RL with Adaptive Regularization (RL-AR), an algorithm that enables safe RL exploration by combining the RL policy with a policy regularizer that hard-codes the safety constraints.
__label__safety_in_machine_learning Unfortunately, recent studies indicate that fine-tuning can increase the risk to the safety of LLMs, even when data does not contain malicious content.
__label__machine_vision Extensive results on the KITTI, 3DMatch, and 3DLoMatch datasets demonstrate that our method achieves state-of-the-art performance compared to both traditional and learning-based methods in various indoor and outdoor scenes.
__label__machine_learning_for_healthcare Surgical video-language pretraining (VLP) faces unique challenges due to the knowledge domain gap and the scarcity of multi-modal data.
__label__graph_neural_networks Graph neural networks (GNNs) provide state-of-the-art results in a wide variety of tasks which typically involve predicting features at the vertices of a graph.
__label__machine_learning_for_other_sciences_and_fields Specifically, we introduce a visiting intention memory network (VIMN) to capture the visiting intentions at each record, along with a shared pool of human travel preference prompts (HTPP) to guide the LLM in understanding users’ travel preferences.
__label__robotics This limited scope hampers their applicability for object manipulation required for animation and simulation.
__label__robotics Code and checkpoints are maintained at https://github.com/OpenDriveLab/CLOVER.
__label__speech_and_audio Speech language models have significantly advanced in generating realistic speech, with neural codec language models standing out.
__label__deep_learning_architectures To address these limitations, we introduce a linear time O(N) proxy based sparse expert selection and pooling approach for context driven feature-set attention.
__label__safety_in_machine_learning Then, we propose an effective two-stage defense method.
__label__machine_vision Evaluating the performance of deep models in new scenarios has drawn increasing attention in recent years due to the wide application of deep learning techniques in various fields.
__label__machine_vision The task shift is addressed by aligning proxy tasks to the downstream tasks, while the semantic shift is handled by leveraging the generalizability of pre-trained encoders.The proposed Few-Shot Attacking FrameWork, denoted as FSAFW, can effectively generate UAPs across various FSL training paradigms and different downstream tasks.
__label__machine_learning_for_other_sciences_and_fields We open source our implementation at https://github.com/Zun-Wang/DEQHNet.
__label__machine_learning_for_other_sciences_and_fields We define a process for extracting a large, high-quality training dataset from the TabLib corpus, proposing methods for tabular data filtering and quality control.
__label__deep_learning_architectures In this paper, we present a practical approach to adaptive depth networks that is applicable to various networks with minimal training effort.
__label__machine_learning_for_physical_sciences We introduce Universal Physics Transformers (UPTs), an efficient and unified learning paradigm for a wide range of spatio-temporal problems.
__label__safety_in_machine_learning Designed as a model-agnostic, plug-and-play module, Aligner can be directly applied to various open-source and API-based models with only one-off training, making it suitable for rapid iteration.
__label__natural_language_processing Extensive experiments with a wide range of LLMs demonstrate the efficacy of our approach.
__label__fairness We study fair allocation of constrained resources, where a market designer optimizes overall welfare while maintaining group fairness.
__label__diffusion_based_models Specifically, Feature Slicer effectively partitions input features into sub-features and Operator Grouping processes each sub-feature with a group of consecutive operators, resulting in significant memory reduction without sacrificing the quality or speed.
__label__natural_language_processing Specifically, we find that increasing the size of the datastore used by a retrieval-based LM monotonically improves language modeling and several downstream tasks without obvious saturation, such that a smaller model augmented with a large datastore outperforms a larger LM-only model on knowledge-intensive tasks.
__label__privacy Unfortunately, existing defense strategies against poisoning attacks rely on the analysis of local updates in plaintext, making them incompatible with SecAgg.
__label__optimization We note that Price's algorithm works for arbitrary order streams whereas our algorithm requires a stronger assumption that the rows are presented in a uniformly random order.
__label__deep_learning_architectures Code and checkpoints are available at https://github.com/yueyang130/DeeR-VLA.
__label__natural_language_processing In enhancing the reasoning capabilities of large language models (LLMs), prior research primarily focuses on specific prompting techniques such as few-shot or zero-shot chain-of-thought (CoT) prompting.
__label__interpretability_and_explainability Concept activation vector (CAV) has attracted broad research interest in explainable AI, by elegantly attributing model predictions to specific concepts.
__label__diffusion_based_models LucidDrag comprises an intention reasoner and a collaborative guidance sampling mechanism.
__label__neuroscience_and_cognitive_science A central aim in computational neuroscience is to relate the activity of large populations of neurons to an underlying dynamical system.
__label__interpretability_and_explainability Visual-TCAV can provide both local and global explanations for any CNN-based image classification model without requiring any modifications.
__label__human-AI_interaction Accordingly, we propose a $\textbf{C}$ollaborative $\textbf{B}$ayesian $\textbf{P}$olicy $\textbf{R}$euse ($\textbf{CBPR}$), a novel Bayesian-based framework that $\textbf{adaptively selects optimal collaborative policies matching the current meta-task from multiple policy networks}$ instead of just selecting actions relying on a single policy network.
__label__optimization Our analysis reveals that both random reshuffling and the recently proposed flip-flop shuffling alone can suffer divergence in C-C problems.
__label__other As such, it does not impact the performance of the original model.
__label__learning_theory We find that strategic interactions can break the conventional view—meaning that performance does not necessarily monotonically improve as model classes get larger or more expressive (even with infinite data).
__label__evaluation However, such transforms are typically applied only after the base models have already been finalized by standard means.
__label__machine_learning_for_other_sciences_and_fields Digital Twins (DTs) are computational models that simulate the states and temporal dynamics of real-world systems, playing a crucial role in prediction, understanding, and decision-making across diverse domains.
__label__graph_neural_networks This offers a new and fundamentally different pipeline for learning on very large non-sparse graphs, whose applicability is demonstrated empirically on node classification tasks and spatio-temporal data processing.
__label__optimization Querying complex models for precise information (e.g.
__label__other This paper presents a case study of chain of thought on problems from Blocksworld, a classical planning domain, and examines the performance of two state-of-the-art LLMs across two axes: generality of examples given in prompt, and complexity of problems queried with each prompt.
__label__machine_learning_for_physical_sciences On the TinyMol dataset, we outperform the `gold-standard' CCSD(T) CBS reference energies by 1.9m$E_h$ and reduce energy errors compared to previous generalized neural wave functions by up to an order of magnitude.
__label__evaluation A common, but often implicit, assumption is that the results of a study will generalize beyond the study itself, e.g.
__label__optimization_for_deep_networks Inspired by the human ability to capture concise and abstract patterns, we hypothesize that generalizable correlations are expected to be easier to imitate.
__label__probabilistic_methods Bayesian reasoning in linear mixed-effects models (LMMs) is challenging and often requires advanced sampling techniques like Markov chain Monte Carlo (MCMC).
__label__deep_learning_architectures However, the current quantized/binarized training approaches are limited by: (1) significant performance loss due to arbitrary approximations of the latent weight gradient through its discretization/binarization function, and (2) training computational intensiveness due to the reliance on full-precision latent weights.
__label__machine_learning_for_physical_sciences We propose a novel approach combining millions of citizen science species observations with textual descriptions from Wikipedia, covering habitat preferences and range descriptions for tens of thousands of species.
__label__natural_language_processing Crucially, in a more difficult instruction-following benchmark, AlpacaEval 2.0, we show that reusing off-the-shelf small models (e.g., $\texttt{zephyr-7b-beta}$ and its untuned version) can improve the length-controlled win rates of both white-box and black-box large models against $\texttt{gpt-4-turbo}$ (e.g., $34.4\% \rightarrow 37.9\%$ for $\texttt{Llama-3-70B-Instruct}$ and $16.0\% \rightarrow 20.1\%$ for $\texttt{gpt-3.5-turbo-instruct}$), despite the small models' low win rates $\approx 10.0\%$.
__label__optimization_for_deep_networks Such formulations are able to explicitly models pairwise correlations between weights during training, leading to a more accurate optimization characterization of the training problem.
__label__machine_vision To improve the generality, we repurpose an open-vocabulary detection foundation model (GroundingDINO) for the counting task, and also extend its capabilities by introducing modules to enable specifying the target object to count by visual exemplars.
__label__diffusion_based_models To mitigate the inherent reconstruction errors of the LDM's VAE decoder, we propose a latent transparency decoder to align the RGBA prediction with the input image, thereby reducing discrepancies.
__label__robotics However, prevailing methods struggle with intricate scenarios and causal relationships, hindering adaptability and interpretability in varied environments.
__label__machine_learning_for_other_sciences_and_fields Besides, they typically face difficulties in aligning the cross-die modules in 3D ICs due to their heuristic representations, which could potentially result in severe data transfer failures.
__label__machine_learning_for_other_sciences_and_fields This millennium-old philosophical problem, known as inductive inference, lies at the heart of epistemology.
__label__diffusion_based_models Unfortunately, conventional generation models often rely on unrealistic assumptions about attackers' knowledge of NIDS components, making them impractical for real-world scenarios.
__label__generative_models This allows us to compare different player styles, as well as synthesize new (human-like) styles, e.g.
__label__robotics Robot videos are best viewed at https://dynamo-ssl.github.io.
__label__probabilistic_methods Moreover, we demonstrate two direct and useful applications of such connection between conformal prediction and information theory: (i) more principled and effective conformal training objectives that generalize previous approaches and enable end-to-end training of machine learning models from scratch, and (ii) a natural mechanism to incorporate side information into conformal prediction.
__label__neuroscience_and_cognitive_science We also found that the effective dimensionality of weights decreases in a network pretrained with random noise.
__label__diffusion_based_models We develop a neural network architecture which, trained in an unsupervised manner as a denoising diffusion model, simultaneously learns to both generate and segment images.
__label__natural_language_processing Initiating with a broad initial vocabulary, we refine our tokenizer by monitoring changes in the model’s perplexity during training, allowing for the selection of a tokenizer that is closely aligned with the model’s evolving dynamics.
__label__safety_in_machine_learning Out-of-distribution (OOD) detection is critical for deploying machine learning models in the open world.
__label__natural_language_processing Prevailing techniques in LLM distillation typically use a black-box model API to generate high-quality pretrained and aligned datasets, or utilize white-box distillation by altering the loss function to better transfer knowledge from the teacher LLM.
__label__probabilistic_methods Numerical results indicate that the new procedures can be highly competitive among existing methods, especially for heavy-tailed errors.
__label__other Autonomous driving system aims for safe and social-consistent driving through the behavioral integration among interactive agents.
__label__safety_in_machine_learning Watermarking is a technical means to dissuade malfeasant usage of Large Language Models.
__label__machine_vision Additionally, PHYRECON models both rendering and physical uncertainty to identify and compensate for inconsistent and inaccurate monocular geometric priors.
"__label__graph_neural_networks We precisely define a class of ``MP-tractable"" MILPs for which MP-GNNs can accurately approximate SB scores."
__label__bandits Statistical inference with interference is widely studied in the offline setting, but far less is known about how to adaptively assign treatments to minimize regret.
__label__machine_learning_for_other_sciences_and_fields We conducted extensive experiments on a range of real-world datasets and biological applications, and our results underscore the superior performance of our proposed algorithm compared to existing state-of-the-art sequence editing methods.
"__label__diffusion_based_models Inspired by the success of diffusion models in tabular data modeling, we introduce 
 \textbf{C}luster \textbf{La}tent \textbf{Va}riable guided \textbf{D}enoising \textbf{D}iffusion \textbf{P}robabilistic \textbf{M}odels (ClavaDDPM)."
__label__learning_theory Instead of approximating the correct labels for training, it considers the entire distribution of all possible labeling entailed by the imprecise information.
__label__learning_theory In this paper, we first show that the training dynamics of the gradient flow of neural networks with random initialization converge uniformly to that of the corresponding NTK regression with random initialization \(f^{\mathrm{GP}}\).
__label__diffusion_based_models However, directly applying these techniques to video models results in unsatisfied frame quality.
__label__causal_inference First, an unidentifiable causal effect may become identifiable when certain variables are functional.
__label__reinforcement_learning We introduce a new method called optimization-based AIL (OPT-AIL), which centers on performing online optimization for reward functions and optimism-regularized Bellman error minimization for Q-value functions.
__label__machine_learning_for_other_sciences_and_fields Specifically, we align two pockets in 3D space with protein-ligand binding priors and build two complex graphs with shared ligand nodes for SE(3)-equivariant composed message passing, based on which we derive a composed drift in both 3D and categorical probability space in the generative process.
__label__machine_learning_for_other_sciences_and_fields To this end, we propose a Hierarchical Representation Learning Framework (HRLF) for the MSA task under uncertain missing modalities.
__label__generative_models Our key insight is to achieve spatial disentanglement through hierarchical modeling of layouts.
__label__machine_learning_for_other_sciences_and_fields These structures are then evaluated and ranked based on their distance to the ground truth, resulting in an automatic preference dataset.
__label__robotics This strategy allows us to leverage abundant and diverse behavior data to enhance generalization and enable rapid adaptation to downstream tasks using minimal annotations.
__label__optimization We show that this concern manifests itself in the classical secretary problem in the learning-augmented setting---the state-of-the-art algorithm can have zero probability of accepting the best candidate, which we deem unfair, despite promising to accept a candidate whose expected value is at least $\max\{\Omega (1) , 1 - O(\varepsilon)\}$ times the optimal value, where $\varepsilon$ is the prediction error.
__label__privacy We develop estimators that are appropriate for such signals---our estimators are $(\varepsilon,\delta)$-differentially private and have sample complexity that is dimension-independent for anisotropic subgaussian distributions.
__label__machine_vision The code is available at https://github.com/rkzheng99/SyncVIS.
__label__machine_learning_for_other_sciences_and_fields We argue that such setup is more aligned with practical scenarios, especially when some users do not have complete personal information (thus assumed with hidden confounding), while others do have (thus assumed without hidden confounding).
__label__generative_models a perspective camera with a fixed focal length, leading to distorted shapes when the assumption fails.
__label__machine_learning_for_social_sciences Also, we employ the auxiliary prediction task to enhance generalization and accuracy.
__label__speech_and_audio This leads us to integrate WavLM-based perceptual loss into MS-STFT adversarial training pipeline, creating an effective and stable training procedure for the speech enhancement model.
__label__diffusion_based_models This inference procedure generally utilizes a trained neural network numerous times to obtain the final output, creating significant latency and energy consumption on digital electronic hardware such as GPUs.
__label__natural_language_processing 2) From a model perspective, this article explores the factors that affect the performance of editing models.
__label__machine_learning_for_physical_sciences This work studies the problem of out-of-distribution fluid dynamics modeling.
__label__robotics An additional methodological innovation contributing to achieving this third goal is an approximated closed-form solution for efficient integration of network dynamics, which eases efficient training.
__label__reinforcement_learning Specifically, A2PO employs a conditional variational auto-encoder to disentangle the action distributions of intertwined behavior policies by modeling the advantage values of all training data as conditional variables.
__label__deep_learning_architectures We show that competitive architectures can be obtained by searching from scratch, and we consistently find large improvements when initialising the search with strong baselines.
__label__interpretability_and_explainability In this work we develop Causal Dependence Plots (CDPs) to visualize how a model's predicted outcome depends on changes in a given predictor *along with consequent causal changes in other predictor variables*.
__label__diffusion_based_models Notably, on challenging inverse problems like 4x super-resolution on the ImageNet dataset, our method can generate high-quality samples in as few as *5* conditional sampling steps and outperforms competing baselines requiring 20-1000 steps.
__label__machine_vision We present Unsupervised SAM (UnSAM) for promptable and automatic whole-image segmentation that does not require human annotations.
__label__machine_vision Yet, scant attention has been directed towards the visual signals utilized by MLLMs, often assumed to be the final high-level features extracted by a frozen visual encoder.
__label__online_learning By designing its regularizer and learning rate to be adaptive to past observations, FTRL is known to work adaptively to various properties of an underlying environment.
__label__optimization_for_deep_networks In this paper, we propose the first 4-bit second-order optimizers, exemplified by 4-bit Shampoo, maintaining performance similar to that of 32-bit ones.
__label__optimization_for_deep_networks Besides, we adopt minimum-variance unbiased estimation for activation gradient and FP8 quantization for whole process.
__label__reinforcement_learning Additionally, our analysis of training dynamics reveals that CurrMask gradually acquires skills of varying complexity by dynamically adjusting its masking scheme.
__label__natural_language_processing The results demonstrate SHED's superiority over state-of-the-art methods across various tasks and LLMs; notably, datasets comprising only 10% of the original data selected by SHED achieve performance comparable to or surpassing that of the full datasets.
__label__machine_vision Comprehensive experiments confirm our framework's advantage over state-of-the-art diffusion-based VSR techniques.
__label__diffusion_based_models The method rests upon the fact that, although the traditional score-based loss is intractable to minimize for generator models, under certain conditions we \emph{can} efficiently compute the \emph{gradients} for a wide class of score-based divergences between a diffusion model and a generator.
__label__deep_learning_architectures Successfully leveraging this complex data, while overcoming the scarcity of high-quality training samples, is key to improving these models' predictive performance.
__label__natural_language_processing Large vision-language models (LVLMs) are ignorant of the up-to-date knowledge, such as LLaVA series, because they cannot be updated frequently due to the large amount of resources required, and therefore fail in many cases.
__label__neuroscience_and_cognitive_science Here, we question this assumption by analyzing three neural datasets used in an impactful study on LLM-to-brain mappings, with a particular focus on an fMRI dataset where participants read short passages.
__label__safety_in_machine_learning ESPO employs an optimization framework with three modes: maximizing rewards, minimizing costs, and balancing the trade-off between the two.
__label__machine_learning_for_healthcare We also construct MutaDescribe, the first large-scale protein mutation dataset with rich textual annotations, which provides cross-modal supervision signals.
__label__learning_theory While existing works consider specific algorithms to realize invariance learning, we show that model has the potential to learn invariance through standard training procedures.
__label__machine_vision Extensive experiments on the nuScenes dataset demonstrate notable improvements, with up to 6.9\%/4.2\% increase in mAP and NDS for 3D detection tasks and up to 4.3\% rise in mIoU for BEV map segmentation tasks, narrowing the performance gap with multi-modal models.
__label__reinforcement_learning Our method is versatile and can be readily adapted to various preference optimization frameworks, including direct preference optimization (DPO).
__label__natural_language_processing Through extensive experiments on various language models and complexity analysis, we demonstrate that aespa is accurate and efficient in quantizing Transformer models.
__label__learning_theory In contrast, non-adaptive pair-wise query algorithms are extremely limited: even for $k=3$, such algorithms require $\Omega(n^2)$ queries, which matches the trivial $O(n^2)$ upper bound attained by querying every pair of points.
__label__natural_language_processing In certain scenarios, such as multi-tenant serving, deploying multiple LLMs becomes necessary to meet complex demands.
__label__interpretability_and_explainability We find that task abilities and the functional components that support them emerge consistently at similar token counts across scale.
__label__probabilistic_methods For classification, GMDI outperforms all approaches, and surpasses the state-of-the-art method, VDI, by up to 3.4%, reaching 99.3%.
__label__machine_learning_for_other_sciences_and_fields Cryo-EM is an increasingly popular method for determining the atomic resolution 3D structure of macromolecular complexes (eg, proteins) from noisy 2D images captured by an electron microscope.
__label__safety_in_machine_learning These simple techniques result in surprisingly effective jailbreaking against aligned LLMs (even with advanced defenses).
__label__fairness We discuss these trade-offs under two paradigms for preference modeling – in the stochastic optimization regime, the market designer has access to a probability distribution over utilities, and in the robust optimization regime they have access to an uncertainty set containing the true utilities with high probability.
__label__generative_models Diffusion models have emerged as a promising approach for behavior cloning (BC), leveraging their exceptional ability to model multi-modal distributions.
__label__safety_in_machine_learning Building on these theoretical understandings, we propose Personalized Magnitude Adversarial Collaborative Filtering (PamaCF).
__label__generative_models Finetuning on this dataset leads to a model that achieves a 67.1 pass@1 on HumanEval+, surpassing CodeLlama-70B-Instruct despite being ten times smaller.
__label__optimization_for_deep_networks Our code is available at https://github.com/xinwangChen/EDT.
__label__neuroscience_and_cognitive_science How reliable are the mechanistic insights derived from this procedure?
__label__machine_vision To resolve the $\ell_0$-minimization, we develop a novel two-stage decoupling strategy, which first decouples the alignment error into a rotation fitting error and a translation fitting error.
__label__interpretability_and_explainability This work uses active learning, a standard pedagogical method, to attempt to improve humans' ability to validate policies in signal temporal logic (STL).
__label__natural_language_processing These methods, while effective, often involve manually intensive prompt engineering.
__label__neuroscience_and_cognitive_science Surface electromyography (sEMG) based gesture recognition offers a natural and intuitive interaction modality for wearable devices.
__label__machine_vision Additionally, a Decoupled Temporal-Spatial Aggregation Module is designed to aggregate information from adjacent points and frames.
__label__reinforcement_learning In many real-world decision problems there is partially observed, hidden or latent information that remains fixed throughout an interaction.
__label__machine_vision We argue that the LQ image contains rich information to restore its HQ counterpart, and hence the given LQ image can be directly taken as the starting point for diffusion, eliminating the uncertainty introduced by random noise sampling.
__label__reinforcement_learning It exhibits capabilities in nuanced credit assignments, trajectory stitching, and adaptation to environmental contingencies.
__label__machine_vision Leveraging the reasoning capabilities and world knowledge of large language models, and augmented by the Segment Anything Model, VideoLISA generates temporally consistent segmentation masks in videos based on language instructions.
__label__causal_inference We then provide a local minimax lower bound to show the instance-dependent optimality of the AIPW estimator using no-regret online learning algorithms.
__label__other Concretely, at the group identification stage, we first estimate the adaptive density of each user point, where areas with higher densities are more likely to be recognized as group centers.
__label__safety_in_machine_learning We show that sometimes, the human's feedback determines the return function uniquely up to an additive constant, but in other realistic cases, there is irreducible ambiguity.
__label__other We theoretically prove that *CSPG* can accelerate the existing graph-based ANNS algorithms by reducing unnecessary explorations.
__label__neuroscience_and_cognitive_science The code for this paper is available at: https://github.com/BINE022/EEGPT
__label__deep_learning_architectures In the reward modeling, we employ a step-wise strategy and incorporate visual constraints into the self-rewarding process to place greater emphasis on visual input.
__label__generative_models Specifically, we show that contrary to sets, elements in a graph are not entirely un-ordered and there is a unique partial order for nodes and edges.
__label__causal_inference In this paper, we introduce a novel approach that achieves broad coverage of causal estimands beyond the SBD, incorporating various sum-product functionals like the FD, while maintaining scalability -- estimated in polynomial time relative to the number of variables and samples.
__label__graph_neural_networks Experimental results on 13 benchmark datasets verify the effectiveness and efficiency of TPNet, where TPNet outperforms other baselines on most datasets and achieves a maximum speedup of $33.3 \times$ compared to the SOTA baseline.
__label__algorithmic_game_theory In this paper, we investigate the question of whether no-swap-regret dynamics have stronger convergence properties in repeated games than regular no-external-regret dynamics.
__label__online_learning We resolve this question by showing that the optimal U-calibration error is $\Theta(\sqrt{KT})$ --- we start with a simple observation that the Follow-the-Perturbed-Leader algorithm of Daskalakis and Syrgkanis (2016) achieves this upper bound, followed by a matching lower bound constructed with a specific proper loss (which, as a side result, also proves the optimality of the algorithm of Daskalakis and Syrgkanis (2016) in the context of online learning against an adversary with finite choices).
__label__causal_inference To fill this gap, we aim to quantify the aleatoric uncertainty of the treatment effect at the covariate-conditional level, namely, the conditional distribution of the treatment effect (CDTE).
__label__neuroscience_and_cognitive_science However, incorporating a realistic noise model of the sensorimotor system — accounting for multiplicative noise in feedback and motor output, as well as internal noise in estimation — makes the problem challenging.
__label__neuroscience_and_cognitive_science This modeling framework offers a promising new avenue for elucidating the computational principles of synaptic plasticity and learning in the brain.
__label__generative_models FOT constrains feature consistency between generated and real signals through the designed forced feature matching mechanism, meanwhile addressing GANs' mode collapse and mixing issues by introducing Wasserstein distance.
__label__natural_language_processing The results show that our approach not only boosts the general reasoning performance of LLMs but also makes considerable strides towards their capacity for abstract reasoning, moving beyond simple memorization or imitation to a more nuanced understanding and application of generic facts.
__label__machine_vision Surprisingly, our method not only realizes consistently increasing generalization ability but also enhances task-specific 3D recognition performances across various 3DDG benchmarks by a clear margin.
__label__probabilistic_methods In this work, we present a novel method for eigenvector estimation that avoids this dependence on coherence.
__label__interpretability_and_explainability A key goal in mechanistic interpretability is circuit analysis: finding sparse subgraphs of models corresponding to specific behaviors or capabilities.
__label__learning_theory A recent successful approach that falls under the JEPA framework is self-distillation, where an online encoder is trained to predict the output of the target encoder, sometimes with a lightweight predictor network.
__label__machine_learning_for_physical_sciences Moreover, back-propagating gradients through the existing solvers is difficult and they hence cannot be easily integrated into modern neural architectures.
__label__learning_theory However, the theoretical analyses of such landscape results often rely on strong assumptions, such as the sampled measurements are (complex) Gaussian.
__label__causal_inference Moreover, relative smoothness of the CQTE lacks the interpretability of smoothness of the CATE making it less clear whether it is a reasonable assumption to make.
__label__speech_and_audio We discover that the transformer-based encoder adopted in recent years is actually capable of performing the alignment internally during the forward pass, prior to decoding.
__label__natural_language_processing Experiments suggest that InfoNCA/NCA surpasses various preference baselines when reward datasets are available.
__label__machine_learning_for_other_sciences_and_fields MKG categorizes information into comprehensive labels such as Name, Formula, and Application, structured around a meticulously designed ontology, thus enhancing data usability and integration.
__label__neuroscience_and_cognitive_science We investigate this question by using multiple unimodal and two types of multi-modal models—cross-modal and jointly pretrained—to determine which type of models is more relevant to fMRI brain activity when participants were engaged in watching movies (videos with audio).
__label__safety_in_machine_learning The code is available at https://github.com/shawkui/Proactive_Defensive_Backdoor.
__label__reinforcement_learning Despite various attempts to combine LLMs with RL, there is commonly a semantic gap between action signals and LLM tokens, which hinders their integration.
__label__natural_language_processing \thm{Can we fine-tune a series of task-specific small models and transfer their knowledge directly to a much larger model without additional training?}
__label__natural_language_processing To date, most studies have concentrated on isolated components within these models, such as the Multilayer Perceptrons and attention head.
__label__causal_inference These results are then generalized to general models and likelihoods, where the same claims hold.
__label__machine_vision This paper addresses the challenge of generalizable object pose estimation, particularly focusing on category-level object pose estimation for unseen object categories.
__label__neuroscience_and_cognitive_science We further validated this mechanism by implementing a heteroscedastic KF that used the same strategy, and it also approached state-of-the-art performance while remaining in the explainable domain of standard KFs.
__label__graph_neural_networks Thus, we propose a loss to enforce the similarity of graph representations to be consistent across different layers.
__label__machine_vision Extensive experiments on five datasets demonstrate that SGNet achieves competitive performance and consistent improvements across a variety of general segmentation models, surpassing the traditional ultra image segmentation methods by a large margin.
__label__machine_vision Implicit neural representations (INRs) have demonstrated success in a variety of applications, including inverse problems and neural rendering.
__label__learning_theory In particular, for every $K \geq 3$ we uncover and characterize a region of the parameter space where exact community recovery is possible using $K$ correlated graphs, even though (1) this is information-theoretically impossible using any $K-1$ of them and (2) none of the latent matchings can be exactly recovered.
__label__reinforcement_learning Our code is available at https://github.com/dyunis/subwords_as_skills.
__label__machine_learning_for_other_sciences_and_fields Such heterogeneous hardware lacks a supporting theoretical framework.
__label__generative_models We propose SelfCodeAlign, the first fully transparent and permissive pipeline for self-aligning code LLMs without extensive human annotations or distillation.
__label__machine_learning_for_other_sciences_and_fields The results thus suggest that AI can play a crucial role in realizing the potential of human expertise in global problem-solving.
__label__learning_theory We also conduct experiments to verify our theorems and the results are in excellent agreement with our theoretical findings.
__label__safety_in_machine_learning This work delves into symmetric moral consistency in large language models and demonstrates that modern LLMs lack sufficient consistency ability in moral scenarios.
__label__deep_learning_architectures Many attempts naturally focus on adjusting learning procedures adaptively.
__label__generative_models Extensive qualitative and quantitative experiments illustrate the superior performance of Ctrl-X on various condition inputs and model checkpoints.
__label__deep_learning_architectures However, forecasters based on Transformers are still suffering from vulnerability to high-frequency signals, efficiency in computation, and bottleneck in full-spectrum utilization, which essentially are the cornerstones for accurately predicting time series with thousands of points.
__label__generative_models However, consistently biasing GFNs towards producing high-utility samples is non-trivial.
__label__machine_vision Large Vision-Language Models (LVLMs) typically encode an image into a fixed number of visual tokens (e.g., 576) and process these tokens with a language model.
__label__natural_language_processing Experimental results demonstrate that \method outperforms existing state-of-the-art prompt-based methods by an average relative improvement of 9.01% across five diverse personalization tasks in the LaMP benchmark.
__label__reinforcement_learning Through empirical evaluation, we demonstrate that the proposed pre-training method enables zero-shot generalization to various cDFA task classes and accelerated policy specialization without the myopic suboptimality of hierarchical methods.
__label__natural_language_processing Recent work utilizes plan-and-solve decomposition to decrease the complexity and leverage self-tests to refine the generated program.
__label__generative_models Understanding when object-centric representations can theoretically be identified is important for scaling slot-based methods to high-dimensional images with correctness guarantees.
__label__diffusion_based_models In this paper, we replace BM with an approximation of its non-Markovian counterpart, fractional Brownian motion (fBM), characterized by correlated increments and Hurst index $H \in (0,1)$, where $H=0.5$ recovers the classical BM.
__label__reinforcement_learning Furthermore, we study the statistical complexity of estimating the feasible reward set with a generative model and analyze a uniform sampling algorithm that turns out to be minimax optimal whenever the sub-optimal experts' performance level is sufficiently close to that of the optimal expert.
__label__machine_vision Extensive experiments conducted in both cloud and edge environments demonstrate that CLFD consistently improves the performance of state-of-the-art (SOTA) methods in both precision and training efficiency.
__label__machine_vision Our experimental results demonstrate the effectiveness of our Slot-VLM, which achieves the state-of-the-art performance on video question-answering.
__label__graph_neural_networks We empirically demonstrate that the learning process on the reduced graph can closely approximate that on the original graph.
__label__generative_models A key factor in the performance of continuous diffusion models stems from the guidance methods, which enhance the sample quality at the expense of diversity.
__label__natural_language_processing We develop custom CUDA kernels for KVQuant, showing that we can achieve up to ~1.7x speedups, compared to baseline fp16 matrix-vector multiplications, for the LLaMA-7B model.
__label__reinforcement_learning This subroutine can be applied to various previous algorithms to obtain improved regret bounds.
__label__generative_models Despite being simple, our approach surpasses StyleGAN2 on FFHQ, ImageNet, CIFAR, and Stacked MNIST datasets, and compares favorably against state-of-the-art GANs and diffusion models.
__label__diffusion_based_models Our findings reveal that: 1) Input control information has unique characteristics compared to conventional inputs like Canny edges and depth maps.
__label__learning_theory As industrial models and designs grow increasingly complex, the demand for optimal control of large-scale dynamical systems has significantly increased.
__label__machine_vision To tackle this, we present Hierarchical Sinkhorn Tree (HST), a pruned tree structure designed to hierarchically measure the local consistency of each coarse correspondence across multiple feature scales, thereby filtering out the local dissimilar ones.
__label__reinforcement_learning We first establish the converse result, where we show that any Federated Q-learning that offers a linear speedup with respect to number of agents in sample complexity needs to incur a communication cost of at least $\Omega(\frac{1}{1-\gamma})$, where $\gamma$ is the discount factor.
__label__other Despite its simplicity, RankUp, with or without RDA, achieves SOTA results in across a range of regression benchmarks, including computer vision, audio, and natural language processing tasks.
__label__learning_theory Our main result is the first high-accuracy nearly-linear time algorithm for solving semi-random matrix completion, and an extension to the noisy observation setting.
__label__machine_vision Despite recent advances, existing methods fail to capture important domain-specific knowledge, while also ignoring differences in data distribution across different domains.
__label__machine_vision Additionally, we employ von Mises-Fisher distributions to structure the feature space, ensuring semantic embeddings within the same class remain consistent across varying inputs.
__label__reinforcement_learning We show that Reciprocators can be used to promote cooperation in temporally extended social dilemmas during simultaneous learning.
__label__graph_neural_networks In this paper, we introduce Continuous Product Graph Neural Networks (CITRUS) that emerge as a natural solution to the TPDEG.
__label__generative_models Video generation primarily aims to model authentic and customized motion across frames, making understanding and controlling the motion a crucial topic.
__label__natural_language_processing We also introduce an MCTS-based inference algorithm that enables DeTikZify to iteratively refine its outputs without the need for additional training.
__label__generative_models Finally, we introduce the temporal attention layers into our egocentric video diffusion pipeline to improve the temporal consistency cross egocentric frames.
__label__bandits Bandits with preference feedback present a powerful tool for optimizing unknown target functions when only pairwise comparisons are allowed instead of direct value queries.
__label__natural_language_processing 2021) to emphasise that because LLMs are simply a method for creating a probability distribution over sequences of words, they can be viewed as simply parroting information in the training data.
__label__other However, this approach does not inform outlier removal with the estimation task, leaving room for improvement.
__label__machine_vision Vision-Language Models seamlessly discriminate among arbitrary semantic categories, yet they still suffer from poor generalization when presented with challenging examples.
__label__diffusion_based_models Previous approaches have suffered from the discrepancy between discrete data and continuous modeling.
__label__natural_language_processing By adopting the latter approach, we can naturally emphasize informative low-frequency words in terms of their vector norm, which becomes evident from the information-geometric perspective, and in terms of the loss functions for imbalanced classification.
__label__deep_learning_architectures We introduce a novel positional encoding strategy for Transformer-style models, addressing the shortcomings of existing, often ad hoc, approaches.
__label__fairness At the same time, machine learning is being increasingly applied to decision-making processes at a societal level, making it imperative for modelers to address inherent biases towards subgroups present in the data.
__label__machine_learning_for_healthcare Such analysis can be done by comparing individuals to a reference one with time series as biomedical data.
__label__natural_language_processing In this work we consider Code World Models, world models generated by a Large Language Model (LLM) in the form of Python code for model-based Reinforcement Learning (RL).
__label__neuroscience_and_cognitive_science It is unclear about the neural circuit mechanism of motor action selection, nor its underlying theory.
__label__generative_models This enables detailed region description and reasoning without the need for substantial training costs or model retraining.
__label__causal_inference We consider the linear causal representation learning setting where we observe a linear mixing of $d$ unknown latent factors, which follow a linear structural causal model.
"__label__learning_theory As a heuristic for improving test accuracy in classification, the ""flooding"" method proposed by Ishida et al."
__label__diffusion_based_models Recent advances in text-guided video editing have showcased promising results in appearance editing (e.g., stylization).
__label__machine_vision By extending the observation, we propose a Random Linear Enhancement (RLE) strategy which includes Moderate Random Linear Enhancement (MRLE)  and Radical Random Linear Enhancement (RRLE)  to push the boundaries of both types of transformation.
__label__algorithmic_game_theory This approach makes the grand coalition a Nash equilibrium with high probability  despite information asymmetry, thereby breaking unravelling.
__label__diffusion_based_models Without the constraints of panorama image generation, we surpass previous methods in supporting complicated indoor space generation beyond a single room, even as complicated as a whole multi-bedroom apartment with irregular shapes and layouts.
__label__neuroscience_and_cognitive_science Our model achieves state-of-the-art performance on the 61-word classification task, surpassing all baselines.
__label__optimization_for_deep_networks It has also been introduced into CL to improve memory representation or learning efficiency.
"__label__algorithmic_game_theory As our main result, for this ""unbalanced"" setting we devise a truthful random mechanism, which outperforms the best known mechanism (with no predictions) by Lu et al.~[2010]."
__label__natural_language_processing To that end, we propose, Code Generation and Emulated EXecution (COGEX).
__label__deep_learning_architectures Over the past years, we have observed an abundance of approaches for modeling dynamic 3D scenes using Gaussian Splatting (GS).
__label__generative_models While time series diffusion models have received considerable focus from many recent works, the performance of existing models remains highly unstable.
__label__graph_neural_networks In this paper, we approach an overlooked yet critical task Graph2Image: generating images from multimodal attributed graphs (MMAGs).
__label__reinforcement_learning However, meta-RL research primarily focuses on adapting to minor variations of a single task.
__label__graph_neural_networks Here, we present $\texttt{IsoNet++}$, an early interaction graph neural network (GNN), based on several technical innovations.
__label__safety_in_machine_learning The pursuit of a performative optimum (PO)—minimizing performative risk—is generally reliant on modeling of the distribution map, which characterizes how a deployed ML model alters the data distribution.
__label__deep_learning_architectures We develop a taxonomy of all such operators based on their computational and algebraic properties, which provides insights into their scaling laws.
__label__optimization_for_deep_networks To remedy this, most robust fine-tuning methods aim to preserve the pre-trained features.
__label__interpretability_and_explainability In contrast, Gradient Descent converges exponentially more slowly.
__label__privacy Firstly, SM3D in FOOGD estimates score model for arbitrary distributions without prior constraints, and detects semantic-shift data powerfully.
__label__reinforcement_learning We consider a standard mathematical formulation, the reverse-KL regularized minimax game between two LLMs for RLHF under general preference oracle.
__label__machine_vision However, such approaches either disregard temporal information in a long time span or sacrifice spatial details, resulting in flawed compression.
__label__generative_models Inference-time alignment methods avoid the complex post-training step and instead bias the generation towards responses that are aligned with human preferences.
__label__natural_language_processing In this paper, we introduce Moderation Using LLM Introspection (MULI), which detects toxic prompts using the information extracted directly from LLMs themselves.
__label__natural_language_processing In particular, LayTextLLM projects each bounding box to a single embedding and interleaves it with text, efficiently avoiding long sequence issues while leveraging autoregressive traits of LLMs.
__label__probabilistic_methods Taken together, these results show that representations learned via temporal contrastive learning follow a Gauss-Markov chain, a graphical model where inference (e.g., prediction, planning) over representations corresponds to inverting a low-dimensional matrix.
__label__optimization Most theoretical results assume that it is possible to obtain unbiased gradient estimators, which is not the case in several recent deep learning and reinforcement learning applications that use Monte Carlo methods.
__label__causal_inference To remedy this shortcoming, we introduce a module of learnable Fourier features, thereby developing a new criterion.
__label__machine_vision To better reuse the VLM resource and fully leverage its potential on different zero-shot image classification tasks, a promising strategy is selecting appropriate Pre-Trained VLMs from the VLM Zoo, relying solely on the text data of the target dataset without access to the dataset’s images.
__label__machine_vision Project page: https://junshengzhou.github.io/DiffGS.
__label__neuroscience_and_cognitive_science The procedure highlights image regions containing shared features driving responses of the model neuron.
__label__probabilistic_methods Furthermore, we prove that the spherical IFT  gradient flow enjoys the best of both worlds by providing the global exponential convergence guarantee for both the MMD and KL energy.
"__label__machine_vision In this paper, rather than focusing on representation architectures, which
is a common focus in many existing works, we propose a novel INR-based video
compression framework, Neural Video Representation Compression (NVRC),
targeting compression of the representation."
__label__safety_in_machine_learning To fill this gap, we first propose CAPGD, a gradient attack that overcomes the failures of existing gradient attacks with adaptive mechanisms.
__label__neuroscience_and_cognitive_science When synaptic turnover is introduced, the learned rule incorporates a form of homeostasis, better maintaining robust sequential dynamics relative to other previously proposed rules.
__label__graph_neural_networks We theoretically demonstrate that the learned representations are divided into distinct partitions based on the number of classes and exhibit enhanced generalization ability across tasks.
__label__safety_in_machine_learning One of its key components is $\textit{dynamic outlier distribution adaptation}$ that effectively adapts a vanilla outlier distribution based on the outlier samples to the true OOD distribution by utilizing the OOD knowledge in the predicted OOD samples during inference.
__label__interpretability_and_explainability In this paper, we instead demonstrate that Transformers learn to approximate second-order optimization methods for ICL.
__label__interpretability_and_explainability ${\textbf{W}_q}^\top\textbf{W}_k$).
__label__machine_vision In this paper, we address a novel problem of selective forgetting for black-box models, named Black-Box Forgetting, and propose an approach to the problem.
__label__fairness Results show that for content moderation, our GPT-3.5-based models either match or outperform GPT-4 on $41$ datasets.
__label__diffusion_based_models Further empirical study also indicates that its negative impact is not negligible even when content shift is not visually perceivable.
__label__machine_vision CODE utilizes the comprehensive descriptions from model itself as visual counterpart to correct and improve response alignment with actual visual content.
__label__safety_in_machine_learning A key source of complexity in next-generation AI models is the size of model outputs, making it time-consuming to parse and provide reliable feedback on.
__label__diffusion_based_models Our estimator, called FLIPD, is easy to implement and compatible with all popular DMs.
__label__machine_vision Furthermore, we develop a coarse-to-fine filling strategy to generate the density fields of the object from the Gaussian reconstruction, allowing for the extraction of object continuums along with their surfaces and the integration of Gaussian attributes into these continuum.
__label__privacy By contrast, previous audits were only (relatively) tight in stronger white-box models, where the adversary can access the model's inner parameters and insert arbitrary gradients.
__label__learning_theory Collecting large quantities of high-quality data can be  prohibitively expensive or impractical, and a bottleneck in machine learning.
__label__generative_models Already, some theoretical results on self-consuming generative models (a.k.a., iterative retraining) have emerged in the literature, showcasing that either model collapse or stability could be possible depending on the fraction of generated data used at each retraining step.
"__label__optimization Recent studies have shown that the ""workhorse"" of digital AI training - stochastic gradient descent (SGD) algorithm converges inexactly when applied to model training on non-ideal devices."
__label__active_learning This work focuses on the active learning in video captioning.
__label__probabilistic_methods We derive a tighter inequality than Cauchy-Schwarz Inequality, leverage it to refine Pearson's $r$, and propose a new correlation coefficient, i.e., rearrangement correlation.
__label__machine_vision Especially, we further theoretically prove that the Gabor wavelet activation possesses a time-frequency tightness property that favors learning the optimal bandwidths in the decoder.
__label__machine_vision The final denoised image is obtained by propagating intermediate MAP solutions that balance the updated likelihood and diffusion prior.
__label__natural_language_processing Therefore, it has a high decoding speed but an unsatisfactory acceptance rate.
__label__graph_neural_networks Node centralities play a pivotal role in network science, social network analysis, and recommender systems.
__label__diffusion_based_models By fine-tuning a pre-trained text-to-image diffusion model with this information, our approach enables fine-grained 3D pose and placement control of individual objects in a scene.
__label__bandits We propose a novel inventory reserving algorithm which draws new insights into the problem.
__label__machine_learning_for_healthcare The code is available at https://github.com/prescient-design/funcmol.
__label__learning_theory Our analysis reveals that for both models, the reversal curse is a consequence of the (effective) model weights *asymmetry*, i.e., the increase of weights from a token $A$ to token $B$ during training does not necessarily cause the increase of the weights from $B$ to $A$, which is caused by the training dynamics under certain choice of loss function and the optimization space of model parameters.
__label__other Our analysis decomposes the NTK matrix into two components.
__label__natural_language_processing As a result of this exploration, we introduce SWE-agent: a system that facilitates language model agents to autonomously use computers to solve software engineering tasks.
__label__graph_neural_networks Equivariant machine learning is an approach for designing deep learning models that respect the symmetries of the problem, with the aim of reducing model complexity and improving generalization.
__label__bandits We propose a new best-of-both-worlds algorithm for bandits with variably delayed feedback.
__label__graph_neural_networks Recent advancements predominantly follow the standard supervised learning paradigm -- feeding an individual problem instance into the network each time and training it to approximate the execution steps of a classical algorithm.
__label__deep_learning_architectures Our codesign therefore addresses critical gaps on both the algorithmic and system fronts, establishing a scalable and efficient alternative for LLM inference in resource-constrained settings.
__label__natural_language_processing Overall, our results show that datastore size should be considered as an integral part of LM efficiency and performance trade-offs.
__label__human-AI_interaction We then demonstrate with human-in the-loop experiments that IDA achieves better control in Lunar Lander and that human participants experience greater autonomy and prefer IDA over pilot-only and traditional SA control.
__label__diffusion_based_models Notably, FlowTurbo reaches an FID of 2.12 on ImageNet with 100 (ms / img) and FID of 3.93 with 38 (ms / img), achieving the real-time image generation and establishing the new state-of-the-art.
__label__other Specifically, we reduce the distributionally robust strategy to a max-min optimization problem, constitute the Stackelberg equilibrium as the solution concept, and estimate the convergence rate.
__label__machine_vision Furthermore, we propose a frequency modulation inter-branch module (FMIM) to modulate the frequency distribution between branches.
__label__generative_models This iterative structure brings the flexibility for our method to generate or refine scenes from various starting points, such as text, floor plans, or pre-arranged environments.
__label__machine_vision Rather than using LLM to connect each specialist, our work aims at end-to-end training on one encoder, one decoder, and one LLM.
__label__privacy We evaluate PANORAMIA on ML models for image and tabular data classification, as well as on large-scale language models.
__label__machine_vision To solve the issue, we propose a novel Domain-Aware Adapter (DA-Ada) tailored for the DAOD task.
__label__natural_language_processing Extensive experiments on two real-world datasets validate the effectiveness of TMEA with a clear improvement over competitive baselines.
__label__machine_learning_for_healthcare Trained on a consortium of 10 public CT datasets, $\textbf{\textit{CAT}}$ demonstrates superior performance in multiple segmentation tasks.
__label__machine_vision Current vision models typically maintain a fixed correspondence between their representation structure and image space.
__label__deep_learning_architectures Notice that a function having only low frequency components may be well-represented by a shallow neural network (SNN), a network having only a few layers.
__label__machine_learning_for_social_sciences The empirical success of PRB demonstrates the value of the proposed fusion approach.
__label__natural_language_processing Specifically, we study the structural properties of the solutions selected by gradient-based optimizers among the many possible minimizers of the NTP objective.
__label__fairness Our experimental results demonstrate SFID's effectiveness across various VLMs tasks including zero-shot classification, text-to-image retrieval, image captioning, and text-to-image generation, by significantly reducing gender biases without compromising performance.
__label__reinforcement_learning Our approach outperforms traditional RL methods and existing GPT agents, demonstrating superior efficiency.
__label__privacy Despite stringent data management measures, attackers can steal massive private data from local clients through multiple Trojans, which control generative behaviors with multiple triggers.
__label__diffusion_based_models Text-driven image synthesis has made significant advancements with the development of diffusion models, transforming how visual content is generated from text prompts.
__label__graph_neural_networks Our code is available at https://github.com/zylMozart/Disentangle_GraphHom.
__label__reinforcement_learning In this work, we establish a novel theoretical result that links the context length of a policy to the time needed to reliably evaluate its performance (i.e., its mixing time) in large scale partially observable reinforcement learning environments that exhibit latent sub-task structure.
__label__diffusion_based_models To achieve pose disentanglement, compactness for generative models, and transferability, we first design the pose extractor to represent the pose as a keypoint-based hybrid representation and the pose applier to learn an implicit deformation field.
__label__machine_vision Extensive experiments on multiple datasets show that Vista outperforms the most advanced general-purpose video generator in over 70% of comparisons and surpasses the best-performing driving world model by 55% in FID and 27% in FVD.
__label__diffusion_based_models Denoising diffusion models (DDMs) offer a flexible framework for sampling from high dimensional data distributions.
__label__safety_in_machine_learning Several primary defense strategies have been proposed to protect LLMs from producing harmful information, mostly focusing on model fine-tuning or heuristical defense designs.
__label__machine_learning_for_physical_sciences However, real-world systems often deviate from strict energy conservation and follow different physical priors.
__label__causal_inference On the other hand, in the same linear case, we show that identification up to SNA is possible under mild conditions, and propose an algorithm, LiNGCReL which provably achieves such identifiability guarantee.
__label__other Statistical heterogeneity of data present at client devices in a federated learning (FL) system renders the training of a global model in such systems difficult.
__label__machine_vision Our approach enables material editing, relighting, and novel view synthesis at interactive rates.
__label__learning_theory We show that proximal samplers based on the Gaussian oracle have a fundamental barrier in that they necessarily achieve only low-accuracy guarantees when sampling from a class of heavy-tailed targets.
__label__learning_theory Our approach naturally leverages properties of Cramér-Chernoff bounds, such as exact optimization of the free parameter in many PAC-Bayes bounds.
__label__privacy This is traditionally formalized via mechanism-agnostic subsampling guarantees that express the privacy parameters of a subsampled mechanism as a function of the original mechanism's privacy parameters.
__label__causal_inference In this paper, we exemplify this incompleteness and then present the *sequential adjustment criterion*, a sound and complete criterion for sequential covariate adjustment.
__label__learning_theory We believe that a theoretically grounded understanding on the role of equivariance would inspire more principled and advanced designs in this field.
__label__optimization_for_deep_networks Here, the loss function poses the bottleneck when training a deep neural network.
__label__optimization In the semi-discrete $2$-Wasserstein problem, we wish to compute the cheapest way to transport all the mass from a continuous distribution $\mu$ to a discrete distribution $\nu$ in $\mathbb{R}^d$ for $d\ge 1$, where the cost of transporting unit mass between points $a$ and $b$ is $d(a,b)=||a-b||^2$.
__label__reinforcement_learning As demonstrations, we propose a supervised and a self-supervised implementation of $I(Z; M)$, and empirically show that the corresponding optimization algorithms exhibit remarkable generalization across a broad spectrum of RL benchmarks, context shift scenarios, data qualities and deep learning architectures.
__label__learning_theory As machine learning models are increasingly deployed in dynamic environments, it becomes paramount to assess and quantify uncertainties associated with distribution shifts.
__label__causal_inference When the number of interventional samples is large enough, we show theoretically that our proposed algorithm will return the true causal graph with high probability.
__label__machine_learning_for_healthcare While optimization-based methods boast generalizability across modalities and robust performance, learning-based methods promise peak performance, incorporating weak supervision and amortized optimization.
__label__machine_vision Extracting extrusion cylinders from raw 3D geometry has been extensively researched in computer vision, while the processing of 3D data through neural networks has remained a bottleneck.
__label__learning_theory Such an algorithm is said to be optimistically universal for the given concept class.
"__label__safety_in_machine_learning It is also of philosophical interest,
as goal-directedness is a key aspect of agency."
__label__machine_vision In this work, we introduce MeshFormer, a sparse-view reconstruction model that explicitly leverages 3D native structure, input guidance, and training supervision.
__label__natural_language_processing In this work, we consider the incapability to memorize domain-specific knowledge embedded in the general corpus with rare occurrences and long-tail distributions as the leading cause for pretrained LMs' inferior downstream performance.
__label__interpretability_and_explainability Experimental results demonstrate that our strategy improves fidelity between the target and surrogate model predictions on several datasets.
__label__machine_vision Extensive experiments demonstrate the effectiveness and efficiency of our method in obtaining re-posable 3D objects.
__label__reinforcement_learning Indeed, we prove that the λ-discrepancy is exactly zero for all Markov decision processes and almost always non-zero for a broad class of partially observable environments.
__label__generative_models Initially, we establish dense mesh correspondences between characters using semantically consistent sensors (SCS), effective across diverse mesh topologies.
__label__deep_learning_architectures Thus, it is necessary to adapt ViT architectures to devices with diverse computational overheads to achieve an accuracy-efficient trade-off.
"__label__machine_vision Extensive experiments on large-scale datasets of unbounded scenes
demonstrate that 3DGS-Enhancer yields superior reconstruction performance and
high-fidelity rendering results compared to state-of-the-art methods."
__label__safety_in_machine_learning Through evaluation, we have found that most existing methods are unable to achieve both supervised and contrastive unlearnability, which poses risks to data protection by availability attacks.
__label__causal_inference In particular, we define a new risk measure, the identifiable robust risk, and its corresponding (population) minimax quantity that is an algorithm-independent measure for the best achievable robustness under partial identifiability.
__label__machine_learning_for_physical_sciences Furthermore, we present numerical experiments in support of our theoretical claims.
__label__learning_theory To the best of our knowledge, this is the first work to formally define and address this problem.
__label__deep_learning_architectures In (1), a bi-branch encoding process is developed to enable the disentangling of variant and invariant correlations by coordinating with a decoy classifier and the decoder reconstruction.
__label__speech_and_audio In particular, ELSA achieves +2.8\% mean audio-to-text and text-to-audio R@1 above the LAION-CLAP baseline, and outperforms by -11.6° mean-absolute-error in 3D source localization over the SeldNET baseline on the TUT Sound Events 2018 benchmark.
__label__diffusion_based_models Diffusion-based methods have achieved remarkable achievements in 2D image or 3D object generation, however, the generation of 3D scenes and even $360^{\circ}$ images remains constrained, due to the limited number of scene datasets, the complexity of 3D scenes themselves, and the difficulty of generating consistent multi-view images.
__label__privacy We further prove information-theoretical lower bounds, showing that the error rate of our algorithm is optimal up to logarithmic factors.
__label__diffusion_based_models Finally, the CVA module introduces decomposing global and local cross-attention, ensuring local fidelity and global consistency of the person image when multiple source image prompts.
__label__reinforcement_learning Moreover, we experimentally show that RegQ converges in environments where Q-learning with linear function approximation has known to diverge.
__label__robotics Humans can communicate and observe media with different modalities, such as texts, sounds, and images.
__label__deep_learning_architectures Linear RNN architectures, like Mamba, can be competitive with Transformer models in language modeling while having advantageous deployment characteristics.
__label__machine_learning_for_healthcare The success of machine learning models relies heavily on effectively representing high-dimensional data.
__label__fairness Recent research in FL has increasingly focused on improving the uniformity of model performance across clients, a fairness principle known as egalitarian fairness.
__label__natural_language_processing This result thus demonstrates that code synthesis can be applied to a much broader class of problems than previously considered.
__label__machine_vision The proposed M$^2$CRL is pre-trained on 7 publicly available endoscopic video datasets and fine-tuned on 3 endoscopic video datasets for 3 downstream tasks.
__label__reinforcement_learning The experimental results show that our design of Feint behaviors can (1) greatly improve the game reward gains; (2) significantly improve the diversity of Multi-Player Games; and (3) only incur negligible overheads in terms of time consumption.
__label__safety_in_machine_learning Unfortunately, our subsequent study shows that this simple Bi-State Optimization (BSO) solution experiences convergence instability when steps invested in its alignment state is too small, leading to downgraded alignment performance.
__label__diffusion_based_models Through multi-layer composition, we demonstrate that our approach allows to build and manipulate images from highly complex prompts with fine-grained control over object appearance and location, granting a higher degree of control than competing methods.
__label__machine_vision Experimental results on different public datasets show that our method achieves state-of-the-art reconstruction quality.
__label__safety_in_machine_learning Empirically, AttnGCG demonstrates consistent performance enhancements across diverse LLMs, with an average improvement of 7\% in the Llama-2 series and 10\% in the Gemma series.
__label__machine_learning_for_other_sciences_and_fields Experimental results demonstrate the effectiveness of our AL paradigm, as well as the proposed diversity and uncertainty methods.
__label__machine_vision The Multi-modal Large Language Model (MLLM) based Referring Expression Generation (REG) task has gained increasing popularity, which aims to generate an unambiguous text description that applies to exactly one object or region in the image by leveraging foundation models.
__label__reinforcement_learning Prominent examples of this task include portfolio optimization or distributing computational workloads across servers.
__label__optimization_for_deep_networks We also empirically show that the student backbones trained by our method transfer well on downstream MS-COCO and ADE20K datasets.
__label__machine_learning_for_physical_sciences We integrate irreducible Cartesian tensor products into message-passing neural networks and prove the equivariance and traceless property of the resulting layers.
__label__causal_inference Our algorithm also enables us to conduct a causal analysis to evaluate spurious correlations among input features of generative models pre-trained on the CelebA dataset.
__label__infrastructure The Lion optimizer has been a promising competitor with the AdamW for training large AI models, with advantages in memory, computation, and sample efficiency.
__label__interpretability_and_explainability With a multiple-choice-list experiment, we initially estimate the degree of risk preference, probability weighting, and loss aversion in a context-free setting for three commercial LLMs: ChatGPT-4.0-Turbo, Claude-3-Opus, and Gemini-1.0-pro.
__label__learning_theory In this work, we provide a unified asymptotic analysis for a family of algorithms that encompasses IRLS, the recently proposed lin-RFM algorithm (which was motivated by feature learning in neural networks), and the alternating minimization algorithm on linear diagonal neural networks.
"__label__graph_neural_networks Concretely, we prove that polynomial-sized message-passing algorithms can represent
the most powerful polynomial time algorithms for Max Constraint Satisfaction
Problems assuming the Unique Games Conjecture."
"__label__causal_inference Causal models are crucial for understanding complex systems and
identifying causal relationships among variables."
__label__machine_vision Experimental results demonstrate that MaVEn significantly enhances MLLMs' understanding in complex multi-image scenarios, while also improving performance in single-image contexts.
__label__reinforcement_learning However, these studies still fail to overcome the following challenges: (1) insufficiently utilizing the historical temporal information among inter-steps, (2) overlooking the local intra-step relationships among states, actions and return-to-gos (RTGs), (3) overfitting suboptimal trajectories with noisy labels.
__label__machine_learning_for_other_sciences_and_fields SARAD trains a Transformer to learn the spatial associations, the pairwise inter-feature relationships which ubiquitously characterize such feedback-controlled systems.
__label__deep_learning_architectures Extending these models to practical multi-party fuzzy VFL typically results in significant performance degradation and increased costs for maintaining privacy.
__label__natural_language_processing However, two challenges remain: (a) interference between different models and (b) heterogeneous data during testing.
__label__interpretability_and_explainability μMoE layers enable scalable expert specialization by performing an implicit computation on prohibitively large weight tensors entirely in factorized form.
__label__machine_vision In this work, we propose a grouping-exploration strategy framework,  Group Explorer Domain Adaptation ($\textbf{GroupEXP-DA}$), to addresses those two issues.
__label__infrastructure FACT is the first federated mechanism that: (1) eliminates federated free riding by using a penalty system, (2) ensures agents provide truthful information by creating a competitive environment, and (3) encourages agent participation by offering better performance than training alone.
__label__human-AI_interaction The RWS strategy, acknowledging that only target objects have supervised positional information, employs dependency tree rules to precisely guide the core instance’s positioning.
__label__interpretability_and_explainability We present both qualitative and quantitative evidence that scaling μMoE layers when fine-tuning foundation models for vision tasks leads to more specialized experts at the class-level, further enabling manual bias correction in CelebA attribute classification.
__label__machine_learning_for_other_sciences_and_fields Our main technical tool is a novel non-uniform online learning framework, which may be of independent interest.
__label__reinforcement_learning Reinforcement learning (RL) in large or infinite state spaces is notoriously challenging, both theoretically (where worst-case sample and computational complexities must scale with state space cardinality) and experimentally (where function approximation and policy gradient techniques often scale poorly and suffer from instability and high variance).
__label__generative_models However, these models are inefficient because they require many diffusion steps to produce aesthetically pleasing samples.
__label__machine_vision In this paper, we develop a single any-to-any model trained on tens of highly diverse modalities and by performing co-training on large-scale multimodal datasets and text corpora.
__label__deep_learning_architectures Our approach outperforms the previous best method in terms of NDS by +1.7%, while also surpassing the leading approach in mAP by +1.4%.
__label__natural_language_processing Our codes are released at https://github.com/danshi777/IRCAN.
__label__machine_learning_for_healthcare This design allows S-MolSearch to adaptively utilize unlabeled data within the learning process.
__label__learning_theory In this paper, we introduce VCoTTA, a variational Bayesian approach to measure uncertainties in CTTA.
__label__machine_vision The findings are validated in both supervised and self-supervised learning, enabling models trained on imbalanced data to achieve CLIP-level performance on diverse recognition tasks.
"__label__interpretability_and_explainability Concretely, we exploit the fact that Leela is a transformer that treats every chessboard square like a token in language models, and give three lines of evidence: (1) activations on certain squares of future moves are unusually important causally; (2) we find attention heads that move important information ""forward and backward in time,"" e.g., from squares of future moves to squares of earlier ones; and (3) we train a simple probe that can predict the optimal move 2 turns ahead with 92% accuracy (in board states where Leela finds a single best line)."
__label__infrastructure The runtime accelerates execution with novel optimizations like RadixAttention for KV cache reuse and compressed finite state machines for faster structured output decoding.
__label__optimization This framework accommodates varying assumptions regarding smoothness and convexity, enabling the application of specific methods with different complexity results.
__label__optimization Multi-block minimax bilevel optimization has been studied recently due to its great potential in multi-task learning, robust machine learning, and few-shot learning.
__label__natural_language_processing Thereby, we present a path toward scalable adversarial training algorithms for robustly aligning LLMs.
__label__machine_vision Consequently, the support information is better utilized, leading to better performance.
__label__machine_vision Lattice vector quantization (LVQ), on the other hand, presents a compelling alternative, which can exploit inter-feature dependencies more effectively while keeping computational efficiency almost the same as scalar quantization.
__label__privacy These methods provide data-dependent low-rank approximations of the kernel matrix for general kernels in a DP manner.
__label__probabilistic_methods An empirical comparison of several graph sparsification techniques confirms our theoretical finding and shows that the metric backbone is an efficient sparsifier in the presence of communities.
__label__learning_theory The second resolving an issue for a clean, unambiguous embedding of (ensembles of) decision trees in this model.
__label__optimization However, existing search space transfer methods either lack an adaptive mechanism or are not flexible enough, making it difficult to efficiently identify promising search space during the optimization process.
__label__privacy Our framework will slightly modify the closeness metric and instead give a simple and efficient application of the sparse vector technique.
__label__natural_language_processing We conduct extensive experiments on publicly available language processing datasets, including the LLaMA-V1|V2|V3 family and OPT, covering various benchmarks.
__label__graph_neural_networks In contrast, we develop a flexible question-answering framework targeting real-world textual graphs, applicable to multiple applications including scene graph understanding, common sense reasoning, and knowledge graph reasoning.
__label__deep_learning_architectures AlphaPruning can be used in conjunction with multiple existing LLM pruning methods.
__label__neuroscience_and_cognitive_science In an application motivated by redundancy reduction theory, we demonstrate that when the inputs are natural image statistics and the target distribution is a spherical Gaussian, the circuit learns a nonlinear transformation that significantly reduces statistical dependencies in neural responses.
__label__evaluation In this work, we develop a statistical framework to bridge this gap.
__label__diffusion_based_models Our experiments in image and motion generation confirm the efficacy of these techniques.
__label__machine_vision Specifically, we propose a multi-view mask strategy for addressing the challenges of endoscopic videos.
__label__diffusion_based_models Hence, we introduce latent partitioning to reduce the training-inference gap and lookahead denoising to leverage the benefit of forward referencing.
__label__diffusion_based_models Building on the success of diffusion models in visual generation, flow-based models reemerge as another prominent family of generative models that have achieved competitive or better performance in terms of both visual quality and inference speed.
__label__other The self-generated manual can not only improve the adaptability but also guide the planning of smaller LLMs while being human-readable.
__label__reinforcement_learning However, extending this framework to a multi-agent context introduces the need to simultaneously learn both local value functions to capture local observations and individual actions, and a joint value function for exploiting centralized learning.
__label__machine_vision For instance, VQGAN-FC is restricted to learning a codebook with a maximum size of 16,384, maintaining a typically low utilization rate of less than 12% on ImageNet.
__label__learning_theory However, existing theoretical work fail to build up an understanding of the connection between this semantic regularity and the innovative power of ICL.
__label__optimization_for_deep_networks In this work, we propose *Learning from Teaching* (**LoT**), a novel regularization technique for deep neural networks to enhance generalization.
__label__reinforcement_learning Existing methods work by emulating the human preference at the single decision (turn) level, limiting their capabilities in settings that require planning or multi-turn interactions to achieve a long-term goal.
__label__safety_in_machine_learning We demonstrate the efficacy of our approach on biography and medical question-answering datasets.
__label__reinforcement_learning The success of Large Language Models (LLMs) sheds light on utilizing such knowledge behind the books.
__label__safety_in_machine_learning Hallucinations in these models create uncertainty about their reliability, raising major concerns about their practical application.
__label__neuroscience_and_cognitive_science Inferring the synaptic plasticity rules that govern learning in the brain is a key challenge in neuroscience.
__label__other Though Bayesian inference can alleviate this issue, a direct posterior inference at clients may result in biased local posterior estimates due to data heterogeneity, leading to a sub-optimal global posterior.
__label__machine_vision We train Artemis on the newly established ViderRef45K dataset with 45K video-QA pairs and design a computationally efficient, three-stage training procedure.
__label__machine_vision In addition, we propose a post-processing approach to mitigate the severe over-segmentation in the online setting.
__label__machine_vision However, traditional LVQ structures are designed/optimized for uniform source distributions, hence nonadaptive and suboptimal for real source distributions of latent code space for Neural image compression tasks.
"__label__safety_in_machine_learning We present PREPARED, the first efficient
technique for provable editing of DNNs."
__label__machine_vision Inspired by this, (i) we propose a Memory-based Vision-Language Tracker (MemVLT).
__label__causal_inference In this work, we aim to provide a theoretical understanding of when extrapolation is possible and offer principled methods to achieve it without requiring an on-support target distribution.
__label__machine_learning_for_physical_sciences A workaround is to abandon the frequency modes exceeding a predefined threshold, but this limits the FNOs' ability to represent high-frequency details and poses non-trivial challenges for hyper-parameter specification.
__label__natural_language_processing These challenges are further exacerbated in LMMs due to the integration of multiple data types and the combinational complexity of multimodal ICDs.
__label__natural_language_processing This has sparked a vigorous debate about whether LLMs are genuinely reasoning or merely memorizing.
__label__other Given a complex, textual segmentation description, DeiSAM leverages Large Language Models (LLMs) to generate first-order logic rules and performs differentiable forward reasoning on generated scene graphs.
__label__deep_learning_architectures To this end, this paper proposes the Coupled SSM model, for coupling state chains of multiple modalities while maintaining independence of intra-modality state processes.
__label__machine_vision To address these issues, we propose a one-step effective diffusion network, namely OSEDiff, for the Real-ISR problem.
__label__probabilistic_methods In practice, however, this recipe entails i) approximating an intractable posterior at each time step; and ii) encapsulating results appropriately to allow for posterior propagation.
__label__neuroscience_and_cognitive_science Finally, we show that ORGaNICs can be trained by backpropagation through time without gradient clipping/scaling, thanks to its intrinsic stability property and adaptive time constants, which address the problems of exploding, vanishing, and oscillating gradients.
__label__graph_neural_networks In this paper, we investigate a novel mechanism for injecting global topological invariance into pooling layers using PH, motivated by the observation that filtration operation in PH naturally aligns graph pooling in a cut-off manner.
__label__diffusion_based_models We introduce Equivariant Neural Diffusion (END), a novel diffusion model for molecule generation in 3D that is equivariant to Euclidean transformations.
__label__machine_vision Large transformer-based foundation models have been commonly used as pre-trained models that can be adapted to different challenging datasets and settings with state-of-the-art generalization performance.
__label__learning_theory In experiments, we show that NCP with a 2-hidden-layer network matches or outperforms leading methods.
__label__other This work discloses the hidden semantic structure within score-based generative models, unveiling their potential as effective discriminative priors.
__label__optimization Extensive experiments are conducted on various datasets to showcase the superior performance of the proposed FIARSE.
"__label__machine_vision Our key insight is a ""decompose-recompose"" approach that factorizes the video scene into the background and object tracks, while also factorizing object motion into 3 components: object-centric deformation, object-to-world-frame transformation, and camera motion."
__label__reinforcement_learning Reinforcement Learning (RL) is a powerful method for controlling dynamic systems, but its learning mechanism can lead to unpredictable actions that undermine the safety of critical systems.
__label__natural_language_processing We confirm much publicly disclosed information about these models, and also make several new inferences: GPT-4o is much more multilingual than its predecessors, training on 10x more non-English data than GPT-3.5, Llama 3 and Claude are trained on predominantly code, and many recent models are trained on 7-16% books.
__label__generative_models We observe that attention, as the core module of MLLMs, connects text prompt tokens and visual tokens, ultimately determining the final results.
__label__machine_learning_for_healthcare Precise image segmentation provides clinical study with instructive information.
__label__safety_in_machine_learning We are also among the first to formulate the settings, goals, and evaluations in LLM unlearning.
__label__optimization We propose to use the *locally evolving set process*, a novel framework to characterize the algorithm locality, and demonstrate that many standard solvers can be effectively localized.
__label__interpretability_and_explainability Under the manifold assumption, we outline a framework for discovering continuous symmetry in data beyond the affine transformation group.
__label__diffusion_based_models This framework models the stereotype problem as a probability distribution alignment problem, aiming to align the stereotype probability distribution of the generated image with the stereotype-free distribution.
__label__machine_vision Additionally, for fast adaptation to off-the-shelf sensors, we generate a pixel-wise affinity map based on the knowledge from the foundation model.
__label__optimization_for_deep_networks However, existing theoretical analyses for sketching-based distributed learning (sketch-DL) either incur a prohibitive dependence on the ambient dimension or need additional restrictive assumptions such as heavy-hitters.
__label__human-AI_interaction We validate our framework using real human predictions from two different human subject studies and show that, in decision support systems based on prediction sets, there is a trade-off between accuracy and counterfactual harm.
__label__reinforcement_learning Understanding cognitive processes in multi-agent interactions is a primary goal in cognitive science.
__label__deep_learning_architectures We find that a simple yet heuristic integration strategy can significantly alleviate the modality imbalance phenomenon.
__label__learning_theory We consider a planted model with two datasets $X,Y$ that consist of $n$ datapoints in $\mathbb{R}^d$, where $Y$ is a noisy version of $X$, up to an orthogonal transformation and a relabeling of the data points.
__label__algorithmic_game_theory We show that any clustering satisfying a weak proportionality notion of Brill and Peters (EC'23) simultaneously obtains the best known approximations to the proportional fairness notion of Chen et al., but also to individual fairness (Jung et al., FORC'20) and the ``core'' (Li et al., ICML'21).
__label__machine_learning_for_other_sciences_and_fields Additionally, we extend our \emph{Double Rounding} to one-shot mixed precision training and develop a Hessian-aware Stochastic Bit-switching (${\bf HASB}$) strategy.
__label__machine_learning_for_other_sciences_and_fields We demonstrate the effectiveness of the proposed method on six inverse problems (three linear and three nonlinear), including a real-world black hole imaging problem.
__label__machine_learning_for_other_sciences_and_fields In this paper, we propose a novel sequence structure learning and modulation approach that endows Transformers with the ability to model and utilize such fixed-sequence structural properties for improved performance on inertial pose estimation tasks.
__label__reinforcement_learning We show the generalizability and robustness of BECAUSE under fewer samples or larger numbers of confounders.
__label__generative_models By using up to 8 sampling steps, we obtain distilled models that outperform not only their one-step versions but also their original many-step teacher models, obtaining new state-of-the-art results on the Imagenet dataset.
__label__machine_vision Additionally, we explore multiple design variants to find the best practice of DyT.
__label__machine_learning_for_physical_sciences A polymer inverse design task for gas separation with feedback from domain experts further demonstrates its practical utility.
__label__machine_vision Then, the pre-trained encoders are used for downstream tasks.
__label__safety_in_machine_learning Inspired by this observation, we introduce LT-Defense, the first searching-free backdoor defense via exploiting the long-tailed effect.
__label__machine_learning_for_other_sciences_and_fields Finally, we introduce two large, geometry-focused datasets of wireless signal propagation in indoor scenes.
__label__natural_language_processing Existing methodologies usually prioritize a global optimization for finding the global optimum, which however will perform poorly in certain tasks.
__label__generative_models Large Language Models (LLMs) are being used for a wide variety of tasks.
__label__natural_language_processing To explore the nature of prompt generalization on unknown domains, we conduct pilot experiments and find that (i) Prompts gaining more attention weight from PLMs’ deep layers are more generalizable and (ii) Prompts with more stable attention distributions in PLMs’ deep layers are more generalizable.
__label__interpretability_and_explainability However, it is unclear whether LLMs accurately learn the underlying preferences in human feedback data.
__label__machine_vision Experiments demonstrate the effectiveness of TextCtrl compared with previous methods concerning both style fidelity and text accuracy.
__label__machine_vision To our knowledge, this is the first work that explicitly emphasizes assessing complex image exposure problems at a pixel level, providing a significant boost to the IEA and exposure-related community.
"__label__speech_and_audio In contrast, sound event localization and detection models are limited to recognizing sounds from a fixed number of classes, and they localize the source to absolute position (e.g., 0.2m) rather than a position described using natural language (e.g., ""next to me"")."
__label__deep_learning_architectures However, Mamba’s downstream learning capabilities remain either unexplored–e.g., mixed-precision (MPFT) and parameter-efficient fine-tuning (PEFT)–or under-evaluated–e.g., in-context learning (ICL).
__label__interpretability_and_explainability In this work, we dramatically expand a notion of *decision sparsity* called the *Sparse Explanation Value* (SEV) so that its explanations are more meaningful.
__label__learning_theory In this work, we study SSL for high dimensional sparse Gaussian classification.
__label__machine_vision We introduces a general detection-based approach to text line recognition, be it printed (OCR) or handwritten text (HTR), with latin, chinese or ciphered characters.
__label__privacy Our numerical experiments demonstrate that models trained using BSR perform on par with the best existing methods, while completely avoiding their computational overhead.
__label__reinforcement_learning These grouped agents train calibration policies coordinately to adjust multiple parameters using MARL.
"__label__learning_theory This framework casts learning with label
    noise as a form of domain adaptation, in particular, domain adaptation
    under posterior drift."
__label__machine_vision Composed Image Retrieval (CIR) facilitates retrieving an image matching a reference image while incorporating specified textual modifications, which is crucial for internet searches and e-commerce.
__label__probabilistic_methods This enables a factor of 1000 times faster inference than the current state of the art and, correspondingly, supports models with several orders of magnitude more features than the current state of the art can consider.
__label__diffusion_based_models We introduce a novel, training-free method for sampling *differentiable representations* (diffreps) using pretrained diffusion models.
__label__learning_theory Armed with this CLT, we propose a hypothesis testing framework as well as an efficient implementation using the Sinkhorn algorithm.
__label__deep_learning_architectures The key to our approach is to leverage the recent progress in differentiable trajectory optimization, which enables computing the gradients of the loss with respect to the parameters of trajectory optimization.
__label__machine_learning_for_social_sciences Our framework is generalizable across various domains with minimal domain-specific customization, ensuring easy transfer to other areas facing similar challenges in data availability.
"__label__learning_theory Our framework extends well-studied notions of stability, including Differential Privacy ($k = 0$), differentially private learning with public data (where the $k$ public datapoints are fixed in advance),
and stable sample compression (where the $k$ datapoints are selected adaptively by the algorithm)."
__label__diffusion_based_models Our approach requires no additional retraining and is compatible with various existing editing methods.
__label__reinforcement_learning Moreover, we define distributional perspectives on action gaps and advantages.
__label__causal_inference Causal discovery is a fundamental problem with applications spanning various areas in science and engineering.
__label__machine_learning_for_healthcare Motivated by a simple observation -- that neighboring instances are likely to have the same label -- we propose a novel, principled, and flexible mechanism to model local dependencies.
__label__learning_theory This is a novel statistical-computational trade-off for density estimation, demonstrating that any data structure must use close to a linear number of samples or take close to linear query time.
__label__machine_vision Extensive experiments on two VIReID benchmarks (i.e., SYSU-MM01, RegDB) and different systems validate the effectiveness of our method.
__label__optimization Several delayed consensus algorithms for networks of single- and double-integrators using only the relative positions are considered.
__label__privacy The algorithm applies noisy Mirror Descent to a dual problem from relaxing the hard constraints for private shadow prices, and then uses the shadow prices to coordinate allocations in the primal problem.
__label__probabilistic_methods Codes are publicly available at \url{https://github.com/KenCao2007/WSM_TPP}.
__label__diffusion_based_models Despite the success of DPM in practice, the mechanism behind it remains to be explored.
__label__online_learning from some distribution).
__label__diffusion_based_models For example, SFD achieves 4.53 FID (NFE=2) on CIFAR-10 with only **0.64 hours** of fine-tuning on a single NVIDIA A100 GPU.
__label__privacy Our source code is publicly available at https://github.com/ntuaislab/Trap-MID.
__label__machine_vision Our method consists of two key components: (i) By constructing fine-grained text style disentanglement and robust text glyph structure representation,  TextCtrl explicitly incorporates Style-Structure guidance into model design and network training, significantly improving text style consistency and rendering accuracy.
__label__deep_learning_architectures In this work we propose a derivative-enhanced deep operator network (DE-DeepONet), which leverages derivative information to enhance the solution prediction accuracy and provides a more accurate approximation of solution-to-parameter derivatives, especially when training data are limited.
__label__safety_in_machine_learning The first is a traditional Label Attack, tricking VLMs into misidentifying class labels, such as confusing Donald Trump for Joe Biden.
__label__fairness * Theoretically, we develop a model of recommendations with user and item fairness objectives and characterize the solutions of fairness-constrained optimization.
__label__graph_neural_networks Applying our characterizations, we also prove that, relative to graph properties definable in monadic second-order logic (MSO), our infinitary and rule-based logics are equally expressive.
__label__other Experimental results demonstrate that LeHaCE provides a more stable, fair, and comprehensive evaluation of object hallucinations in LVLMs compared to existing methods.
__label__optimization_for_deep_networks We thus complement existing works linking $L2$-regularization with low-rank regularization, and in particular, explain why such regularization on the matrix product affects early stages of training.
__label__probabilistic_methods With the most common variational objective function, known as the evidence lower bound (ELBO), only convergence to a *local* optimum can be guaranteed.
__label__optimization_for_deep_networks First, we construct the mean-field limit of large-scale Transformers, showing that as the model width and depth go to infinity, gradient flow converges to the Wasserstein gradient flow, which is represented by a partial differential equation.
__label__learning_theory Further, we introduce an entropic regularization of this statistic, and establish a central limit theorem (CLT) and consistency of the bootstrap procedure for the empirical statistic.
__label__machine_vision In addition, this enables more fine-grained and controllable multimodal generation capabilities and allows studying the distillation of models trained on diverse data and objectives into one unified model.
__label__machine_vision We conduct extensive experiments and benchmark SaSPA against both traditional and generative data augmentation techniques.
__label__reinforcement_learning In multi-agent reinforcement learning (MARL), parameter sharing is commonly employed to enhance sample efficiency.
__label__optimization_for_deep_networks Moreover, recognizing the impact of gradient amplitude of adaptive decay rates, we implement an adaptive learning rate mechanism for gradient compensation to prevent inadequate learning of over-small or over-large gradient.
__label__diffusion_based_models Our comparative studies show that CAF not only outperforms rectified flow with reflow procedures in terms of speed and accuracy but also demonstrates substantial improvements in preserving coupling for fast generation.
__label__learning_theory With these conditions, we interpret the learnability of the representative Deep MIL algorithms and validate them through empirical studies.
__label__learning_theory We characterize conditions on the signal to noise ratio (SNR) of the model parameters giving rise to benign versus non-benign, or harmful, overfitting: in particular, if the SNR is high then benign overfitting occurs, conversely if the SNR is low then harmful overfitting occurs.
__label__machine_vision In this study, we introduce LOVA3, an innovative framework named ``Learning tO Visual Question Answering, Asking and Assessment,'' designed to equip MLLMs with these additional capabilities.
__label__evaluation In this paper, we challenge the go-to approach of *data-only testing* and introduce *Context-Aware Testing* (CAT) which uses context as an inductive bias to guide the search for meaningful model failures.
__label__machine_vision Towards better scalability, we present a framework, Agent-to-Sim, that learns simulatable 3D agents in a 3D environment from monocular videos.
__label__natural_language_processing To address the representation density problem, we introduce a simple but effective contrastive learning strategy, namely SignCL, which encourages gloss-free models to learn more discriminative feature representation in a self-supervised manner.
__label__neuroscience_and_cognitive_science Moreover, we benchmark all AI models in point-light displays of two standard video datasets in computer vision.
__label__optimization_for_deep_networks Recognizing that **$\epsilon$-softmax**-enhanced losses may slightly reduce fitting ability on clean datasets, we further incorporate them with one symmetric loss, thereby achieving a better trade-off between robustness and effective learning.
__label__graph_neural_networks Furthermore, we prove that, under certain expressivity conditions, ScaleGMN can simulate the forward and backward pass of any input feedforward neural network.
__label__diffusion_based_models Recent advances in generative modeling with diffusion processes (DPs) enabled breakthroughs in image synthesis.
"__label__neuroscience_and_cognitive_science In this work, we address these challenges by linking dynamic divisive normalization (DN) to the stability of ""oscillatory recurrent gated neural integrator circuits'' (ORGaNICs), a biologically plausible recurrent cortical circuit model that dynamically achieves DN and that has been shown to simulate a wide range of neurophysiological phenomena."
__label__privacy Graph neural networks (GNNs) have attracted considerable attention due to their diverse applications.
__label__machine_learning_for_healthcare We expect this foundation model can promote the development of volumetric medical image analysis.
__label__natural_language_processing Experiments on several benchmarks (GSM8K, StrategyQA, MBPP and HumanEval) demonstrate that SCMoE can consistently enhance Mixtral 8x7B’s reasoning capability across various domains.
__label__diffusion_based_models It also provides perspectives on the choices of time and variance schedules in sampling: when the score is well trained, the design in [Song et al., 2021] is more preferable, but when it is less trained, the design in [Karras et al., 2022] becomes more preferable.
__label__speech_and_audio To address this, we introduce global and local Transformer blocks that can directly handle long sequences more efficiently without chunking and dual-path processing.
__label__machine_vision Experimental results demonstrate that CGFormer achieves state-of-the-art performance on the SemanticKITTI and SSCBench-KITTI-360 benchmarks, attaining a mIoU of 16.87 and 20.05, as well as an IoU of 45.99 and 48.07, respectively.
__label__interpretability_and_explainability This method classifies an image by comparing it to a set of learned prototypes, providing explanations of the form ``this looks like that.''
__label__machine_learning_for_physical_sciences We introduce two implementations of AM-FNO, based on the recently developed, appealing Kolmogorov–Arnold Network (KAN) and Multi-Layer Perceptrons (MLPs) equipped with orthogonal embedding functions respectively.
__label__interpretability_and_explainability Formal specifications have been proposed as ways to produce human-interpretable policies for autonomous systems that can still be learned from examples.
__label__natural_language_processing Furthermore, Text2NKG supports four typical NKG schemas: hyper-relational schema, event-based schema, role-based schema, and hypergraph-based schema, with high flexibility and practicality.
__label__machine_learning_for_other_sciences_and_fields Power efficiency is plateauing in the standard digital electronics realm such that new hardware, models, and algorithms are needed to reduce the costs of AI training.
__label__algorithmic_game_theory Unexpectedly, we find that common diversity-promoting approaches do not work in the presence of dual influence, while relevancy-optimizing methods like top-$k$ truncation can prevent polarization and improve diversity of the system.
__label__online_learning In particular, we prove that in the full feedback model,  $k$ queries are enough to achieve an optimal regret of $\Theta(\min\{\sqrt T, \frac{T}{k}\})$.
__label__human-AI_interaction In this paper, we take one state-of-the-art intractable cognitive model and propose a tractable surrogate that is suitable for deployment in preference learning.
__label__diffusion_based_models To address this issue, we propose COrrespondence-guided Video Editing (COVE), leveraging the inherent diffusion feature correspondence to achieve high-quality and consistent video editing.
__label__diffusion_based_models We employ Meta-Exploration to train an additional scheduler model dedicated to scheduling contextualized noise for each sentence.
__label__deep_learning_architectures Specifically, in each layer, we use two different ways to represent an image: a fine-grained regular grid and a coarse-grained set of semantic slots.
__label__optimization By leveraging the submodular property, we propose a natural greedy strategy that simultaneously maximizes both bounds.
__label__privacy Lastly, since any distribution can be reduced to a continuous distribution, our algorithm is successfully carried to multiple other families of distributions and thus has numerous applications.
__label__interpretability_and_explainability These methods search for hypothesised consistency structures of latent knowledge.
__label__probabilistic_methods This has proved to be data-efficient for univariate predictions, however, existing constructions for higher dimensional densities are only possible by relying on restrictive assumptions on the model's multivariate structure.
__label__natural_language_processing The code is available at https://github.com/Waste-Wood/MeanLearn.
__label__machine_vision Existing works mainly rely on introducing external semantic or depth priors to supervise the optimization of 3D representations.
__label__natural_language_processing Instead of looking for a mode in the distribution, we generate multiple samples from high-density areas through the Metropolis-Hastings algorithm, a simple Markov chain Monte Carlo approach.
__label__other We present the first mini-batch kernel $k$-means algorithm.
__label__learning_theory In this paper, we derive *entrywise* error bounds for low-rank approximations of kernel matrices obtained using the truncated eigen-decomposition (or singular value decomposition).
__label__machine_vision Experiments on four standard benchmark datasets demonstrate the superiority of our method with reasonable baselines.
__label__safety_in_machine_learning This framework extends the application of conformalized multiple testing to complex selective settings.
__label__optimization Compared to prior state of the art implementations, MUVERA achieves consistently good end-to-end recall and latency across a diverse set of the BEIR retrieval datasets, achieving an average of 10% improved recall with 90% lower latency.
__label__machine_learning_for_other_sciences_and_fields This paper proposes a hybrid system combining LLMs and rule-based reasoning to detect accounting error vulnerabilities in smart contracts.
__label__machine_vision We then define adversarial attacks by replacing the projected gradient descent (PGD) with the mirror descent associated with the learned Bregman divergence, and use them to improve the state-of-the-art in robustness through adversarial training for common image corruptions.
__label__machine_vision Our key idea lies in exploring the self-supervisions inherent in the binocular stereo consistency between each pair of binocular images constructed with disparity-guided image warping.
__label__diffusion_based_models Inspired by the observations, we proceed to explore the influence of each token in the text prompt during the two stages.
__label__diffusion_based_models Furthermore, Resfusion can be easily applied to image generation and emerges with strong versatility.
__label__machine_learning_for_other_sciences_and_fields We provide extensive experiments and comprehensive theoretical analyses to demonstrate the feasibility and efficiency of NeurKItt.
__label__natural_language_processing Consequently, each attention head flexibly processes tokens using multiple RoPE angles dynamically selected by the router to attend to the needed positions.
__label__generative_models M-FFF is straightforwardly adapted to any manifold with a known projection.
__label__reinforcement_learning VMOC naturally integrates maximum entropy intrinsic rewards to promote the exploration of diverse and effective options.
__label__machine_learning_for_other_sciences_and_fields This severely constrains the scope of application for post-prediction inference.
__label__generative_models Large language models (LLMs) with billions of parameters excel at predicting the next token in a sequence.
__label__learning_theory Despite previous efforts to understand the data generation process of diffusion models, existing theoretical support remains highly suboptimal in the presence of low-dimensional structure, which we strengthen in this paper.
__label__optimization_for_deep_networks Additionally, we show that these initial LRs result in a sparse set of learned features, with a clear focus on those most relevant for the task.
__label__active_learning This paper introduces a novel metric learning method for Coreset to perform slice-based active learning in 3D medical segmentation.
__label__machine_vision ** This work provides several implications.
__label__learning_theory In this work, we generalize the KT algorithm to speed up supervised learning problems involving kernel methods.
"__label__neuroscience_and_cognitive_science We also compare
with recent algorithms for using LLMs to generate and revise hypotheses, finding
that our online inference method yields higher accuracy at recovering the true
underlying rule, and provides better support for designing optimal experiments."
__label__generative_models Recently, Deep Image Prior (DIP) has emerged as an effective unsupervised one-shot learner, delivering competitive results across various image recovery problems.
__label__machine_learning_for_social_sciences However, when the number of alternatives, $m$, is large, eliciting the prediction report or even the vote over $m$ alternatives might be too costly.
__label__robotics Extensive experiments demonstrate the effectiveness of STP as well as unleash its generality and data efficiency by further post-pre-training and hybrid pre-training.
__label__learning_theory This contrasts dramatically with negative results due to Gold and Angluin in a well-studied model of language learning where the goal is to identify an unknown language from samples; the difference between these results suggests that identifying a language is a fundamentally different problem than generating from it.
__label__reinforcement_learning We also show that our approach can be easily applied to popular architectures such as ResNets and transformers while recovering and in some cases even slightly improving the performance of the base model in common stationary benchmarks.
__label__machine_vision Code: https://github.com/nikosips/UDON.
__label__optimization_for_deep_networks Furthermore, we outperform QLoRA for fine-tuning LLaMA and show competitive performance against other memory-efficient pre-training methods on the large-scale C4 dataset.
__label__optimization_for_deep_networks Building on this observation, we introduce a novel model fusion technique that optimizes both the desired metric and loss through multi-objective Bayesian optimization.
__label__learning_theory Extending beyond prior research that primarily focused on sequential feature learning, we investigate the non-sequential scenario, emphasizing the pivotal role of inter-feature interactions in expediting training and enhancing performance, particularly with an uninformed initialization strategy.
__label__optimization Extensive experiments on both simulated and real-world datasets demonstrate that BPQP achieves a significant improvement in efficiency—typically an order of magnitude faster in overall execution time compared to other differentiable optimization layers.
__label__machine_vision Moreover, we develop a novel baseline method built on mainstream pretrained vision models, combining visual place recognition and object association into one Transformer decoder architecture.
__label__probabilistic_methods We propose Wasserstein gradient boosting, a novel extension of gradient boosting, which fits a new weak learner to alternative pseudo residuals that are Wasserstein gradients of loss functionals of probability distributions assigned at each input.
__label__bandits Additionally, we reveal a significant relationship between linear ensemble sampling and Linear Perturbed-History Exploration (LinPHE), showing that LinPHE is a special case of linear ensemble sampling when the ensemble size equals $T$.
__label__probabilistic_methods Although many VI methods that take correlation into account have been proposed, these methods generally are not scalable enough to capture the correlation among data instances, which often arises in applications with graph-structured data or explicit constraints.
__label__machine_learning_for_healthcare Experimental results indicate that our method outperforms existing approaches in 13 out of 15 molecular property prediction benchmarks in MoleculeNet dataset and 8 out of 12 benchmarks in the QM9 benchmark, achieving new state-of-the-art results on average.
"__label__optimization Finally, the second attention layer is a classifier that
compares these features with the feature at the output position, and uses the resulting similarity scores to generate the desired output."
__label__graph_neural_networks We theoretically prove CIA-LRA's effectiveness by deriving an OOD generalization error bound based on PAC-Bayesian analysis.
__label__optimization We propose adaptive versions of GD and ProxGD that are based on observed gradient differences and, thus, have no added computational costs.
__label__machine_vision Existing methods primarily focus on recalling unknown objects, neglecting to explore the reasons behind them.
__label__optimization_for_deep_networks We analyze the training dynamics of one-layer multi-head transformers to {in-contextly} predict unlabeled inputs given partially labeled prompts, where the labels contain Gaussian noise and the number of examples in each prompt  are not sufficient to determine the template.
__label__machine_learning_for_healthcare To tackle these issues, we propose a hierarchical knowledge augmentation approach and a novel Procedure-Encoded Surgical Knowledge-Augmented Video-Language Pretraining (PeskaVLP) framework.
__label__natural_language_processing Without updating the parameters of LLMs, the average accuracy improved by 6.7 and 8.0 across all languages and low-resource languages on the MGSM dataset, respectively.
__label__interpretability_and_explainability Extensive experiments and ablation studies demonstrate that our model outperforms state-of-the-art models by up to 10.1\% in prediction accuracy across a wide range of tasks.
__label__machine_vision The code for this project is publicly available at  https://github.com/peiyao-w/BaFormer.
__label__other They are then optimized through M$^3$-Impute’s novel feature correlation unit (FCU) and sample correlation unit (SCU) that enable explicit consideration of feature and sample correlations for imputation.
__label__reinforcement_learning In this paper we study RL with regular safety properties.
__label__machine_learning_for_other_sciences_and_fields In this study, we investigate the design of priority queues within the learning-augmented framework, where algorithms use potentially inaccurate predictions to enhance their worst-case performance.
__label__machine_learning_for_healthcare A major challenge of the long measurement times in magnetic resonance imaging (MRI), an important medical imaging technology, is that patients may move during data acquisition.
__label__reinforcement_learning The results showcase that the discussion greatly changes players' utilities by affecting their beliefs, emphasizing the significance of discussion tactics.
__label__natural_language_processing Moreover, our experiments demonstrate, for the first time, the feasibility of aligning a single LLM to represent an exponentially vast spectrum of human preferences through various optimization methods.
__label__machine_vision To address these challenges, we first introduce a cooperative unfolding network that jointly models atmospheric scattering and image scenes, effectively integrating physical knowledge into deep networks to restore haze-contaminated details.
__label__optimization Beyond the standard bilevel optimization formulation, we extend our discussion to conditional bilevel optimization and also two special cases: minimax and compositional optimization.
__label__privacy To the best of our knowledge, this is the first study on designing truthful (and privacy-preserving) mechanisms for high dimensional sparse linear regression.
__label__optimization_for_deep_networks Using these insights, we show how $\eta_{\text{init}}$ can be properly chosen by utilizing the loss catapult mechanism, which saves on the number of warmup steps, in some cases completely eliminating the need for warmup.
__label__natural_language_processing We conduct extensive experiments on multiple challenging tasks such as arithmetic, knowledge reasoning, and multimodal benchmarks spanning GSM8K, MMLU, SQA, and VQA, demonstrating that our DSA method achieves significant performance gains on the LLaMA-1|2|3, Mistral, and OPT models.
__label__machine_vision Comprehensive experimental results on a series of vision-centric and VQA benchmarks indicate that our Lumen model not only achieves or surpasses the performance of existing LMM-based approaches in a range of vision-centric tasks while maintaining general visual understanding and instruction following capabilities.
__label__human-AI_interaction However, accurate modeling of human choice behavior is challenging due to a range of context effects that arise from how humans contrast and evaluate options.
__label__generative_models Many recent state-of-the-art results in language tasks were achieved using compound systems that perform multiple Language Model (LM) calls and aggregate their responses.
__label__human-AI_interaction We demonstrate on large-scale human data that this model produces significantly better inferences on static and actively elicited data than existing Bradley-Terry variants.
__label__learning_theory Multiple works have developed the theory of implicit bias for binary classification under the assumption that the loss satisfies an *exponential tail property*.
__label__causal_inference We show that if we observe all bivariate distributions, the CMAXENT solution reduces to a logistic regression in the causal direction and Linear Discriminant Analysis (LDA) in the anticausal direction.
__label__diffusion_based_models 2) On the methodological front, we propose a simple yet effective method called Mixture of Low-rank Experts (MoLE) by considering low-rank modules trained on close-up hand and face images respectively as experts.
__label__optimization_for_deep_networks The attention mechanism in the Transformer architecture is a critical component of LLMs, as it allows the model to selectively focus on specific input parts.
__label__machine_vision Then, we propose a prompt-based Composition Incremental Learner (CompILer), to overcome the ambiguous composition boundary.
__label__natural_language_processing Large language models (LLMs) have significantly advanced performance across a spectrum of natural language processing (NLP) tasks.
__label__privacy We also quantify their behavior empirically on real and simulated  prediction tasks.
__label__diffusion_based_models This becomes stark in the case of diffusion models where a large number of steps gives the best samples, but the quality degrades rapidly with smaller number of steps.
__label__optimization The resulting policy is potentially history-dependent and non-Markovian.
__label__machine_vision To tackle the challenge of inaccurate interactive motion recovery in complex scenes, we propose LiveScene, a scene-level language-embedded interactive radiance field that efficiently reconstructs and controls multiple objects.
__label__optimization_for_deep_networks The code (including the W2 CUDA kernels) is attached and will be made public.
__label__machine_vision Specifically, we elaborately design three pretext tasks: 1) Text-guided Image Colorization, aims to establish the correspondence between the person-related image regions and the fine-grained color-part textual phrases.
__label__graph_neural_networks In response to a user's questions, our method provides textual replies and highlights the relevant parts of the graph.
__label__machine_vision Under this unsupervised multi-domain setting, we have identified inherent model bias within CLIP, notably  in its visual and text encoders.
__label__neuroscience_and_cognitive_science These points are referred to as the noise equilibria because, at these points, noise contributions from different directions are balanced and aligned.
__label__optimization_for_deep_networks SIRIUS is evaluated on 6 models with 8 difficult generation tasks in reasoning, deduction, and coding and shows consistent effectiveness and efficiency.
__label__safety_in_machine_learning Particularly, we propose a novel Unlearnable Segmentation (UnSeg) framework to train a universal unlearnable noise generator that is capable of transforming any downstream images into their unlearnable version.
__label__optimization By deriving and applying a parallelized form of Newton's method, they achieve large speedups over sequential evaluation.
__label__evaluation That is, there is a high probability that repeating the study under different conditions will yield similar results.
__label__graph_neural_networks Moreover, we theoretically provide a finite-sample coverage guarantee of SNAPS.
__label__generative_models Based on this, we further propose the LCGen method, which guides text-to-3D to obtain different priors with different certainty from various viewpoints, aiding in view-consistent generation.
__label__generative_models In many real-world applications, we often have access to large, expensively trained unconditional diffusion models, which we aim to exploit for improving conditional sampling.
__label__neuroscience_and_cognitive_science Using this approach, we construct a multi-subject model trained on the combined data from 21 subjects performing a behavioral task.
__label__machine_learning_for_physical_sciences EScAIP leverages a novel multi-head self-attention formulation within graph neural networks, applying attention at the neighbor-level representations.
__label__learning_theory The code is available at https://github.com/s-kumano/perturbation-learning.
__label__graph_neural_networks Further, we notice that there is recent literature suggesting that multi-task training on CLRS can improve the reasoning accuracy of certain tasks, implying intrinsic connections between different algorithmic tasks.
__label__reinforcement_learning However, when facing multiple input modalities, existing dynamics modeling methods (e.g., DeepMDP) usually stumble in addressing the complex and volatile relationship between different modalities.
__label__privacy To address this limitation, we propose increasing the density around high-quality pseudo-private data—recovered samples through model inversion that exhibit characteristics of the private training data—by slightly tuning the generator.
__label__optimization_for_deep_networks Adam has been shown to outperform gradient descent on large language models by a larger margin than on other tasks, but it is unclear why.
__label__machine_learning_for_physical_sciences Our method outperforms widely applied classical approaches and state-of-the-art deep learning and reinforcement learning methods.
__label__graph_neural_networks We characterize the computational power of neural networks that follow the graph neural network (GNN) architecture, not restricted to aggregate-combine GNNs or other particular types.
__label__other Extensive verification on large-scale real-world datasets, including nuPlan and WOMD, demonstrates that BeTop achieves state-of-the-art performance in both prediction and planning tasks.
__label__deep_learning_architectures In this work, we present a unifying framework that enables searching among all linear operators expressible via an Einstein summation.
__label__machine_vision Current multimodal Large Language Models (MLLMs) suffer from ''hallucination'', occasionally generating responses that are not grounded in the input images.
__label__machine_vision Extensive experiments on two public benchmarks, Scan2CAD and ROBI, show that our method achieves a new state-of-the-art performance on the multi-instance point cloud registration task.
__label__diffusion_based_models We leverage the query-key self-attention mechanism of ViTs to explore the interconnections among different anatomical parts in human pose skeletons.
__label__machine_vision Empirical studies show our MLLM named Octopus improves accuracy on popular MLLM tasks and is up to 5× faster on visual grounding tasks.
__label__deep_learning_architectures Pre-trained language models have been proven to possess strong base capabilities, which not only excel in in-distribution language modeling but also show powerful abilities in out-of-distribution language modeling, transfer learning and few-shot learning.
__label__diffusion_based_models However, these methods do not approximate Optimal Transport (OT) maps, which are known to have desirable properties.
__label__reinforcement_learning At the core of Cert-LSVI-UCB is an innovative certified estimator, which facilitates a fine-grained concentration analysis for multi-phase value-targeted regression, enabling us to establish an instance-dependent regret bound that is constant w.r.t.
__label__reinforcement_learning To resolve this issue, we study transfer RL with latent low rank structure.
__label__optimization For both settings, we design algorithms that combine a sampling strategy with a classic approximation algorithm for correlation clustering and study their theoretical guarantees.
__label__machine_vision Correspondences in point cloud registration are prone to outliers, significantly reducing registration accuracy and highlighting the need for precise inlier identification.
__label__machine_vision We optimize all parameters jointly via ray-traced differentiable rendering.
__label__machine_learning_for_other_sciences_and_fields To exploit the phenomenon and yet dismiss non-anomalous descent, SARAD performs anomaly detection via autoencoding in the association space.
__label__machine_vision However, the resulting adaptations suggest that there is still a substantial inter-domain gap left to be minimized.
__label__online_learning Continual learning (CL) empowers pre-trained vision-language (VL) models to efficiently adapt to a sequence of downstream tasks.
__label__machine_vision This paradigm overlooks the geometric features intrinsic to the lanes themselves and are prone to being influenced by inherent endpoint shifts in lane detection.
__label__algorithmic_game_theory We show that partial information release can counter-intuitively benefit the learner’s accuracy, allowing qualified agents to pass the classifier while preventing unqualified agents from doing so.
__label__natural_language_processing We analyze the capabilities of Transformer language models in learning compositional discrete tasks.
__label__machine_learning_for_social_sciences We evaluate our LLM agent framework and compare it with state-of-the-art personal mobility generation approaches, demonstrating the effectiveness of our approach and its potential applications in urban mobility.
__label__generative_models Later, textual learners are integrated with token-wise routing, blending the outputs of both modality learners collaboratively.
__label__natural_language_processing By applying our method to the LLaMA, Llama-2, Llama-3, and Mistral models, we achieve < 0.1 perplexity degradation with 3-bit quantization on both Wikitext-2 and C4, outperforming existing approaches.
__label__other Our method also has no trainable parameter.
__label__other We introduce a new concept of traversal of rationale that facilitates efficient embedding of rationale.
"__label__safety_in_machine_learning Finally, we show that agents that leverage ""Universalization""-based reasoning, a theory of moral thinking, are able to achieve significantly better sustainability."
__label__optimization_for_deep_networks decoupleQ gets rid of any tricks for dealing with outliers, sensitive channels, etc., and focuses only on the basic optimization objective to achieve high model accuracy on extreme low bit quantization.
__label__natural_language_processing The allure of these models lies in their ability to substantially increase the parameter count without a corresponding increase in FLOPs.
__label__neuroscience_and_cognitive_science Simulations show that the goal-reducer can be integrated into RL frameworks like Deep Q-learning and Soft Actor-Critic.
__label__machine_vision To address these limitations, we propose AltO, an unsupervised learning framework for estimating homography in multimodal image pairs.
__label__infrastructure Our evaluations demonstrate that in pure pipeline parallelism settings, our methods outperform 1F1B by from 7\% to 55\% in terms of throughput.
__label__generative_models Despite its popularity and success, few studies have elucidated the underlying mechanisms that contribute to its effectiveness.
__label__probabilistic_methods While the choice of prior strongly affects the resulting posterior distribution, computational tractability and lack of interpretability of the weight space typically limit the Laplace approximation to isotropic Gaussian priors, which are known to cause pathological behavior as depth increases.
__label__deep_learning_architectures Our study reveals that MGDL excels at representing functions containing high-frequency information.
__label__natural_language_processing In particular, we observe more preferable responses than binary labels and significant improvements where modestly-confident labels are in the majority.
__label__probabilistic_methods Our work takes an important step towards understanding the expressive power of tree-structured PCs, and our techniques may be of independent interest in the study of structure learning algorithms for PCs.
__label__reinforcement_learning This setting arises in many application domains, such as self-driving cars, healthcare, and finance, where expert demonstrations are made using contextual information, which is not recorded in the data available to the learning agent.
__label__learning_theory Occupancy-based PG naturally handles arbitrary offline data distributions, and, with one-line algorithmic changes, can be adapted to optimize any differentiable objective functional.
__label__interpretability_and_explainability In the real datasets, ChronoEpilogi is shown to reduce the number of TS variables by 96% (on average) by conserving or even improving forecasting performance.
__label__machine_vision Additionally, results on roaming datasets demonstrate that ODGS effectively restores fine details, even when reconstructing large 3D scenes.
__label__safety_in_machine_learning However, its effectiveness in improving the AR of SSMs remains unclear.
"__label__graph_neural_networks However, key limitations of *ℓ*-step MPGNNs are that their ""receptive field"" is typically limited to the *ℓ*-hop neighborhood of a node and that information exchange between distant nodes is limited by over-squashing."
__label__machine_learning_for_physical_sciences Incorporating physics-informed priors, such as in Hamiltonian Neural Networks (HNNs), achieves high-precision modeling for energy-conservative systems.
__label__machine_vision In addition, we propose a new interpretable intervention-based metric, to measure the quality of factors encoding in the representation.
__label__machine_vision We show that albedo can be recovered from our latent intrinsics without using any example albedos, and that the albedos recovered are competitive with SOTA methods.
__label__diffusion_based_models The CVD framework includes a novel cross-video synchronization module that promotes consistency between corresponding frames of the same video rendered from different camera poses using an epipolar attention mechanism.
__label__deep_learning_architectures We investigate two spectral adaptation mechanisms, namely additive tuning and orthogonal rotation of the top singular vectors, both are done via first carrying out Singular Value Decomposition (SVD) of pretrained weights and then fine-tuning the top spectral space.
__label__reinforcement_learning We propose a formalization of this search for diverse skills, building on a previous definition based on the mutual information between states and skills.
__label__safety_in_machine_learning In addition, introducing attentional truncation can mitigate the overfitting over complex interactions between tokens in deep ViT layers to further improve the transferability.
__label__diffusion_based_models Moreover, given the same computational resources, a ReNO-optimized one-step model outperforms widely-used open-source models such as SDXL and PixArt-alpha, highlighting the efficiency and effectiveness of ReNO in enhancing T2I model performance at inference time.
__label__probabilistic_methods However, in practical situations, there are many complicated problems that require master craftsmanship to combine these individual models  into a  single giant model.
__label__algorithmic_game_theory Kalogiannis et al.
__label__safety_in_machine_learning In this paper, we propose a framework for Synthesis with Efficient Exact Verification (SEEV).
__label__neuroscience_and_cognitive_science Here, we develop methods to efficiently select which neurons to stimulate such that the resulting neural responses will best inform a dynamical model of the neural population activity.
__label__active_learning We develop a tractable version of the framework to obtain Colander (Confidence functions for Efficient and Reliable Auto-labeling), a new post-hoc method specifically designed to maximize performance in TBAL systems.
__label__machine_vision TokenCLIPose leverages the rich semantic representations endowed by language for inducing keypoint-specific context, even for occluded keypoints.
__label__machine_vision To address this, recent research incorporates equivariant representation learning, which captures transformation-sensitive information.
__label__machine_vision The code is publicly available.
__label__reinforcement_learning It can guide the direction of artificial intelligence (AI) research toward social decision-making in multi-agent systems, which includes uncertainty from character heterogeneity.
__label__diffusion_based_models Diffusion models are capable of generating photo-realistic images that combine elements which do not appear together in natural images, demonstrating their ability to compositionally generalize.
__label__machine_vision Our method, dubbed Partial2Global, adopts a transformer-based list-wise ranker to provide a more comprehensive comparison within several alternatives, and a consistency-aware ranking aggregator to generate globally consistent ranking.
__label__deep_learning_architectures In (2), a strategic sample augmentation based on disentangled latent features with RWC loss is designed to reinforce the training of a more generalizable model.
__label__learning_theory In this paper, we propose a method called Stratified Prediction-Powered Inference (StratPPI), in which we show that the basic PPI estimates can be considerably improved by employing simple data stratification strategies.
__label__machine_vision We propose a video editing framework, NaRCan, which integrates a hybrid deformation field and diffusion prior to generate high-quality natural canonical images to represent the input video.
"__label__machine_vision We apply *DeepStack* to both language and vision transformer in LMMs, and 
validate the effectiveness of *DeepStack* LMMs with extensive empirical results."
__label__machine_vision Although the performance of Temporal Action Segmentation (TAS) has been improved in recent years, achieving promising results often comes with a high computational cost due to dense inputs, complex model structures, and resource-intensive post-processing requirements.
__label__machine_vision This allows for collectively guiding the network's meta-learning process with the aim of learning generalizable image feature embeddings, while not introducing any extra computational cost in the inference phase.
__label__machine_vision This unified framework allows TabPedia to seamlessly integrate VTU tasks, such as table detection, table structure recognition, table querying, and table question answering, by leveraging the capabilities of large language models (LLMs).
__label__machine_vision RLT also works without training, increasing model throughput by 35% with only 0.1% drop in accuracy.
__label__optimization Notably, a VCM trained at a specific DVN depth can steadily find better solutions by simply extending the testing depth, which narrows the gap to 0.034\% on benchmarks.
__label__diffusion_based_models Extensive experiments and user studies demonstrate that Hyper-SD achieves SOTA performance from 1 to 8 inference steps for both SDXL and SD1.5.
__label__optimization Experiments on multiple benchmarks demonstrate the proposed method is very competitive in finding preference-guided optimal solutions.
__label__machine_learning_for_physical_sciences Through empirical evaluations on various benchmark data sets, we consistently observe on-par or better performance than that of state-of-the-art spherical and Cartesian models.
__label__reinforcement_learning Specifically, it assigns small scaling parameters to pairs with ambiguous preferences, leading to more comparable rewards, and large scaling parameters to those with clear preferences for more distinct rewards.
__label__evaluation While effort has been put into characterizing the memorized data and linking encoder memorization to downstream utility, little is known about where the memorization happens inside SSL encoders.
__label__learning_theory The first boosts transferability via an exchange protocol between the clients and the server that includes information about cross-client Jacobian (gradient) norms.
__label__graph_neural_networks This paper investigates the capacity of GNNs to represent strong branching (SB), the most effective yet computationally expensive heuristic employed in the branch-and-bound algorithm.
__label__machine_vision Our study not only uncovers the mechanisms behind CLIP's generalizability beyond data imbalance but also provides transferable insights for the research community.
__label__interpretability_and_explainability Experimental results demonstrate that CMR achieves better accuracy-interpretability trade-offs to state-of-the-art CBMs, discovers logic rules consistent with ground truths, allows for rule interventions, and allows pre-deployment verification.
__label__reinforcement_learning To address this challenge, we revisit the idea that multi-task RL is bottlenecked by imbalanced training losses created by uneven return scales across different tasks.
__label__reinforcement_learning This paper develops a new Q-learning algorithm, called RegQ, that converges when linear function approximation is used.
__label__generative_models The customized requirements promote the development of few-shot diffusion models, which use limited $n_{ta}$ target samples to fine-tune a pre-trained diffusion model trained on $n_s$ source samples.
__label__evaluation To address this gap, we introduce an all-around LMM-based NR-IQA model, which is capable of producing qualitatively comparative responses and effectively translating these discrete comparison outcomes into a continuous quality score.
__label__infrastructure We find that aside from being heavily bound by memory bandwidth, certain inherent inefficiencies exist in all unfused implementations of neighborhood attention, which in most cases undo their theoretical efficiency gain.
__label__machine_learning_for_physical_sciences We demonstrate the versatility of our approach for both fully data-driven and for physics-aware neural solvers.
"__label__generative_models To address these issues, we develop ""Deep Dependency Regularized Knockoff (DeepDRK),"" a distribution-free deep learning method that effectively balances FDR and power."
__label__causal_inference However, unlike standard policy learning, the presence of a human expert can mitigate some of these risks.
__label__machine_vision Additionally, an MoE-based gating module is applied between two types of adapters to further enhance the multimodal interaction.
__label__generative_models By employing example-specific projected gradient iterations under the guidance of this joint machine, we refine synthesized images and achieve an improved FID scores on the ImageNet 64x64 dataset for both Consistency-Training and Consistency-Distillation techniques.
__label__speech_and_audio The primary challenges stem from the inherent complexities involved in direct translation tasks and the scarcity of data.
__label__interpretability_and_explainability On the theoretical side, we prove that a transformer with $O(\log_2(k))$ layers can represent the in-context conditional empirical distribution by composing induction heads to track the previous $k$ symbols in the sequence.
__label__fairness Our experimental findings indicate that regression tasks, which are relatively unexplored from literature, can achieve significant fairness improvement through VFair regardless of any prior, whereas classification tasks usually do not because of their quantized utility measurements.
__label__machine_vision We present Mixture of Nested Experts (MoNE), which utilizes a nested structure for experts, wherein individual experts fall on an increasing compute-accuracy curve.
__label__graph_neural_networks Cross-domain recommendation (CDR) offers a promising solution to the data sparsity problem by enabling knowledge transfer across source and target domains.
__label__diffusion_based_models Our method outperforms existing approaches in terms of perceptual quality while retaining a competitive performance in relation to fidelity metrics.
__label__machine_vision Despite the significant achievements of Vision Transformers (ViTs) in various vision tasks, they are constrained by the quadratic complexity.
__label__fairness Our results show more nuanced interactions of modern finetuned models with group robustness than was previously known.
__label__machine_vision To improve the efficiency while keeping the high performance, we present a novel perspective centered on per-segment classification.
__label__machine_vision This method utilizes 3D Gaussian representation and tile-based splatting techniques, bypassing the expensive neural field querying.
__label__learning_theory Specifically, we consider a smooth target function supported on a low-dimensional manifold, then prove that ConvResNeXts can adapt to the function smoothness and low-dimensional structures and efficiently learn the function without suffering from the curse of dimensionality.
__label__machine_learning_for_other_sciences_and_fields Building on this realistic and generalized contextual market model, we introduce MarketFCNet, a deep learning-based method for approximating market equilibrium.
__label__safety_in_machine_learning This calls into question the reliability of diffusion-based purification after mitigating the gradient dilemma and scrutinizing its resubmit risk.
__label__privacy We introduce One-Shot Label-Only (OSLO) membership inference attacks (MIAs), which accurately infer a given sample's membership in a target model's training set with high precision using just a single query, where the target model only returns the predicted hard label.
__label__generative_models string manipulation, dynamic programming, etc.)
__label__machine_vision Inspired by Matryoshka Representation Learning, we introduce the Matryoshka Query Transformer (MQT), capable of encoding an image into $m$ visual tokens during inference, where $m$ can be any number up to a predefined maximum.
__label__safety_in_machine_learning Driven by the rapid development of Large Language Models (LLMs), LLM-based agents have been developed to handle various real-world applications, including finance, healthcare, and shopping, etc.
__label__generative_models Furthermore, we propose a set of data augmentation techniques within this metric space to create new data samples.
__label__privacy We show that a ''frequency domain'' operation called *low-pass filtering* can be used to effectively reduce the impact of DP noise.
__label__reinforcement_learning We propose an inexact version of $h$-PMD where lookahead action values are estimated.
__label__deep_learning_architectures To tackle the built-in maximization problem, we derive the regret value by invoking a pre-trained model, subsequently utilizing it as the reward during the model training.
__label__interpretability_and_explainability We will publicly release our dataset and code.
__label__natural_language_processing Conventional methods in TKGR typically depend on deep learning algorithms or temporal logical rules.
__label__machine_vision The physical uncertainty further facilitates physics-guided pixel sampling to enhance the learning of slender structures.
__label__bandits Additionally, GG exhibits problem-independent guarantees on top of best problem-dependent guarantees.
"__label__graph_neural_networks In this work, we delve into the robustness analysis of representative robust GNNs and provide a unified robust estimation point of view to
understand their robustness and limitations."
__label__machine_vision In the paper, we first justify the use of the Gauss-Laguerre quadrature and then demonstrate this plug-and-play attribute by implementing it in two different NeRF models.
__label__safety_in_machine_learning In this paper, we investigate time series OOD generalization via pre-trained Large Language Models (LLMs).
__label__privacy DP mechanisms add noise during training to limit the risk of information leakage.
__label__natural_language_processing While promising, many-shot ICL can be bottlenecked by the available amount of human-generated outputs.
__label__machine_learning_for_other_sciences_and_fields We assess challenges in multi-modal learning of phenomics and molecular modalities such as experimental batch effect, inactive molecule perturbations, and encoding perturbation concentration.
__label__natural_language_processing Upon well-designed PedCorpus, we propose PediatricsGPT, the first Chinese pediatric LLM assistant built on a systematic and robust training pipeline.
__label__machine_vision Extensive evaluations across eleven datasets in three domains demonstrate our model's effectiveness compared to the latest AD methods.
__label__reinforcement_learning We propose a stochastic Hyper Policy Gradient Descent (HPGD) algorithm to solve CB-RL, and demonstrate its convergence.
__label__machine_learning_for_other_sciences_and_fields However, existing approaches for HOBRE rely heavily on uni-modal models like SCFMs for supervised fine-tuning or general LLMs for prompting, resulting in sub-optimal performance.
__label__optimization This enables us to overcome the obstacles in existing analysis and establish for the first time the weak convergence of the joint process $(x_k, \theta_k)$.
__label__deep_learning_architectures However, they are often unstable during training, and have no guarantees of convergence/termination at the solution.
__label__machine_learning_for_physical_sciences Metriplectic systems are learned from data in a way that scales quadratically in both the size of the state and the rank of the metriplectic data.
"__label__deep_learning_architectures We empirically demonstrate the advantages of Elliptical Attention over the baseline dot-product attention and state-of-the-art attention methods on various practical tasks, including object classification, image
segmentation, and language modeling across different data modalities."
__label__machine_vision Our approach demonstrates significant superiority in novel view synthesis, interactive scene control, and language grounding performance through extensive experiments.
__label__reinforcement_learning Moreover, to reduce the large variance of diffusion policies, we also develop an efficient behavior policy through action selection.
"__label__reinforcement_learning Together, our results serve as a
  first step toward a unified statistical and algorithmic theory for
reinforcement learning under latent dynamics."
__label__natural_language_processing subareas), and explains sophisticated concepts that classical methods (e.g.
__label__generative_models Our extensive experiments on LVMs and LLMs demonstrate that finetuning only a small fraction of the parameters in the base model significantly outperforms LoRA while enabling both rapid switching and multi-adapter fusion.
__label__machine_learning_for_other_sciences_and_fields SAND also outperforms standard statistical methods for functional imputation like kernel smoothing and PACE.
__label__machine_learning_for_other_sciences_and_fields Phenomic experiments, designed to capture cellular morphology, utilize microscopy based techniques and demonstrate a high throughput solution for uncovering molecular impact on the cell.
__label__natural_language_processing In addition, we carry out an extensive analysis across different domains and architectures, showing that fractal parameters are robust.
__label__natural_language_processing For $\textit{O}$2 (untested scalability), our study shows that $G_{\text{stack}}$ is scalable and consistently performs well, with experiments up to 7B LLMs after growth and pre-training LLMs with 750B tokens.
__label__learning_theory In response, the sigmoid gating function has been recently proposed as an alternative and has been demonstrated empirically to achieve superior performance.
__label__optimization_for_deep_networks We provide a thorough convergence analysis and a comprehensive practical discussion for $($FG$)^2$U, complemented by extensive empirical evaluations, showcasing its superior performance in diverse large-scale bi-level optimizaiton tasks.
__label__safety_in_machine_learning However, automated data quality control faces unique challenges in collaborative settings where sharing is not allowed directly between data silos.
__label__speech_and_audio Experiments show that CA-SSLR reduces the number of trainable parameters, mitigates overfitting, and excels in under-resourced and unseen tasks.
__label__algorithmic_game_theory We observe that if Bob is almost myopic and chooses his favorite piece too often, then he can be systematically exploited by Alice through a strategy akin to a binary search.
__label__online_learning For example, in multi-level caching architectures, the expected cost of fetching a memory block is a function of its probability of being in a mid-level cache rather than the main memory.
__label__learning_theory In this work, we introduce a data-dependent setting where DNC forms due to feature learning through the average gradient outer product (AGOP).
__label__evaluation However, in the task of emotional classification, due to the psychological similarities between emotions, misclassifying a certain emotion into one class may be more severe than another, e.g., misclassifying 'excitement' as 'anger' apparently is more severe than as 'awe'.
__label__learning_theory In particular, we find that although pure masked attention still suffers from exponential collapse to a rank one subspace, sparse or local masked attention can provably slow down the collapse rate.
__label__natural_language_processing Our analysis of IRL-extracted reward functions further indicates benefits for more robust reward functions via tighter integration of supervised and preference-based LLM post-training.
__label__causal_inference Empirical evaluations demonstrate the robustness of our method in detecting diverse dependencies, excelling in undirected graph estimation and showing competitive performance in completed partially directed acyclic graph estimation via a novel two-step approach.
__label__natural_language_processing This is particularly important because generating correct solutions in one attempt proves challenging for complex tasks.
__label__safety_in_machine_learning To effectively defend against the FJAttack with limited safety examples under LMaaS, we propose the Backdoor Enhanced Safety Alignment method inspired by an analogy with the concept of backdoor attacks.
__label__reinforcement_learning But what if sometimes rewards are unobservable, e.g., situations of partial monitoring in bandits and the recent formalism of monitored Markov decision process?
__label__algorithmic_game_theory However, the tractability of $\Phi$-equilibria in such games remains elusive.
__label__optimization We first propose a sufficient condition for the performative control problem to admit a unique PSC solution with a problem-specific structure of distributional sensitivity propagation and aggregation.
__label__safety_in_machine_learning In this paper, we aim to prevent such learning behavior of applying frequency shortcuts from a data-driven perspective.
__label__active_learning To overcome this challenge, the concept of active finetuning has emerged, aiming to select the most appropriate samples for model finetuning within a limited budget.
__label__optimization_for_deep_networks Then, we demonstrate that the gradient flow reaches a global minimum consistent with the PDE solution when the weight decay regularization parameter is sufficiently small.
__label__diffusion_based_models Extensive experiments in face privacy and artistic style protection, demonstrate the superior generalization of our method in comparison to existing techniques.
__label__causal_inference A central step in designing these algorithms is establishing the relationships between UMN interventional CRL and score functions associated with the statistical models of different interventional environments.
__label__machine_learning_for_other_sciences_and_fields These tokens serve as an effective protein structure representation.
__label__generative_models Multimodal large language models (MLLMs), initiated with a trained LLM, first align images with text and then fine-tune on multimodal mixed inputs.
__label__natural_language_processing But do LLMs actually recognize their own outputs when they give those texts higher scores, or is it just a coincidence?
__label__bandits Leveraging this new result, we then present an almost tight characterization (up to log factor) of the minimax regret in online MABs and sub-optimality in offline MABs under both LTC and CTL settings, respectively.
__label__machine_vision With this understanding, we revisit key challenges in OCL from both empirical and theoretical perspectives, highlighting two critical issues beyond the well-documented catastrophic forgetting: (\romannumeral1) Model's ignorance: the single-pass nature of OCL challenges models to learn effective features within constrained training time and storage capacity, leading to a trade-off between effective learning and model throughput; (\romannumeral2) Model's myopia: the local learning nature of OCL on the current task leads the model to adopt overly simplified, task-specific features and \textit{excessively sparse classifier}, resulting in the gap between the optimal solution for the current task and the global objective.
__label__deep_learning_architectures Neural architecture search (NAS) finds high performing networks for a given task.
__label__machine_vision Our MetaUAS significantly outperforms previous zero-shot, few-shot, and even full-shot anomaly segmentation methods.
__label__safety_in_machine_learning We prove a probabilistic guarantee that the resulting conformal interval around $f$ contains a function approximately satisfying $\mathcal{P}$.
__label__machine_vision The code will be released.
__label__learning_theory However, there is a noticeable gap in analysis for multiclass classification, with only a handful of results which themselves are restricted to the cross-entropy loss.
__label__reinforcement_learning A natural question is if there is a limit to the performance benefits of increasing the context length if the computation needed is available.
__label__machine_learning_for_physical_sciences Recently, data-driven approaches for weather forecasting based on deep learning have shown great promise, achieving accuracies that are competitive with operational systems.
"__label__generative_models To circumvent the issue, we propose a simple and effective approach called Local Perplexity Ranking (LPR), which replaces the ""noisy"" candidates with their nearest neighbors that are more likely to be clean."
__label__neuroscience_and_cognitive_science To achieve this, combining data across multiple subjects is crucial.
__label__generative_models Moreover, we devise an anchor loss to enhance the appearance details and facilitate the learning of dynamic NeRF.
__label__active_learning Machine learning systems are widely used in many high-stakes contexts in which experimental designs for assigning treatments are infeasible.
__label__natural_language_processing In particular, we find that DAA methods deteriorate not only across a wide range of KL-budgets, but also often before even a single epoch of the dataset is completed.
__label__infrastructure We develop three main techniques to speed up attention on Hopper GPUs: exploiting asynchrony of the Tensor Cores and TMA to (1) overlap overall computation and data movement via warp-specialization and (2) interleave block-wise matmul and softmax operations, and (3) block quantization and incoherent processing that leverages hardware support for FP8 low-precision.
__label__natural_language_processing Traditional approaches focus on aligning models during the instruction tuning or reinforcement learning stages, referred to in this paper as `\textit{post alignment}'.
__label__safety_in_machine_learning Large Language Models (LLMs) are becoming a prominent generative AI tool, where the user enters a query and the LLM generates an answer.
__label__reinforcement_learning In this paper, we propose a novel framework for IRL-based IL that prioritizes task alignment over conventional data alignment.
__label__interpretability_and_explainability Experiments demonstrate our method's superior performance in disentanglement and reconstruction.
__label__algorithmic_game_theory Despite this barrier, \citet{Huang2008:Computing} altered the algorithm to compute exact extensive-form correlated equilibria.
__label__other Tensor multiplication with learned weight matrices is the fundamental building block in deep learning models.
__label__machine_vision Then, we propose a 3D dual-masking instance matching module to estimate the pose between the model point cloud and each object proposal.
__label__natural_language_processing Furthermore, a two-step training scheme is introduced to first train to embeded the external capabilities into LLMs and then train the collaborative utilization of the external capabilities and the built-in capabilities in LLMs.
__label__evaluation Extensive experiments on Meta-Dataset demonstrate that CoPA achieves the _state-of-the-art_ performance more efficiently.
__label__machine_vision We introduce ChatCam, a system that navigates camera movements through conversations with users, mimicking a professional cinematographer's workflow.
__label__diffusion_based_models To address this challenge, we propose AdaFlow, an imitation learning framework based on flow-based generative modeling.
__label__infrastructure Previous methods employed low-rank adaptation (LoRA) for efficient federated fine-tuning but utilized traditional FL aggregation strategies on LoRA adapters.
__label__machine_vision In this paper, we introduce a novel approach to reduce vision compute by leveraging redundant vision tokens ``skipping layers'' rather than decreasing the number of vision tokens.
__label__infrastructure However, the prohibitive costs of tensor communication render it a theoretically plausible yet practically inefficient solution.
__label__reinforcement_learning This process enables the LLM to understand the environment better, facilitating the generation of meaningful rollouts.
__label__machine_vision While detecting such cases may be critical, evaluating a model's vulnerability at a per-instance level using adversarial attacks is computationally too intensive and unsuitable for real-time deployment scenarios.
__label__human-AI_interaction Our ICAL agent surpasses the state-of-the-art in dialogue-based instruction following in TEACh, multimodal web agents in VisualWebArena, and action anticipation in Ego4D.
__label__robotics We achieve this with a framework called MeMo which learns (Me)aningful, (Mo)dular controllers.
__label__safety_in_machine_learning Our finding underscores the need for a shift in research/industry emphasis from invisible watermarks to semantic-preserving watermarks.
__label__machine_learning_for_social_sciences Large Language Model Multi-Agent Systems (LLM-MAS) have greatly progressed in solving complex tasks.
__label__safety_in_machine_learning We demonstrate that this kind of representational alignment can also support safely learning and exploring human values in the context of personalization.
__label__generative_models Despite the empirical success, no theoretical work specifically analyzes few-shot diffusion models.
__label__graph_neural_networks To address this limitation, we propose Adaptive Long-range aware TransformER (ALTER), a brain graph transformer to capture long-range dependencies between brain ROIs utilizing biased random walk.
__label__machine_vision Unlike previous efforts simply treating the generated descriptions as mutually equivalent text classifiers, SDSGG is equipped with an advanced renormalization mechanism to adjust the influence of each text classifier based on its relevance to the presented scene (this is what the term “specific” means).
__label__graph_neural_networks Graph self-supervised pre-training, which trains GNN encoders without manual labels to generate high-quality graph representations, has garnered widespread attention.
"__label__learning_theory The training dynamics of linear networks are well studied in two distinct
setups: the lazy regime and balanced/active regime, depending on the
initialization and width of the network."
__label__privacy DPConvCNP learns from simulated data how to map private data to a DP predictive model in one forward pass, and then provides accurate, well-calibrated predictions.
__label__evaluation This will help communities establish more sophisticated human assessment protocols.
__label__evaluation We will open-source I2EBench, including all instructions, input images, human annotations, edited images from all evaluated methods, and a simple script for evaluating the results from new IIE models.
__label__generative_models Post-training Quantization (PTQ) has emerged as a fast and data-efficient solution that can significantly reduce computation and memory footprint by using low-bit weights and activations.
__label__machine_vision Based on the above intuition, we decouple the training process of NeRF in the ray dimension softly and propose a Ray-decoupled Training Framework for neural rendering (Rad-NeRF).
__label__optimization Various numerical experiments are conducted to demonstrate the effectiveness of the proposed algorithm in solving large-scale OT problems.
__label__learning_theory Thus, we thoroughly analyze these gaps, to guide surrogate loss selection, covering: comparisons across different comp-sum losses, conditions where gaps become zero, and general conditions leading to small gaps.
__label__machine_vision This paper presents M$^3$GPT, an advanced $\textbf{M}$ultimodal, $\textbf{M}$ultitask  framework for $\textbf{M}$otion comprehension and generation.
__label__reinforcement_learning In this paper, we formalize a novel reinforcement learning model which explicitly represents the information structure.
__label__reinforcement_learning However, existing unsupervised skill discovery methods often learn entangled skills where one skill variable simultaneously influences many entities in the environment, making downstream skill chaining extremely challenging.
__label__natural_language_processing Experimental results show that this architecture achieves state-of-the-art performance on MUSIC-AVQA-R, notably obtaining a significant improvement of 9.32\%.
__label__reinforcement_learning These selections serve as subgoals that indicate subtasks and guide policy.
__label__machine_vision Both the boundary poses and linear transforms can be efficiently learned from the whole dataset via clustering.
__label__diffusion_based_models To address this limitation, we outline three key objectives for SFR: (1) promoting diversity across identities (inter-class diversity), (2) ensuring diversity within each identity by injecting various facial attributes (intra-class diversity), and (3) maintaining identity consistency within each identity group (intra-class identity preservation).
__label__machine_vision To do so, we revise the ‘cloning’ of Gaussians into a relocalization scheme that approximately preserves sample probability.
__label__machine_vision We observe that smaller ViTs are intrinsically the sub-networks of a larger ViT with different widths.
__label__machine_vision This work aims to pave the way for further advancements in SAR object detection.
__label__machine_learning_for_physical_sciences We propose _Meta Flow Matching_ (MFM), a practical approach to integrating along these vector fields on the Wasserstein manifold by amortizing the flow model over the initial populations.
__label__machine_vision It consistently outperforms regression-based methods and averaging ensemble approaches on 14 benchmarks across 3 image restoration tasks, including super-resolution, deblurring and deraining.
__label__online_learning We demonstrate that it characterizes the instance-optimal mistake bounds for deterministic learning algorithms in the realizable setting.
__label__optimization Key to our proofs is directional smoothness, a measure of gradient variation that we use to develop upper-bounds on the objective.
__label__machine_vision Recent AD methods have evolved with the advent of large pre-trained vision-language models, enhancing few-shot anomaly detection capabilities.
__label__natural_language_processing We validate our predictions empirically by training models with 3B parameters across different FLOPs budgets.
__label__causal_inference Moreover, based on the identification theory, we develop a cluster fusion-like method to discover valid IV sets and estimate the causal effects of interest.
__label__safety_in_machine_learning Large language models (LLMs) for code are typically trained to align with natural language instructions to closely follow their intentions and requirements.
__label__robotics This system empowers robots to handle tasks using various modalities, whether in combinations like text-image, audio-image, text-point cloud, or in isolation.
__label__machine_vision With CCA, visual tokens can better interact with instruction tokens, thereby enhancing model's perception capability and alleviating object hallucination.
__label__deep_learning_architectures We propose an algebraic geometric framework to study the expressivity of linear activation neural networks.
__label__optimization [Orlin, STOC 1988]).
__label__machine_learning_for_other_sciences_and_fields CANDY dexterously exploits inter-view similarities as \textit{context} to uncover false negatives.
__label__interpretability_and_explainability Detailed decompositions quantify the importance of each variable to each term in the aggregate decomposition, which can provide a deeper understanding and suggest more targeted interventions.
__label__learning_theory Despite its widespread use in practice, the softmax gating may lead to unnecessary competition among experts, potentially causing the undesirable phenomenon of representation collapse due to its inherent structure.
__label__machine_vision Accordingly, we develop a Distribution Guidance Network (DGNet), which comprises a weakly supervised learning branch and a distribution alignment branch.
__label__optimization_for_deep_networks Ultimately, GNM-PT enhances generalization across all classes while simultaneously reducing computational overhead.
__label__learning_theory This rate reveals the structural properties of the Transformer and suggests the types of sequential relationships it is best suited for approximating.
__label__learning_theory Moreover, our flexible framework can be extended to different domains, tasks, and architectures.
__label__reinforcement_learning One of the fundamental challenges for offline reinforcement learning (RL) is ensuring robustness to data distribution.
__label__natural_language_processing Current Multi-modal Large Language Models (MLLMs), constrained by their training data, lack the capability to function effectively as operation assistants.
__label__safety_in_machine_learning Extensive experiments demonstrate the capability to tackle deepfakes and the robustness in surviving diverse input transformations.
__label__reinforcement_learning The RL teacher can leverage previously discovered environment structures and generate environments at the frontier of the student's capabilities by observing the student policy's representation.
__label__diffusion_based_models Langevin Dynamics is a Stochastic Differential Equation (SDE) central to sampling and generative modeling and is implemented via time discretization.
__label__natural_language_processing GPT-4o with Sketchpad sets a new state of the art on all tasks, including V*Bench (80.3%), BLINK spatial reasoning (83.9%), and visual correspondence (80.8%).
__label__natural_language_processing Additionally, as model capabilities and usage patterns evolve, there may be a costly need to add or relabel data to modify safety behavior.
"__label__machine_vision In this paper, we address this underexplored problem by presenting a framework for ""renovating"" names in open-vocabulary segmentation benchmarks (RENOVATE)."
__label__deep_learning_architectures However, developing MLLMs for real-world robots is challenging due to the typically limited computation and memory capacities available on robotic platforms.
__label__algorithmic_game_theory Our main results include a characterization of scenarios where a single signal suffices and a computationally efficient algorithm to compute optimal signaling schemes.
__label__causal_inference In recent years, an increasing emphasis has been placed on heterogeneity in treatment effects, leading to the development of various methods for estimating Conditional Average Treatment Effects (CATE).
__label__diffusion_based_models We alleviate the issue in this work from two perspectives.
__label__machine_vision Latest methods employ supervised learning or pretrained priors to learn a signed distance function (SDF).
__label__machine_vision Considering the lack of study and evaluation on 3DDG, we also create three new benchmarks, namely base-to-new, cross-dataset and few-shot generalization benchmarks, to enrich the field and inspire future research.
__label__graph_neural_networks Graph classification is a challenging problem owing to the difficulty in quantifying the similarity between graphs or representing graphs as vectors, though there have been a few methods using graph kernels or graph neural networks (GNNs).
__label__machine_vision A feature extractor that achieves this goal even under significant viewpoint changes must recognise not just semantic categories in a scene, but also understand how different objects relate to each other in three dimensions.
__label__optimization A general meta-algorithm is devised to convert algorithms for linear/quadratic maximization into ones that optimize upper-linearizable/quadratizable functions, offering a unified approach to tackling concave and DR-submodular optimization problems.
__label__learning_theory Prospective ERM, roughly speaking, incorporates time as an input in addition to the data.
__label__safety_in_machine_learning This paper studies the vulnerabilities of transformer-based Large Language Models (LLMs) to jailbreaking attacks, with a particular focus on the optimization-based Greedy Coordinate Gradient (GCG) strategy.
__label__active_learning We are the first to show, under general regularity assumptions, that such decision rules converge uniformly to the smallest possible uncertainty obtainable from the accessible data.
__label__diffusion_based_models We propose Reward Preference Optimization (RPO), which offers a simpler setup (requiring only 3\% of the negative samples used by DreamBooth) and fewer gradient steps for fine-tuning.
__label__reinforcement_learning In contrast, PFM utilizes flow matching techniques to directly learn from preference data, thereby reducing the dependency on extensive fine-tuning of pre-trained models.
__label__algorithmic_game_theory For the canonical setting where the agent’s actions result in success or failure, we present a simple, optimal solution for the principal: Initially provide a linear contract with scalar $\alpha > 0$, then switch to a zero-scalar contract.
__label__machine_learning_for_healthcare Experimental results on real-world EHR data demonstrate the efficacy of the proposed AutoDP framework.
__label__natural_language_processing Theoretically, we show that the proposed algorithms converge to the stationary solutions of the IRL problem.
__label__learning_theory We prove that this family of surrogate losses benefits from $H$-consistency bounds, and thus Bayes-consistency, across any general multi-label loss.
__label__machine_learning_for_healthcare However, current methods often neglect the fact that the contribution to prognosis differs with tissue types.
__label__generative_models In this work, we propose an iterative algorithm to approximate the MAP estimator efficiently to solve a variety of linear inverse problems.
__label__human-AI_interaction We prove that IDA performance is lower bounded by human performance, so that IDA does not negatively impact human control.
__label__deep_learning_architectures The attention mechanism within the transformer architecture enables the model to weigh and combine tokens based on their relevance to the query.
__label__generative_models While standard evaluation scores for generative models are mostly reference-based, a reference-dependent assessment of generative models could be generally difficult due to the unavailability of applicable reference datasets.
__label__natural_language_processing However, the quality of these exemplars in the prompt greatly impacts performance, highlighting the need for an effective automated exemplar selection method.
__label__causal_inference Remarkably, these guarantees match the best-known results for more restrictive single-node interventions.
__label__deep_learning_architectures As a drop-in replacement for attention layers, \name outperforms BERT by 0.8 points on the GLUE benchmark and ViT by 2% Top-1 accuracy on ImageNet.
__label__safety_in_machine_learning We focus on the setting where a public model is fine-tuned before serving users for specific usage, where the model should improve on the downstream task while maintaining alignment.
__label__safety_in_machine_learning However, employing consistent negative labels across different OOD datasets often results in semantic misalignments, as these text labels may not accurately reflect the actual space of OOD images.
__label__machine_learning_for_other_sciences_and_fields We present experimental results to demonstrate that SARAD achieves state-of-the-art performance, providing robust anomaly detection and a nuanced understanding of anomalous events.
__label__machine_vision Advanced diffusion models (DMs) perform impressively in image super-resolution (SR), but the high memory and computational costs hinder their deployment.
__label__generative_models Moreover, we identify a mode trapping issue with EDSD, and propose a mode shifting regularization with spatial feature guided sampling to avoid such issue.
__label__machine_learning_for_other_sciences_and_fields Experimental results on benchmark datasets showcase that CoupleNet outperforms state-of-the-art methods, exhibiting particularly superior performance in low-sequence similarities scenarios,  adeptly identifying infrequently encountered functions and effectively capturing remote homology relationships in proteins.
__label__generative_models The proposed approach leverages an auxiliary task for semantic smoothing in vector-quantized token space, analogous to the Gaussian blur in continuous pixel space.
__label__reinforcement_learning Then, we show how these can be used to derive near-optimal guarantees of an optimistic exploration algorithm.
__label__privacy We provide new lower bounds on the privacy guarantee of _multi-epoch_ Adaptive Batch Linear Queries (ABLQ) mechanism with _shuffled batch sampling_, demonstrating substantial gaps when compared to _Poisson subsampling_; prior analysis was limited to a single epoch.
__label__machine_vision Besides, by incorporating human subjects, we establish a human baseline, creating a high- quality environment specifically designed to assess trackers’ visual search abilities in videos across STDChallenge.
__label__machine_vision By excluding background noise distractions, the model is encouraged to focus on character morphology and generalize the ability to recognize complex samples when trained with only simple synthetic data.
__label__evaluation We present sDBSCAN, a scalable density-based clustering algorithm in high dimensions with cosine distance.
__label__reinforcement_learning Our results cover the full spectrum between observing the immediate rewards before acting to observing all the rewards before the interaction starts.
__label__human-AI_interaction In this work, we ask: Can LLMs and VLMs generate their own examples from generic, sub-optimal demonstrations?
__label__generative_models Recently, token-based generation approaches have demonstrated their effectiveness in synthesizing visual content.
__label__machine_vision While current benchmarks for object hallucination primarily concentrate on the presence of a single object class rather than individual entities, this work systematically investigates multi-object hallucination, examining how models misperceive (e.g., invent nonexistent objects or become distracted) when tasked with focusing on multiple objects simultaneously.
__label__optimization_for_deep_networks Our code is available at https://github.com/LIONS-EPFL/SAMPa.
__label__bandits Nevertheless, the problem in its general form, i.e., _unknown_ graph and _unknown_ stochastic intervention models, remains open.
__label__learning_theory For a point set $S$, the map $\varphi_\ell:\mathbb{R}^d \to N^{-1/2}\{-1,1\}^N$ has the property that storing  $\varphi_\ell(S)$ (a sketch of $S$) allows one to report squared distances between points up to some  multiplicative  $(1\pm \epsilon)$ error with high probability.
__label__robotics To better preserve the information of environment and fully exploit the reasoning ability of LLM, we propose to represent the observed scene with 3D scene graph.
__label__reinforcement_learning Policy Mirror Descent (PMD) is a powerful and theoretically sound methodology for sequential decision-making.
__label__online_learning Our findings underscore the effectiveness of WATT across diverse datasets, including CIFAR-10-C, CIFAR-10.1, CIFAR-100-C, VisDA-C, and several other challenging datasets, effectively covering a wide range of domain shifts.
__label__diffusion_based_models Our research demonstrates that stable diffusion is a promising approach to robust watermarking, able to withstand even stable-diffusion-based attack methods.
__label__machine_vision We observe that an audio signal may contain background noise interference.
__label__reinforcement_learning This challenging optimization landscape leads to sub-optimal performance on several benchmark tasks.
__label__diffusion_based_models This limited guidance interval improves the record FID in ImageNet-512 significantly, from 1.81 to 1.40.
__label__machine_learning_for_physical_sciences Applying principles from operator learning and implicit models for shape encoding, our approach effectively addresses the prediction of highly variable frequency response functions occurring in dynamic systems.
__label__natural_language_processing To address this problem, this paper introduces Multi-scale Positional Encoding (Ms-PoE) which is a simple yet effective plug-and-play approach to enhance the capacity of LLMs to handle the relevant information located in the middle of the context, without fine-tuning or introducing any additional overhead.
__label__natural_language_processing We explore this primarily in context of math reasoning, developing a prompt-guided interaction procedure  to get a powerful  LLM to assign sensible skill labels to math questions, followed by having it perform semantic clustering to obtain coarser families of skill labels.
__label__generative_models Our work delineates the viability of an integrated approach to multimodal generation within the visual text domain, setting a foundation for subsequent inquiries.
__label__optimization To the best of our knowledge, this is the first proposal that achieves a globally optimal m-sparse Sharpe ratio with a theoretically-sound guarantee.
__label__interpretability_and_explainability However, our current understanding in this regard remains limited with many fundamental questions about how transformers learn Markov chains still unanswered.
__label__machine_vision Specifically, we propose a Predicting Center Module (PCM) that shares parameters with the original encoder with extra cross-attention to predict centers.
__label__learning_theory For Gaussian distribution $\mathcal{D} = N(0,\Sigma)$, this algorithm achieves error $O(\varepsilon^{3/4})$.
__label__online_learning These algorithms significantly improve over the state-of-the-art in terms of query complexity.
__label__natural_language_processing However, both strategies have drawbacks: input reduction has no guarantee of covering the part with needed information, while window extension struggles with focusing on the pertinent information for solving the task.
__label__machine_learning_for_other_sciences_and_fields ), focusing on independently modeling each segment.
__label__safety_in_machine_learning Performative prediction aims to model scenarios where predictive outcomes subsequently influence the very systems they target.
__label__machine_learning_for_other_sciences_and_fields Considering the shortage of existing CAT solutions in student ranking, this paper emphasizes the importance of aligning test outcomes (student ranks) with the true underlying abilities of students.
__label__learning_theory To address this problem, we introduce lower-bounded expansion properties to characterize the instability in update rules which can serve as general tools for lower-bound analysis.
__label__generative_models This results in a model that, given a prespecified broad set of possible symmetries, learns to what extent, if at all, those symmetries are actually present.
__label__diffusion_based_models Meta-Diffu$B$ achieves state-of-the-art performance compared to previous S2S-Diffusion models and fine-tuned pre-trained language models (PLMs) across four Seq2Seq benchmark datasets.
__label__machine_vision Moreover, RPM integrates with VLM to construct the ${\bf R}$elationship ${\bf P}$rompt ${\bf N}$etwork (${\bf RPN}$), achieving OVSS without any segmentation-specific networks.
__label__human-AI_interaction This limits human autonomy that may have deleterious effects on performance.
__label__probabilistic_methods Accordingly, we reconsider the fundamentals and ask what can be learned using just observational data.
__label__optimization However, their approach inherits cubic computational complexity and numerical instability.
__label__fairness Specifically, the algorithm first scores each new example by its influence on fairness and accuracy evaluated on the validation dataset, and then selects a certain number of examples for training.
"__label__diffusion_based_models In this
paper, we define semantic binding as the task of associating a given object with its
attribute, termed attribute binding, or linking it to other related sub-objects, referred
to as object binding."
__label__diffusion_based_models They can explore task-specific and task-shared knowledge during training, and aggregate all low-rank weights of old concepts based on their contributions during inference.
__label__learning_theory Empirically, we concretize the variance-bias balance via synthetic experiments and demonstrate the effectiveness of SkMM for finetuning in real vision tasks.
__label__online_learning A switching regret is defined relative to any segmentation of the trial sequence, and is equal to the sum of the static regrets of each segment.
__label__machine_learning_for_physical_sciences This approach achieves up to $\sim50\%$ reductions in mean absolute error on both experimental and simulated data, over state-of-the-art models based on convolutional neural networks, and scales to devices with 100+ qubits.
__label__graph_neural_networks Rethink convolution-based graph neural networks (GNN)---they characteristically suffer from limited expressiveness, over-smoothing, and over-squashing, and require specialized sparse kernels for efficient computation.
__label__online_learning Most studies on population dynamics focus on the problem of prediction rather than control.
__label__neuroscience_and_cognitive_science These abstract representations define notions like objects and shapes, but at the cost of spatial specificity.
__label__generative_models Inspired by the recent emergence of neural network diffusion, we present Tina, a text-conditioned neural network diffusion for train-once-for-all personalization.
__label__optimization However, the vast majority of work in this space assumes that the prediction itself is non-probabilistic, even if it is generated by some stochastic process (such as a machine learning system).
__label__machine_learning_for_other_sciences_and_fields These challenges can adversely affect user experience and seller benefits, making them crucial to address.
__label__machine_vision This motivates our research to explore techniques for improving the capability of small-size image restoration standing on the success secret of large receptive filed.
__label__other Involving thousands of clients performing heterogeneous NLP tasks and client resources, our experiments validate the efficacy of FlexLoRA, with the federated global model achieving consistently better improvement over SOTA FL methods in downstream NLP task performance across various heterogeneous distributions.
__label__graph_neural_networks We introduce novel building blocks, of independent technical interest, that allow for equivariance or invariance with respect to individual scalar multipliers or their product and use them in all components of ScaleGMN.
__label__generative_models Moreover, our method can accurately predict images from unknown viewpoints.
__label__deep_learning_architectures Recent State Space Models (SSMs) such as S4, S5, and Mamba have shown remarkable computational benefits in long-range temporal dependency modeling.
__label__diffusion_based_models Comprehensive ablations enhance the interpretability of our pipeline.
__label__machine_learning_for_healthcare Neural stochastic differential equations (Neural SDEs) are an attractive modeling technique for this problem, which parameterize the drift and diffusion terms of an SDE with neural networks.
__label__learning_theory We show that two gating regimes naturally arise and, in each of them, we formulate an identifiability condition for the expert functions and derive the corresponding convergence rates.
__label__deep_learning_architectures The extracted dynamic subgraph can effectively capture the data distribution shift  by incorporating the inferred environment variables into the node-wise dependencies.
__label__machine_vision To address this, we propose De-focus Attention Networks, which employ learnable bandpass filters to create varied attention patterns.
__label__graph_neural_networks Current approaches rely on large number of samples from meta-learning to construct memories, and heavy fine-tuning of the GNN parameters that lead to the loss of past knowledge.
__label__bandits Our contributions advance the theoretical foundation of ensemble sampling, bringing its regret bounds in line with the best known bounds for other randomized exploration algorithms.
__label__neuroscience_and_cognitive_science We argue for the computational advantages of a recurrent architecture with complex-valued weights.
__label__optimization This paper explores adaptive variance reduction methods for stochastic optimization based on the STORM technique.
__label__causal_inference We also apply the resulting model to real-world climate data and successfully answer downstream causal questions in line with existing literature on climate change.
__label__graph_neural_networks Recently, there has been an emerging trend to integrate persistent homology (PH) into graph neural networks (GNNs) to enrich expressive power.
__label__machine_vision DiffGS is a powerful and efficient 3D generative model which is capable of generating Gaussian primitives at arbitrary numbers for high-fidelity rendering with rasterization.
__label__machine_vision To solve the above issues, we propose a Slow And Fast parameter-Efficient tuning (SAFE) framework.
__label__learning_theory In this work, we study the experts problem in the distributed setting where an expert's cost needs to be aggregated across multiple servers.
__label__reinforcement_learning Recent advances in data-driven imitation learning and offline reinforcement learning have highlighted the use of expert data for skill acquisition and the development of hierarchical policies based on these skills.
__label__machine_vision With an aim to investigate the reasons behind this finding, we conduct controlled experiments to study various underlying factors, and reveal that CLIP's pretext task forms a dynamic classification problem wherein only a subset of classes is present in training.
__label__online_learning To preserve a compact representation, an equivalent number of low-opacity Gaussians that minimally impact the representation are removed before processing subsequent frames.
__label__optimization Experiments show that our approach outperforms existing methods as it can significantly reduce costs on real-world cities.
__label__optimization Zero-shot optimization involves optimizing a target task that was not seen during training, aiming to provide the optimal solution without or with minimal adjustments to the optimizer.
__label__machine_vision Specifically, we decompose the input 3D scene into a set of object proposals, each assigned a unique identifier token, which enables efficient object referencing and grounding during user-assistant interactions.
__label__graph_neural_networks Experiments show that our graph token embeddings help the LLM predictor achieve state-of-the-art performance on unseen datasets and tasks compared to other methods using LLMs as predictors.
__label__interpretability_and_explainability Our theoretical analysis leads us to propose a strategy for model reconstruction that we call Counterfactual Clamping Attack (CCA) which trains a surrogate model using a unique loss function that treats counterfactuals differently than ordinary instances.
__label__generative_models The core innovations include a diffusion-based holistic facial dynamics and head movement generation model that works in a face latent space, and the development of such an expressive and disentangled face latent space using videos.
__label__learning_theory Our work thus proposes a unified surrogate loss framework benefiting from strong consistency guarantees for any multi-label loss, significantly expanding upon previous work which only established Bayes-consistency and for specific loss functions.
"__label__causal_inference We show that by making simple yet
often realistic independence assumptions, it is possible 
to uniquely estimate the probability of an interventional formula (including
the well-studied notions of probability of sufficiency and necessity)."
__label__deep_learning_architectures Using two SSM heads with different discretization processes and input-dependent parameters, Chimera is provably able to learn long-term progression, seasonal patterns, and desirable dynamic autoregressive processes.
__label__bandits In this work, we propose approximate posterior sampling algorithms for contextual bandits with a diffusion model prior.
__label__optimization_for_deep_networks In particular, we find that warmup helps counteract large angular updates as well as a limited critical batch size early in training.
__label__machine_vision more accuracy with the same GMACs and up to 7 p.p.
__label__learning_theory To the best of our knowledge, ours are the first theoretical results on benign or tempered overfitting that: (1) apply to deep NNs, and (2) do not require a very high or very low input dimension.
__label__causal_inference In the theoretical investigations, we demonstrate that our design has a higher probability of correctly identifying the best set of subgroups compared to conventional designs.
__label__optimization Our method adaptively adjusts the importance of each class during training, mitigates overfitting on dominant classes and enhances model adaptability across diverse datasets.
__label__learning_theory That is, these algorithms are computationally efficient given an efficient ERM for the class.
__label__privacy Experimental results show that our algorithm runs orders of magnitude faster than their approach, while achieving similar empirical accuracy.
__label__diffusion_based_models Our method, termed Hollowed Net, enhances memory efficiency during fine-tuning by modifying the architecture of a diffusion U-Net to temporarily remove a fraction of its deep layers, creating a hollowed structure.
__label__learning_theory However, the potential impacts of the derivative-free setting on the learning guarantees of SCO remains unclear and merits further investigation.
__label__natural_language_processing We find clear advantages for IRL-based imitation, in particular for retaining diversity while maximizing task performance, rendering IRL a strong alternative on fixed SFT datasets even without online data generation.
__label__graph_neural_networks Extensive experiments on four GCIL benchmarks show that i) our task prototype-based method can achieve 100% task ID prediction accuracy on all four datasets, ii) our GCIL model significantly outperforms state-of-the-art competing methods by at least 18% in average CIL accuracy, and iii) our model is fully free of forgetting on the four datasets.
__label__neuroscience_and_cognitive_science We then prove that many highly degenerate (non-strict) saddles of the loss including the origin become much easier to escape (strict) in the equilibrated energy.
__label__optimization SkipPredict prioritizes predicted short jobs over long jobs, and for the long jobs, SkipPredict applies a second round of more detailed “expensive predictions” to approximate Shortest Remaining Processing Time for these jobs.
__label__natural_language_processing Nonetheless, these existing approaches encounter two significant limitations.
__label__diffusion_based_models Specifically, to handle the challenging hand generation caused by sparse motion guidance, we propose a novel Key Point-based Fine-grained Hand Modeling module by amplifying positional information from raw hand key points and constructing a corresponding key point-based codebook.
__label__optimization We also present numerical results on a nonconvex/PL problem with synthetic data and on distributionally robust optimization problems with real data, illustrating our theoretical findings.
__label__natural_language_processing Specifically, we iteratively prompt an LLM to propose and implement new preference optimization loss functions based on previously evaluated performance metrics.
__label__learning_theory For instance, we can predict the amount by which the strong model will improve over the weak model, and also choose among different weak models to train the strong model, based on its misfit error.
"__label__deep_learning_architectures To further reduce stored parameters, we introduce a ""divide-and-share"" paradigm that breaks the barriers of low-rank decomposition across matrix dimensions, modules, and layers by sharing  parameters globally via a vector bank."
__label__reinforcement_learning Reinforcement learning (RL) algorithms are typically based on optimizing a Markov Decision Process (MDP) using the optimal Bellman equation.
__label__learning_theory Our extensive experiments validate that our approach consistently improves self-training performances by 8% to 16% across diverse distribution shift scenarios without a computational overhead.
__label__natural_language_processing Ambiguous answers are essential for knowledge-seeking, but it may go beyond the knowledge boundary of LLMs.
"__label__online_learning To tackle this problem, we propose CAMS, a contextual active model selection algorithm that relies on two novel components: (1) a contextual model selection mechanism, which leverages context information to make informed decisions about which model is likely to perform best for a given context, and (2)
an active query component, which strategically chooses when to request labels for data points, minimizing the overall labeling cost."
__label__machine_vision Through the alignment of 3D and 2D space, our model can directly integrate RGB information, further enhancing the understanding of 3D anomalies in a plug-and-play manner.
__label__optimization However, due to the complex three-level optimization structure, existing algorithms often suffer from issues such as high computing costs due to the second-order model derivatives or high memory consumption in storing all blocks' parameters.
__label__robotics arms, legs, or fingers), but each robot must be trained from scratch to control all the actuators of all the parts together.
__label__diffusion_based_models We propose Resfusion, a general framework that incorporates the residual term into the diffusion forward process, starting the reverse process directly from the noisy degraded images.
__label__robotics Our key insight is to leverage a humanoid motion representation that provides human-like motor skills and significantly speeds up training.
__label__optimization_for_deep_networks This urges the development of more computation and communication efficient training algorithms.
"__label__machine_learning_for_healthcare To address the challenge of large search spaces and high sampling costs, 
we design a relaxation mechanism that uses an approximation strategy to efficiently explore optimal subgraph configurations."
__label__causal_inference Extensive experiments are conducted to verify the theoretical results and the effectiveness of the proposed dynamic learning framework.
__label__robotics We compare to state-of-the-art imitation learning and LVM baselines and see that QueST’s architecture leads to strong performance on several multitask and few-shot learning benchmarks.
__label__generative_models Benefiting from accurate motion learning, we could achieve straightforward mesh animation.
__label__machine_vision Although the neural radiance field (NeRF) exhibits high-fidelity visualization on the rendering task, it still suffers from rendering defects, especially in complex scenes.
__label__probabilistic_methods Experts’ labels are often costly, requiring efficient use of their efforts, and can at the same time be unreliable, requiring careful adjustment of the degree to which any expert is trusted.
__label__diffusion_based_models Rigorous theoretical proofs and extensive experiments also demonstrate the advantages of this simple gated residual mechanism consistent with dynamic modeling in improving the  fidelity and consistency of generated content and supporting large-scale scalable training.
__label__reinforcement_learning Crucially,  the planner and the neural policy  play a synergistic role in DMPS.
__label__deep_learning_architectures CosAE, however, encodes frequency coefficients, i.e., the amplitudes and phases, in its bottleneck.
__label__reinforcement_learning Motivated by this observation, we aim to understand the bottlenecks in current offline RL algorithms.
__label__other However, they have limitations when tasked with generating images in dynamic, evolving domains.
__label__machine_vision The superior performance further confirms the feasibility of using LLMs for understanding visual tables when all concepts work in synergy.
__label__machine_vision Current methods for ego-body pose estimation rely on temporally dense sensor data, such as IMU measurements from spatially sparse body parts like the head and hands.
__label__reinforcement_learning Partner diversity is known to be crucial for training a robust generalist cooperative agent.
__label__natural_language_processing This simple modification can be easily applied to any DPO-based methods and mitigate over-optimization and objective mismatch, which prior works suffer from.
__label__optimization_for_deep_networks We support our theoretical findings with experiments on both DUFM and real data, which show the emergence of the low-rank structure in the solution found by gradient descent.
__label__natural_language_processing From a theoretical perspective, both our approach and existing methods can be clearly categorized: word representations are distributed according to an exponential family with either uniform or Zipfian base measures.
__label__interpretability_and_explainability Most currently deployed LLMs undergo continuous training or additional finetuning.
__label__neuroscience_and_cognitive_science We study the optimal memorization capacity of modern Hopfield models and Kernelized Hopfield Models (KHMs), a transformer-compatible class of Dense Associative Memories.
__label__optimization_for_deep_networks We have created a Python package called Modula that automatically normalizes weight updates in the modular norm of the architecture.
__label__machine_vision Our alignment is conducted in an agentic workflow controlled by an LLM-based assistant named AVAgent.
__label__optimization However, existing works have found that naively incorporating momentum into ASGD can impede the convergence.
__label__optimization_for_deep_networks In this work, we introduce a novel approach to train the factors of a Tucker decomposition of the weight tensors.
__label__evaluation Large Language Models (LLMs) have transformed natural language processing and extended their powerful capabilities to multi-modal domains.
__label__safety_in_machine_learning This paper presents Conformal Alignment, a general framework for identifying units whose outputs meet a user-specified alignment criterion.
__label__natural_language_processing To address these challenges, we propose Generate, Improve and Fix with Monte Carlo Tree Search (GIF-MCTS), a new code generation strategy for LLMs.
__label__fairness In this framework, we first propose a new fairness regularization term that can lead to a fair partition of data.
__label__generative_models Variance exploding (VE) based diffusion models, an important class of diffusion models, have shown state-of-the-art (SOTA) performance.
"__label__generative_models Our theoretical results suggest that this non-monotonicity is due to the diversity of query difficulties within a task: more LM calls lead to higher performance on ""easy"" queries, but lower performance on ""hard"" queries, and non-monotone behavior can emerge when a task contains both types of queries."
__label__diffusion_based_models We also propose a novel parameterization technique for learning the forward process.
__label__natural_language_processing Specifically, IN2 training leverages a synthesized long-context question-answer dataset, where the answer requires (1) **fine-grained information awareness** on a short segment (~128 tokens) within a synthesized long context (4K-32K tokens), and (2) the **integration and reasoning** of information from two or more short segments.
__label__optimization Existing second-order and high-order methods for variational inequalities require precise computation of derivatives, often resulting in prohibitively high iteration costs.
__label__machine_vision Open-vocabulary models utilize class names as text prompts to generalize to categories unseen during training.
__label__machine_vision Many existing approaches tend to assume that clients are isomorphic and all of them belong to either single-view clients or multi-view clients.
__label__algorithmic_game_theory We introduce the set of $k$-mediator deviations, which generalize the untimed communication deviations recently introduced by Zhang, Farina and Sandholm [2024] to the case of having multiple mediators, and we develop algorithms for minimizing the regret with respect to this set of deviations in $N^{O(k)}/\epsilon^2$ rounds.
__label__graph_neural_networks Experimenting on 23 datasets, UltraQuery in the zero-shot inference mode shows competitive or better query answering performance than best available baselines and sets a new state of the art on 15 of them.
__label__diffusion_based_models Masked pose images are used to smoothly refine the attention maps based on target pose-related features in a hierarchical manner, transitioning from coarse to fine levels.
__label__deep_learning_architectures In this paper, we present Depth-wise Separable Convolution Pruning (DEPrune), a novel pruning method applied to both point-wise and depth-wise convolutions.
__label__learning_theory Credit attribution is crucial across various fields.
__label__machine_vision We introduce a new task called Adaptable Error Detection (AED), which aims to identify behavior errors in few-shot imitation (FSI) policies based on visual observations in novel environments.
__label__machine_vision To handle multiple unknown degradations, PCE recasts it into a conditional restoration problem by implicitly establishing a conditional map between degradations and ground truths.
__label__optimization As gradient descent is sensitive to its hyperparameters, we need to tune the hyperparameters carefully using a grid search.
__label__fairness However, spurious correlation remains an ongoing challenge, primarily due to the difficulty in correctly detecting these samples.
"__label__learning_theory Motivated by this challenge, we propose a pipeline for constructing stable classifiers from data, using bagging (i.e., resampling and averaging) to produce stable continuous scores, and then using a stable relaxation of argmax, which we call the ""inflated argmax"", to convert these scores to a set of candidate labels."
__label__graph_neural_networks Graph Neural Networks (GNNs) have demonstrated strong performance in graph mining tasks due to their message-passing mechanism, which is aligned with the homophily assumption that adjacent nodes exhibit similar behaviors.
__label__deep_learning_architectures This is achieved by eliminating the corresponding weights from the network, without the need for retraining.
__label__natural_language_processing In this paper, we introduce DeAR (_Decompose-Analyze-Rethink_), a framework that iteratively builds a reasoning tree to tackle intricate problems within a single large language model (LLM).
__label__diffusion_based_models To address this, we propose a novel fine-tuning objective, dubbed Direct Consistency Optimization, which controls the deviation between fine-tuning and pretrained models to retain the pretrained knowledge during fine-tuning.
__label__optimization Numerical experiments across different tasks validate the effectiveness of our proposed methods.
__label__fairness Unlike previous work that deals with specific objectives for quality and fairness, we deal with all objectives for fairness and quality in two general classes encompassing most of the special cases addressed in previous work.
__label__bandits Under minor regularity assumptions, our algorithm achieves an optimal regret bound of $\tilde{\mathcal{O}}(T^{2/3})$, improving the existing results.
__label__natural_language_processing Experimental evaluations on various LLMs using different benchmarks demonstrate that RTD establishes a new paradigm for augmenting models to downstream tasks.
__label__interpretability_and_explainability We propose an algorithm that is guaranteed to decrease the error monotonically and which scales to large datasets without any approximation.
__label__natural_language_processing However, uncertain correspondences between inter-modal or intra-modal cues, such as weak inter-modal associations, description diversity, and modality absence, still severely hinder the effective exploration of aligned entity similarities.
__label__deep_learning_architectures Across 15 model based RL tasks and 35 imitation learning tasks with high-dimensional image and point cloud inputs, DiffTORI outperforms prior state-of-the-art methods in both domains.
__label__optimization_for_deep_networks The fusion mechanism leverages the strengths of each modality while minimizing their weaknesses.
__label__learning_theory Although memorization is widely believed to have a close relationship with the strong generalizability of deep learning when using overparameterized models, to the best of our knowledge, there exists no theoretical study on the generalizability of memorization neural networks.
__label__machine_vision Particularly, the proposed FADA consists of two branches, i.e., low- and high- frequency branches.
__label__safety_in_machine_learning Existing techniques aimed at improving alignment, such as refusal training, are often bypassed.
__label__optimization We theoretically prove the convergence of OrMo with both constant and delay-adaptive learning rates for non-convex problems.
__label__neuroscience_and_cognitive_science Conversely, the neuroscience-inspired model significantly outperforms all optical flow models on this task.
__label__neuroscience_and_cognitive_science Additionally, we present a novel space-time-resolved decoding technique, demonstrating how temporal resolution in decoding can advance our understanding of neural representations.
__label__online_learning We study online composite optimization under the Stochastically Extended Adversarial (SEA) model.
__label__machine_vision This paper introduces a novel probabilistic framework that unifies various recent proposals in long-tail learning.
__label__natural_language_processing Additionally, DropBP calculates the sensitivity of each layer to assign an appropriate drop rate, thereby stabilizing the training process.
__label__reinforcement_learning We define this optimism as a logical constraint between a program and a planner.
__label__learning_theory We validate our design choices with both simulations and real data experiments.
__label__machine_vision We build the Mapverse benchmark, sourced from the Ithaca365 and nuPlan datasets, to evaluate our method in unsupervised 2D segmentation, 3D reconstruction, and neural rendering.
__label__human-AI_interaction The proposed LNLN features a dominant modality correction (DMC) module and dominant modality based multimodal learning (DMML) module, which enhances the model's robustness across various noise scenarios by ensuring the quality of dominant modality representations.
__label__optimization We explore the theoretical possibility of learning $d$-dimensional targets with $W$-parameter models by gradient flow (GF) when $W<d$.
__label__machine_learning_for_physical_sciences The inferred gradient fields can then be used to rapidly generate sample trajectories that mimic the dynamics of the physical system on a population level over varying physics parameters.
__label__safety_in_machine_learning Finally, we identify two cases where ICL fails: one in logic reasoning tasks that require generalizing to new, unseen patterns, and another in analogy completion where relevant word pairs appear only in fixed training positions.
__label__algorithmic_game_theory We show that Team-FP reaches near TNE in ZSPTGs with a quantifiable error bound.
__label__algorithmic_game_theory Our main contribution is the first (randomized) non-wasteful algorithm that simultaneously achieves a $1/2$ approximation to class envy-freeness (CEF) while simultaneously ensuring an equivalent approximation to the class proportionality (CPROP) and utilitarian social welfare (USW) objectives.
__label__evaluation Despite the development of various CDR methods, two critical questions remain underexplored: when should these methods be applied, and how can the information unique to the foreground group be quantified?
__label__probabilistic_methods Reliable prediction of protein variant effects is crucial for both protein optimization and for advancing biological understanding.
__label__machine_vision Existing SOTA methods employ attention-based transformers to learn the 3D hand pose and shape, yet they do not fully achieve robust and accurate performance, primarily due to inefficiently modeling spatial relations between joints.
__label__machine_vision The AGL task is associated with two important challenges.
__label__deep_learning_architectures We provide new insights and a paradigm for the long-tailed learning problem, greatly expanding its applicability in practical scenarios.
__label__generative_models In particular, Ctrl-X supports novel structure and appearance control with arbitrary condition images of any modality, exhibits superior image quality and appearance transfer compared to existing works, and provides instant plug-and-play functionality to any T2I and text-to-video (T2V) diffusion model.
__label__machine_vision However, these methods still struggle with task ambiguity in in-context segmentation, as not all in-context examples can accurately convey the task information.
__label__generative_models Fortunately, with a pre-defined ordering strategy, 3D meshes can be represented as sequences, and the generation process can be seamlessly treated as an auto-regressive problem.
__label__machine_vision Code is available at https://github.com/liuyijungoon/TAL.
__label__machine_vision Recent advancements in 3D perception have led to a proliferation of network architectures, particularly those involving multi-modal fusion algorithms.
__label__generative_models For instance, many LLM-like video models learn the distribution of discrete tokens derived from 3D VAEs within the VQVAE framework, while most diffusion-based video models capture the distribution of continuous latent extracted by 2D VAEs without quantization.
__label__machine_vision We show comprehensive state-of-the-art results of the proposed method over large datasets like Imagenet.
__label__reinforcement_learning We also show a simple extension of IsCiL for task unlearning scenarios.
__label__learning_theory 2.
__label__machine_learning_for_physical_sciences Our scalable architecture combines aspects of graph neural networks with efficient approximations to the physics of errors in quantum programs.
__label__machine_learning_for_healthcare With multi-modal biological data, patient characterization can be enriched from two distinct views: the biological view and the phenotype view.
__label__deep_learning_architectures In the medical multi-modal frameworks, the alignment of cross-modality features presents a significant challenge.
__label__reinforcement_learning The algorithm exploits these new samples to  complete the matrix estimation using a CUR-like method.
__label__generative_models However, genomic sequences are functionally heterogeneous, consisting of multiple connected regions (e.g., Promoter Regions, Exons, and Introns) where elements within each region come from the same probability distribution, but the overall sequence is non-homogeneous.
__label__optimization Recently, beginning with the landmark ColBERT paper, multi-vector models, which produce a set of embedding per data point, have achieved markedly superior performance for IR tasks.
__label__interpretability_and_explainability These settings carry natural collections of interpretable features—for example, “there is a knight on F3”—which we leverage into metrics for SAE quality.
__label__learning_theory It extends the applicability of the algorithm by: (1) allowing the subgroups' underlying input distributions to be different, unknown, and heavy-tailed; (2) recovering all subgroups followed by a significant proportion of batches even for infinite $k$; (3) removing the separation requirement between the regression vectors; (4) reducing the number of batches and allowing smaller batch sizes.
__label__machine_vision Inspired by that, we propose to integrate the strong learning capacity of the video diffusion model with the rich motion information of an event camera as a motion simulation framework.
__label__machine_vision However, the domain mismatch between the pre-training and the downstream CL tasks calls for finetuning of the CLIP on the latter.
__label__diffusion_based_models Extensive experiments show that our MCM achieves state-of-the-art video diffusion distillation performance.
__label__machine_learning_for_healthcare Histopathology Whole Slide Image (WSI) analysis serves as the gold standard for clinical cancer diagnosis in the daily routines of doctors.
__label__safety_in_machine_learning Through comprehensive experiments and ablation studies, we demonstrate that our ProTransformer significantly enhances the robustness of transformer models across a variety of prediction tasks, attack mechanisms, backbone architectures, and data domains.
__label__generative_models This approach maintains global consistency while incorporating diverse and high-quality spatiotemporal details from local videos, enhancing both the consistency and fidelity of long video generation.
__label__machine_vision To achieve generalized image restoration, all-in-one methods have recently been proposed and shown potential for multiple restoration tasks using one single model.
__label__generative_models Through extensive experiments, we demonstrate that DP-Attacker has the capability to significantly decrease the success rate of DP for all scenarios.
__label__privacy Recent algorithms can numerically compute the privacy parameters to arbitrary precision but must be carefully applied.
__label__natural_language_processing We notice that a better zero-shot CoT reasoning needs the prompt to obtain semantic information from the question then the rationale aggregates sufficient information from the question directly and via the prompt indirectly.
__label__deep_learning_architectures Theoretical upper bounds on the approximation error of $\rm CALDERA$ are established using a rank-constrained regression framework, and the tradeoff between compression ratio and model performance is studied by analyzing the impact of target rank and quantization bit budget.
__label__graph_neural_networks The reduction to the canonicalization perspective further uncovers equivalences between previous methods.
__label__machine_learning_for_physical_sciences This region is defined as a set of vectors where the inner products with both the gradients of the PDE residual loss and the boundary loss are non-negative.
__label__reinforcement_learning Preferences serve as an alternative but recent work rarely considers preference learning given multiple tasks.
__label__bandits Our first technical novelty is its derivation, which utilizes a time-uniform PAC-Bayesian bound with a uniform prior/posterior, despite the latter being a rather unpopular choice for deriving CSs.
__label__deep_learning_architectures We show that when the dataset exhibits strong symmetries, the permutation matrices will converge to regular group representations and our weight-sharing networks effectively become regular group convolutions.
__label__probabilistic_methods Bayesian optimization is an effective technique for black-box optimization, but its applicability is typically limited to low-dimensional and small-budget problems due to the cubic complexity of computing the Gaussian process (GP) surrogate.
__label__diffusion_based_models This model, fine-tuned from a 2D diffusion model, directly generates pixel-aligned 3D Gaussians as an immediate 3D scene representation for consistent denoising.
__label__machine_learning_for_physical_sciences Additionally, we demonstrate Stormer’s favorable scaling properties, showing consistent improvements in forecast accuracy with increases in model size and training tokens.
"__label__safety_in_machine_learning We first observe that the vector direction of LLM parameters remains stable after the model has converged during pretraining, 
with negligible perturbations through subsequent training steps, including continued pretraining, supervised fine-tuning, and RLHF, 
which makes it a sufficient condition
to identify the base model."
"__label__learning_theory In practice, higher-order cumulants, which quantify
the non-Gaussian correlations between three or more variables, are particularly
important for the performance of neural networks."
"__label__reinforcement_learning All code and data from this
study have been made pulicly available at https://github.com/histmeisah/Large-Language-Models-play-StarCraftII"
__label__machine_learning_for_healthcare This trade-off is primarily constrained by k-space measurements, which traverse specific trajectories in the spatial Fourier domain (k-space).
__label__machine_vision Our experimental results demonstrate that our method surpasses existing approaches in terms of rendering quality.
__label__optimization Furthermore, we present extensions of this approach to training accurate sparse DNNs, and validate it experimentally at scale.
__label__learning_theory In this framework, an agent, whose target is to maximize the total reward under the initial inventory, selects an action in each round upon observing a random request, leading to a reward and resource consumptions that are further associated with an unknown random external factor.
__label__learning_theory We develop a unifying framework for information-theoretic lower bound in statistical estimation and interactive decision making.
__label__probabilistic_methods This procedure updates the LSA estimate together with a set of randomly perturbed LSA estimates upon the arrival of subsequent observations.
__label__other While this question is fairly well understood in low-dimensions, we establish some of the first upper and lower bounds for high-dimensional point sets.
__label__interpretability_and_explainability In the context of continuous symmetry detection, current state of the art experiments are limited to the detection of affine transformations.
__label__machine_vision We evaluate these models in four tasks: Vision-Language Scene Reasoning, Visual Grounding, Segmentation, and Registration, each focusing on different aspects of scene understanding.
__label__causal_inference While some adaptive experimental strategies exist for the identification of the single best subgroup, they commonly do not enable the identification of the best set of subgroups.
__label__reinforcement_learning We compare E2GN2 to other common function approximators using common MARL benchmarks MPE and SMACv2.
__label__natural_language_processing For instance, leading approaches achieve a perplexity (PPL) of 10 or greater on Wikitext compared to the dense model's 5.12 PPL, but MaskLLM achieves a significantly lower 6.72 PPL solely by learning the masks with frozen weights.
__label__machine_learning_for_other_sciences_and_fields This additional context enables black-box LLMs to enhance recovery accuracy.
__label__active_learning The algorithm is capable of safely tracking a time-varying safe region without the need for explicit change detection.
__label__deep_learning_architectures Time series forecasting typically needs to address non-stationary data with evolving trend and seasonal patterns.
__label__safety_in_machine_learning In our black-box setting, all fingerprinting steps are internally conducted by the LLMs owners.
__label__learning_theory Despite its empirical success, however, it has remained unclear whether this method can provably recover the performance of its optimal, prior-aware counterparts.
__label__machine_learning_for_other_sciences_and_fields Extensive experiments on four different circuit benchmarks demonstrate that our method can precisely generate circuits with up to 1200 nodes.
__label__machine_vision By coupling them in a tight manner, we can fully leverage the potential of both models.
__label__generative_models Our approach combines a novel differentiable simulation-based loss function with physically inspired regularization, serving as either a refinement or a post-processing module for existing frameworks.
__label__active_learning The optimization process is guided by an acquisition function that selects points to acquire in each round of BO.
__label__machine_vision Code is available at https://github.com/prstrive/SCGaussian.
__label__deep_learning_architectures We propose a new paradigm to formulate the visual object tracking problem as a denoising learning process.
__label__machine_vision By leveraging the low-pass filtering properties of the regularized temporal representations, ConVRT effectively mitigates turbulence-induced temporal frequency variations and promotes temporal consistency.
__label__machine_vision 1) Model-level: we propose a Pixel-level IEA Network (P-IEANet) that utilizes Haar discrete wavelet transform (DWT) to analyze, decompose, and assess exposure from both lightness and structural perspectives, capable of generating pixel-level assessment results under no-reference scenarios.
__label__privacy We design a new algorithm **Fed-SVT** and show that it achieves an $m$-fold regret speed-up under both pure DP and approximate DP constraints over the single-player counterparts.
__label__machine_vision Occupancy prediction has increasingly garnered attention in recent years for its fine-grained understanding of 3D scenes.
__label__machine_learning_for_social_sciences Agent-based models (ABMs) are proliferating as decision-making tools across policy areas in transportation, economics, and epidemiology.
__label__learning_theory The existing theory bounds for multi-label learning, which preserve the coupling among different components, are invalid for LSRL.
__label__reinforcement_learning We evaluate GCPO on a challenging multi-goal long-horizon task: fixed-wing UAV velocity vector control.
__label__machine_vision To address this challenge, we draw inspiration from human visual cognition, and propose the Visual Fourier Prompt Tuning (VFPT) method as a general and effective solution for adapting large-scale transformer-based models.
__label__learning_theory Our main result is that there is an agent that is able to generate in the limit for every countable list of candidate languages.
__label__machine_vision The source code is available at \url{https://github.com/yuxi120407/CLIPCEIL}.
__label__natural_language_processing In xRAG, the only trainable component is the modality bridge, while both the retriever and the language model remain frozen.
__label__machine_vision For example, if RGB channels are always present, the other channels can focus on extracting information that cannot be captured by the RGB channels.
__label__probabilistic_methods Recent works in Variational Inference have examined alternative criteria to the commonly used exclusive Kullback-Leibler divergence.
__label__machine_learning_for_physical_sciences The problem is commonly solved using genetic algorithms; we show that we can enhance such methods by inducing a library of abstract textual concepts.
__label__online_learning Our reservoir thus contains $K$ previous intermediate weight vectors with high survival times.
__label__machine_vision Extensive experiments are performed on two benchmark datasets, EPIC-Kitchens and Human-Animal-Cartoon (HAC), with various modality combinations, demonstrating the effectiveness of our method under multi-source and single-source settings.
__label__robotics Although existing Vision-Language-Action (VLA) models for robots can handle a range of basic tasks, they still face challenges in two areas: (1) insufficient reasoning ability to tackle complex tasks, and (2) high computational costs for VLA model fine-tuning and inference.
__label__natural_language_processing First, by using LFMs to identify desirable behaviour to imitate, we improve in task-completion rate over strong behavioural cloning baselines on three distinct language grounding environments (Touchdown, ScienceWorld, and ALFWorld).
__label__optimization These solvers offer several advantages over traditional methods and other learning-based methods, particularly for large-scale CO problems.
__label__probabilistic_methods This has led to the rise of simulation-based inference (SBI), a class of machine learning-enabled techniques for approaching inverse problems with stochastic simulators.
__label__graph_neural_networks In this paper, we develop a novel Hierarchical Cluster-based GAE (HC-GAE), that can learn effective structural characteristics for graph data analysis.
__label__optimization We validate this result with numerical experiments.
__label__machine_learning_for_other_sciences_and_fields Additionally, a risk-control component in FinCon enhances decision quality by episodically initiating a self-critiquing mechanism to update systematic investment beliefs.
__label__machine_vision Although achieving groundbreaking progress, such design has certain drawbacks: 1) preceding subtasks require massive high-quality 3D annotations as supervision, posing a significant impediment to scaling the training data; 2) each submodule entails substantial computation overhead in both training and inference.
__label__reinforcement_learning This becomes problematic in continual learning settings, where the resulting learning rate schedule may decay to near zero too quickly relative to the timescale of the learning problem.
__label__neuroscience_and_cognitive_science Prediction is a fundamental capability of all living organisms, and has been proposed as an objective for learning sensory representations.
__label__other Empirically, we analyze different components of *$\mathcal{H}$-LLM* to understand *why* and *when* it works, demonstrating the potential of self-healing ML.
__label__natural_language_processing Concretely, we steer the agent model to self-synthesize knowledge from both expert and sampled trajectories.
__label__online_learning As a by-product, we extend MetaAd to the FLM setting and get provable competitive algorithms.
__label__machine_vision It is important to estimate an accurate signed distance function (SDF) from a point cloud in many computer vision applications.
__label__optimization_for_deep_networks Experimental results obtained on main-stream architectures and tasks demonstrate Pareto-superiority over other state-of-the-art solutions, in terms of the trade-off between generalization and memory footprint.
__label__interpretability_and_explainability It provably reduces the upper bound of generalization error.
__label__probabilistic_methods Many popular methods for in-distribution (ID) calibration, such as isotonic and Platt’s sigmoidal regression, exhibit excellent ID calibration performance.
__label__algorithmic_game_theory Next, we consider the problem of revenue maximization in this environment.
__label__natural_language_processing However, the underlying rules for the effectiveness of MM-ICL remain under-explored.
__label__optimization In the high signal-to-noise ratio (SNR) regime, we show that T-DGD achieves comparable statistical accuracy to DGD, while the communication cost is logarithmic in the number of parameters.
__label__causal_inference As the Kolmogorov complexity is not computable, we instantiate our model using Minimum Description Length and show that the resulting score identifies the causal direction.
__label__optimization_for_deep_networks Our training proposal proves to be optimal in locally approximating the original unfactorized dynamics and stable for the initialization.
__label__neuroscience_and_cognitive_science From decades of neuroscience research, we know that synaptic weights are constantly changing, controlled in part by chemicals such as neuromodulators.
__label__optimization This paper puts forth a theoretical foundation for gradient-based training on analog devices.
__label__generative_models Meanwhile, developments in neural surface representation learning incorporate non-linear positional encoding, addressing issues like spectral bias; however, this poses challenges in applying mesh extraction techniques based on CPWA functions.
__label__deep_learning_architectures Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories, in particular they constituted the first Large Language Models (LLMs).
__label__graph_neural_networks This insight explains how edge deletions can improve generalization, thus connecting spectral gap optimization to a seemingly disconnected objective of reducing computational resources by pruning graphs for lottery tickets.
__label__neuroscience_and_cognitive_science We additionally find that QA-Emb can be effectively approximated with an efficient model, and we explore broader applications in simple NLP tasks.
__label__human-AI_interaction In this work, we introduce GUIDE, a framework for real-time human-guided reinforcement learning by enabling continuous human feedback and grounding such feedback into dense rewards to accelerate policy learning.
__label__natural_language_processing We prepare six (two synthetic and four real) observational datasets, paired with corresponding ground truth in the form of randomized trials, which we used to systematically evaluate each step of our pipeline.
__label__deep_learning_architectures Enabling MPFT and PEFT in Mamba architectures is challenging due to recurrent dynamics and highly customized CUDA kernels, respectively.
__label__machine_vision We demonstrate the effectiveness of our method in task arithmetic, few-shot recognition and test-time adaptation, with supervised or unsupervised objectives.
__label__reinforcement_learning We study our agent on gridworlds, and on task planning, finding our approach is more sample-efficient compared to deep RL, more compute-efficient compared to ReAct-style agents, and that it can transfer its knowledge across environments by editing its code.
__label__bandits The problem is fairly understood in toy settings with linear target functions or over finite small domains that limits practical interest.
"__label__optimization_for_deep_networks This characteristic is strongly correlated to a network's initialization state, establishing criterion autonomy from the learning state ($\textit{stateless}$) and thus setting a new fundamental basis for the expansion of compression strategies in regards to the ""When to Prune"" question."
__label__probabilistic_methods We develop a new geometric view of reparametrizations from which we explain the success of linearization.
__label__other With the assistance of ST$_k$ Loss, we surpass the state-of-the-art (SOTA) on both CIFAR-100-LT and Places-LT leaderboards.
__label__reinforcement_learning Evaluated in the DeepMind Control suite, our framework termed Meta-Controller demonstrates superior few-shot generalization to unseen embodiments and tasks over modular policy learning and few-shot IL approaches.
__label__generative_models Latent diffusion models (LDM) have revolutionized text-to-image generation, leading to the proliferation of various advanced models and diverse downstream applications.
__label__learning_theory Extensive experiments on six datasets show that RecPiece achieves higher performances but comparable or even fewer parameters compared to previous anchor-based KGE models, indicating that our model can select better anchors in a more scalable way.
__label__diffusion_based_models However, these models still struggle with inconsistencies and implausibility in new views generation, especially for challenging changes in viewpoint.
__label__natural_language_processing Transformer-based large language models (LLMs) typically have a limited context window, resulting in significant performance degradation when processing text beyond the length of the context window.
__label__privacy Finally, we perform numerical experiments to validate our theoretical analysis.
__label__bandits And by trivially treating every set as a unique arm one deduces that $\sqrt{ {n \choose k} T }$ is also achievable using standard multi-armed bandit algorithms.
__label__machine_vision In computer vision, gaze following is defined as the prediction of the pixel coordinates where a person in the image is focusing their attention.
__label__graph_neural_networks Detecting such subgraph patterns is important in many applications; therefore, establishing graph neural networks (GNNs) that can detect such patterns and run fast on large graphs is demanding.
__label__probabilistic_methods Extensive experiments validate the effectiveness of our method.
__label__probabilistic_methods With this formulation, the information from each sample can be shared not only with neighbors but also across dimensions, thus fostering a more global search strategy.
__label__reinforcement_learning To the best of our knowledge, these are the first randomized RL algorithms for the MNL transition model that achieve both computational and statistical efficiency.
__label__speech_and_audio Frieren regresses the conditional transport vector field from noise to spectrogram latent with straight paths and conducts sampling by solving ODE, outperforming autoregressive and score-based models in terms of audio quality.
__label__neuroscience_and_cognitive_science Remarkably, MP outperforms all existing AI models with a maximum improvement of 29\% in top-1 action recognition accuracy on these conditions.
__label__machine_learning_for_healthcare To address these limitations, we propose a novel framework called Med-MICN (Medical Multi-dimensional Interpretable Concept Network).
__label__diffusion_based_models However, training a diffusion model is computationally expensive, creating a pressing need to adapt off-the-shelf diffusion models for downstream generation tasks.
__label__fairness To this end, we propose a simple but effective method named VFair to minimize the variance of training losses inside the optimal set of empirical losses.
__label__reinforcement_learning Unlike previous approaches that relied on tracking-based methods for multi-humanoid HOI, CooHOI is inherently efficient, does not depend on motion capture data of multi-humanoid interactions, and can be seamlessly extended to include more participants and a wide range of object types.
__label__reinforcement_learning As a key part of the proof, constructional results are established to demonstrate that the transformer architecture is sufficiently rich to realize celebrated multi-agent game-playing algorithms, in particular, decentralized V-learning and centralized VI-ULCB.
__label__online_learning Quantitatively, our upper bounds improve upon existing multiclass upper bounds in Hanneke et al.
"__label__reinforcement_learning We then demonstrate the utility of this methodology by assessing
the hyperparameter sensitivity of several commonly used normalization variants of
PPO."
__label__robotics Robo-Instruct introduces RoboSim to synthesize a consistent world state on the fly by inferring properties relevant to the program being checked, and simulating actions accordingly.
__label__privacy It helps prevent effective unlearning from interfering with the retained performance.
__label__reinforcement_learning Rather than using the global timestep in the Adam update, Adam-Rel uses the *local* timestep within an epoch, essentially resetting Adam's timestep to 0 after target changes.
__label__optimization While recent works offer first-order methods for unconstrained bilevel problems, the constrained setting remains relatively underexplored.
__label__graph_neural_networks Standard iterative methods require accessing the whole graph per iteration, making them time-consuming for large-scale graphs.
__label__machine_vision Finally, compared with current state-of-the-art methods, our lightest model achieves superior RayIoU on the Occ3D-nuScenes dataset at near 2x FPS, while our heaviest model surpasses previous best results by 6.1 RayIoU.
__label__probabilistic_methods Symmetries have proven useful in machine learning models, improving generalisation and overall performance.
__label__infrastructure We introduce SGLang, a system for efficient execution of complex language model programs.
__label__optimization Our methodology is based on the interpretation of diffusion processes as energy-minimizing trajectories in the probability space via the so-called JKO scheme, which we study via its first-order optimality conditions.
__label__natural_language_processing While several approaches have been proposed to enhance LLMs' context awareness, achieving both effectiveness and efficiency remains challenging.
__label__machine_vision We show that MooG can provide a strong foundation for different vision tasks when compared to “on-the-grid” baselines.
__label__human-AI_interaction In this work, we propose a unified modeling approach for human-AI alignment in chess that coherently captures human style across different skill levels and directly captures how people improve.
__label__safety_in_machine_learning We also propose a novel training objective that enhances the consistency of multi-modal features while encourages differentiated uni-modal features between natural and adversarial examples.
__label__natural_language_processing In this paper, we propose a novel unlearning framework called Unlearning from Logit Difference (ULD), which introduces an assistant LLM that aims to achieve the opposite of the unlearning goals: remembering the forget documents and forgetting the retain knowledge.
__label__diffusion_based_models The form of our inference process is consistent with the DDPM.
__label__robotics In this work, we introduce Robo-Instruct, which brings the best of both worlds --- it promotes the diversity of Self-Instruct while providing the correctness of simulator-based checking.
__label__evaluation Such incoherence creates fragility: using a generative model to solve related but subtly different tasks can lead to failures.
__label__deep_learning_architectures In this paper, we introduce ControlSynth Neural ODEs (CSODEs).
__label__deep_learning_architectures While recent radar-camera fusion methods have made significant progress by fusing information in the bird's-eye view (BEV) representation, they often struggle to effectively capture the motion of dynamic objects, leading to limited performance in real-world scenarios.
__label__safety_in_machine_learning As such, their success relies on an underlying assumption that verification is a \emph{one-time} and \emph{privacy-preserving} process, which does not necessarily hold in practice.
__label__natural_language_processing Continual Pre-Training (CPT) on Large Language Models (LLMs) has been widely used to expand the model’s fundamental understanding of specific downstream domains (e.g., math and code).
__label__optimization In particular, we propose Extrapolated FedProx (FedExProx), and study three extrapolation strategies: a constant strategy (depending on various smoothness parameters and the number of participating devices), and two smoothness-adaptive strategies; one based on the notion of gradient diversity (FedExProx-GraDS), and the other one based on the stochastic Polyak stepsize (FedExProx-StoPS).
__label__machine_learning_for_social_sciences Large-scale simulation models of complex socio-technical systems provide decision-makers with high-fidelity testbeds in which policy interventions can be evaluated and _what-if_ scenarios explored.
__label__algorithmic_game_theory Motivated by its economic applications, this paper studies the robustness of this approximation to natural strategic manipulations in which each random reward is associated with a self-interested player who may selectively reveal his realized reward to the searcher in order to maximize his probability of being selected.
__label__online_learning The platform also monitors equilibrium prices influenced by both demand and supply.
__label__other With a long history in network science, community detection typically relies on objective functions, optimised with custom-tailored search algorithms, but often without leveraging recent advances in deep learning.
__label__speech_and_audio State-of-the-art audio foundation models, such as CLAP, which learn to map between audio scenes and natural textual descriptions, are trained on non-spatial audio and text pairs, and hence lack spatial awareness.
__label__deep_learning_architectures While prior works have delved into the implications of dataset shift on calibration, existing CE estimators either (i) assume access to labeled data from the target domain, often unavailable in practice, or (ii) are derived under a covariate shift assumption.
__label__generative_models In this study, we posit that online feedback is key and improves DAP methods.
__label__online_learning We address these challenges by developing an algorithm that handles delayed feedback, balancing exploration and exploitation using confidence bounds and optimism.
__label__natural_language_processing Notably, these biases often diverge from typical human emotional responses, occasionally leading to unexpected drops in cooperation rates, even under positive emotional influence.
__label__deep_learning_architectures We conduct extensive evaluation on 95 datasets curated by TabZilla from OpenML, upon which we establish a new state-of-the-art with LoCalPFN -- even with respect to tuned tree-based models.
__label__reinforcement_learning Offline model-based reinforcement learning (MBRL) enhances data efficiency by utilizing pre-collected datasets to learn models and policies, especially in scenarios where exploration is costly or infeasible.
__label__learning_theory As the PAC model fails to explain the behavior of learning curves, recent research has explored an alternative universal learning model and has ultimately revealed a distinction between optimal universal and uniform learning rates (Bousquet et al., 2021).
__label__deep_learning_architectures Extensive experiments on seven real-world datasets across domains showcase the superiority of EpoD against baselines, and toy example experiments further verify the powerful interpretability and rationality of our EpoD.
__label__natural_language_processing Our findings indicate that all aspects are important for performance, with better preference data leading to the largest improvements, followed by the choice of learning algorithm, the use of improved reward models, and finally the use of additional unlabeled prompts for policy training.
__label__interpretability_and_explainability We find that there is a strong disconnect between the existing research and the practical requirements for AR.
__label__other Recently, motivated by diminishing privacy concerns, mitigating potential attacks, and reducing communication overhead, one-shot federated learning (i.e., limiting client-server communication into a single round) has gained popularity among researchers.
__label__machine_learning_for_other_sciences_and_fields The proposed model enables fast single-step solution generation while retaining the option of multi-step sampling to trade for sampling quality, which offers a more effective and efficient alternative backbone for neural solvers.
__label__other On top of that, we introduce a simple yet effective test-time merging mechanism to dynamically merge discriminative LoRAs for test-time task customization.
__label__natural_language_processing We formulate the construction of such an LLM agent as a reinforcement learning (RL) problem, in which the LLM serves as the policy model.
__label__optimization Our result recovers the convergence rate of gradient descent in the centralized setting, showing its tightness.
__label__optimization_for_deep_networks Our update equation is comparable to the widely used SGD and much more efficient than existing BLO-based methods.
__label__machine_learning_for_healthcare The efficacy of our method in the centerline extraction and segmentation tasks has been substantiated through experimental evaluations across various datasets.
__label__human-AI_interaction Together, these findings present three important future research directions: 1) Improving the ability to generate collaborative agents with white-box models, 2) Better learning methods to facilitate collaboration rather than individualized coordination, and 3) Mixed-initiative interfaces that enable users, who may vary in ability, to improve collaboration.
__label__graph_neural_networks For instance, graph neural networks experience instability due to the convergence of node representations (over-smoothing), which can occur after only a few iterations of message-passing, reducing their effectiveness in downstream tasks.
__label__machine_vision Existing state-of-the-art dense object detection techniques tend to produce a large number of false positive detections on difficult images with complex scenes because they focus on ensuring a high recall.
__label__graph_neural_networks Moreover, we argue that the full potential of the compatibility matrix is not completely achieved due to the existence of incomplete and noisy semantic neighborhoods in real-world heterophilous graphs.
__label__deep_learning_architectures This benefits from the powerful model function understanding ability of the large language model (LLM).
__label__machine_vision Photographs captured in unstructured tourist environments frequently exhibit variable appearances and transient occlusions, challenging accurate scene reconstruction and inducing artifacts in novel view synthesis.
__label__natural_language_processing In particular, little is known about the proportions of different domains, languages, or code represented in the data.
__label__learning_theory Our reduction builds on the result of Chen et al.
__label__active_learning Batched Energy-Entropy acquisition for BO (BEEBO) enables tight control of the explore-exploit trade-off of the optimization process and generalizes to heteroskedastic black-box problems.
__label__machine_vision Continual learning requires to overcome catastrophic forgetting when training a single model on a sequence of tasks.
__label__probabilistic_methods Deep discriminative approaches like random forests and deep neural networks have recently found applications in many important real-world scenarios.
__label__safety_in_machine_learning Adversarial Training (AT) is a mainstream approach to enhancing Adversarial Robustness (AR) and has been validated on various traditional DNN architectures.
__label__optimization Our main result shows that if the targets are described by a particular $d$-dimensional probability distribution, then there exist models with as few as two parameters that can learn the targets with arbitrarily high success probability.
__label__causal_inference Finally, we present a novel capstone example using MC-EIF for optimal portfolio selection.
__label__safety_in_machine_learning While some defenses have been proposed, they have not been adapted to newly proposed attacks and more challenging threat models.
__label__natural_language_processing This novel perspective allows for a precise measurement of the importance of each layer across the model.
__label__reinforcement_learning Empirical results on offline multi-objective and safe tasks demonstrate the capability of our framework to infer policies that align with real preferences while meeting the constraints implied by the provided demonstrations.
__label__natural_language_processing These results highlight the effectiveness of KG-FIT in incorporating open-world knowledge from LLMs to significantly enhance the expressiveness and informativeness of KG embeddings.
__label__optimization However, the existing works consider only with-replacement sampling of stochastic gradients.
__label__other Experiments confirm our findings on both synthetic and real-world sequence classification tasks.
__label__safety_in_machine_learning In this work, we revisit patch-based attacks against person detectors and introduce a camera-agnostic physical adversarial attack to mitigate this limitation.
__label__bandits Empirically, we corroborate our theoretical findings via numerical simulations.
__label__reinforcement_learning In this work, we take a close look at the learning process itself under the multi-level training in Procgen.
__label__optimization The phenomenon seems elusive, and our current theoretical understanding remains severely incomplete.
__label__other Prior research has primarily focused on employing data-free knowledge distillation to optimize data generators and ensemble models for better aggregating local knowledge into the server model.
__label__fairness For instance, if commonly used criteria, such as independence or sufficiency, are satisfied for a prediction score $S$ used for binary classification, they need not be satisfied after an application of a simple thresholding operation on $S$ (as commonly used in practice).
__label__safety_in_machine_learning We begin with a theoretical prediction, show that it applies to learning human morality judgments, then show that our results generalize to ten different aspects of human values -- including ethics, honesty, and fairness -- training AI agents on each set of values in a multi-armed bandit setting, where rewards reflect human value judgments over the chosen action.
__label__optimization Experiments on $48$ benchmark test problems, including synthetic problems, RNA inverse design and protein structure prediction, fully demonstrate the effectiveness of our proposed approach.
__label__bandits Moreover, in the non-asymptotic setting, the complexity of previous batch algorithms is usually conditioned on the event that the best arm is returned (with a probability of at least $1-\delta$), which is potentially unbounded in cases where a sub-optimal arm is returned.
__label__neuroscience_and_cognitive_science It is shown that QKFormer achieves significantly superior performance over existing state-of-the-art SNN models on various mainstream datasets.
__label__other SuperClass demonstrated superior performance on various downstream tasks, including classic computer vision benchmarks and vision language downstream tasks.
__label__other In this work, we study the problem of communicating multiple samples from an unknown probability distribution using as few bits as possible.
__label__machine_vision To this end, we introduce a unified framework that jointly performs point tracking and segmentation, providing synergistic effects between the two tasks.
__label__interpretability_and_explainability DeiT, DINO, DINOv2, Swin, MaxViT), we gain insights into the roles of different components concerning particular image features.
__label__fairness Moreover, we show that by employing a novel clique-based concept balancing strategy, we can mitigate these imbalances, leading to enhanced performance on downstream tasks.
__label__reinforcement_learning Extensive experiments show that DDM consistently achieves competitive performance in challenging multi-modal visual environments.
__label__diffusion_based_models Here, we propose an efficient gradient-free decoder inversion for LDMs, which can be applied to diverse latent models.
"__label__optimization By being robust to ""stragglers"" and adaptively ignoring slow computations, Freya PAGE offers significantly improved time complexity guarantees compared to all previous methods, including Asynchronous SGD, Rennala SGD, SPIDER, and PAGE, while requiring weaker assumptions."
__label__bandits In this work, we provide improved Bayes regret bounds for hierarchical Bayesian bandit algorithms in the multi-task linear bandit and semi-bandit settings.
__label__machine_vision Additionally, we explore the local diffusion prior inherent in low-resolution diffusion models, enabling direct handling of high-resolution noisy images.
__label__reinforcement_learning Reinforcement learning from human feedback (RLHF) is a prevalent approach to align AI systems with human values by learning rewards from human preference data.
__label__bandits This problem aims to find the arm of the largest mean with a fixed confidence level when the bandit model has been sampled from the known prior.
"__label__natural_language_processing By
evaluating on a wide range of downstream tasks, including InfiniteBench, RULER, PG-19, and Needle In A Haystack, and models including LLaMA-3-1M, GLM-4-1M, Yi-200K, Phi-3-128K, and Qwen2-128K, we demonstrate that MInference effectively reduces inference latency by up to 10x for pre-filling on an A100, while maintaining accuracy."
__label__causal_inference One common assumption in this context is that the data in source and target sites (where the candidate policy is due to be deployed) come from the same distribution.
__label__learning_theory Due to the utilization of solely unlabeled samples, there exists significant uncertainty in model updates, leading CTTA to encounter severe error accumulation issues.
__label__learning_theory We study distributed goodness-of-fit testing for discrete distribution under bandwidth and differential privacy constraints.
__label__deep_learning_architectures SURMs remain competitive with baselines, often providing significant quality improvements while using a smaller parameter budget.
__label__machine_learning_for_healthcare Specifically, our method surpasses previous baselines by 5.0\%, 2.1\%, 4.6\%, and 5.8\% at a 40\% noise rate on the HAM10000, APTOS-2019, BloodMnist, and OrgancMnist datasets, respectively.
__label__optimization We derive closed-form formulas that calculate the mean response time of jobs with size predictions accounting for the prediction cost.
__label__robotics With the advancement of embodied AI, robots are increasingly capable of satisfying human demands.
__label__bandits Specifically, we prove the following regret bounds: $\Theta(\ln T)$ in the deterministic setting, $\Omega(T)$ in the stochastic setting, and $\tilde{\Theta}(T^{2/3})$ in the stochastic setting when sellers' and buyers' valuations are independent of each other.
__label__machine_vision In this paper, we propose an orthogonal solution called the Retrieval-augmented Framework for Image Restoration (ReFIR), which incorporates retrieved images as external knowledge to extend the knowledge boundary of existing LRMs in generating details faithful to the original scene.
__label__machine_vision Additionally, Normal-GS leverages optimized normals and Integrated Directional Encoding (IDE) to accurately model specular effects, enhancing both rendering quality and surface normal precision.
__label__reinforcement_learning We further establish conditions for the mixing networks under which the multi-agent IL objective function exhibits convexity within the Q function space.
__label__deep_learning_architectures Federated Learning (FL) is an evolving paradigm that enables multiple parties to collaboratively train models without sharing raw data.
__label__infrastructure To address the limited memory constraints, we introduce a novel self-ensemble and batch-agnostic early-exit strategy for TTA, which enables continuous adaptation with small batch sizes for reduced memory usage, handles distribution shifts, and improves latency efficiency.
__label__machine_vision Existing models for individuals often fall short in this task due to the inherent distribution discrepancies among entity skeletons, leading to suboptimal backbone optimization.
__label__learning_theory Motivated by prior empirical work, we initiate the study of techniques to develop efficient ERM learning algorithms for data-driven algorithm design by enumerating the pieces of the sum dual loss functions for a collection of problem instances.
__label__machine_vision To address this, we introduce MoE Jetpack, a framework designed to fine-tune the abundant and easily accessible dense checkpoints into MoE models.
__label__reinforcement_learning Furthermore, we remark that $R^3M$ is versatile and can be extended to various preference optimization methods, including direct preference optimization (DPO).
__label__generative_models }, image), resulting in suboptimal performance when the codebook is applied to multi-modal downstream tasks (\emph{e.g.
__label__other Existing approaches often sacrifice one for the other.
__label__neuroscience_and_cognitive_science We demonstrate how plasticity acting on both excitatory and inhibitory synapses can better shape excitatory cell dynamics to scaffold timing representations.
__label__probabilistic_methods Moreover, we prove the non-asymptotic validity of the confidence intervals for parameter estimation with LSA based on multiplier bootstrap.
__label__diffusion_based_models By deactivating these memorization neurons, we can avoid the replication of training data at inference time, increase the diversity in the generated outputs, and mitigate the leakage of private and copyrighted data.
__label__interpretability_and_explainability Reinforcement learning from human feedback (RLHF) is widely used to train large language models (LLMs).
__label__machine_learning_for_social_sciences However, one fundamental question remains: can LLM agents really simulate human behavior?
__label__graph_neural_networks rule-based modal logic.
__label__deep_learning_architectures Specifically, we showcase our framework on the Symmetric Positive Definite (SPD) manifold and special orthogonal group, i.e., the set of rotation matrices.
__label__diffusion_based_models This schedule reduces noise prediction errors, enabling more faithful editing that preserves the original content of the source image.
__label__bandits Causal knowledge about the relationships among decision variables and a reward variable in a bandit setting can accelerate the learning of an optimal decision.
__label__reinforcement_learning Intelligent agents must be generalists, capable of quickly adapting to various tasks.
__label__optimization Lastly, we numerically demonstrate on common machine learning datasets that our bounds are indeed much tighter, thus offering a bridge between theory and practice.
__label__reinforcement_learning This paper studies first-order policy optimization for robust average cost Markov decision processes (MDPs).
__label__machine_learning_for_physical_sciences However, current flow-based models are limited to a single initial population and a set of predefined conditions which describe different dynamics.
__label__natural_language_processing We propose a learning framework, PRELUDE that infers a description of the user's latent preference based on historic edit data and using it to define a prompt policy that drives future response generation.
__label__reinforcement_learning Extensive policy evaluation, selection, and learning experiments highlight the versatility and favorable performance of LS.
__label__other We present an optimal method for encoding cluster assignments of arbitrary data sets.
__label__other Subsequently, DeiSAM segments objects by matching them to the logically inferred image regions.
__label__neuroscience_and_cognitive_science Our findings indicate that goal-driven RNNs employ chronological memory subspaces to track information over short time spans, enabling testable predictions with neural data.
__label__other The results further demonstrate the great potential of our ZOPP for real-world scenarios.
__label__generative_models Most recent approaches are motivated heuristically and lack a unifying framework, obscuring connections between them.
__label__safety_in_machine_learning And by integrating with existing methods, the classification performance can be significantly improved on noisy datasets, typically by 22.8% on 80% symmetric CIFAR-10 with M-correction.
__label__learning_theory While setting the threshold is non-trivial and is usually done with validation data, this simple technique has proved remarkably effective in terms of accuracy.
__label__optimization So far, the focus has mostly been on online algorithms, where information-theoretic barriers are overcome using predictions about the unknown future.
__label__machine_learning_for_other_sciences_and_fields When data gathered from the real world is polluted, the absence of global information will damage the robust prediction capability of these algorithms.
__label__natural_language_processing Finally, we empirically show that ALT on FLD$^{\times2}$ substantially enhances the reasoning capabilities of state-of-the-art LLMs, including LLaMA-3.1-70B.
__label__natural_language_processing We introduce Language Feedback Models (LFMs) that identify desirable behaviour --- actions that help achieve tasks specified in the instruction - for imitation learning in instruction following.
__label__machine_vision The proposed process is guided by texture clues for effective appearance editing, and regularized by scene depth for preserving original geometric structure.
__label__learning_theory This paper studies the problem of \emph{entropy identity testing}: given sample access to a distribution $p$ and a fully described distribution $q$ (both are discrete distributions over the support of size $k$), and the promise that either $p = q$ or $ | H(p) - H(q) | \geqslant \varepsilon$, where $H(\cdot)$ denotes the Shannon entropy, a tester needs to distinguish between the two cases with high probability.
__label__machine_vision Our objective in this paper is to probe large vision models to determine to what extent they ‘understand’ different physical properties of the 3D scene depicted in an image.
__label__safety_in_machine_learning High-quality public datasets significantly prompt the prosperity of deep neural networks (DNNs).
__label__diffusion_based_models We present an novel framework for efficiently and effectively extending the powerful continuous diffusion processes to discrete modeling.
__label__reinforcement_learning We address this challenge by introducing a novel approach based on learning a world model of the environment using conditional mean embeddings.
__label__machine_vision Extensive experiments demonstrate that our proposed UPS achieves state-of-the-art performance relative to leading lightweight SISR methods, as verified by various popular benchmarks.
__label__diffusion_based_models We demonstrate that these limitations can be sidestepped by reframing the generative process as a discrete optimal control episode.
__label__deep_learning_architectures (2023a,b) developed a systematic method to show a constructive approximation theorem from *scalar-valued joint-group-invariant* feature maps, covering a formal deep network.
__label__optimization_for_deep_networks However, the training of a sparse DNN encounters great challenges in achieving optimal generalization ability despite the efforts from the state-of-the-art sparse training methodologies.
__label__optimization Byzantine fault tolerance mechanisms have been proposed to address these issues, but they often assume full participation from all clients, which is not always practical due to the unavailability of some clients or communication constraints.
__label__graph_neural_networks To efficiently reduce the search space of potential molecules, we further introduce a Molecule Extraction Policy Network for molecule extraction.
__label__natural_language_processing We demonstrate the theoretical advantages of Cal-DPO over existing approaches.
__label__deep_learning_architectures ICL in Multi-Headed Attention (MHA) with absolute positional embedding has been the focus of more study than other sequence model varieties.
__label__reinforcement_learning In this paper, we propose a new algorithm that substantially enhances behavior-regularization based on conservative policy iteration.
__label__machine_vision We have conducted extensive experiments on various DAOD benchmarks and approaches, and the experimental results show that the proposed DAS correlates well with the performance of DAOD models and can be used as an effective tool for model selection after training.
__label__generative_models However, accurate recognition relies on massive inertial signal samples, which are hard to collect for the Chinese context due to the vast number of characters.
__label__safety_in_machine_learning We exhibit applications of our methodology on a collection of supervised learning datasets for (shape-constrained) properties such as monotonicity and concavity.
__label__diffusion_based_models With an isotropic architecture that chains a series of transformer blocks, DiTs demonstrate competitive performance and good scalability; but meanwhile, the abandonment of U-Net by DiTs and their following improvements is worth rethinking.
"__label__learning_theory It offers streamlined learning via a single unconditional training phase, allowing 
efficient inference without the need for retraining even when conditioning changes."
__label__generative_models However, they often fail to produce realistic geometric details, resulting in overly smooth surfaces or geometric details inaccurately baked in albedo maps.
__label__reinforcement_learning To tackle these challenges, we propose a novel approach based on in-context learning and decision-time search named Opponent Modeling with In-context Search (OMIS).
__label__machine_vision Project page: \url{https://a-sa-m.github.io/}.
__label__safety_in_machine_learning Data-poisoning backdoor attacks are serious security threats to machine learning models, where an adversary can manipulate the training dataset to inject backdoors into models.
__label__machine_vision Specifically, we divide the point cloud into different patches and use a lightweight yet effective Inner Mamba to capture local geometric information.
__label__causal_inference In this paper, we expand upon this framework by integrating hierarchical and density-based clustering algorithms.
__label__graph_neural_networks This paper presents a Hierarchical Distance Structural Encoding (HDSE) method to model node distances in a graph, focusing on its multi-level, hierarchical nature.
__label__machine_learning_for_physical_sciences When solving partial differential equations (PDEs), classical numerical methods often require fine mesh grids and small time stepping to meet stability, consistency, and convergence conditions, leading to high computational cost.
__label__active_learning Rigorously establishing the safety of black-box machine learning models with respect to critical risk measures is important for providing guarantees about the behavior of the model.
__label__deep_learning_architectures We also show that MatrixNet respects group relations allowing generalization to group elements of greater word length than in the training set.
__label__machine_vision Firstly, most existing methods only adopt a single latent space to align visual and linguistic features, which has a limited ability to represent complex visual-linguistic patterns, especially for fine-grained tasks.
__label__deep_learning_architectures Extensive experiments on real-world datasets demonstrate that CCM can (1) boost the performance of CI and CD models by an average margin of 2.4% and 7.2% on long-term and short-term forecasting, respectively; (2) enable zero-shot forecasting with mainstream time series forecasting models; (3) uncover intrinsic time series patterns among channels and improve interpretability of complex time series models.
__label__optimization Given the high economic and environmental costs of using large vision or language models, analog in-memory accelerators present a promising solution for energy-efficient AI.
__label__safety_in_machine_learning Thus, we aim to raise awareness among practitioners about this inherent tradeoff, empowering them to make informed decisions and potentially prioritize safety over perceptual performance.
__label__machine_vision Specifically, our model is based on the pose-normalized query/patch pairs and enhanced by the proposed intrinsic patch geometry representation, modeling the intrinsic 3D patch geometry feature by learnable multi-head memory banks.
__label__bandits In this work we enforce fairness by rewarding the platform with the _fair gain from trade_, defined as the minimum between sellers' and buyers' utilities.
__label__privacy However, many datasets possess an inherent low-dimensional structure.
__label__learning_theory Furthermore, we complement our algorithmic results with Statistical Query lower bounds, providing evidence that our estimators are likely to have nearly optimal sample complexity.
__label__interpretability_and_explainability (1) Optimal Test-Case Selection: At any time in the testing process, our approach evaluates the policy in the states that are most critical for safety.
__label__natural_language_processing Further, we show that decoder-only Transformer language models can lose sensitivity to specific tokens in the input, which relates to the well-known phenomenon of over-squashing in graph neural networks.
__label__safety_in_machine_learning However, during the development and deployment of FedGL models, they are susceptible to illegal copying and model theft.
__label__safety_in_machine_learning In this paper, we revisit the generation process and identify a universal principle: Deepfake images inherently contain information from both source and target identities, while genuine faces maintain a consistent identity.
__label__interpretability_and_explainability Lossless compression of large-scale scientific floating-point data is critical yet challenging due to the presence of high-order information and noise that arises from model truncation and discretization errors.
__label__fairness We demonstrate that existing methods optimizing for equal and proportional representation metrics may fail to promote MPR.
__label__privacy We compare DPConvCNP with a DP Gaussian Process (GP) baseline with carefully tuned hyperparameters.
__label__neuroscience_and_cognitive_science Stochastic optimal control theory offers a mathematical framework to explain these processes at the algorithmic level through optimality principles.
__label__natural_language_processing This paper outlines key characteristics of agents' behavior in the Bayesian-IL framework, including predictions that are supported by experimental verification with various LLMs.
__label__deep_learning_architectures As machine learning models in critical fields increasingly grapple with multimodal data, they face the dual challenges of handling a wide array of modalities, often incomplete due to missing elements, and the temporal irregularity and sparsity of collected samples.
__label__safety_in_machine_learning Code is available at https://github.com/mala-lab/AdaptOD.
__label__online_learning We present the first algorithm for online weighted paging that does not know page weights in advance, but rather learns from weight samples.
__label__reinforcement_learning Driven by the discoveries above, we propose a simple and effective method, called Policy Path Trimming and Boosting (PPTB), as a general plug-in improvement to DRL algorithms.
__label__evaluation While recent advancements in large multimodal models (LMMs) have significantly improved their abilities in image quality assessment (IQA) relying on absolute quality rating, how to transfer reliable relative quality comparison outputs to continuous perceptual quality scores remains largely unexplored.
__label__machine_vision We further present a video prediction pipeline empowered by motion graph, exhibiting substantial performance improvements and cost reductions.
__label__generative_models FlowLLM first fine-tunes an LLM to learn an effective base distribution of meta-stable crystals in a text representation.
__label__natural_language_processing To address this limitation, we propose calibrated direct preference optimization (Cal-DPO), a simple yet effective algorithm.
__label__reinforcement_learning We propose a new algorithm for model-based distributional reinforcement learning (RL), and prove that it is minimax-optimal for approximating return distributions in the generative model regime (up to logarithmic factors), the first result of this kind for any distributional RL algorithm.
__label__probabilistic_methods Bayesian methodologies for handling count-valued time series have gained prominence due to their ability to infer interpretable latent structures and to estimate uncertainties, and thus are especially suitable for dealing with noisy and incomplete count data.
__label__safety_in_machine_learning To address this issue, we introduce JAMBench, a harmful behavior benchmark designed to trigger and evaluate moderation guardrails.
__label__safety_in_machine_learning While the vanilla few-shot jailbreaking may be inefficient, we propose improved techniques such as injecting special system tokens like [/INST] and employing demo-level random search from a collected demo pool.
__label__graph_neural_networks Recognizing this, we explore another GNN structure called the second-order folklore GNN (2-FGNN) that overcomes this limitation, and the aforementioned universal approximation theorem can be extended to the entire MILP space using 2-FGNN, regardless of the MP-tractability.
__label__neuroscience_and_cognitive_science In neuroscience, recurrent neural networks (RNNs) are modeled as continuous-time dynamical systems to more accurately reflect the dynamics inherent in biological circuits.
__label__fairness We extend the theory of proportionally fair clustering to non-centroid clustering by considering a variety of cost functions, both metric and non-metric, for a data point to be placed in a cluster with other data points.
__label__deep_learning_architectures Transformers, renowned for their self-attention mechanism, have achieved state-of-the-art performance across various tasks in natural language processing, computer vision, time-series modeling, etc.
__label__graph_neural_networks Link-MoE utilizes various GNNs as experts and  strategically selects the appropriate expert for each node pair based on various types of pairwise information.
__label__interpretability_and_explainability But how can we evaluate this hypothesis?
__label__safety_in_machine_learning We find that embedding space attacks circumvent model alignments and trigger harmful behaviors more efficiently than discrete attacks or model fine-tuning.
__label__other However, tackling time-series analysis tasks usually involves designing and training a separate model from scratch leveraging training data and domain expertise specific to the task.
__label__learning_theory Our analysis technique involves the development of novel properties on the attention gradient and further in-depth analysis of how these properties contribute to the convergence of the training process.
__label__machine_vision Most existing ZSTAD methods utilize a foreground-based approach, limiting the integration of text and visual features due to their reliance on pre-extracted proposals.
__label__safety_in_machine_learning All the components of WildJailbreak contribute to achieving balanced safety behaviors of models
__label__probabilistic_methods This work addresses these issues by introducing a variational spatio-temporal state-space GP that handles linear and non-linear physical constraints while achieving efficient linear-in-time computation costs.
__label__neuroscience_and_cognitive_science We validate the method on multi-session datasets containing widefield calcium imaging recordings across the dorsal cortex.
__label__graph_neural_networks For instance, in graph machine learning, training Graph Neural Networks (GNNs) on coarsened graphs leads to drastic savings in time and memory.
__label__machine_vision Here, we propose a simpler network architecture based on Deep Sets.
__label__probabilistic_methods It solves distribution-valued supervised learning, where the output values of the training dataset are probability distributions.
__label__probabilistic_methods Remarkably, both algorithms work down to the information theoretic threshold.
__label__learning_theory * sampling, shuffling, and Markovian sampling, affect the convergence speed of UD-SGD by considering the impact of agent dynamics on the limiting covariance matrix as described in the Central Limit Theorem (CLT).
__label__natural_language_processing To answer this, we conduct a thorough empirical study on prompt optimization and draw two major insights.
__label__probabilistic_methods Current uncertainty quantification is memory and compute expensive, which hinders practical uptake.
__label__diffusion_based_models Firstly, the FLC module combines the low-level texture feature from the VAE encoder with the high-level semantic feature from the image encoder, addressing the issue of missing detail information due to the absence of a dedicated person image feature extractor.
__label__optimization Experiments for two image interpolation applications verify the restoration performance, parameter efficiency and robustness to covariate shift of our graph-based unrolled networks compared to conventional transformers.
__label__optimization To handle the interdependency between the neural networks, we adopt sequential and parallelized versions of coordinate descent for training.
__label__algorithmic_game_theory In this paper, we consider the core learning problem in stochastic cooperative games, where the reward distribution is unknown.
__label__natural_language_processing Finally, we show that our method can tackle new types of scenarios that are out of reach of existing methods.
__label__optimization To reduce the computational complexity, we apply quasi-Newton approximations and show they converge comparably, use less memory, and are faster, compared to full-Newton.
__label__machine_vision In light of the interleaved embedding space, we introduce FIND-Bench, which introduces new training and evaluation annotations to the COCO dataset for interleaved segmentation and retrieval.
__label__privacy This occurs for some parameters that could realistically be chosen for DP-SGD.
__label__machine_learning_for_other_sciences_and_fields Specifically, we introduce the Residual Cycle Forecasting (RCF) technique, which utilizes learnable recurrent cycles to model the inherent periodic patterns within sequences, and then performs predictions on the residual components of the modeled cycles.
__label__evaluation Standardized benchmarks drive progress in machine learning.
__label__optimization_for_deep_networks ESPACE also reduces GEMM execution time and prefill inference latency on existing hardware.
__label__natural_language_processing The result is a quantized model where all matrix multiplications are performed in 4 bits, without any channels identified for retention in higher precision.
__label__generative_models Our empirical results indicate the method's scalability and interpretability applied to large-scale generative models.
__label__machine_vision The underlying synchronization problem seeks to recover camera poses (locations and orientations up to a global transformation) from the block trifocal tensor.
__label__machine_vision Our method, PDF-Embedding, transforms the replication level of each image-replica pair into a probability density function (PDF) as the supervision signal.
__label__optimization_for_deep_networks The code for this paper is available at \url{https://github.com/hsijiaxidian/AGN}.
__label__machine_vision Current approaches for open-vocabulary scene graph generation (OVSGG) use vision-language models such as CLIP and follow a standard zero-shot pipeline – computing similarity between the query image and the text embeddings for each category (i.e., text classifiers).
__label__machine_vision Designing computationally efficient network architectures remains an ongoing necessity in computer vision.
__label__probabilistic_methods Diffusion models have emerged as effective distribution estimators in vision, language, and reinforcement learning, but their use as priors in downstream tasks poses an intractable posterior inference problem.
__label__natural_language_processing For this target, we introduce a 1-bit model compressing framework named OneBit, including a novel 1-bit parameter representation method to better quantize LLMs as well as an effective parameter initialization method based on matrix decomposition to improve the convergence speed of the quantization framework.
__label__diffusion_based_models Through the carefully designed usage guidance, we effectively enhance the text representation capability of the LLM for prompt encoding and eliminate its inherent positional bias.
__label__machine_vision However, the optimal scene encoding strategies for various scenarios remain unclear, particularly compared to their image-based counterparts.
__label__safety_in_machine_learning However, the safety of cross-modality scenarios, which are more commonly encountered in practical applications involving images captured by infrared cameras, has not received adequate attention.
__label__diffusion_based_models Extensive qualitative and quantitative evaluations demonstrate that $\textit{Bifröst}$ significantly outperforms existing methods, providing a robust solution for generating realistically composited images in scenarios demanding intricate spatial understanding.
"__label__learning_theory To this end, a learner in this model can guide the learning via ""validity queries"", which allow it to ascertain the validity of individual examples."
__label__machine_learning_for_social_sciences (2) The fused representation is then used to construct Matryoshka representations with multi-dimensional and multi-granular embedded representations learned by the global homogeneous model header and the local heterogeneous model header.
__label__machine_vision The key innovation of DITS is to leverage the inherent proximity of text and video embeddings, defining a truncated diffusion flow from the fixed text embedding to the video embedding, enhancing controllability compared to adopting the isotropic Gaussian.
__label__natural_language_processing Despite significant gains of up to 5% in mathematical evaluation when scaling up reward models, we surprisingly observe marginal improvements in other categories.
__label__diffusion_based_models Domain knowledge is particularly important in real-world scenarios, where invalid generated graphs hinder deployment in practical applications.
__label__optimization This rediscovery yields a specific step size schedule we call Random Function Descent (RFD).
__label__machine_vision This approach forms an effective backbone for self-supervised representation learning, and we demonstrate that a simple off-the-shelf equivariant network operating in the pre-trained latent space can achieve results on par with meticulously-engineered, handcrafted networks designed to handle complex, nonlinear symmetries.
__label__diffusion_based_models This paper introduces the Transfer Guided Diffusion Process (TGDP), a novel approach distinct from conventional finetuning and regularization methods.
__label__other Moreover, we derive a novel closed-form formula for effective resistance, and describe its relation to diffusion distances.
__label__diffusion_based_models A computational bottleneck, built into the neural architecture, encourages the denoising network to partition an input into regions, denoise them in parallel, and combine the results.
__label__natural_language_processing The results show that our proposed method leads to high-quality and diverse outputs across multiple language pairs (English$\leftrightarrow$\{German, Russian\}) with two strong decoder-only LLMs (Alma-7b, Tower-7b).
__label__neuroscience_and_cognitive_science For the vision task, the proposed method converts Swin Transformer into an SNN without post-training or conversion-aware training, achieving state-of-the-art SNN accuracy on ImageNet dataset, i.e., 80.0\% with 28.7M parameters.
__label__probabilistic_methods This map is both invertible and neighborhood-preserving, with controllable numerical error, with the result that uncertainties in the data are correctly propagated to the latent space.
__label__graph_neural_networks However, real-world graphs typically exhibit geometrically heterogeneous characteristics, rendering the confinement to a single geometric paradigm insufficient for capturing their intricate structural complexities.
__label__generative_models Generating videos with realistic and physically plausible motion is one of the main recent challenges in computer vision.
__label__natural_language_processing We argue that the key to accomplishing this task lies in distinguishing writing styles of different authors, rather than simply classifying the text into human-written or AI-generated text.
__label__other First, we give a simple and efficient way to construct a navigable graph with average degree $O(\sqrt{n \log n })$ for any set of $n$ points, in any dimension, for any distance function.
__label__neuroscience_and_cognitive_science Inspired by these biological mechanisms, we propose a novel neuromorphic SSL framework that integrates spike-based neural encoding and computation.
__label__learning_theory Training machine learning and statistical models often involves optimizing a data-driven risk criterion.
__label__online_learning L-ARC updates a threshold function within a reproducing kernel Hilbert space (RKHS), with the kernel determining the level of localization of the statistical risk guarantee.
__label__robotics Majority of prior arts adhere to an open-loop philosophy and lack real-time feedback, leading to error accumulation and undesirable robustness.
__label__probabilistic_methods The first algorithm employs global information to simultaneously match all the graphs, whereas the second algorithm first partially matches the graphs pairwise and then combines the partial matchings by transitivity.
__label__graph_neural_networks Here, we introduce Quantized Graph Convolution Networks (QGCNs), the first framework for GNNs that formally and directly extends CNNs to graphs.
__label__diffusion_based_models In this paper, we point out that suboptimal noise-data mapping leads to slow training of diffusion models.
__label__reinforcement_learning Each GVF computes the expected return for a given policy, based on a unique reward.
__label__machine_vision Estimating the homography between two images is crucial for mid- or high-level vision tasks, such as image stitching and fusion.
__label__deep_learning_architectures We extend work done by Garg et al.
__label__reinforcement_learning However, existing techniques for world modelling do not guarantee that the effect of actions are represented in such systematic ways.
__label__natural_language_processing Intrigued by the claims of emergent planning capabilities in large language models (LLMs), works have been proposed to investigate the planning effectiveness of LLMs, without considering any utilization of off-the-shelf planning techniques in LLMs.
__label__machine_vision This novel loss function enables improvements in memory efficiency by three orders of magnitude, mitigating a key bottleneck of previous methods and allowing much larger models to be trained with the same computational resources.
__label__optimization Moreover, to facilitate rapid evaluation of NMS methods for researchers, we introduce NMS-Bench, the first benchmark designed to comprehensively assess various NMS methods.
__label__human-AI_interaction Compared to classical machine learning (ML) models, generative models offer a new usage paradigm where (i) a single model can be used for many different tasks out-of-the-box; (ii) users interact with this model over a series of natural language prompts; and (iii) the model is ideally evaluated on binary user satisfaction with respect to model outputs.
__label__natural_language_processing To address these restrictions, we propose Text2NKG, a novel fine-grained n-ary relation extraction framework for n-ary relational knowledge graph construction.
__label__other FRLC handles multiple OT objectives (Wasserstein, Gromov-Wasserstein, Fused Gromov-Wasserstein), and marginal constraints (balanced, unbalanced, and semi-relaxed) with linear space complexity.
__label__machine_learning_for_other_sciences_and_fields *f*-RAG is based on a pre-trained molecular generative model that proposes additional fragments from input fragments to complete and generate a new molecule.
__label__privacy We start with  new near-optimal bounds for the classic mean estimation problem but with sparse data, improving upon existing algorithms particularly for the high-dimensional regime.
__label__machine_learning_for_social_sciences However, acquiring high-quality *human-annotated* samples can be highly challenging and even infeasible in social domains.
__label__machine_vision For SR tasks emphasizing texture details, we propose an image-guided domain adaptation method.
__label__machine_vision With several carefully designed modules, KptLLM adeptly handles various modality inputs, facilitating the interpretation of both semantic contents and keypoint locations.
__label__machine_learning_for_other_sciences_and_fields However, they encounter limitations in effectively capturing long-range interactions in large molecular systems.
__label__safety_in_machine_learning Based on the study, we propose SampDetox, which strategically combines lightweight and high noise.
__label__robotics Furthermore, the instructions and programs generated by Self-Instruct may be subtly inconsistent --- such as the program missing a step implied by the instruction.
__label__diffusion_based_models We present empirical evidence supporting the effectiveness of our method.
__label__learning_theory A central technical challenge arises from the dependency of each outer-level update on the concurrent stage of inner optimization in bilevel programming.
__label__other Our study demonstrates the effectiveness of LP-FT for fine-tuning language models.
__label__privacy For example, the déjà vu method shows that for certain representation learning models and training images, it is sometimes possible to correctly predict the foreground label given only the representation of he background – better than through dataset-level correlations.
__label__machine_vision In this paper, we propose a novel unsupervised model selection approach for domain adaptive object detection, which is able to select almost the optimal model for the target domain without using any target labels.
__label__bandits Experimental results showcase the importance of using our Lipschitz reward estimator and the overall effectiveness of our algorithms.
__label__machine_vision In contrast, 3D Gaussian splatting (3DGS) uses an explicit and discrete representation, hence the reconstructed surface is built by the huge number of Gaussian primitives, which leads to excessive memory consumption and rough surface details in sparse Gaussian areas.
__label__other We also notice the superiority of our approach in open-world generalization, with an average Accuracy improvement of 10.59\% (absolute) across all test sets.
__label__reinforcement_learning In this work, we build on these insights to provide a lightweight but effective approach for training RNNs in online RL.
__label__deep_learning_architectures Despite these advantages, several applications pose new challenges to PEFT beyond mere parameter efficiency.
"__label__neuroscience_and_cognitive_science Consequently, this approach mitigates the issue of poor feature quality typically 
  extracted from low SNR signals."
__label__learning_theory We further demonstrate the approximation capability of PQCs via numerical experiments.
__label__reinforcement_learning While originally developed for continuous control problems, Proximal Policy Optimization (PPO) has emerged as the work-horse of a variety of reinforcement learning (RL) applications, including the fine-tuning of generative models.
__label__reinforcement_learning In this paper, we propose a novel Structural Information principles-based Effective Exploration framework, namely SI2E.
__label__generative_models In this game, an attacker and a defender communicate around a target word only visible to the attacker.
__label__evaluation However, evaluating these models poses a significant challenge.
__label__safety_in_machine_learning Conformal prediction (CP) is an emerging uncertainty quantification framework that allows us to construct a prediction set to cover the true label with a pre-specified marginal or conditional probability.
__label__reinforcement_learning Computationally, our proposed loss function is strictly convex and univariate with respect to each scaling parameter, enabling its efficient optimization through a simple second-order algorithm.
__label__reinforcement_learning Additionally, our method is easy to implement and requires no prior knowledge of the coverage ratio (or even an upper bound on it), which altogether make it the strongest known algorithm for this setting to date.
__label__machine_vision We propose a novel unsupervised method to learn pose and part-segmentation of articulated objects with rigid parts.
__label__neuroscience_and_cognitive_science Our visual experience in daily life are dominated by dynamic change.
__label__optimization Our code is available at https://github.com/mazumder-lab/ALPS.
__label__probabilistic_methods Algorithms that utilize bandit feedback to optimize top-k recommendations are vital for online marketplaces, search engines, and content platforms.
__label__privacy Many high-stakes applications require machine learning models that protect user privacy and provide well-calibrated, accurate predictions.
__label__other Data markets can increase the supply of data, particularly in data-scarce domains such as healthcare, by incentivizing potential data providers to join the market.
__label__reinforcement_learning However, existing UED approaches focus primarily on the random generation of environments for open-ended agent training.
__label__learning_theory In particular we allow the predictor  to learn as it receives larger parts of the input, with the ultimate goal of designing online learning algorithms specifically tailored for the algorithmic task at hand.
__label__diffusion_based_models On top of this, we present effective feature selection solutions for several popular diffusion models.
__label__machine_vision Continual learning (CL) with Vision-Language Models (VLMs) has overcome the constraints of traditional CL, which only focuses on previously encountered classes.
__label__causal_inference Although a sound and complete graphical identification criterion, known as the adjustment criterion (Shpitser, 2010), exists for static contexts, sequential contexts present challenges.
__label__graph_neural_networks The key idea behind SNAPS is that nodes with high feature similarity or direct connections tend to have the same label.
__label__machine_learning_for_other_sciences_and_fields Specifically, we begin by transforming time series into the modality of text tokens.
__label__learning_theory To complement this upper bound on $\epsilon$, we show that for each $d\in\mathbb{N}$ and each viable $k\in[2^d]$, a construction of a $(k,\epsilon)$-secluded (unit cube) partition with $\epsilon\geq\frac{\log_4(k)}{d}\cdot\frac{1}{8\log_4(d+1)}$.
__label__evaluation Therefore, evaluation methods that can adapt and generate evaluation data with controlled complexity are urgently needed.
__label__reinforcement_learning In this paper, we develop a bilevel optimization framework for meta-RL (BO-MRL) to learn the meta-prior for task-specific policy adaptation, which implements multiple-step policy optimization on one-time data collection.
__label__learning_theory Our analysis encompasses scenarios where the source and the target domain are related via i) a bounded density ratio, and ii) a measure-preserving transformation.
__label__optimization_for_deep_networks We propose a simple modification of SAM, termed SAMPa, which allows us to fully parallelize the two gradient computations.
__label__fairness Then, we theoretically propose the  optimal egalitarian fairness bounds that an FL coalition can achieve while maintaining core stability under various types of altruistic behaviors.
__label__learning_theory Recent work of Foster et al.
__label__diffusion_based_models MGAPO presents significant challenges.
__label__machine_learning_for_healthcare Moreover, our method standardizes the value distribution (mean and variance) of each gene to have standard distributions regardless of the gene.
__label__machine_vision Finally, experimental results on three widely used datasets validate that the new PsHD outperforms state-of-the-art with 3.9% improvements on average, and also achieves 1.5%  improvements while reducing 60% memory buffer size, highlighting the potential of utilizing unlabeled data in SSCL.
__label__neuroscience_and_cognitive_science Through guiding the diffusion model to activate individual latent factors, we verify that the neural dynamics of latent factors in the disentangled neural subspace provide interpretable quantifications of the behaviors of interest.
__label__natural_language_processing We iteratively optimize the policy based on the feedback from a base EA model.
__label__reinforcement_learning By exposing the underlying structure of a reward function, they enable the decomposition of an RL task, leading to impressive gains in sample efficiency.
__label__optimization Unfortunately, existing efforts to expedite conformalization often carry strong assumptions and are developed specifically for certain models, or they only offer approximate solution sets.
__label__learning_theory We show that for any $(k,\epsilon)$-secluded partition where each member has at most unit measure, it must be that $k \geq(1+2\varepsilon)^d$, and consequently, for the interesting regime $k\in[2^d]$ it must be that $\epsilon\leq\frac{\log_4(k)}{d}$.
__label__interpretability_and_explainability Machine learning (ML) algorithms can often differ in performance across domains.
"__label__diffusion_based_models First of all, we show that this diffusion model achieves an improved memory efficiency and data efficiency
while still maintaining the  capability to generate entire images via positional encoding."
__label__other We also give results for the more general scenario when $t$ negatives are allowed.
__label__reinforcement_learning Heuristics encode prior human knowledge about how a task should be done, providing valuable hints for RL algorithms.
__label__optimization_for_deep_networks In this paper, we propose a new adaptive gradient method named ADOPT, which achieves the optimal convergence rate of $\mathcal{O} ( 1 / \sqrt{T} )$ with any choice of $\beta_2$ without depending on the bounded noise assumption.
__label__machine_learning_for_other_sciences_and_fields Current approaches to pocket generation often suffer from time-intensive physical computations or template-based methods, as well as compromised generation quality due to the overlooking of domain knowledge.
__label__optimization_for_deep_networks To address these issues, we propose SalientLoRA, which adaptively optimizes intrinsic ranks of LoRA via salience measurement.
__label__learning_theory In a recent and somewhat surprising work, Burns et al.
__label__learning_theory Finally, we adapt our proof techniques to obtain a new ``neighborhood'' variant of the cubical KKM lemma (or cubical Sperner's lemma): For any coloring of $[0,1]^d$ in which no color is used on opposing faces, it holds for each $\epsilon\in(0,\frac12]$ that there is a point where the open $\epsilon$-radius $\ell_\infty$-ball intersects at least $(1+\frac23\epsilon)^d$ colors.
__label__learning_theory In particular, we find that these exact rate curves (varying along $\gamma$) exhibit the periodic plateau behavior and the polynomial approximation barrier.
__label__natural_language_processing Our code is open-sourced at https://github.com/cxcscmu/MATES.
__label__optimization In various settings, the polynomials depend on external parameters that may be random.
__label__natural_language_processing We fine-tune the LLM using labeled data of actions and the PPO algorithm.
__label__machine_vision Additionally, a gradient modulation is employed to adopt appropriate clues for capturing interaction regions across various egocentric scenarios.
__label__machine_vision Although learnable approaches like Q-Former and Perceiver Resampler have been developed to reduce the vision token burden, they overlook the context causally modeled by LLMs (i.e., key-value cache), potentially leading to missed visual cues when addressing user queries.
__label__diffusion_based_models Scene initialization refers to generating the initial layout for the traffic in a scene, and scene rollout refers to closed-loop simulation for the behaviors of the agents.
__label__algorithmic_game_theory Comparison data elicited from people are fundamental to many machine learning tasks, including reinforcement learning from human feedback for large language models and estimating ranking models.
__label__machine_vision We observe that human daily actions are confronted with temporal mismatch across different datasets, as they are usually partial observations of their complete action sequences.
__label__online_learning M${}^{\natural}$-concave functions, a.k.a.
__label__generative_models We have released all models and codes to promote the exploration of AR/VAR models for visual generation and unified learning.
__label__reinforcement_learning This improves the state-of-the-art sample complexity in general parameterized CMDPs by a factor of $\mathcal{O}((1-\gamma)^{-1}\epsilon^{-2})$ and achieves the theoretical lower bound in $\epsilon^{-1}$.
__label__machine_learning_for_physical_sciences However, traditional numerical methods, such as finite element and finite difference methods, often face challenges when dealing with nonlinear solvers, particularly in the presence of multiple solutions.
__label__safety_in_machine_learning Further, to obtain a more reliable set of predicted OOD samples on long-tailed ID data, a novel $\textit{dual-normalized energy loss}$ is introduced in AdaptOD, which leverages class- and sample-wise normalized energy to enforce a more balanced prediction energy on imbalanced ID samples.
__label__causal_inference Standard imitation methods do not generally apply when the learner and the expert's sensory capabilities mismatch and demonstrations are contaminated with unobserved confounding bias.
__label__online_learning While such algorithms enjoy low theoretical regret, in real-world deployment they can be sensitive to individual outliers that cause the algorithm to over-correct.
__label__machine_vision Accordingly, we introduce Coordinate Field, which is a composition of coordinate frames of all local shapes.
__label__privacy Unlike previous simulation-based local differential privacy mechanisms, PPR exactly preserves the joint distribution of the data and the output of the original local randomizer.
__label__causal_inference Relaxing the causal sufficiency and parametric assumptions and leveraging recent advancements in causal discovery and confounding analysis with non-i.i.d.
__label__deep_learning_architectures Sparse Mixture of Experts (SMoE) has become the key to unlocking unparalleled scalability in deep learning.
__label__machine_learning_for_other_sciences_and_fields Offline black-box optimization aims to maximize a black-box function using an offline dataset of designs and their measured properties.
__label__machine_learning_for_other_sciences_and_fields Our code and pre-trained models are publicly available.
__label__generative_models In our paper, we present the HairFast model, which uniquely solves these problems and achieves high resolution, near real-time performance, and superior reconstruction compared to optimization problem-based methods.
__label__bandits We demonstrate the sample efficiency improvements afforded by AdaptiveSoftmax on real and synthetic data to corroborate our theoretical results.
__label__neuroscience_and_cognitive_science While significant strides have been made in understanding learning in artificial neural networks, applying this knowledge to biological networks remains challenging.
__label__learning_theory Yet, under the strongly-aligned regime, KM suffers the saturation effect, while TKM can be continuously improved as the alignment becomes stronger.
__label__algorithmic_game_theory (NeurIPS’23).
__label__generative_models Empirically, we demonstrate that our method is competitive on several challenging conditional generation tasks, including an infinite-dimensional inverse problem.
__label__deep_learning_architectures We introduce UniTS, a unified multi-task time series model that utilizes task tokenization to integrate predictive and generative tasks into a single framework.
__label__machine_learning_for_healthcare Effective extraction of features for the disease areas is crucial for disease classification on radiographic images.
__label__optimization This paper presents a comprehensive analysis on Adafactor in a non-convex smooth setting, demonstrating its convergence to find a stationary point at a rate of $\tilde{\mathcal{O}}(1/\sqrt{T})$.
__label__machine_vision However, the neural priors from 2D pretrained models are often noisy and blurry, which struggle to precisely guide the learning of radiance fields.
__label__bandits We introduce a computationally efficient method based on the EZ-diffusion model, combining choices and response times to estimate the underlying human utility function.
__label__evaluation Moreover, we show how PromptEval can be useful in LLM-as-a-judge and best prompt identification applications.
__label__diffusion_based_models Applying these methods sequentially for multi-attribute edits increases computational demands and efficiency losses.
__label__optimization_for_deep_networks Recurrent neural networks (RNNs) notoriously struggle to learn long-term memories, primarily due to vanishing and exploding gradients.
__label__fairness We demonstrate the efficacy of our model primarily on tabular datasets such as UCI Adult and Heritage Health.
__label__generative_models This paper presents a method for estimating the hallucination rate for in-context learning (ICL) with generative AI.
__label__other However, topological conjugacies have historically been challenging to compute.
__label__optimization_for_deep_networks We view the DPS problem as posterior inference in a novel Bayesian model where the posterior distributions of the instance-wise weights and the main neural network parameters are inferred under a reasonable prior and likelihood model.
__label__infrastructure The ranking information offers valuable guidance for scheduling requests.
__label__reinforcement_learning In this paper, we investigate (i) how EL-based state abstractions compare to NL-based ones for RL in hard-exploration, procedurally-generated environments, and (ii) how properties of the referential games used to learn ELs impact the quality of the RL exploration and learning.
__label__machine_vision To address this, we propose the method UNION.
__label__privacy Alongside our method, we establish MMUBench, a new benchmark for MU in MLLMs and introduce a collection of metrics for its evaluation.
__label__machine_learning_for_healthcare We illustrate our results with several tasks, such as removal of ECG noise and artifacts (baseline wander, electrode motion), reconstruction of a 12-lead ECG from a single lead (useful for ECG reconstruction of smartwatch experiments), and unsupervised explainable anomaly detection.
__label__natural_language_processing Native alignment aims to prevent unaligned content from the beginning, rather than relying on post-hoc processing.
__label__diffusion_based_models To address these issues, we first establish a large-scale panoramic video-text dataset containing millions of consecutive panoramic keyframes with corresponding panoramic depths, camera poses, and text descriptions.
__label__machine_vision TAPTR borrows designs from DEtection TRansformer (DETR) and formulates each tracking point as a point query, making it possible to leverage well-studied operations in DETR-like algorithms.
__label__causal_inference Additionally, we demonstrate its utility with real data by analyzing the heterogeneous effects of 401(k) plan participation on wealth.
__label__online_learning Our problem incorporates three practical challenges: (a) initially, the platform lacks knowledge of the demand side beforehand, necessitating a balance between exploring (learning the demand curve) and exploiting (maximizing revenue) simultaneously; (b) since only equilibrium prices and quantities are observable, traditional Ordinary Least Squares (OLS) estimators would be biased and inconsistent; (c) buyers are rational and strategic, seeking to maximize their consumer surplus and potentially misrepresenting their preferences.
__label__optimization_for_deep_networks Our method is simple, efficient, and accurate, enabling Jacobian nuclear norm regularization to scale to high-dimensional deep learning problems.
__label__generative_models Existing diffusion-based text-to-3D generation methods primarily focus on producing visually realistic shapes and appearances, often neglecting the physical constraints necessary for downstream tasks.
__label__machine_vision To accurately predict real-world dynamics at high resolution, we propose two novel losses to promote the learning of moving instances and structural information.
__label__machine_learning_for_other_sciences_and_fields Extensive experiments show that PocketFlow outperforms baselines on multiple benchmarks, e.g., achieving an average improvement of 1.29 in Vina Score and 0.05 in scRMSD.
__label__safety_in_machine_learning Despite growing interest, much of the existing research has focused on varied unlearning method designs to boost effectiveness and efficiency.
__label__generative_models This bias and uncertainty can be substantial enough to impede statistical convergence rates, even in seemingly straightforward analyses like mean calculation.
__label__machine_learning_for_healthcare To tackle this problem, we design a novel retrieval-augmented framework for incorporating similar structure information in known protein structures.
__label__learning_theory Then, we prove that by employing a crowdsourcing strategy involving multiple annotators, a carefully designed loss function can establish the desired model identifiability under reasonable conditions.
__label__privacy We demonstrate that our attacks outperform existing methods in terms of both effectiveness and computational efficiency.
__label__machine_vision MM-Det achieves state-of-the-art performance in DVF, demonstrating the effectiveness of our algorithm.
__label__natural_language_processing Finally, we show that the self-discovered reasoning structures are universally applicable across model families: from PaLM 2-L to GPT-4, and from GPT-4 to Llama2, and share commonalities with human reasoning patterns.
__label__machine_vision They suffer from long training time and slow inference speed.
__label__neuroscience_and_cognitive_science In these tasks, both human participants and neural networks successfully identified the relevant stimulus features within a few trials, demonstrating effective generalisation.
__label__evaluation We introduce PromptEval, a method for estimating performance across a large set of prompts borrowing strength across prompts and examples to produce accurate estimates under practical evaluation budgets.
__label__optimization This family contains regularizers that are commonly employed in practice and shares properties with regularizers parameterized by deep neural networks.
__label__learning_theory We address this question based on the framework of *data-driven algorithm design*, which connects the amount of data sufficient for establishing generalization bounds to the *pseudo-dimension* of performance metrics.
__label__natural_language_processing We conduct extensive experiments on 10 challenging reasoning-intensive tasks, and achieve significant performance improvements over previous SOTA methods: 11\% on Game of 24, 20\% on Geometric Shapes and 51\% on Checkmate-in-One.
__label__machine_vision This allows for the creation of dataset-specific prompts that improve generalization performance, while maintaining human comprehension.
__label__graph_neural_networks In this paper we address both by identifying and manipulating the key contributors to a problem's ``hardness'', known as cores.
__label__machine_vision More importantly, our FAM is fully-automated without the need for pre-cutting and can deal with highly-complex topologies, since its learning process adaptively finds reasonable cutting seams and UV boundaries.
__label__natural_language_processing Large language models (LLMs) targeting different deployment scales and sizes are currently produced by training each variant from scratch; this is extremely compute-intensive.
__label__deep_learning_architectures Although some works try to integrate both, they apply the two operators simultaneously at the finest pixel granularity.
__label__causal_inference These general laws inspire us that the estimator designs is not merely about eliminating bias, reducing variance, or simply achieve a bias-variance trade-off.
__label__generative_models We address this challenge and propose a novel theoretically-justified, lightweight, unbalanced EOT solver.
__label__other However, are large-scale soft labels necessary for large-scale dataset distillation?
__label__graph_neural_networks To address this issue, many residual methods have emerged.
__label__machine_vision We leverage the concept of Continual Learning and develop an incremental training strategy atop pre-trained MLLMs, enabling their expansion to new modalities using uni-modal data, without executing joint-modal pretraining.
__label__active_learning We investigate the convergence rates and data sample sizes required for training a machine learning model using a stochastic gradient descent (SGD) algorithm, where data points are sampled based on either their loss value or uncertainty value.
__label__diffusion_based_models Diffusion models are initially designed for image generation.
__label__optimization We show that, under some smoothness conditions, this bias is of order $O(\alpha)$.
__label__natural_language_processing Overall, we introduce a novel problem towards LLM reliability, an interactive MEDIQ benchmark and a novel question-asking system, and highlight directions to extend LLMs’ information-seeking abilities in critical domains.
__label__active_learning We present Causal Amortized Active Structure Learning (CAASL), an active intervention design policy that can select interventions that are adaptive, real-time and that does not require access to the likelihood.
__label__other Unfortunately, this comes at a cost: employing top-of-the-line models often requires paying thousands of dollars for API calls, while the resulting datasets are static and challenging to audit.
__label__infrastructure Longer KV cache requires larger memory, limiting the batch-size thus decreasing throughput.
__label__diffusion_based_models Experimental results highlight the capability of our framework to generate interactions with multiple human characters and its potential to work with off-the-shelf physics-based character simulators.
__label__privacy Our experiments demonstrate that FBPs provide informative trajectories describing the evolving states of clients and their contributions to the global model, thereby enabling the identification of clusters of clients with similar behaviours.
__label__reinforcement_learning Empirical studies demonstrate that OPT-AIL outperforms previous state-of-the-art deep AIL methods in several challenging tasks.
__label__reinforcement_learning To address this challenge, we propose Text-Aware Diffusion for Policy Learning (TADPoLe), which uses a pretrained, frozen text-conditioned diffusion model to compute dense zero-shot reward signals for text-aligned policy learning.
__label__machine_vision However, when directly turning to person representation learning, these general pre-training methods suffer from unsatisfactory performance.
__label__robotics Traditional approaches often leverage categorical semantic information for navigation guidance, which struggles when only partial objects are observed or detailed and functional representations of the environment are lacking.
__label__natural_language_processing Consequently, these methods are often inapplicable for out-of-distribution (OOD) data and newly emerged large language models (LLMs).
__label__machine_vision We estimate the range-wise ensemble weights on a reference set and store them in a lookup table (LUT) for efficient ensemble inference on the test set.
__label__safety_in_machine_learning Inspired by the success of vision-language foundation models, previous efforts achieved zero-shot adversarial robustness by aligning adversarial visual features with text supervision.
__label__machine_vision Skeleton-based multi-entity action recognition is a challenging task aiming to identify interactive actions or group activities involving multiple diverse entities.
__label__robotics At test time, we only require the object mesh and desired trajectories for grasping and transporting.
__label__machine_vision Additionally, \ModelName proves effective for long context inference, achieving results comparable to full text input while maintaining computational efficiency.
__label__machine_learning_for_physical_sciences Additionally, the FIDE framework incorporates the Generalized Extreme Value (GEV) distribution within its generative modeling framework, ensuring fidelity to both block maxima and overall data distribution.
"__label__generative_models From that, we construct an additional Low-Rank Residual Attention (LoRRA) block that acts as the ""modality learner"" to expand the learnable space and compensate for the attention shift."
__label__safety_in_machine_learning Theoretically, we show that CT-SSF surpasses previous methods defined in the output space.
__label__generative_models Experimental results show that GDPO achieves state-of-the-art performance in various graph generation tasks with complex and diverse objectives.
__label__optimization In response, we introduce first-order optimization expedited with approximately parallelized iterations (OptEx), the first general framework that enhances the time efficiency of FOO by leveraging parallel computing to directly mitigate its requirement of many sequential iterations for convergence.
__label__safety_in_machine_learning To address this, we propose an optimization-based objective for defending LLMs against jailbreaking attacks and an algorithm, Robust Prompt Optimization (RPO), to create robust system-level defenses.
__label__other The superior generation capabilities of Denoised Diffusion Probabilistic Models (DDPMs) have been effectively showcased across a multitude of domains.
__label__robotics Real-world experiments show that our method achieves a superior trade-off between grasping performance and time costs.
__label__online_learning She then reveals her type *only if* she makes a purchase.
__label__neuroscience_and_cognitive_science MiSO consists of three key components: 1) a neural activity alignment method to merge stimulation-response samples across sessions, 2) a statistical model trained on the merged samples to predict the brain's response to untested stimulation parameter configurations, and 3) an online optimization algorithm to adaptively update the stimulation parameter configuration based on the model's predictions.
__label__machine_learning_for_other_sciences_and_fields To solve this dilemma, we root monotonicity (a fundamental psychometric theory on educational assessments) in a co-factorization framework and present an autoencoder-like nonnegative matrix co-factorization (AE-NMCF), which improves the accuracy of estimating the student's knowledge proficiency via an encoder-decoder learning pipeline.
__label__machine_vision Thereby, HydraViT achieves adaptability across a wide spectrum of hardware environments while maintaining performance.
__label__bandits TRIPLE is built on a novel connection established between prompt optimization and fixed-budget best arm identification (BAI-FB) in multi-armed bandits (MAB); thus, it is capable of leveraging the rich toolbox from BAI-FB systematically and also incorporating unique characteristics of prompt optimization.
__label__machine_vision Furthermore, we demonstrate that not all soft labels are created equal; they must contain *structured information* to be beneficial.
__label__reinforcement_learning We therefore propose to use the Emergent Communication paradigm, where artificial agents are free to learn an emergent language (EL) via referential games, to bridge this gap.
__label__machine_vision This boosts significantly the learning of complex domains which are characterised by a large number of classes and long-tail distributions.
__label__privacy In this manner, the adversary can collect a dataset with a similar distribution from public APIs.
__label__deep_learning_architectures Additionally, $\mathbf{L}$ and $\mathbf{R}$ are readily amenable to low-rank adaptation, consequently enhancing the zero-shot performance.
__label__natural_language_processing We identify two key factors influencing the effectiveness of IM: (1) The ratio between instruction length and output length in the training data; and (2) The number of training examples.
__label__deep_learning_architectures The theory is based on two local neuronal operations: scaling which is commutative, and balancing which is not commutative.
__label__diffusion_based_models Notably, our RealCompo can be seamlessly extended with a wide range of spatial-aware image diffusion models and stylized diffusion models.
__label__other It still remains an open problem to address EDG in the domain-incremental setting, where source domains are non-static and arrive sequentially to mimic the evolution of training domains.
__label__learning_theory Traditional knowledge graph embedding (KGE) models map entities and relations to unique embedding vectors in a shallow lookup manner.
__label__safety_in_machine_learning In machine learning, model calibration and predictive inference are essential for producing reliable predictions and quantifying uncertainty to support decision-making.
__label__neuroscience_and_cognitive_science We provide an extremely simple but effective way to train high-accuracy spiking neural networks.
__label__other In this work, motivated by the advantage of hyperbolic spaces in modeling hierarchical relationships, we propose a novel approach HypStructure: a Hyperbolic Structured regularization approach to accurately embed the label hierarchy into the learned representations.
__label__generative_models We present a new MOtion FeaTure (MOFT) by eliminating content correlation information and filtering motion channels.
__label__natural_language_processing We translate this principle mathematically into a novel Active-Passive-Constraint (APC) score, a constraint-wise sum of statement-to-response natural language inference (NLI) scores weighted by constraint-query relevance scores.
__label__natural_language_processing However, the potential of optimized prompts on domain generalization has been under-explored.
__label__diffusion_based_models In this paper, we study a particular failure mode in diffusion models, which we term ***mode interpolation***.
__label__diffusion_based_models Experimentally, our algorithm achieves lower error than all the existing IDO techniques for stochastic optimal control for three out of four control problems, in some cases by an order of magnitude.
__label__optimization This paper considers the problem for finding the  $(\delta,\epsilon)$-Goldstein stationary point of Lipschitz continuous objective, which is a rich function class to cover a great number of important applications.
__label__machine_learning_for_healthcare We demonstrate the effectiveness of the proposed approach across a range of oracle models, including pretrained proxy models and GPU-accelerated docking.
__label__learning_theory Recently, several studies suggest that transformers learn a mesa-optimizer during autoregressive (AR) pretraining to implement ICL.
__label__machine_vision Specifically, this method introduces layer-wise learnable blocks to the VFMs, which hinges on alternately learning two representations during training: (i) Learning visual prompt.
__label__natural_language_processing Large Language Models (LLMs) are distinguished by their massive parameter counts, which typically result in significant redundancy.
__label__machine_learning_for_physical_sciences Despite recent popularity of attention-based neural architectures in core AI fields like natural language processing (NLP) and computer vision (CV), their potential in modeling complex physical systems remains under-explored.
__label__machine_learning_for_other_sciences_and_fields This approach allows the theorem to be tackled incrementally by outlining the overall theorem at the first level and then solving the intermediate conjectures at deeper levels.
__label__neuroscience_and_cognitive_science We validate our method on several datasets consisting of both continuous and spiking neural data, where we obtain lower dimensional latent dynamics than current state of the art methods.
__label__learning_theory In particular, the results on approximation rates enable us to concretely analyze the differences between the Transformer and classical sequence modeling methods, such as recurrent neural networks.
__label__diffusion_based_models Additionally, we train a warping module to align the hair color with the target region.
__label__other In addition, we conduct extensive experiments on benchmark datasets.
__label__optimization On the LLaMA3-8B model with 70\% sparsity, ALPS achieves a 29\% reduction in test perplexity on the WikiText dataset and a 8\% improvement in zero-shot benchmark performance compared to existing methods.
__label__neuroscience_and_cognitive_science Furthermore, we find that in static tasks, the accuracy of SNNs at each time step increases as the membrane potential evolves from zero.
__label__online_learning We study the fundamental problem of sequential probability assignment, also known as online learning with logarithmic loss, with respect to an arbitrary, possibly nonparametric hypothesis class.
__label__machine_learning_for_other_sciences_and_fields In particular, we demonstrate an 8.1 times improvement in zero shot molecular retrieval of active molecules over the previous state-of-the-art, reaching 77.33% in top-1% accuracy.
__label__machine_vision While existing works have mixed opinions of its application in visual tasks, the exploration of its internal workings and the optimization of its performance remain urgent and worthy research questions given its status as a novel model.
__label__machine_learning_for_healthcare Machine learning catalyzes a revolution in chemical and biological science.
__label__safety_in_machine_learning Gradient Cuff exploits the unique properties observed in the refusal loss landscape, including functional values and its smoothness, to design an effective two-step detection strategy.
"__label__probabilistic_methods We demonstrate this result empirically for both
real and simulated networks, and examine how this relates
to other embedding tools for network data."
__label__generative_models Certain types of motion that accompany speech can provide this illustrative mode of communication.
__label__neuroscience_and_cognitive_science This serves as a strong motivation for the neuro-science community to investigate the interpretability of these models for deepening our understanding of multi-modal information processing in brain.
__label__bandits We study lifelong learning in linear bandits, where a learner interacts with a sequence of linear bandit tasks whose parameters lie in an $m$-dimensional subspace of $\mathbb{R}^d$, thereby sharing a low-rank representation.
__label__natural_language_processing Furthermore, we showcase the transferability of our steering vectors across different models/LoRAs and highlight the synergistic benefits of applying multiple vectors simultaneously.
__label__causal_inference We further show that our AU-learner has several strengths in that it satisfies Neyman-orthogonality and is doubly robust.
__label__machine_vision Code for replicating the experiments is available at https://github.com/fpv-iplab/Differentiable-Task-Graph-Learning.
__label__neuroscience_and_cognitive_science We further demonstrate that our proposed method systematically increases models' ability to predict responses in macaque inferior temporal cortex.
__label__deep_learning_architectures However, theoretical analysis of the relationship between parameter space symmetries and these phenonmena is difficult.
__label__deep_learning_architectures A classifier is, in its essence, a function which takes an input and returns the class of the input and implicitly assumes an underlying distribution which does not change.
__label__deep_learning_architectures To address these limitations, we propose a novel contrastive framework name *QUEST: Quadruple Multimodal Contrastive Learning with Constraints and Self-Penalization*.
__label__machine_learning_for_social_sciences In addition,  we probe the biases of agent trust and  differences in agent trust towards other LLM agents and humans.
__label__safety_in_machine_learning Moreover, we indicate the biased message-passing schemes for graph structures and propose the personalized preference module.
__label__generative_models This insight then allows us to compute, from a small number of samples, the number of LM calls that maximizes system performance, and define an analytical scaling model for both systems.
__label__other In this paper, we propose a novel Reinforcement Learning (RL) Guided SSL method, RLGSSL, that formulates SSL as a one-armed bandit problem and deploys an innovative RL loss based on weighted reward to adaptively guide the learning process of the prediction model.
__label__natural_language_processing We also discover that a model exhibits emergent abilities on certain tasks---regardless of the continuity of metrics---when its pre-training loss falls below a specific threshold.
__label__generative_models However, generating multiple high-quality and diverse responses without sacrificing accuracy remains a challenge, especially when using greedy sampling.
__label__learning_theory Empirical evaluations on several standard real-world datasets demonstrate that our theoretical bounds highly correlate with empirical generalization performance, leading to improved classifier design via our regularizers.
__label__probabilistic_methods We improve upon this idea by incorporating inference-time optimization with self-supervised loss to iteratively improve the solutions and employ a teacher-student framework that provides a better initial network, which in turn, helps reduce the number of inference-time optimization steps.
__label__generative_models As large language models (LLMs) become increasingly prevalent across many real-world applications, understanding and enhancing their robustness to adversarial attacks is of paramount importance.
__label__bandits Though pure exploration has been extensively studied in standard adaptive experimental design settings, we believe this is the first work considering a setting where noise is confounded.
__label__natural_language_processing Our findings reveal that, intriguingly, CoT reasoning paths can be elicited from pre-trained LLMs by simply altering the \textit{decoding} process.
__label__generative_models We further provide theoretical and empirical insights on how high sparsity in SHiRA can aid multi-adapter fusion by reducing concept loss.
__label__privacy To enhance diversity while reducing errors in the approximated models, we apply edit distance to quantify the diversity within a group of approximated models and introduce a theoretically guaranteed criterion to evaluate each model's error.
__label__speech_and_audio This is exemplified by instances generated in a single channel where one speaker's utterance is seamlessly mixed with another's interjections or laughter, indicating the latter's role as an attentive listener.
__label__natural_language_processing Large Language Models (LLMs) have shown remarkable performance in various natural language tasks, but they often struggle with planning problems that require structured reasoning.
"__label__diffusion_based_models We further introduce a theoretically grounded ""denoising schedule"" to improve sampling and learning efficiency."
__label__machine_vision This paper introduces a self-supervised method for BID that does not require GT images.
__label__evaluation However, while popular, the system's suitability for assessing entities with constant skill levels, such as LLMs, remains relatively unexplored.
__label__machine_vision To verify our viewpoint, we present the Automated Multi-level Preference (**AMP**) framework for MLLMs.
__label__other However, common machine learning applications like classification require point-level information and features to be available.
__label__optimization Instead, we propose a different distributional perspective, where we seek to find an idealized data distribution which maximizes a pretrained model's performance.
__label__machine_vision We present both qualitative and quantitative results across a diverse range of applications that largely improve upon baselines.
__label__graph_neural_networks Rehearsal-based methods, which consolidate old knowledge with a replay memory buffer, are a de facto solution due to their straightforward workflow.
__label__machine_vision We further apply recent pixel-aligned Gaussian Splatting to regularize depth prediction and improve detection performance with little computation overhead.
__label__learning_theory Semirandom adversaries are a useful tool to determine the extent to which an algorithm has overfit to statistical assumptions on the input.
__label__learning_theory Specifically, we build a *quantum-assisted differentiable simulator* for efficient gradient estimation that is more accurate and robust than classical methods relying on stochastic approximation.
__label__diffusion_based_models Most existing theoretical investigations of the accuracy of diffusion models, albeit significant, assume the score function has been approximated to a certain accuracy, and then use this a priori bound to control the error of generation.
__label__probabilistic_methods In this paper, we address these issues, and present: (i) a pipeline for building DAG-shaped PICs out of arbitrary variable decompositions, (ii) a procedure for training PICs using tensorized circuit architectures, and (iii) neural functional sharing techniques to allow scalable training.
__label__machine_learning_for_other_sciences_and_fields In  2024, $247.88 M was lost in 20 smart contract exploits.
__label__diffusion_based_models To further enhance multi-color hairstyle editing, we fine-tuned a CLIP model using a multi-color hairstyle dataset.
"__label__other Concretely, MPI comprises two key procedures:
$\textbf{1) Masked Pre-training}$ involves training model to reconstruct massive natural images with random masking for generalizable representations, gathering the potential for valid zero-shot denoising on images with varying noise degradation and even in distinct image types."
__label__machine_vision Our numerical and visual comparisons show our superiority over the state-of-the-art results on the widely used benchmarks.
__label__generative_models This leads to model divergence and visual artifacts due to the absence of proper guidance for initialized positions of Gaussians and densification to manage their scales adaptively.
__label__machine_vision Additionally, we introduce depth distortion and near-far regularizations to improve the accuracy of scene structure for down- stream tasks.
__label__optimization We illustrate the relevance and effectiveness of PD-LMC in several applications.
__label__active_learning In this paper, we advance the third approach by proposing two new methods.
__label__generative_models To address this, one straightforward solution is to incorporate a personalized diffusion model with a text-driven editing framework.
__label__interpretability_and_explainability This work targets ante hoc interpretability, and specifically Concept Bottleneck Models (CBMs).
__label__reinforcement_learning Exploration in sparse-reward reinforcement learning (RL) is difficult due to the need for long, coordinated sequences of actions in order to achieve any reward.
__label__optimization_for_deep_networks In light of such revelation, we propose $S^{2} - SAM$, characterized by a **S**ingle-step **S**harpness_**A**ware **M**inimization that is tailored for **S**parse training.
__label__algorithmic_game_theory We focus on two learning tasks; in the first, the input is vectors of utilities of an action (decision or policy) for individuals in a group and their associated social welfare as judged by a policy maker, whereas in the second, the input is pairwise comparisons between the welfares associated with a given pair of utility vectors.
__label__reinforcement_learning This paper presents OmniJARVIS, a novel Vision-Language-Action (VLA) model for open-world instruction-following agents in Minecraft.
__label__machine_vision To utilize more appropriate ranking metric and leverage more comprehensive information among the alternative set, we propose a novel in-context example selection framework to approximately identify the global optimal prompt, i.e.
__label__deep_learning_architectures SMoE has the potential to exponentially increase in parameter count while maintaining the efficiency of the model by only activating a small subset of these parameters for a given sample.
__label__natural_language_processing \name can also use specialist vision models during the sketching process (e.g., draw bounding boxes with object detection models, draw masks with segmentation models), to further enhance visual perception and reasoning.
__label__reinforcement_learning In this work, we propose to learn preference representations aligned with preference labels, which are then used as conditions to guide the conditional generation process of diffusion models.
__label__other RCC does not require any training and its worst-case complexity scales quasi-linearly with the size of the largest cluster.
__label__reinforcement_learning Training Embodied AI agents for instruction-following with Reinforcement Learning (RL) poses a strong exploration challenge.
__label__learning_theory We simulate various regression tasks to verify our theory and demonstrate the practical feasibility of our proposed algorithm.
__label__machine_vision Extensive results on the DTU dataset and Tanks\&Temple benchmark demonstrate the effectiveness of our method.
__label__machine_vision We introduce a new INR training framework, STRAINER that learns transferable features for fitting INRs to new signals from a given distribution, faster and with better reconstruction quality.
__label__interpretability_and_explainability To understand this phenomenon in depth, we formulate the low-rank matrix completion problem as a masked language modeling (MLM) task, and show that it is possible to train a BERT model to solve this task to low error.
__label__machine_vision Test-time prompt tuning, which learns prompts online with unlabelled test samples during the inference stage, has demonstrated great potential by learning effective prompts on-the-fly without requiring any task-specific annotations.
__label__safety_in_machine_learning While recent studies have explored the Rashomon effect across various machine learning algorithms, its impact on gradient boosting---an algorithm widely applied to tabular datasets---remains unclear.
__label__algorithmic_game_theory We show how natural robustness of the $1$-median (also known as the geometric median) of a set of points leads to an algorithm for single-facility location with MAC predictions.
__label__generative_models Diffusion models have demonstrated great success in text-to-video (T2V) generation.
__label__machine_learning_for_healthcare We demonstrate $\texttt{sisPCA}$'s connections with autoencoders and regularized linear regression and showcase its ability to identify and separate hidden data structures through extensive applications, including breast cancer diagnosis from image features, learning aging-associated DNA methylation changes, and single-cell analysis of malaria infection.
__label__probabilistic_methods Such tasks are commonly encountered in protein design, robotics, and clinical medicine where evaluating the oracle function is prohibitively expensive.
__label__diffusion_based_models By drawing an analogy between the denoising process and stochastic gradient descent (SGD), we implement a filtering mechanism that aggregates attention maps, enhancing generation reliability and authenticity.
__label__optimization_for_deep_networks Our results imply ambient dimension-independent communication complexity for sketch-DL.
__label__safety_in_machine_learning (3) GREAT Score can be used for remote auditing of privacy-sensitive black-box models, as demonstrated by our robustness evaluation on several online facial recognition services.
__label__diffusion_based_models The watermark can be verified by reversing the generation process.
__label__neuroscience_and_cognitive_science Moreover, we introduce a learned receptive field layer that sheds light on the CNN-based model's data processing during training, enhancing understanding of its structure and interpretive capacity.
__label__deep_learning_architectures Upon acceptance of our paper, the code will be open sourced.
__label__machine_learning_for_other_sciences_and_fields In this paper, we propose a graph structure based on Arithmetic Normal Form (ANF) to efficiently handle the XOR operation bottleneck.
__label__learning_theory Our theories and claims are supported by empirical results on several image classification tasks with various types of neural networks.
__label__machine_vision Recent advancements in 3D Gaussian Splatting (3DGS) have enabled high-fidelity novel view synthesis at real-time speeds.
__label__active_learning On the other hand, for smoothness, one needs descriptors that are smooth and continuous on the shape.
__label__machine_vision In the meantime, we ensure the image-text alignment by aligning text embeddings of the class descriptions and their corresponding image embedding while further removing the domain-specific features.
__label__machine_learning_for_healthcare The discovery of dynamical systems is crucial across a range of fields, including pharmacology, epidemiology, and physical sciences.
__label__robotics However, in real-world scenarios, we often lack the time to gather enough trajectory points before making predictions, e.g., when a car suddenly appears due to an obstruction, the system must make immediate predictions to prevent a collision.
__label__learning_theory Our approach excels in practical applications and is underpinned by a solid theoretical framework, ensuring its reliability and effectiveness across diverse contexts.
__label__reinforcement_learning Nodes with the best heuristic values in OPEN are most probably picked into this subset, but sometimes may not be included, which enables SeeA$^*$ to explore other promising branches.
__label__machine_learning_for_physical_sciences Controlling the evolution of complex physical systems is a fundamental task across science and engineering.
__label__reinforcement_learning Consequently, the QVPO algorithm leverages the exploration capabilities and multimodality of diffusion policies, preventing the RL agent from converging to a sub-optimal policy.
__label__learning_theory Whereas multiple nearly-linear time algorithms have been established in the more specialized fully-random setting where each entry is revealed with probablity exactly $p$, the only known nearly-linear time algorithm in the semi-random setting is due to [CG18], whose sample complexity has a polynomial dependence on the inverse accuracy and condition number and thus cannot achieve high-accuracy recovery.
__label__diffusion_based_models We establish a mathematical framework for guided diffusion to systematically study its optimization theory and algorithmic design.
__label__interpretability_and_explainability Our results indicate that some algorithmic capabilities are present in transformers (and accessible via appropriately structured inputs) even before these models are trained.
__label__learning_theory In the asymptotic regime of large models, we provide an exact and rigorous analysis and relate the generalization errors (in regression) and classification errors (in binary classification) for the pretrained and fine-tuned models.
__label__diffusion_based_models Can we generate a control policy for an agent using just one demonstration of desired behaviors as a prompt, as effortlessly as creating an image from a textual description?
__label__machine_learning_for_other_sciences_and_fields The theoretical framework, on the one hand, enables us to optimize the parameters of data-modulating masks, and on the other hand, provides a fundamental connection between the number of data frames that can be recovered from a single measurement to the parameters of the untrained NN.
__label__learning_theory Moreover, we extend this to an efficient algorithm for exact graph matching whenever this is information-theoretically possible, positively resolving an open problem of Rácz and Sridhar (NeurIPS 2021).
__label__machine_vision Most early methods, though, assume that the given image pairs are from the same camera or have minor lighting differences.
__label__neuroscience_and_cognitive_science Our tests validated ActSort's performance across different experimental conditions and datasets from multiple animals.
__label__reinforcement_learning We then show that with increasing step size, our robust policy mirror descent achieves a linear convergence rate in the optimality gap, and with constant step size, our algorithm converges to an $\epsilon$-optimal policy with an iteration complexity of $\mathcal{O}(1/\epsilon)$.
__label__machine_vision To overcome this, we introduce a novel framework, Meta-Exploiting Frequency Prior for Cross-Domain Few-Shot Learning, which is crafted to comprehensively exploit the cross-domain transferable image prior that each image can be decomposed into complementary low-frequency content details and high-frequency robust structural characteristics.
__label__optimization This algorithm enables flexible estimator choices, such as neural networks or kernel based methods, as well as non-quadratic loss functions, which may be suitable for structural equations beyond the setting of continuous outcomes and additive noise.
__label__privacy In this work, we first provide a theoretical understanding of the efficacy of DP training by analyzing the per-iteration improvement of loss through the lens of the Hessian.
__label__machine_learning_for_physical_sciences FDEs play a fundamental role in physics, mathematics, and optimal control.
__label__learning_theory However, the majority of these analyses have been limited to an unrealistic regression error structure, assuming independent and identically distributed errors with zero mean and common variance.
__label__natural_language_processing Both are drop-in replacements for existing PEFTs and learn interventions that are 15x--65x more parameter-efficient than LoRA.
__label__optimization Algorithms with predictions is a recent framework for decision-making under uncertainty that leverages the power of machine-learned predictions without making any assumption about their quality.
__label__optimization_for_deep_networks Sharpness-Aware Minimization (SAM) has attracted significant attention for its effectiveness in improving generalization across various tasks.
__label__machine_vision Visual-Language Alignment (VLA) has gained a lot of attention since CLIP's groundbreaking work.
__label__machine_learning_for_physical_sciences Boltzmann Generators are a generative machine learning method that addresses this issue by learning a transformation via a normalizing flow from a simple prior distribution to the target Boltzmann distribution of interest.
__label__safety_in_machine_learning Despite these empirical successes, the theoretical understanding of ACF's effectiveness in terms of both performance and robustness remains unclear.
__label__neuroscience_and_cognitive_science iii) _Spiking Patch Embedding with Deformed Shortcut (SPEDS)_, enhances spiking information transmission and integration, thus improving overall performance.
__label__optimization However, UL-based solvers face two practical issues: (I) an optimization issue, where UL-based solvers are easily trapped at local optima, and (II) a rounding issue, where UL-based solvers require artificial post-learning rounding from the continuous space back to the original discrete space, undermining the robustness of the results.
__label__natural_language_processing This paper investigates the factors influencing these decision boundaries and explores methods to enhance their generalizability.
__label__diffusion_based_models The former applies the distillation objective solely to the motion representation, while the latter mitigates training-inference discrepancies by mixing distillation trajectories from both the low- and high-quality video domains.
__label__learning_theory Despite various existing HSIC estimators designed since its introduction close to two decades ago, the fundamental question of the rate at which HSIC can be estimated is still open.
__label__diffusion_based_models We investigate the training methodologies and the decoder architecture of LiteVAE and propose several enhancements that improve the training dynamics and reconstruction quality.
__label__learning_theory The nearest neighbor rule is fundamental prediction strategy, but it is only known to be consistent under strong statistical or geometric assumptions: the instances come i.i.d.
__label__machine_vision We present MV2Cyl, a novel method for reconstructing 3D from 2D multi-view images, not merely as a field or raw geometry but as a sketch-extrude CAD.
__label__reinforcement_learning The quality of a world model hinges on the richness of data stored in the agent's replay buffer, with expectations of reasonable generalization across the state space surrounding recorded trajectories.
__label__machine_vision Thanks to ASG, we have significantly improved the ability of 3D-GS to model scenes with specular and anisotropic components without increasing the number of 3D Gaussians.
__label__causal_inference Under this formulation, we cast the extrapolation problem into a latent-variable identification problem.
__label__graph_neural_networks Inspired by the success of foundation models in applications such as ChatGPT, as graph data has been ubiquitous, one can envision the far-reaching impacts that can be brought by Graph Foundation Models (GFMs) with broader applications in the areas such as scientific research, social network analysis, drug discovery, and e-commerce.
"__label__optimization_for_deep_networks Remarkably, selecting only 50K
data for SFT, S2L achieves a $32.7$% accuracy on the challenging MATH
benchmark, improving Phi-2 by $16.6$%."
__label__machine_vision Transduction is a powerful paradigm that leverages the structure of unlabeled data to boost predictive accuracy.
__label__generative_models We show that the best-of-$n$ sampling distribution is essentially equivalent to the policy learned by RLHF if we apply a particular monotone transformation to the reward function.
__label__diffusion_based_models Video editing is an emerging task, in which most current methods adopt the pre-trained text-to-image (T2I) diffusion model to edit the source video in a zero-shot manner.
__label__machine_vision Building on these findings, we introduce the Anchor Former (AcFormer), a novel vision-language connector designed to leverage the rich prior knowledge obtained from these visual anchors during pretraining, guiding the aggregation of information.
__label__privacy We provide theoretical insights into the impacts of trapdoor's effectiveness and naturalness on deceiving MI attacks.
__label__neuroscience_and_cognitive_science By investigating the role of latent learning progress in human goal selection, we pave the way for more effective and personalized learning experiences as well as the advancement of more human-like autotelic machines.
__label__neuroscience_and_cognitive_science We use multiple datasets and different types of perceptual representations to show that the representations encoded by transformer models are able to predict: (i) labels associated with odorants‌‌ provided by experts; (ii) continuous ratings provided by human participants with respect to pre-defined descriptors; and (iii) similarity ratings between odorants provided by human participants.
__label__machine_vision Mixture of Experts (MoE) networks demonstrate scalability while maintaining same inference-time costs, but they come with a larger parameter footprint.
__label__deep_learning_architectures We train MemoryFormer from scratch and conduct extensive experiments on various benchmarks to demonstrate the effectiveness of the proposed model.
__label__machine_vision Extensive experimental results across 16 datasets demonstrate the efficacy of our approach, particularly outperforming the state-of-the-art by an average of $2.6\%$ on 10 datasets with CLIP ViT-B/16 and achieving an average margin of $1.5\%$ on ImageNet and its five distribution shifts with CLIP ViT-B/16.
"__label__causal_inference In particular, we prove, that the task
asking whether, for a given feasible correlation matrix, there 
are exactly one or two or more parameter sets explaining the observed 
matrix, is hard for $\forall \mathbb{R}$, the co-class of the existential theory 
of the reals."
__label__reinforcement_learning However, the use of diffusion policies in online RL is hindered by the intractability of policy likelihood approximation, as well as the greedy objective of RL methods that can easily skew the policy to a single mode.
"__label__learning_theory To achieve such provable ""forward-error"" guarantees, our methods rely on a new $O(n^{\omega+\eta})$ stability analysis for the Cholesky factorization, and a smoothed analysis for computing spectral gaps, which can be of independent interest."
__label__robotics Videos of the robot are best viewed at baku-robot.github.io.
__label__optimization One of its benefits is to enable strong convergence guarantees through smooth-like analyses, even for objectives with exploding or vanishing curvature.
__label__probabilistic_methods Conformal Prediction (CP) is a distribution-free uncertainty estimation framework that constructs prediction sets guaranteed to contain the true answer with a user-specified probability.
__label__generative_models During property optimization, the latent prompt is inferred from target properties and constraints through posterior sampling and then used to guide the autoregressive molecule generation.
__label__machine_learning_for_physical_sciences We show that modelling solutions as flows of pointclouds over the group of interest $G$ improves generalization and data-efficiency.
"__label__optimization_for_deep_networks In this paper, we introduce a novel criterion for model compression, named ""Expressiveness""."
__label__privacy The DPConvCNP outperforms the GP baseline, especially on non-Gaussian data, yet is much faster at test time and requires less tuning.
__label__machine_vision Both cooperate to fully leverage diverse semantics and multi-grained complementary information.
__label__privacy To address these limitations, we define feature sensitivity in evaluating feature unlearning according to Lipschitz continuity.
__label__natural_language_processing Besides, our auxiliary model, LLaMA-2-13B, is effective in discovering many ambiguous answers, including correct answers neglected by GPT-4 and delusive wrong answers GPT-4 struggles to identify.
__label__learning_theory That means that, in contrast with other algorithms, GD has no advantage over naive ERMs.
__label__algorithmic_game_theory We say a threshold policy is $\alpha$(-strategically)-robust if it (a) achieves the  $\alpha$-approximation to the prophet value for strategic players; and (b) meanwhile remains a  $\frac{1}{2}$-approximation in the standard non-strategic setting.
__label__diffusion_based_models First, we eliminate the regression loss and the need for expensive dataset construction.
__label__interpretability_and_explainability Furthermore, our method significantly improves the model's rationale correctness, improving localization by 7.5\% and disentanglement by 36.5\%.
__label__diffusion_based_models We attribute these issues to the incorrect learning of the embedding alignment for the concept.
__label__neuroscience_and_cognitive_science Model comparisons and ablation studies reveal that our design choices, including (\romannumeral1) temporal modeling based on region-level tokens by utilizing 1D depthwise convolution to fuse channels in the ventral sensorimotor cortex (vSMC) and superior temporal gyrus (STG) and (\romannumeral2) self-supervision through discrete codex-guided mask modeling, significantly contribute to this performance.
"__label__reinforcement_learning Prior work towards this objective has been largely restricted to perturbation-based data augmentation where new data points are created by perturbing the original ones,
which has been impressively effective for tasks where the RL agent observe control states as images with perturbations including random cropping, shifting, etc."
__label__reinforcement_learning Next, we conduct experiments on pluralistic language datasets representing diverse user preferences and demonstrate improved reward function accuracy.
__label__deep_learning_architectures By observing that composition of low frequency functions can effectively approximate a high-frequency function, we propose to learn a function containing high-frequency components by composing several SNNs, each of which learns certain low-frequency information from the given data.
__label__evaluation In this work, we present RaVL, which takes a fine-grained perspective on VLM robustness by discovering and mitigating spurious correlations using local image features rather than operating at the global image level.
__label__machine_vision The source code is available on our project page (https://github.com/esw0116/ODGS).
__label__other Experimental results demonstrate a better generalization behavior with better calibrated uncertainty estimates of FedMDMI.
__label__learning_theory We study online classification with general hypothesis classes where the true labels are determined by some function within the class, but are corrupted by *unknown* stochastic noise, and the features are generated adversarially.
__label__other However, most existing representation learning methods ignore this hierarchy, treating labels as permutation invariant.
__label__natural_language_processing Inspired by this theory, we propose an ID-induced prompt synthesis framework for evaluating LLMs so that the evaluation set continually updates and refines according to model abilities.
__label__machine_vision (2) Interleavable.
__label__diffusion_based_models Extensive experiments show that our RealCompo consistently outperforms state-of-the-art text-to-image models and spatial-aware image diffusion models in multiple-object compositional generation while keeping satisfactory realism and compositionality of the generated images.
__label__other Comprehensive experiments demonstrate that FuseFL outperforms existing OFL and ensemble FL by a significant margin.
"__label__learning_theory It elucidates insightful theories and
useful tools to understand and optimize neural networks."
__label__online_learning Additionally, we present an efficient implementation of our algorithm that significantly reduces computational complexity.
__label__interpretability_and_explainability Finite-sample experiments demonstrate that our procedure, with a simple choice of the slider, works well across a wide range of settings.
__label__neuroscience_and_cognitive_science Humans are autotelic agents who learn by setting and pursuing their own goals.
__label__generative_models We further present a multi-part editing pipeline that enables us to synthesize different textures across various regions.
__label__natural_language_processing The resulting LLM-based program synthesis algorithm is broadly applicable: Across loop invariant synthesis, visual reasoning puzzles, and competition programming problems, we find that our new method can solve more problems using fewer language model calls.
__label__neuroscience_and_cognitive_science We find significant differences between these RNNs and those trained to path integrate only a single agent.
__label__reinforcement_learning Pre-training policies from task-agnostic datasets has emerged as a promising approach, yet existing methods still necessitate substantial interactions via RL to learn new tasks.
__label__machine_learning_for_other_sciences_and_fields As one of the pioneers in this field, we propose the Large Language Models Enhancement framework for Sequential Recommendation (LLM-ESR).
__label__machine_vision Our results show that using curvature as input leads to significant a increase in performance on segmentation and classification tasks, while allowing far less computational overhead than current methods.
__label__diffusion_based_models Simulation with realistic and interactive agents represents a key task for autonomous vehicle (AV) software development in order to test AV performance in prescribed, often long-tail scenarios.
__label__optimization_for_deep_networks We theoretically establish the equivalence between the policy performance and action-value weighted decision difference, and introduce action-value weighted PBC (Av-PBC) as a more effective OBD objective.
__label__machine_learning_for_social_sciences Additionally, CogMir framework demonstrates its potential as a valuable platform for encouraging more research into the social intelligence of LLM Agents.
__label__deep_learning_architectures To address this issue, we propose a novel long-tailed learning paradigm that aims to tackle distribution shift in real-world scenarios and accommodate different user preferences for the trade-off between head and tail classes.
__label__diffusion_based_models Empirically, we perform extensive experiments to demonstrate the robustness advantage of AdvUnlearn across various DM unlearning scenarios, including the erasure of nudity, objects, and style concepts.
__label__natural_language_processing Project is available at https://minicache.vmv.re .
__label__reinforcement_learning Prior work has shown that incorporating pre-trained visual representations (PVRs) enhances sample efficiency and generalization.
__label__machine_learning_for_physical_sciences These transferable Boltzmann Generators allow approximate sampling from the target distribution of unseen systems, as well as efficient reweighting to the target Boltzmann distribution.
__label__machine_vision By recovering complete actions and resampling from these full sequences, we can generate strong augmentations for unseen domains.
__label__optimization We provide space complexity lower bounds for data structures that approximate logistic loss up to $\epsilon$-relative error on a logistic regression problem with data $\mathbf{X} \in \mathbb{R}^{n \times d}$ and labels $\mathbf{y} \in \\{-1,1\\}^d$.
__label__reinforcement_learning Our experimental results support that our framework can achieve better performance, even under conditions of misspecified transition models.
__label__causal_inference Training on datasets generated from different classes of causal models, unambiguously identifiable in isolation, improves the test generalization.
__label__machine_learning_for_other_sciences_and_fields Code is available at https://github.com/DILab-USTCSZ/CMuST.
__label__neuroscience_and_cognitive_science The versatility of our framework is demonstrated in the magnetoencephalogram (MEG) data modality.
__label__speech_and_audio Audio samples and code are available at http://frieren-v2a.github.io.
__label__other samples from a target distribution $P$, while the transmitter and the receiver have access to independent samples from a reference distribution $Q$.
__label__safety_in_machine_learning We conducted experiments on three widely used cross-modality datasets, namely RegDB, SYSU, and LLCM.
__label__machine_vision Since the PEFT strategy is conducted symmetrically to the two CLIP modalities, the misalignment between them is mitigated.
__label__probabilistic_methods CMPE essentially distills a continuous probability flow and enables rapid few-shot inference with an unconstrained architecture that can be flexibly tailored to the structure of the estimation problem.
__label__optimization_for_deep_networks This matrix is decomposed into a product of down-projection and up-projection matrices, with the bottleneck dimensionality being crucial for reducing the number of learnable parameters, as exemplified by prevalent methods like LoRA and Adapter.
__label__reinforcement_learning We present a constrained problem based on the satisfaction of regular safety properties with high probability and we compare our setup to the some common constrained Markov decision processes (CMDP) settings.
__label__other In this work, we provide a causal view to find that this performance drop of OFL methods comes from the isolation problem, which means that local isolatedly trained models in OFL may easily fit to spurious correlations due to the data heterogeneity.
__label__robotics Furthermore, PAD demonstrates superior generalization to unseen tasks in real-world robot manipulation settings with 28.0\% success rate increase compared to the strongest baseline.
__label__other Specifically, we demonstrate that well-chosen concept sets can improve sample efficiency and out-of-distribution robustness in the appropriate regimes.
__label__other The cost of ranking becomes significant in the new stage of deep learning.
__label__graph_neural_networks Our proof leverages the capability of GNNs to simulate a variant of the gradient descent algorithm on a carefully selected potential function.
__label__machine_vision First, this framework integrates feature-level information integration into the diffusion process, allowing adaptive degradation removal and multi-modal information fusion.
__label__causal_inference Causal sufficiency is both unrealistic and empirically untestable.
__label__learning_theory Most existing theoretical analyses studied the in-context learning (ICL) ability of transformers for linear function classes, where it is typically shown that the minimizer of the pretraining loss implements one gradient descent step on the least squares objective.
__label__machine_learning_for_healthcare It highlights the weaknesses in current evaluation metrics and provides a clear framework for analysis.
__label__generative_models Large, autoregressive models trained on databases of chemical compounds have yielded powerful generators, but we still lack robust strategies for generating molecules with desired properties.
__label__generative_models Our approach extends recently proposed one-step methods to the multi-step case, and provides a new perspective by interpreting these approaches in terms of moment matching.
__label__natural_language_processing Large Language Models (LLMs) have exhibited an impressive ability to perform In-Context Learning (ICL) from only a few examples.
__label__algorithmic_game_theory We analyze this model under four approval-based voting rules: Chamberlin-Courant (CC), Proportional Approval Voting (PAV), Approval Voting (AV), and Satisfaction Approval Voting (SAV).
__label__optimization Compared with existing approaches, our method requires weaker assumptions and attains the optimal convergence rate without the additional $\mathcal{O}(\log T)$ term.
__label__learning_theory As a result, a Language Model trained with increasingly many examples can build a deeper representation of the grammar's structure, thus reaching good performance despite the high dimensionality of the problem.
__label__optimization The resulting approximate NLP solution is then utilized to warm-start an interior point solver.
"__label__natural_language_processing We determine the optimal pattern for each attention head offline and dynamically build sparse
indices based on the assigned pattern during inference."
__label__learning_theory Based on this finding, we theoretically characterize the training trajectory as following the hierarchical invariant manifold traversal process, generalizing the characterization of Li et al.
__label__machine_learning_for_physical_sciences These models assume that a sparse latent function is transformed by a monotonic nonlinearity to emit measurable fitness.
__label__machine_vision Employing conventional pruning techniques can remarkably reduce data requirements but would suffer from a degradation in performance.
__label__machine_vision Traditionally, SSL mandates that all classes possess labeled instances.
__label__optimization Our setting encompasses many application domains in which the similarity function is costly to compute and inherently noisy.
__label__generative_models Furthermore, we introduce a consistency alignment loss to encourage coherent subgoal images and align with their corresponding instructions, mitigating potential hallucinations and semantic conflicts between the two planning manners.
__label__interpretability_and_explainability By allowing flexibility in that reference and by considering how distances along the hypercube translate to distances in feature space, we can derive sparser and more meaningful explanations for various types of function classes.
__label__natural_language_processing Direct Alignment Algorithms (DDAs), such as Direct Preference Optimization (DPO) have emerged as alternatives to the classical RLHF pipeline.
__label__generative_models We mend this problem by adding an embedding to each digit that encodes its position relative to the start of the number.
__label__infrastructure The MFP module prunes local models with fused knowledge obtained from both local and remaining domains, ensuring robustness to domain shifts.
__label__natural_language_processing In an effort to reduce the inference cost, post-training quantization (PTQ) has become a popular approach, quantizing weights and activations to lower precision, such as INT8.
__label__interpretability_and_explainability To address this gap, we propose a novel Neuron Attribution method tailored for MLLMs, termed NAM.
"__label__learning_theory One common approach to transfer learning is referred to as ""model-based"", and involves using a model that is pretrained on samples from a source distribution, which is easier to acquire, and then fine-tuning the model on a few samples from the target distribution."
__label__deep_learning_architectures In this work, we propose Asynchronous Perception Machine (APM), a computationally-efficient architecture for test-time-training (TTT).
__label__natural_language_processing Alignment is a procedure to fine-tune pre-trained large language models (LLMs) to follow natural language instructions and serve as helpful AI assistants.
__label__evaluation However, in this paper, we find that there naturally exists a gap, which resembles the modality gap, between the prototype and image instance embeddings extracted from the frozen pre-trained backbone, and simply applying the same transformation during the adaptation phase constrains exploring the optimal representation distributions and shrinks the gap between prototype and image representations.
__label__machine_vision In the field of 3D Human Pose Estimation (HPE), scalability and generalization across diverse real-world scenarios remain significant challenges.
__label__other To address this, we initiate a topological formation that serves as a compliant behavioral foreground to guide downstream trajectory generations.
__label__machine_vision It then discards non-informative patches and progressively moves downward on the selected ones for a fine-grained search.
__label__machine_vision Extensive experiments on public benchmarks demonstrate the superiority of our method, compared to the state-of-the-art multi-task learning approaches.
__label__learning_theory We analyze two anisotropic scenarios: homogeneous, with identical covariance matrices, and heterogeneous, with distinct matrices per cluster.
__label__deep_learning_architectures Additionally, we explore the impact of varying amounts of eye-gaze data on model performance, highlighting the feasibility and utility of integrating this auxiliary data into multi-modal alignment framework.
__label__machine_learning_for_physical_sciences These time-evolving prompt embeddings are then incorporated into basic forecasting models to overcome temporal distribution shifts.
__label__natural_language_processing The KV cache stores key-value states of previously generated tokens, significantly reducing the need for repetitive computations and thereby lowering latency in autoregressive generation.
__label__deep_learning_architectures This paper introduces DiffTORI, which utilizes $\textbf{Diff}$erentiable $\textbf{T}$rajectory $\textbf{O}$ptimization as the policy representation to generate actions for deep $\textbf{R}$einforcement and $\textbf{I}$mitation learning.
__label__optimization We give exact expressions for the risk and learning rate curves in terms of a deterministic solution to a system of ODEs.
__label__deep_learning_architectures Numerical experiments validate the effectiveness of our approach.
__label__machine_learning_for_other_sciences_and_fields Moreover, we demonstrate the optimality of our solution and discuss some possible applications.
__label__optimization However, there has been no progress since, either in terms of achievability or impossibility.
__label__deep_learning_architectures Specifically, the neural networks learned in each grade adeptly capture some low-frequency information, allowing their compositions with SNNs learned in the previous grades effectively representing the high-frequency features.
__label__other Recently, first works have started incorporating such objectives into loss functions for deep graph clustering and pooling.
__label__evaluation In this work, we introduce Dynamic Evaluation of LLMs via Adaptive Reasoning Graph Evolvement (DARG) to dynamically extend current benchmarks with controlled complexity and diversity.
__label__optimization Such problems are minimised ‘at infinity’ and have many possible solutions; we study which solution is preferred by the algorithm depending on the mirror potential.
__label__interpretability_and_explainability 2022, Xue et al.
__label__graph_neural_networks Our theoretical results extend well-known transferability theorems for GNNs to the case of several simultaneous graphs (GtNNs) and provide a strict improvement on what is currently known even in the GNN case.
__label__online_learning Embracing the framework of prediction with expert advice, we maintain a set of experts for each type of functions and aggregate their predictions via a meta-algorithm.
__label__learning_theory We confirm the effectiveness of our results by experiments on the problems of non-negativity-constrained regression, density estimation, and intensity estimation.
__label__deep_learning_architectures Another challenge is the interpretability of LLMs, which is crucial for understanding how LLMs function.
__label__diffusion_based_models After that, *DenoiseRep* fuses the parameters of feature extraction and denoising layers, and *theoretically demonstrates* its equivalence before and after the fusion, thus making feature denoising computation-free.
__label__other We show that our convergence bound is immune to the extent of data heterogeneity, confirming the stability of the proposed algorithm against multi-level non-i.i.d.
__label__other We find that RUM substantially improves top-performing unlearning algorithms.
__label__safety_in_machine_learning This paper investigates this phenomenon through the lens of information theory, revealing a fundamental tradeoff between uncertainty and perception.
__label__deep_learning_architectures Outlier Features (OFs) are neurons whose activation magnitudes significantly exceed the average over a neural network's (NN) width.
__label__other This paper proposes a novel perspective inspired by neural collapse to solve the spurious correlation problem through the alternate execution of environment partitioning and learning semantic masks.
__label__machine_vision Therefore, a new Character Unidirectional Alignment Loss is proposed to correct this error and unify the representation of the same characters in all samples by aligning the character features in the student model with the reference features in the teacher model.
__label__graph_neural_networks Efficient computation of graph diffusion equations (GDEs), such as Personalized PageRank, Katz centrality, and the Heat kernel, is crucial for clustering, training neural networks, and many other graph-related problems.
__label__safety_in_machine_learning This paper addresses this gap by systematically analyzing the Rashomon effect and predictive multiplicity in gradient boosting algorithms.
__label__graph_neural_networks For instance, a money laundering transaction might involve an abnormal account and the broader community it interacts with.
__label__machine_vision 2) Utilizing a combination of visual cues and hierarchical text prompts, GPT-4V precisely identifies and aligns materials with the corresponding components of 3D objects.
__label__reinforcement_learning We propose a novel linear programming (LP) based primal-dual algorithm for convex MDPs that incorporates ``uncertainty'' parameters to improve data efficiency while requiring only partial data coverage assumption.
__label__online_learning Our algorithm overcomes the memory issue by reducing the problem to Bandit Convex Optimization (BCO) without memory and addresses general strongly-convex costs using recent advancements in BCO from \citep{suggala2024second}.
__label__reinforcement_learning In this paper, we introduce a dual policy approach, Diffusion Trusted Q-Learning (DTQL), which comprises a diffusion policy for pure behavior cloning and a practical one-step policy.
__label__active_learning We show that TVSAFEOPT compares favorably against SAFEOPT on synthetic data, both regarding safety and optimality.
__label__learning_theory We introduce a novel _compositional_ PAC-Bayes framework that provides a general recipe to analyze a broad spectrum of models including those with heterogeneous layers.
__label__bandits We use tools from discrete Fourier analysis to develop a sparse linear representation of the unit-specific reward $r_n: [\mathcal{A}]^N \rightarrow \mathbb{R} $, and propose simple, linear regression-based algorithms to minimize regret.
__label__deep_learning_architectures Extensive experiments on five benchmarks demonstrate that START outperforms existing SOTA DG methods with efficient linear complexity.
__label__machine_learning_for_other_sciences_and_fields Domain-specific languages (DSLs) have become integral to various software workflows.
__label__algorithmic_game_theory While Online Gradient Descent and other no-regret learning procedures are known to efficiently converge to a coarse correlated equilibrium in games where each agent's utility is concave in their own strategy, this is not the case when utilities are non-concave -- a common scenario in machine learning applications involving strategies parameterized by deep neural networks, or when agents' utilities are computed by neural networks, or both.
__label__machine_vision However, these prompts are continuous, which lack sufficient abstraction for task knowledge representation, making them less effective for continual learning.
__label__natural_language_processing Using insights from our theoretical results we then propose three interventions which improve the efficacy of debate.
__label__fairness This blurring of lines between machine- and human-written text presents new challenges in distinguishing one from the other – a task further complicated by the frequent updates and closed nature of leading proprietary LLMs.
__label__deep_learning_architectures The key idea to our approach is that we can view both Transformers and SSMs as applying different forms of mixing matrices over the token sequences.
__label__machine_learning_for_other_sciences_and_fields Such structural information can either be acquired by learning or specified based on users' prior knowledge.
"__label__interpretability_and_explainability In particular, LLMs are  capable of ""lying"", knowingly outputting false statements."
__label__machine_learning_for_other_sciences_and_fields In human-centric applications like healthcare and education, the \textit{heterogeneity} among patients and students necessitates personalized treatments and instructional interventions.
__label__learning_theory This paper aims to discuss the impact of random initialization of neural networks in the neural tangent kernel (NTK) theory, which is ignored by most recent works in the NTK theory.
__label__reinforcement_learning Policy Optimization (PO) methods are among the most popular Reinforcement Learning (RL) algorithms in practice.
__label__machine_vision The balance-aware query and optimized feature tokens are respectively taken as the Query and Key&Value of transformer decoder to perform joint object detection and instance segmentation.
__label__natural_language_processing Large language models (LLMs) have shown impressive capabilities in real-world applications.
__label__deep_learning_architectures Results suggest performance on par with or surpassing the current state-of-the-art, without hyperparameter optimizations or ``task search'' of any kind.
__label__diffusion_based_models We prove that the optimal diffusion model for the target domain integrates pre-trained diffusion models on the source domain with additional guidance from a domain classifier.
__label__diffusion_based_models We propose a plug-and-play framework for constructing efficient samplers for inverse problems, requiring only *pre-trained* diffusion or flow-matching models.
__label__generative_models While applicable across various scenes, these strategies may not always be ideal, as certain scenes could benefit more from specific viewpoints.
__label__machine_vision They lack sufficient mining of available WSIs, severely limiting WSI classification performance.
__label__learning_theory Kernel techniques are among the most influential approaches in data science and statistics.
__label__probabilistic_methods To this end, we propose a novel approach FasMe for Fast and Sample-efficient Meta Precision Matrix Learning, which first extracts meta-knowledge through a multi-task learning diagram.
__label__natural_language_processing We leverage it to synthesize 6 million math problems for pre-training our JiuZhang3.0 model.
__label__deep_learning_architectures In this paper, we propose QT-ViT models that improve the previous linear self-attention using quadratic Taylor expansion.
__label__safety_in_machine_learning This involves deriving the evidence lower bounds (ELBOs) for these distributions, approximating the likelihood using the ELBO, and calculating classification probabilities via Bayes' theorem.
__label__optimization We develop federated vanilla and entropy-regularized natural policy gradient (NPG) methods in the tabular setting under softmax parameterization, where gradient tracking is applied to estimate the global Q-function to mitigate the impact of imperfect information sharing.
__label__diffusion_based_models This paper introduces a novel algorithmic framework encompassing existing methods as special cases, unifying the study of training-free guidance into the analysis of an algorithm-agnostic design space.
__label__generative_models Specifically, we propose spatio-temporal compositional diffusion to precisely follow complex textual semantics by manipulating and composing the attention maps of denoising networks spatially and temporally.
__label__natural_language_processing Moreover, we demonstrate reliable information retrieval cannot be solved with scale, reversed tokens, or even naive bidirectional-attention training.
__label__reinforcement_learning It is beneficial but, with limited datasets, errors in the model and the issue of value overestimation among out-of-distribution states can worsen performance.
__label__optimization_for_deep_networks Camera-LiDAR fusion models significantly enhance perception performance in autonomous driving.
__label__machine_learning_for_other_sciences_and_fields Deep learning technologies have demonstrated remarkable performance in vulnerability detection.
__label__machine_learning_for_other_sciences_and_fields To tackle this problem, we propose a novel method, namely **Neur**al **K**rylov **It**era**t**ion (**NeurKItt**), for accelerating linear system solving.
__label__privacy Advances in generative models have made it possible for AI-generated text, code, and images to mirror human-generated content in many applications.
__label__graph_neural_networks However, their scalability is hindered by the need to process large numbers of subgraphs.
__label__natural_language_processing As their usage grows, concerns regarding their safety are rising, especially in maintaining harmless responses when faced with malicious instructions.
__label__machine_learning_for_physical_sciences To tackle this, some existing works learn symmetry in a data-driven manner, parameterizing and learning expected symmetry through data.
__label__learning_theory Our results establish a sharp condition that can distinguish between the small test error phase and the large test error regime, based on the signal-to-noise ratio in the data model.
__label__natural_language_processing Our code can be found at https://github.com/thunlp/InfLLM.
__label__deep_learning_architectures All source code are available at https://github.com/liuchongming1999/Dendritic-integration-inspired-CNN-NeurIPS-2024.
__label__graph_neural_networks Interestingly, and in a sharp departure from previous proposals, this operation on coarsened graphs is oriented, even when the original graph is undirected.
__label__machine_vision Existing image-based methods, such as LISA, struggle with video tasks due to the additional temporal dimension, which requires temporal dynamic understanding and consistent segmentation across frames.
__label__machine_learning_for_healthcare Diverging from traditional approaches based on pixel-level classification, our proposed method, named GraphMorph, focuses on branch-level features of tubular structures to achieve more topologically accurate predictions.
__label__privacy Existing algorithms for user-level DP SCO are impractical in many large-scale machine learning scenarios because: (i) they make restrictive assumptions on the smoothness parameter of the loss function and require the number of users to grow polynomially with the dimension of the parameter space; or (ii) they are prohibitively slow, requiring at least $(mn)^{3/2}$ gradient computations for smooth losses and $(mn)^3$ computations for non-smooth losses.
__label__diffusion_based_models To this end, we introduce AsCAN---a hybrid architecture, combining both convolutional and transformer blocks.
__label__natural_language_processing Fine-tuning with paraphrased data is a common approach to enhance knowledge injection, yet it faces two significant challenges: high computational costs due to repetitive external model usage and limited sample diversity.
__label__interpretability_and_explainability The ICL inference process of the attention layer aligns with the training procedure of its  dual model, generating token representation predictions that are equivalent to the dual model's test outputs.
__label__machine_vision Seams, distortions, wasted UV space, vertex-duplication, and varying resolution over the surface are the most prominent issues of the standard UV-based texturing of meshes.
__label__diffusion_based_models Additionally,  we introduce a prior noise injection method to improve the texture details in the generated image.
__label__fairness This induces the question: *what is the tradeoff between these objectives, and what are the characteristics of (multi-objective) optimal solutions?
__label__privacy In this work, we revisit the problem of DP ReLU regression in high-dimensional regimes.
__label__other Real-world machine learning systems often encounter model performance degradation due to distributional shifts in the underlying data generating process (DGP).
__label__optimization The best-known communication complexity among non-accelerated algorithms is achieved by DANE, a distributed proximal-point algorithm that solves local subproblems at each iteration and that can exploit second-order similarity among individual functions.
__label__generative_models We then identify training images with significant loss deviations after the unlearning process and label these as influential.
__label__machine_vision Moreover, we construct a comprehensive diffusion video dataset, called Diffusion Video Forensics (DVF), across a wide range of forgery videos.
__label__diffusion_based_models Benefiting from the inherent ability of the LLMs and our innovative designs, the prompt understanding performance of  LI-DIT easily surpasses state-of-the-art open-source models as well as mainstream closed-source commercial models including Stable Diffusion 3, DALL-E 3, and Midjourney V6.
__label__machine_vision We also demonstrate that such a trained diffusion model can even convert an irrelevant instance into a relevant one, yielding highly effective synthetic data for training.
__label__probabilistic_methods On a variety of recent high dimensional benchmark tasks in control and molecular design, our approach significantly outperforms standard SVGPs and is capable of achieving comparable rewards with up to $10\times$ fewer function evaluations.
__label__reinforcement_learning Learning world models offers a promising avenue for goal-conditioned reinforcement learning with sparse rewards.
__label__other However, CL models typically face a trade-off between preserving old task knowledge and excelling in new task performance.
__label__privacy By incorporating state-of-the-art smoothing techniques, we show that the framework also provides provable guarantees regarding the false positive rate for misclassifying a watermarked image, even in the presence of adversarial attacks targeting watermark removal.
__label__generative_models We showcase that L4GM that is only trained on synthetic data generalizes well on in-the-wild videos, producing high quality animated 3D assets.
__label__online_learning A long-standing open question in COCO is whether an online policy can simultaneously achieve $O(\sqrt{T})$ regret and $\tilde{O}(\sqrt{T})$ CCV without any restrictive assumptions.
__label__reinforcement_learning We further provide tight bounds for the ratio given the worst-case dynamics.
__label__algorithmic_game_theory Our modular framework can be combined with any existing regret minimizer over cones to compute a Nash equilibrium in two-player zero-sum EFGs with perfect recall, through the self-play framework.
__label__diffusion_based_models As such, adapters have been widely adopted by open-source communities, accumulating a database of over 100K adapters—most of which are highly customized with insufficient descriptions.
__label__safety_in_machine_learning Scaling machine learning models significantly improves their performance.
__label__reinforcement_learning In this paper, we propose a novel policy optimization framework that maximizes Return on Investment (ROI) of a policy using a fixed dataset within a Markov Decision Process (MDP) equipped with a cost function.
__label__speech_and_audio In this study, we introduce a novel model framework TransVIP that leverages diverse datasets in a cascade fashion yet facilitates end-to-end inference through joint probability.
__label__machine_learning_for_other_sciences_and_fields However, real-world scientific applications often have limited data and complex landscapes, making data-hungry models inefficient or impractical.
__label__diffusion_based_models To alleviate this problem, we employ the flow matching framework for simulation-free training of NODEs, which directly regresses the parameterized dynamics function to a predefined target velocity field.
__label__interpretability_and_explainability The inner workings of neural networks can be better understood if we can fully decipher the information encoded in neural activations.
__label__causal_inference To address this issue, we propose a novel Covariate Shift Corrected Pearson Chi-squared Conditional Randomization (csPCR) test.
__label__safety_in_machine_learning Although the recently proposed gray-box attacks have improved practicality, they suffer from semantic loss during the training process, which limits their targeted attack performance.
__label__interpretability_and_explainability We analyze a mechanism used in two LMs to selectively inhibit items in a context in one task, and find that it underlies a commonly used abstraction across many context-retrieval behaviors.
__label__learning_theory We propose a simple  proper learning algorithm, the Perspectron, that has sample complexity $\widetilde{O}((\epsilon\gamma)^{-2})$ and achieves classification error at most $\eta+\epsilon$ where $\eta$ is the Massart noise rate.
__label__machine_vision To address this limitation, we introduce a novel adaptation framework, AWT (Augment, Weight, then Transport).
__label__natural_language_processing Our code can be found at https://github.com/ShiLuohe/ReferenceTrustableDecoding.
__label__interpretability_and_explainability engaged in social and technical processes.
__label__robotics We present various experiments across tasks and robot configurations, providing built plans and training results.
__label__deep_learning_architectures We then use a hash algorithm to retrieve a correlated subset of vectors dynamically based on the input embedding.
__label__machine_vision The project page of this work is https://andyzaq.github.io/GF-SAM/.
__label__natural_language_processing Evident suggests that modules carrying knowledge in a Transformer module are primarily the MLP blocks, thus we propose \textbf{iReVa}, a method that explicitly initializes and retrofits key-value pairs into MLP blocks to construct a new mapping of a piece of knowledge without damaging the irrelevant knowledge.
__label__online_learning Finally, if the action sets are unconstrained, we demonstrate that it can be simply extended to achieve an $O(n\sqrt{T\log T}+d\log T)$ regret bound for strongly convex and smooth functions.
__label__machine_vision Contrastive Language-Image Pre-training (CLIP) achieves impressive performance on tasks like image classification and image-text retrieval by learning on large-scale image-text datasets.
"__label__deep_learning_architectures We demonstrate APM’s ability to recognize out-of-distribution images without dataset-specific pre-training, augmentation or
any-pretext task."
__label__reinforcement_learning In popular MuJoCo and DeepMind Control Suite (DMC) environments, we find common phenomena for TD3 and RAD agents: (1) the activity of policy network parameters is highly asymmetric and policy networks advance monotonically along a very limited number of major parameter directions; (2) severe detours occur in parameter update and harmonic-like changes are observed for all minor parameter directions.
__label__natural_language_processing The model also effectively aligns unseen objectives, marking the first step towards generalizable multi-objective preference alignment.
__label__optimization_for_deep_networks Particularly SEvo-enhanced AdamW with moment estimate correction demonstrates consistent improvements across a spectrum of models and datasets, suggesting a novel technical route to effectively utilize graph structural information beyond explicit GNN modules.
__label__deep_learning_architectures It has recently shown impressive efficiency in dealing with high-resolution inputs across various vision tasks.
__label__machine_learning_for_healthcare Then patients are encoded from these two distinct views.
__label__learning_theory Transformers have achieved great success in recent years.
__label__reinforcement_learning The option framework in hierarchical reinforcement learning has notably advanced the automatic discovery of temporally-extended actions from long-horizon tasks.
__label__optimization Under the additional assumption of Lipschitz continuous gradients, we further design a parameter-free version by tracking the Hessian Lipschitz constant locally and ensuring the iterates remain bounded.
__label__machine_vision Recently, 3D Gaussian Splatting (3DGS) has garnered researchers' attention due to their outstanding rendering quality and real-time speed.
__label__generative_models Finally, we discuss how our method's success suggests that other strong guarantees can be obtained on LLM performance via modifying the input representations.
__label__machine_learning_for_social_sciences In this study, we investigate the under-explored intervention planning aimed at disseminating accurate information within dynamic opinion networks by leveraging learning strategies.
__label__diffusion_based_models Image editing serves as a practical yet challenging task considering the diverse demands from users, where one of the hardest parts is to precisely describe how the edited image should look like.
__label__machine_vision Based on this finding, we propose a Depth-Aware Hybrid Feature Fusion (DH-Fusion) strategy that guides the weights of point cloud and RGB image modalities by introducing depth encoding at both global and local levels.
__label__machine_vision These four types of minimal changes are specifically designed to test the models' understanding of objects, attributes of objects (such as color, material, shape), counts of objects, and spatial relationships between objects.
__label__machine_vision The traditional approach of pixel-level alignment is ineffective for diffusion-processed frames because of iterative disruptions.
__label__machine_vision Accurately estimating depth in 360-degree imagery is crucial for virtual reality, autonomous navigation, and immersive media applications.
__label__deep_learning_architectures Dense linear layers are the dominant computational bottleneck in large neural networks, presenting a critical need for more efficient alternatives.
__label__interpretability_and_explainability The proposed approach is based on sound mathematical theory which describes the internal layers of a Transformer architecture as sequential deformations of the input manifold.
__label__infrastructure Pipeline parallelism has been widely explored, but most existing schedules lack a systematic methodology.
__label__natural_language_processing However, existing methods only focus on utilizing this naturally formed activation sparsity in a post-training setting, overlooking the potential for further amplifying this inherent sparsity.
__label__generative_models We derive *BonBon Alignment* as a method for achieving this.
__label__machine_vision Existing methods in this research area have predominantly centered on pinpointing the gaze target by predicting a gaze heatmap or gaze point.
__label__diffusion_based_models Additionally, we present a query blurring method that is equivalent to blurring the entire attention weights without incurring quadratic complexity in the number of tokens.
__label__reinforcement_learning This problem has been extensively studied in both supervised learning and off-policy reinforcement learning (RL), where a number of remedies have been proposed.
__label__bandits We also show that in some cases, our approach efficiently leverages the semi-bandit feedback and outperforms bandit feedback approaches, not only in exponential regimes where $P\gg d$ but also when $P\leq d$, which is not covered by existing analyses.
__label__speech_and_audio To make the visual scene model audio adaptive, we propose a point densification and pruning strategy to optimally distribute the Gaussian points, with the per-point contribution in sound propagation (e.g., more points needed for texture-less wall surfaces as they affect sound path diversion).
__label__machine_vision In this paper, we develop a Multi-view Masked Contrastive Representation Learning (M$^2$CRL) framework for endoscopic video pre-training.
__label__natural_language_processing We run extensive experiments on benchmarks concerning truthfulness and bias with six open-source LLMs of different sizes and model families.
__label__reinforcement_learning Training a probabilistic model conditioned on observed return to predict action can fail to generalize, as the trajectories that achieve a return in the dataset might have done so due to a suboptimal behavior adversary.
__label__reinforcement_learning Yet, it poses challenges due to substantial parameter size and limited scalability, which is particularly critical in sequential decision-making scenarios where resources are constrained such as in robots and drones with limited computational power.
__label__machine_learning_for_healthcare Predicting the effects of protein mutations is crucial for analyzing protein functions and understanding genetic diseases.
__label__machine_learning_for_physical_sciences However, the numerical analysis of FDEs has faced challenges due to its unrealistic computational costs and has been a long standing problem over decades.
__label__fairness We prove that high-degree test nodes tend to have a lower probability of misclassification regardless of how GNNs are trained.
__label__reinforcement_learning In this work, we propose a simple yet effective offline adaptation framework for multi-objective RL problems without assuming handcrafted target preferences, but only given several demonstrations to implicitly indicate the preferences of expected policies.
__label__optimization In this work, we propose Multi-Stage Predict+Optimize, a novel extension catering to applications where unknown parameters are revealed in sequential stages, with optimization decisions made in between.
__label__safety_in_machine_learning To bridge this gap, we propose a novel analysis that analyzes the transferability of the representations of pre-trained models to downstream tasks in terms of their relatedness to a given reference task.
__label__reinforcement_learning In most existing studies, specific preferences must be provided during deployment to indicate the desired policies explicitly.
__label__probabilistic_methods We study Treeffuser on synthetic and real data and show that it outperforms existing methods, providing better calibrated probabilistic predictions.
__label__machine_vision Extensive experiments demonstrate the superiority of OVM3D-Det over baselines in both indoor and outdoor scenarios.
__label__learning_theory We introduce a new approach to the mixture of experts model that consists in imposing local differential privacy on the gating mechanism.
__label__causal_inference For another, we can capture important aspects of causality using compatibility: we can use compatibility to understand cyclic causal graphs, and to demonstrate structural compatibility, we must essentially produce a causal model.
__label__natural_language_processing However, existing methods that integrate spatial layouts with text have limitations, such as producing overly long text sequences or failing to fully leverage the autoregressive traits of LLMs.
__label__machine_learning_for_social_sciences This step enables representation fusion to adapt to local data distribution.
__label__online_learning For smooth and strongly convex time-varying functions, we establish an $\mathcal{O}((\sigma_{\max}^2 + \Sigma_{\max}^2)\log(\sigma_{1:T}^2 + \Sigma_{1:T}^2))$ regret bound, where $\sigma_{\max}^2$ and $\Sigma_{\max}^2$ denote the maximal stochastic variance and the maximal adversarial variation, respectively.
__label__probabilistic_methods Differential equations are important mechanistic models that are integral to many scientific and engineering applications.
__label__machine_vision We introduce a Prompt Optimization Prompt that not only guides LLMs in creating effective prompts but also stores past prompts with their performance metrics, providing rich in-context information.
__label__generative_models Experimental results demonstrate that ours achieves a significantly faster rendering speed (×100) compared to state-of-the-art 3D consistent GANs with comparable 3D generation capability.
__label__deep_learning_architectures Experimental evaluations conducted on various synthetic and real-world noisy datasets demonstrate significant improvements over existing transition matrix-based methods.
__label__machine_learning_for_other_sciences_and_fields However, current tabular generative methods that learn either the joint distribution $ p(\mathbf{x}, y) $ or the class-conditional distribution $ p(\mathbf{x} \mid y) $ often overfit on small datasets, resulting in poor-quality synthetic data, usually worsening classification performance compared to using real data alone.
__label__safety_in_machine_learning Hence, prior SoTA methods either approximate this $Q^*$ using $Q^{\pi_{\text{sft}}}$ (derived from the reference $\texttt{SFT}$ model) or rely on short-term rewards, resulting in sub-optimal decoding performance.
__label__machine_vision Traditional UDF learning methods typically require extensive training on large datasets of 3D shapes, which is costly and often necessitates hyperparameter adjustments for new datasets.
__label__online_learning In this paper, we illustrate that this gap can be filled in the worst case, where $\bar{d}$ is very close to the maximum delay $d$.
__label__optimization_for_deep_networks Our results are well supported by experiments illustrating the phenomenon beyond linear networks and regression tasks.
__label__safety_in_machine_learning This underscores the importance of utilizing multiple modalities for OOD detection.
__label__machine_vision Many prior methods either support only specific categories of images, such as portraits, or require special capture conditions, like using a flashlight.
__label__evaluation Meanwhile, further analyses also indicate that CoPA can learn better representation clusters, enlarge the gap, and achieve the minimum validation loss at the enlarged gap.
__label__machine_vision We argue that the most critical component of a scene synthesis system is to accurately establish the size and position of various objects within a restricted area.
__label__machine_learning_for_healthcare It achieves significant performance improvements over both hand-crafted and automated state-of-the-art methods, also maintains a feasible search cost at the same time.
__label__graph_neural_networks This observation motivates us to propose a novel algorithm named $\textit{Similarity-Navigated Adaptive Prediction Sets}$ (SNAPS), which aggregates the non-conformity scores based on feature similarity and structural neighborhood.
__label__fairness Even when the Pareto front is polynomial, our algorithm may take exponential time, and we prove that this is inevitable unless P = NP.
__label__machine_vision However, existing works struggle to flexibly adapt to diverse user-specific needs in data grouping, which may require manual understanding of each clustering.
__label__other However, WTA for supervised STDP classification faces unbalanced competition challenges.
__label__diffusion_based_models Through a series of experiments on standard molecule generation benchmarks, we demonstrate the competitive performance of END compared to several strong baselines for both unconditional and conditional generation.
__label__generative_models Thanks to their simple architecture, Restricted Boltzmann Machines (RBMs) are powerful tools for modeling complex systems and extracting interpretable insights from data.
__label__machine_vision To address this problem, we consider the iterative curve-adjustment update process as a dynamic system and formulate it as a Neural Ordinary Differential Equations (NODE) for the first time, and this allows us to learn a continuous dynamics of the latent image.
__label__machine_vision To boost the summarizing process, we theoretically demonstrate the derivation error in the previous character contrastive loss, which mistakenly causes the sparsity in the intra-class distribution and exacerbates ambiguity on challenging samples.
__label__safety_in_machine_learning We find that all but the most powerful LLM agents fail to achieve a sustainable equilibrium in GovSim, with the highest survival rate below 54%.
__label__machine_learning_for_other_sciences_and_fields The robustness of the proposed framework is also highlighted through an ablation study.
__label__other However, the increasing reliance on pretrained models for molecular tasks renders traditional in-domain DP methods incompatible.
__label__deep_learning_architectures This study confirms that the proposed method offers a promising solution to address the spectral bias of DNNs.
__label__machine_learning_for_healthcare Concurrently, Large Language Models (LLMs) have emerged as adept automated planners across domains of robotic control and navigation.
__label__safety_in_machine_learning Notably, PerpCorrect demonstrates practical utility by requiring only a modest amount of validation data and being compatible with various alignment techniques.
__label__natural_language_processing Extensive evaluation on language understanding, language generation, and instruction tuning tasks show that LTE consistently outperforms SOTA baselines.
__label__natural_language_processing We also present the ChatRAG Bench, which encompasses ten datasets covering comprehensive evaluations on RAG, table-related QA, arithmetic calculations, and scenarios involving unanswerable questions.
__label__safety_in_machine_learning Our results reveal that, across four different pixel-level watermarking schemes, the proposed method consistently achieves superior performance compared to existing attack techniques, with lower detection rates and higher image quality.
__label__generative_models Despite being simple and effective, this method results in sub-optimal cross-modal alignment by over-emphasizing the text tokens that are less correlated with or even contradictory with the input images.
__label__neuroscience_and_cognitive_science An example of such dynamics is the formation of sequences, a ubiquitous motif in neural activity.
__label__reinforcement_learning In addition, we build a superiority-based DRL algorithm.
__label__machine_vision Simply optimizing the appearance as prior methods do is often insufficient for modeling continuous textures in the given reference image.
__label__machine_vision Its overall idea is to separately tackle the content and style information by frequency tokens throughout the learning process.
__label__learning_theory In academic research, proper citation acknowledges prior work and establishes original contributions.
__label__reinforcement_learning We apply the proposed method to federated temporal difference learning with linear function approximation and analyze the corresponding complexity improvements.
__label__diffusion_based_models Despite rigorous filtering, these pre-training datasets often inevitably contain corrupted pairs where conditions do not accurately describe the data.
__label__generative_models However, these works mainly focus on confined categories or synthetic 3D assets, which are discouraged from generalizing to challenging in-the-wild scenes and fail to be employed with 2D synthesis directly.
__label__deep_learning_architectures SOFTS achieves superior performance over existing state-of-the-art methods with only linear complexity.
__label__machine_vision To achieve this, we propose CineGPT, a GPT-based autoregressive model for text-conditioned camera trajectory generation.
__label__causal_inference First, we bridge the gap with existing identifiability theory and show that constraints on the training data distribution implicitly define a prior on the test observations.
__label__machine_learning_for_other_sciences_and_fields Additionally, to address the inefficiency of matrix transformations due to the vast discrete space, we use semantic labels derived from quantization or RQ-VAE to replace item IDs, enhancing efficiency and improving cold start issues.
__label__natural_language_processing In this paper, we propose to study emergent abilities in the lens of pre-training loss, instead of model size or training compute.
__label__robotics HPTs outperform several baselines and enhance the fine-tuned policy performance by over 20% on unseen tasks in multiple simulator benchmarks and real-world settings.
__label__deep_learning_architectures These $\(\mathbf{\Delta W}\)$ matrices are structured for efficient parameterization, often using techniques like low-rank approximations or scaling vectors.
__label__generative_models The rapid progress in generative models has resulted in impressive leaps in generation quality, blurring the lines between synthetic and real data.
__label__deep_learning_architectures To address this important problem, we propose in this work to use MSHAs and Convs in parallel \textbf{at different granularity levels} instead.
__label__human-AI_interaction Recognizing the complex, non-linear nature of human learning, we introduce a skill-aware attention mechanism to dynamically integrate players’ strengths with encoded chess positions, enabling our model to be sensitive to evolving player skill.
__label__privacy To address this challenge, we introduce Federated Behavioural Planes (FBPs), a novel method to analyse, visualise, and explain the dynamics of FL systems, showing how clients behave under two different lenses: predictive performance (error behavioural space) and decision-making processes (counterfactual behavioural space).
__label__machine_vision Detection based approaches have until now largely been discarded for HTR because reading characters separately is often challenging, and character-level annotation is difficult and expensive.
__label__evaluation In summary, while LLMs exhibit the ability to memorize common patterns of popular DS API usage through massive training, they overall lack genuine comprehension of the underlying numerical constraints.
__label__machine_learning_for_other_sciences_and_fields Our approach offers a principled, scalable, and incremental roadmap for the gradual integration of self-trainable analog computational primitives into existing digital accelerators.
__label__diffusion_based_models The project page is available at https://lonelvino.github.io/SYE/.
__label__learning_theory In both cases, we find that experts formulated as feed-forward networks with commonly used activation such as $\mathrm{ReLU}$ and $\mathrm{GELU}$ enjoy faster convergence rates under the sigmoid gating than those under softmax gating.
__label__optimization We propose a novel view in which these performative effects are modelled as push forward measures.
__label__machine_learning_for_other_sciences_and_fields The impressive improvements on both few-shot streaming data and new domain tasks against existing SOAT methods are achieved.
__label__fairness However, we also present a new polynomial-time algorithm for computing the entire Pareto front when the cluster centers are fixed, and for perhaps the most natural fairness objective: minimizing the sum, over all clusters, of the imbalance between the two groups in each cluster.
__label__machine_learning_for_other_sciences_and_fields The collected data in recommender systems generally suffers selection bias.
__label__machine_vision The development of neural rendering presents a new solution to this field to learn 3D dynamics from 2D images by inverse rendering.
__label__optimization First-order optimization (FOO) algorithms are pivotal in numerous computational domains, such as reinforcement learning and deep learning.
__label__safety_in_machine_learning Recent research has highlighted the importance of data quality in scaling large language models (LLMs).
__label__probabilistic_methods To mitigate this limitation, a non-stationary PGDS is proposed to allow the underlying transition matrices to evolve over time, and the evolving transition matrices are modeled by the specifically-designed Dirichlet Markov chains.
__label__machine_vision However, existing visual object tracking algorithms still fall short of matching human performance in maintaining tracking over time, particularly in complex scenarios requiring robust visual search skills.
__label__machine_learning_for_social_sciences To facilitate our study, we collect a large dataset of questions from competitive forecasting platforms.
__label__machine_vision To enable the learning of SpelsNet with the proposed point-to-BRep adjacency supervision, we extend two existing CAD datasets with the required annotations, and conduct a thorough experimental validation on them.
__label__diffusion_based_models To overcome these limitations, this work introduces a new family of  $\textit{Factor Graph Diffusion Models}$ (FG-DMs) that models the joint distribution of images and conditioning variables, such as semantic, sketch, depth or normal maps via a factor graph decomposition.
__label__neuroscience_and_cognitive_science Traditional methods often require customized models and extensive trials, lacking interpretability in visual reconstruction tasks.
__label__other With our curated ablation studies and theoretical analyses, we discover that (i) MP improves the CF performance primarily by additional representations passed from neighbors during the forward pass instead of additional gradient updates to neighbor representations during the model back-propagation and (ii) MP usually helps low-degree nodes more than high-degree nodes.
__label__fairness ConBias represents visual datasets as knowledge graphs of concepts, enabling meticulous analysis of spurious concept co-occurrences to uncover concept imbalances across the whole dataset.
__label__interpretability_and_explainability Crucially, this differs from standard methods based on independence or holding other predictors constant, such as regression coefficients or Partial Dependence Plots (PDPs).
__label__natural_language_processing Where the updated knowledge resides in memories is a fundamental question for model editing.
__label__evaluation Our code is available at https://github.com/jiayuww/SpatialEval.
__label__natural_language_processing We provide a new angle, reformulating inverse soft-Q-learning as a temporal difference regularized extension of MLE.
__label__neuroscience_and_cognitive_science However, popular methods often pre-train temporal models based on brain-level tokens, overlooking that brain activities in different regions are highly desynchronized during tasks.
__label__generative_models The Schrödinger Bridge (SB) problem offers a powerful framework for combining optimal transport and diffusion models.
__label__optimization_for_deep_networks Here, key-query, as well as value-projection parameter matrices, are multiplied directly with each other: $W_K^TW_Q$ and $PW_V$.
__label__machine_vision Prior multi-frame optical flow methods typically estimate flow repeatedly in a pair-wise manner, leading to significant computational redundancy.
__label__machine_vision Moreover, fine-tuning with image-text pairs auto-collected via SELMA shows comparable performance to fine-tuning with ground truth data.
__label__machine_vision Videos are a potential source for real-world 3D data, but finding diverse yet corresponding views of the same content have shown to be difficult at scale.
__label__optimization Our theoretical findings are illustrated by numerical experiments on synthetic tasks.
__label__optimization_for_deep_networks Connections in biological neural networks are sparse, as they only exist between few neurons.
__label__optimization_for_deep_networks In this study, we introduce a framework for solving bilevel optimization problems, where the variables in both the lower and upper levels are constrained on Riemannian manifolds.
__label__machine_vision In this paper, we tackle the challenge of generating high-quality hash codes for cross-modal retrieval in the presence of incomplete labels, which creates uncertainty in distinguishing between positive and negative pairs.
__label__causal_inference We show that LLMs and CDs are mutually beneficial and the constructed feedback provably also helps with the factor proposal.
__label__learning_theory Specifically, we show that the improvement in performance achieved by strong models over their weaker counterparts is quantified by the *misfit error* incurred by the strong model on labels generated by the weaker model.
__label__generative_models Specifically, we enhance the source image conditioned editability of a personalized diffusion model via a novel Editability Driven Score Distillation (EDSD) objective.
__label__machine_vision Moreover, to mitigate the inherent lack of smoothness in explicit representation methods, we introduce a smooth regularization term that keeps our model from the chaos of deformation prediction.
__label__graph_neural_networks Specially, TFGDA employs a structure alignment strategy named STSA to encode graphs' topological structure information into the latent space, greatly facilitating the learning of transferable features.
__label__machine_vision Nevertheless, GCD presents unique challenges, particularly the absence of priors for new classes, which can lead to the teacher's misguidance and unsynchronized learning with the student, culminating in suboptimal outcomes.
__label__machine_vision However, image pyramids process multiple resolutions of images using the same large-scale model, which requires significant computational cost.
__label__probabilistic_methods For practical use in protein engineering, it is important that we can also provide reliable uncertainty estimates for our predictions, and while prediction accuracy has seen much progress in recent years, uncertainty metrics are rarely reported.
__label__privacy Given that training a 3DGS requires a significant amount of time and computational cost, it is crucial to protect the copyright, integrity, and privacy of such 3D assets.
__label__machine_learning_for_other_sciences_and_fields Vina Score, while maintaining strong molecular properties.
__label__machine_vision Our analysis suggests that this underperformance is partially due to generator artifacts and inaccurate task-relevant visual details in the synthetic images.
__label__machine_vision In this paper, we propose an efficient spatio-temporal interactive reconstruction network to jointly perform inter-frame feature alignment and intra-frame feature filtering in a coarse-to-fine manner.
__label__infrastructure To be specific, we find that mainstream quantization methods would prevent the base LLM from being shared among tasks, so current LLM serving systems are infeasible to integrate LLM quantization with multiple LoRA adapters to achieve memory-efficient multi-task serving.
__label__deep_learning_architectures We show that Transformers, existing SSMs such as Mamba, and hybrid architectures such as Jamba are special cases of B'MOJO and describe a basic implementation, to be open sourced,  that can be stacked and scaled efficiently in hardware.
__label__interpretability_and_explainability We experimentally compare our method to an existing method known as LieGAN and show that our method is competitive at detecting affine symmetries for large sample sizes and superior than LieGAN for small sample sizes.
__label__learning_theory Finally, we validate our theory in deep neural networks trained on image classification.
__label__machine_vision Experimental results on synthetic and our collected real-world dataset demonstrate that SfPUEL significantly outperforms existing SfP and single-shot normal estimation methods.
__label__machine_vision First, our method employs masked autoencoders to impute hand trajectories by leveraging the spatiotemporal correlations between the head pose sequence and intermittent hand poses, providing uncertainty estimates.
__label__natural_language_processing As a step towards answering this question, we study inductive out-of-context reasoning (OOCR), a type of generalization in which LLMs infer latent information from evidence distributed across training documents and apply it to downstream tasks without in-context learning.
__label__learning_theory We start by studying the asymptotic quantiles of the excess risk of sequences of empirical risk minimizers.
__label__optimization_for_deep_networks $($FG$)^2$U circumvents the memory and approximation issues associated with classical bi-level optimizaiton approaches, and delivers significantly more accurate gradient estimates than existing large-scale bi-level optimizaiton approaches.
__label__deep_learning_architectures Federated learning (FL), through its privacy-preserving collaborative learning approach, has significantly empowered decentralized devices.
__label__other The results validated the superiority of VAP over existing LLMs-based reasoning frameworks.
__label__natural_language_processing To this end, in this paper, we propose a novel Tackling uncertain correspondences method for Multi-modal Entity Alignment (TMEA).
__label__safety_in_machine_learning Under the new assumption that the human's partial observability is known and accounted for, we then analyze how much information the feedback process provides about the return function.
__label__machine_vision We showcase the improved location consistency of our trained feature extractor directly on a multi-view consistency task, as well as the downstream task of scene-stable panoptic segmentation, significantly outperforming previous state-of-the-art.
__label__diffusion_based_models To address these limitations, we introduce Neural Flow Diffusion Models (NFDM), a novel framework that enhances diffusion models by supporting a broader range of forward processes beyond the standard Gaussian.
__label__infrastructure Federated learning (FL) has emerged as a prominent machine learning paradigm in edge computing environments, enabling edge devices to collaboratively optimize a global model without sharing their private data.
__label__diffusion_based_models Ultra-high-resolution image generation poses great challenges, such as increased semantic planning complexity and detail synthesis difficulties, alongside substantial training resource demands.
__label__reinforcement_learning By identifying and solving the previously unrecognized problem of forgoing reward in early episodes, First-Explore represents a significant step towards developing meta-RL algorithms capable of human-like exploration on a broader range of domains.
__label__interpretability_and_explainability This includes the dynamic integration of information among tokens and the static integration of information within tokens in Transformers, as well as the presence of conjunctive errors therein.
__label__machine_vision Is this actually true?
__label__machine_vision Although synthetic face images can partially mitigate potential legal risks while maintaining effective face recognition (FR) performance, FR models trained by face images synthesized by existing generative approaches frequently suffer from performance degradation problems due to the insufficient discriminative quality of these synthesized samples.
__label__reinforcement_learning Project page: https://github.com/Alexander-Yao/Swift-Sampler.
__label__optimization_for_deep_networks Comparative experiments of PiSSA and LoRA across 11 different models, ranging from 184M to 70B, encompassing 5 NLG and 8 NLU tasks, reveal that PiSSA consistently outperforms LoRA under identical experimental setups.
__label__machine_learning_for_other_sciences_and_fields We demonstrate the effectiveness of our approach through theoretical and empirical analysis, highlighting its potential to reason about model and prediction uncertainty due to data quality issues in training data.
__label__causal_inference Accurate estimation of treatment effects is essential for decision-making across various scientific fields.
__label__machine_learning_for_other_sciences_and_fields It uses versatile representations based on geometric algebra and is equivariant with respect to E(3), the symmetry group of the underlying physics.
__label__machine_learning_for_physical_sciences This paper introduces a new Lagrangian-Eulerian combined paradigm to tackle the tanglesome fluid dynamics.
__label__privacy We instantiate ARS on deep image classification to certify predictions against adversarial examples of bounded $L_{\infty}$ norm.
__label__human-AI_interaction Cognitive science has proposed several models that capture these intricacies but, due to their intractable nature, work on preference learning has, in practice, had to rely on tractable but simplified variants of the well-known Bradley-Terry model.
__label__machine_vision The two components of SSDiff can perform favorably against the APFM when utilizing a LoRA-like branch-wise alternative fine-tuning method.
__label__machine_vision SNA outperforms alternative local attention modules on image denoising, and we compare the superpixels learned from denoising with those learned with supervision.
__label__reinforcement_learning In Deep Reinforcement Learning (RL), it is a challenge to learn representations that do not exhibit catastrophic forgetting or interference in non-stationary environments.
__label__bandits In this context, each arm is characterized by a bounded support reward distribution and strategically aims to maximize its own utility by retaining a portion of the observed reward, potentially disclosing only a fraction of it to the player.
__label__other Offboard perception aims to automatically generate high-quality 3D labels for autonomous driving (AD) scenes.
__label__optimization Our algorithms achieve provable approximation guarantees and we demonstrate in several experiments that the (approximate) Pareto set contains good clusterings that cannot be found by considering one of the objectives separately.
__label__causal_inference As many practical fields transition to provide personalized decisions, data is increasingly relevant to support the evaluation of candidate plans and policies (e.g., guidelines for the treatment of disease, government directives, etc.).
__label__generative_models At the spatial level, we disentangle the computations of visible and [MASK] tokens by encoding visible tokens independently, while decoding [MASK] tokens conditioned on the fully encoded visible tokens.
"__label__online_learning We then turn to the online learning problem, 
both in the stochastic and adversarial settings."
__label__natural_language_processing For continual editing, we devise a knowledge-sharding mechanism where different sets of edits reside in distinct subspaces of parameters, and are subsequently merged into a shared memory without conflicts.
__label__machine_vision Code and models are available at \url{https://dk-liang.github.io/UniSeg3D/}.
__label__generative_models A broad range of experiments demonstrate the effectiveness and merits of our algorithms.
__label__safety_in_machine_learning Moreover, building upon the theoretical analysis, we propose TPA, a Theoretically Provable Attack that optimizes a surrogate of the derived bound to craft adversarial examples.
__label__machine_vision The fundamental problem in VICL is how to select the best prompt to activate its power as much as possible, which is equivalent to the ranking problem to test the in-context behavior of each candidate in the alternative set and select the best one.
__label__reinforcement_learning A matching-based policy network then predicts actions from a few demonstrations, producing an adaptive policy that is robust to over-fitting.
__label__learning_theory We show that when trained from scratch, the training process can be split into an initial sample-intensive stage where the correlation is boosted from zero to a nontrivial value, followed by a more sample-efficient stage of further improvement.
__label__natural_language_processing Then, the self-verification stage only requires computing the remaining layers over the \emph{early-exited} hidden states in parallel.
__label__machine_vision We also analyze common open-set noise detection mechanisms based on prediction entropy values.
__label__diffusion_based_models Our approach introduces a common action space, which is a textual embedding space focused solely on actions, enabling precise customization without actor-related details.
__label__machine_learning_for_other_sciences_and_fields Within this domain, cognitive diagnosis (CD) is a key research focus that aims to diagnose students' proficiency levels in specific knowledge concepts.
__label__safety_in_machine_learning One way to elicit capabilities more robustly is to fine-tune the LLM to complete the task.
__label__deep_learning_architectures Based on the velocity and the occupancy score map obtained from the MFE module, the MGTF module aligns and fuses feature maps across multiple timestamps in a recurrent manner.
__label__causal_inference Specifically, ${\cal O}((\log \frac{1}{\delta})^{4})$ samples suffice for latent graph recovery up to ancestors with probability $1 - \delta$, and ${\cal O}((\frac{1}{\epsilon}\log \frac{1}{\delta})^{4})$ samples suffice for latent causal variables recovery that is $\epsilon$ close to the identifiability class with probability $1 - \delta$.
__label__machine_learning_for_other_sciences_and_fields In particular, it utilizes the understanding capabilities of LLMs to annotate the financial meaning of variables in smart contracts, and employs rule-based reasoning to propagate the information throughout a contract's logic and to validate potential vulnerabilities.
"__label__learning_theory We show that
the global minimizer of the regularized loss of DNNs can fit for example
the composition of two functions $f^{*}=h\circ g$ from a small number
of observations, assuming $g$ is smooth/regular and reduces the dimensionality
(e.g."
__label__neuroscience_and_cognitive_science Additionally, the inherent instability of sEMG signals, combined with their sensitivity to distribution shifts in real-world settings, compromises model robustness.
__label__generative_models Specifically, we present by contrasting image inputs, the difference in prediction logits on each text token provides strong guidance of visual correlation.
__label__human-AI_interaction Our results demonstrate that introducing language grounding not only maintains task performance but also accelerates the emergence of communication.
__label__graph_neural_networks Additionally, it finds data with high confidence of being clustered into the same group as the generated data to serve as their neighbors, thereby enriching the neighborhoods of graphs.
__label__interpretability_and_explainability And (2) the distributions of optimized models follow a convergence trend to their shared population mean as the capacity increases.
__label__machine_learning_for_physical_sciences Many problems in physical sciences are characterized by the prediction of space-time sequences.
__label__natural_language_processing Despite the recent observation that large language models (LLMs) can store substantial factual knowledge, there is a limited understanding of the mechanisms of how they acquire factual knowledge through pretraining.
__label__learning_theory The era of proliferation of large language and image generation models begs the question of what happens if models are trained on the synthesized outputs of other models.
__label__machine_vision Our FleVRS represents a significant step towards a more intuitive, comprehensive, and scalable understanding of visual relationships.
__label__machine_learning_for_physical_sciences Here, we argue that solving both problems within the same framework can lead to consistent gains in accuracy and robustness.
__label__evaluation We evaluated both state-of-the-art open-source and closed-source models.
__label__machine_learning_for_other_sciences_and_fields To this end, we introduce a monotonic data augmentation framework, CMCD, to tackle the data sparsity issue and thereby achieve accurate and fair CD results.
__label__learning_theory From the overall study, a high-level conjecture from our analysis and empirical validations is that DEQ has potential advantages in learning certain high-frequency components.
__label__algorithmic_game_theory Our analysis reveals a counterintuitive insight that suboptimal, and perhaps even reward deteriorating, strategic updates are key to driving play to equilibrium along a satisficing path.
__label__machine_vision In this work, we rethink the set of 3D Gaussians as a random sample drawn from an underlying probability distribution describing the physical representation of the scene—in other words, Markov Chain Monte Carlo (MCMC) samples.
__label__learning_theory In this work we remove both of these requirements and instead provide bounds in terms of a measure of distance between data points: notably these bounds hold with high probability even when $d_0$ is held constant versus $n$.
__label__machine_vision The metric validates the intrinsic nature of the compositional relations among parts, objects, and scenes in a hierarchy-agnostic domain.
__label__machine_vision This limitation hinders their performance and restricts their application in real-world multimodal scenarios.
__label__machine_vision To address this challenging trade-off, we introduce SlimSAM, a novel data-efficient SAM compression method that achieves superior performance with extremely less training data.
__label__machine_learning_for_other_sciences_and_fields Hydra expands the expressivity of heterogeneous reconstruction methods and thus broadens the scope of cryo-EM to increasingly complex samples.
__label__other Such an algorithm relies on the assumption that the input graph fits into main memory and it does not seem to be straightforward to adapt it to very large graphs.
__label__diffusion_based_models At each inference step, we denoise patches rather than the entire latent space, minimizing memory demands so that a single GPU can handle the process, regardless of the image's resolution.
__label__neuroscience_and_cognitive_science These models learn a shared set of nonlinear basis functions, which are linearly combined via a learned weight vector to represent a neuron's function.
__label__safety_in_machine_learning To investigate the effectiveness of noise in eliminating different types of backdoors, we conducted a preliminary study, which demonstrates that backdoors with low visibility can be easily destroyed by lightweight noise and those with high visibility need to be destroyed by high noise but can be easily detected.
__label__learning_theory Finally, we show that any pure DP learner can be transformed in a black-box manner to a replicable learner, with time complexity polynomial in the confidence and accuracy parameters, but exponential in the representation dimension of the underlying hypothesis class.
__label__optimization_for_deep_networks Specifically, we show that when the training data are linearly separable, the iterates of Adam converge towards a linear classifier that achieves the maximum $\ell_\infty$-margin in direction.
__label__robotics Exploiting the structural modularity specific to coded simulations, we propose to use a factored partially observable Markov decision process representation that allows us to reduce context dependence during each step of the generation.
__label__safety_in_machine_learning Our extensive experiments on four LLMs demonstrate that JAM achieves higher jailbreak success ($\sim$ $\times$ 19.88) and lower filtered-out rates ($\sim$ $\times$ 1/6) than baselines.
__label__natural_language_processing Finally, we successfully train the model on sequence length 128 and achieve better performance at evaluation sequence length 8192, compared with other static positional encoding methods, revealing the benefit of the adaptive positional encoding method.
__label__natural_language_processing During inference, experts collaboratively retrieve demonstrations for the input query to enhance the  ICL performance.
__label__algorithmic_game_theory The problem exhibits computational intractability for CC and PAV, and polynomial solvability for AV and SAV.
__label__machine_vision In this paper, we propose \textbf{UniKE}, a novel multimodal editing method that establishes a unified perspective and paradigm for intrinsic knowledge editing and external knowledge resorting.
__label__learning_theory In this paper, we propose methodologies for aggregating prediction intervals to obtain one with minimal width and adequate coverage on the target domain under unsupervised domain shift, under which we have labeled samples from a related source domain and unlabeled covariates from the target domain.
__label__online_learning The resulting learning rate is surprisingly simple compared to the existing learning rates for BOBW algorithms for problems with a minimax regret of $\Theta(T^{2/3})$.
__label__reinforcement_learning As suggested by our analysis, in our experiments, we hence find that simply adding TD3 gradients to the finetuning process of ODT effectively improves the online finetuning performance of ODT, especially if ODT is pretrained with low-reward offline data.
__label__interpretability_and_explainability To address this issue, we propose Language-Guided CAV (LG-CAV) to harness the abundant concept knowledge within the certain pre-trained vision-language models (e.g., CLIP).
__label__fairness Empirical evaluations on multiple benchmarks demonstrate that DPR achieves state-of-the-art performance over existing baselines that do not use bias labels.
__label__machine_learning_for_other_sciences_and_fields Given a mathematical domain axiomatized in dependent type theory, we first combine methods for constrained decoding and type-directed synthesis to sample valid conjectures from a language model.
__label__online_learning We propose an alternative and more natural approach based on optimistic estimations of the constraints.
__label__speech_and_audio We call these visual spectrograms *images that sound*.
__label__machine_vision Prior works use diffusion models to generate driving images conditioned on the 3D object layout.
__label__natural_language_processing Experimentally, D-LLMs show superior performance, in terms of computational cost and KV storage utilization.
__label__optimization Extensive experiments across multiple datasets, including CIFAR, Tiny-ImageNet, and ImageNet-1K, demonstrate the superior performance of our method, highlighting its effectiveness in producing diverse and representative synthetic datasets with minimal computational expense.
__label__optimization Extensive experiments using both real-world and synthetic data demonstrate that \textbf{JOBCD} consistently outperforms state-of-the-art solutions, by large margins.
__label__machine_vision Previous efforts have tried to address this by directly generating prompts from input queries instead of selecting from a set of candidates.
__label__neuroscience_and_cognitive_science Specifically, the left-right difference in brain correlation follows a scaling law with the number of parameters.
__label__other These two initialization schemes are seemingly similar.
__label__generative_models More importantly, the high-accuracy fitting of the Gaussians allows us to achieve a high-quality representation with orders of magnitude fewer parameters than previous structured representations for comparable quality, ranging from one to two orders of magnitude.
__label__optimization_for_deep_networks SVD decomposes a matrix into the product of a left unitary matrix, a diagonal matrix of scaling values, and a right unitary matrix.
__label__diffusion_based_models After that, the diffusion model completes the details of generated images by information from themselves.
__label__machine_learning_for_social_sciences This impedes conditioning on partially observed summary statistics, fails to explore the multimodal matrix distribution over a discrete combinatorial support, and incurs discretisation errors.
__label__privacy Furthermore, we introduce probabilistic variation, a more reliable membership signal based on LLM memorization rather than overfitting, from which we rediscover the neighbour attack with theoretical grounding.
__label__machine_vision As one of the fundamental video tasks in computer vision, Open-Vocabulary Action Recognition (OVAR) has recently gained increasing attention, with the development of vision-language pre-trainings.
__label__machine_vision To overcome these limitations, we propose FineCLIP, which keeps the global contrastive learning to preserve the visual-semantic consistency and further enhances the fine-grained understanding through two innovations: 1) A real-time self-distillation scheme that facilitates the transfer of representation capability from global to local features.
__label__machine_vision Despite recent improvements in deep learning models, the lack of paired training datasets remains a significant obstacle.
__label__optimization_for_deep_networks To establish that this behavior is caused by class imbalance, we show empirically that it can be reproduced across architectures and data types, on language transformers, vision CNNs, and linear models.
"__label__optimization_for_deep_networks In this paper, we leverage recent advances in semi-structured sparse training to apply DST in the domain of classification
with large output spaces, where memory-efficiency is paramount."
__label__machine_vision Inspired by previous works in supervised AT, we then incorporate a self-supervised double perturbation scheme to self-supervised learning (SSL), which promotes robustness transferable to downstream classification.
__label__machine_vision This combination of strong face identity embeddings and our neural representation enables accurate reconstruction of not only facial features but also accessories and hair, and can be meshed to provide render-ready assets for gaming and telepresence.
"__label__learning_theory In contrast, in this work,
we unpack the predictor and integrate the learning problem it gives rise for within the algorithmic challenge."
__label__learning_theory Finally, we discuss issues and perspectives on multi-dimensional input settings.
__label__safety_in_machine_learning }$, pseudo OOD samples) to train OOD detectors.
__label__safety_in_machine_learning We show that in the proposed solution, the mean squared error of the estimation decays with the rate of $\mathcal{O}(S^3 N^{-3})$ and $\mathcal{O}(S^{\frac{8}{5}}N^{\frac{-3}{5}})$ in noiseless and noisy computation settings, respectively, where $N$ is the number of worker nodes with at most $S$ slow servers (stragglers).
__label__reinforcement_learning To overcome the limitation that LLMs are inherently text-based and may be incompatible with numerical environmental data, KALM fine-tunes the LLM to perform bidirectional translation between textual goals and rollouts.
__label__machine_vision The existing computing devices have not adapted the calculation of the attention mechanism well, which leads to a burden on computation quantity and inference latency.
"__label__algorithmic_game_theory We show that
fictitious play converges to the equitable utility profile of $(1/2, 1/2)$ at a rate of $O(1/\sqrt{T})$."
__label__generative_models Drawing inspiration from modern 3D animation pipelines, we introduce DreamMesh4D, a novel framework combining mesh representation with geometric skinning technique to generate high-quality 4D object from a monocular video.
__label__interpretability_and_explainability Overall, our analysis demonstrates that appropriate pre-trained representations (e.g., Fourier features) can unlock the ability of Transformers to learn precise mechanisms for algorithmic tasks.
__label__machine_vision Furthermore, to compensate for the information lost when transferring raw features to the BEV space, we propose a Depth-Aware Local Feature Fusion (DLF) module, which adaptively adjusts the weights of original voxel features and multi-view image features in multi-modal local features via depth encoding.
__label__natural_language_processing Two common strategies have emerged: 1) reducing the input length, such as retrieving relevant chunks by Retrieval-Augmented Generation (RAG), and 2) expanding the context window limit of LLMs.
"__label__natural_language_processing The first of these uses the term ""stochastic parrots"" from Emily Bender et al (""On the dangers of stochastic parrots: Can language models be too big?"""
__label__machine_vision Despite their differences, (3) perceptual tokens are implicitly aligned to textual tokens inside LLMs, we call this the implicit multimodal alignment effect (IMA), and argue that this is linked to architectural design, helping LLMs to generalize.
__label__other Empirically, extensive experiments conducted on three real-world datasets demonstrate the superiority of S-DPO to effectively model user preference and further boost recommendation performance while providing better rewards for preferred items.
__label__other We sidestep these challenges and introduce a simple and lightweight approach to adjust pretrained model predictions via optimal transport.
__label__generative_models We provide the code, along with a blog post and video tutorial on the project page: https://s-sahoo.com/mdlm
__label__learning_theory The learnware paradigm aims to enable users to leverage numerous existing well-trained models instead of building machine learning models from scratch.
__label__diffusion_based_models Our method's efficacy is demonstrated on the class-conditional ImageNet generation benchmark, where DiMR-XL variants surpass previous diffusion models, achieving FID scores of 1.70 on ImageNet $256 \times 256$ and 2.89 on ImageNet $512 \times 512$.
__label__optimization Prior to our work, the best complexity bound was $\mathcal{O}(1/{\varepsilon})$, regardless of the strong convexity of the constraint function.
__label__learning_theory In this work, we systematically investigate the implicit regularization of matrix factorization for solving matrix completion problems.
__label__machine_vision We carefully conduct such conversion to minimize the domain gap between training and test cases.
__label__machine_vision In the realm of Multimodal Large Language Models (MLLMs), vision-language connector plays a crucial role to link the pre-trained vision encoders with Large Language Models (LLMs).
__label__learning_theory Finally, we deal with the more realistic case where we only have access to a finite set of expert demonstrations and a generative model and provide bounds on the error made when working with samples.
__label__natural_language_processing Specifically, LLM-DA harnesses the capabilities of LLMs to analyze historical data and extract temporal logical rules.
__label__learning_theory This paper proposes a theoretical framework based on probably approximately correct (PAC) learning theory to assess the instance-level learnability of deep multiple instance learning (Deep MIL) algorithms.
__label__causal_inference However, we show that there still remains a gap in that there exist causal directions that are identifiable while the algorithm fails to identify them.
__label__learning_theory We initiate a theoretical study for this framework, investigating learning scenarios where the target class of transformations is either known or unknown.
__label__machine_vision Our code is available at https://github.com/Alexander-Yao/Multi-Sub.
__label__optimization_for_deep_networks Although recent works show that neural network can operate in a neural tangent kernel (NTK) regime that does not allow feature learning, many works also demonstrate the potential for neural networks to go beyond NTK regime and perform feature learning.
__label__machine_vision To this end, we present RobIR, an implicit inverse rendering approach that uses ACES tone mapping and regularized visibility estimation to reconstruct accurate BRDF of the object.
__label__safety_in_machine_learning For example, our method achieves >80% (mostly >95%) ASRs on Llama-2-7B and Llama-3-8B without multiple restarts, even if the models are enhanced by strong defenses such as perplexity detection and/or SmoothLLM, which is challenging for suffix-based jailbreaking.
__label__interpretability_and_explainability We confirm these connections by qualitatively examining a subset of our proofs.
__label__natural_language_processing Our work presents the first exploration into human-like step-skipping ability and provides fresh perspectives on how such cognitive abilities can benefit AI models.
__label__learning_theory Recent works show that Transformers can be Turing-complete in terms of expressivity, but this does not address the learnability objective.
__label__diffusion_based_models Early methods attempt to incorporate safety filters into models to mitigate the risk of generating harmful images but such external filters do not inherently detoxify the model and can be easily bypassed.
"__label__natural_language_processing Given that stock relations are often unidirectional, such as the ""supplier-consumer"" relationship, causal relations are more appropriate to capture the impact between stocks."
"__label__reinforcement_learning The performance of modern reinforcement learning algorithms critically relies
on tuning ever increasing numbers of hyperparameters."
__label__privacy Such guarantees are of particular importance for privacy accounting, i.e., tracking privacy over multiple iterations.
__label__deep_learning_architectures Specifically, the real and imaginary parts of the frequency components can be viewed as the coefficients of cosine and sine basis functions at tiered frequency levels, respectively.
"__label__machine_learning_for_healthcare To alleviate the computational complexity of long sequences in large WSIs, methods like HIPT use region-slicing, and TransMIL employs Nystr\""{o}mformer as an approximation of full self-attention."
__label__optimization In particular, SNNs have shown promise in solving combinatorial optimization.
__label__other Experimental results show that SpaFL improves accuracy while requiring much less communication and computing resources compared to sparse baselines.
__label__natural_language_processing We find that while affine alignment is fundamentally an asymmetric notion of similarity, it is still informative of extrinsic similarity.
__label__machine_vision Implicit neural representation gains popularity in modeling the continuous 3D surface for 3D representation and reconstruction.
"__label__interpretability_and_explainability Distinct from previous work, \method is task-agnostic and can be used, without training, 
to explain and even replace traditional dense CLIP representations, maintaining high downstream performance while significantly improving their interpretability."
__label__neuroscience_and_cognitive_science Given the coupling between neural activity and behavior, we ask what type of neural variability does not compromise behavioral performance.
__label__machine_vision Unlike recent prior-free MVS methods that work in a pair-wise manner, our method simultaneously considers all the source images.
__label__diffusion_based_models *DenoiseRep* views each embedding layer in a backbone as a denoising layer, processing the cascaded embedding layers as if we are recursively denoise features step-by-step.
__label__machine_vision To further address the challenge in highly sparse point clouds, we propose a 3D voxel generation strategy to densify foreground features thanks to linear group RNN as a natural property of auto-regressive models.
__label__machine_vision In this way, we can significantly reduce feature variations.
__label__diffusion_based_models To bridge this gap, this work leverages the diffusion process to reframe noisy inverse problems as a two-variable constrained optimization task by introducing an auxiliary optimization variable that represents a 'noisy' sample at an equivalent denoising step.
__label__other The code is available at https://github.com/news-vt/SpaFL_NeruIPS_2024
__label__probabilistic_methods In these scenarios, it is desirable to construct architectures that can flexibly depart from exact equivariance in a data-driven way.
__label__natural_language_processing This paper seeks to bridge this gap by comprehensively comparing the performance of representative IO and EO techniques both isolation and combination on a diverse set of challenging tasks.
__label__learning_theory In multi-output regression, we identify a previously neglected challenge that arises from the inability of training distribution to cover all combinations of input features, leading to combinatorial distribution shift (CDS).
__label__interpretability_and_explainability Finally, we demonstrate that failures at either stage can prevent a model from learning a generalizable solution to our fairly simple tasks.
__label__optimization Classical worst-case optimization theory neither explains the success of optimization in machine learning, nor does it help with step size selection.
__label__reinforcement_learning In this work, we present a simplified version of constraint inference that performs as well or better than prior work across a collection of continuous-control benchmarks.
__label__natural_language_processing Furthermore, probe sampling is also able to accelerate other prompt optimization techniques and adversarial attack methods, leading to acceleration of $1.8\times$ for AutoPrompt, $2.4\times$ for APE and $2.4\times$ for AutoDAN.
__label__privacy To approximately solve the problem, we propose a generic algorithm called Noisy Dual Mirror Descent.
__label__reinforcement_learning These scores are then used to extract a few important rows and columns whose entries are further sampled.
__label__optimization_for_deep_networks We introduce a simple Markov Chain sequence modeling task in order to study how this in-context learning capability emerges.
__label__safety_in_machine_learning The new paradigm of fine-tuning-as-a-service introduces a new attack surface for Large Language Models (LLMs): a few harmful data uploaded by users can easily trick the fine-tuning to produce an alignment-broken model.
"__label__diffusion_based_models To alleviate this, we propose LucidDrag, which shifts the focus from ""how to drag"" to ""what-then-how"" paradigm."
__label__machine_vision In addition, a novel counting loss is proposed, that directly optimizes the detection task and avoids the issues of the standard surrogate loss.
"__label__deep_learning_architectures SwitchHead can also be combined with MoE feedforward layers, resulting in fully-MoE ""SwitchAll"" Transformers."
__label__optimization_for_deep_networks However, existing activation mechanisms often struggle to suppress signals from other irrelevant channels entirely, and these signals have been verified to be detrimental to the network's final decision.
__label__other Our empirical analysis demonstrates the effectiveness of our proposed low precision ensembling method compared to existing ensemble approaches.
__label__graph_neural_networks Unlike traditional continuous GNNs that utilize integer-order or single fractional-order differential equations, DRAGON uses a learnable probability distribution over a range of real numbers for the derivative orders.
__label__deep_learning_architectures Codes will be released soon.
__label__machine_learning_for_physical_sciences We demonstrate that optimally selecting these functions significantly enhances the convergence of the solution.
__label__probabilistic_methods Understanding how the collective activity of neural populations relates to computation and ultimately behavior is a key goal in neuroscience.
__label__reinforcement_learning Due to various reasons, however, such data typically takes the form of rankings over pairs of trajectory segments, which fails to capture the varying strengths of preferences across different pairs.
__label__natural_language_processing In this paper, we show an exciting  phenomenon that SVD-based weight pruning can enhance ICL performance, and more surprising, pruning weights in deep layers often results in more stable performance improvements than in shallow layers.
__label__machine_learning_for_other_sciences_and_fields Specifically, LLMs are only capable of performing shallow (level-1) causal reasoning, primarily attributed to the causal knowledge embedded in their parameters, but they lack the capacity for genuine human-like (level-2) causal reasoning.
__label__machine_vision However, we believe that higher-level 3D-aware tasks, such as articulating dynamic scene changes and motion planning, require a fundamental and explicit 3D understanding beyond current spatial VQA datasets.
__label__machine_vision The ability to promptly respond to environmental changes is crucial for the perception system of autonomous driving.
__label__reinforcement_learning In this work, we show that the conditions developed under full access to the expert's policy cannot guarantee transferability in the more practical scenario where we have access only to demonstrations of the expert.
__label__other Federated Semi-Supervised Learning (FSSL) addresses this label deficiency problem, targeting situations where only the server has a small amount of labeled data while clients do not.
__label__deep_learning_architectures Existing LLM pruning strategies typically assign uniform pruning ratios across layers, limiting overall pruning ability; and recent work on layerwise pruning of LLMs is often based on heuristics that can easily lead to suboptimal performance.
__label__reinforcement_learning In this paper, we analyze the sample and communication complexity of the federated linear stochastic approximation (FedLSA) algorithm.
__label__reinforcement_learning Specifically, we focus on least-squares TD (LSTD) prediction for finite state Markov chains, and show that LSTD can achieve relative accuracy far more efficiently than MC.
__label__interpretability_and_explainability In experiments, we show PCA trees are able to identify a wealth of low-dimensional and cluster structure in image and document datasets.
__label__reinforcement_learning Building on the principled theory of online convex optimization, we present a parameter-free optimizer for lifelong RL, called TRAC, which requires no tuning or prior knowledge about the distribution shifts.
__label__graph_neural_networks Despite the significant progress of pre-trained graph neural networks, there haven’t been GFMs that can achieve desired performance on various graph-learning-related tasks.
__label__natural_language_processing Extensive experiments validate that AmoebaLLM not only sets new standards in LLM adaptability but also successfully delivers subnets that achieve state-of-the-art trade-offs between accuracy and efficiency.
__label__other We apply ST$_k$ to the Average Top-k Loss (AT$_k$), which inherently faces a Top-k problem.
__label__evaluation We hope our research provides standardized protocols and serves as a foundation to spur further explorations in the strategic reasoning of LLMs.
__label__diffusion_based_models This paper proposes a novel method, namely BiDM, for fully binarizing weights and activations of DMs, pushing quantization to the 1-bit limit.
__label__deep_learning_architectures Despite their benefits, MoE models face significant challenges during inference, including inefficient memory management and suboptimal batching, due to misaligned design choices between the model architecture and the system policies.
__label__privacy The code is available here: https://github.com/facebookresearch/VLMDejaVu.
__label__deep_learning_architectures We find that our proposed design offers substantial performance gains over existing sequence modeling methods.
__label__safety_in_machine_learning We demonstrate that scaling correction of gradient changes using gradient variance across different layers can produce highly transferable adversarial examples.
__label__diffusion_based_models Based on this observation and novel theoretical insights, we present Diff-Tuning, a frustratingly simple transfer approach that leverages the chain of forgetting tendency.
__label__reinforcement_learning Exploration in reinforcement learning (RL) remains an open challenge.
__label__reinforcement_learning Experimental results on the D4RL benchmark indicate that our method outperforms previous state-of-the-art baselines in most tasks, clearly demonstrate its superiority over behavior regularization.
__label__robotics The process of satisfying daily demands is a fundamental aspect of humans' daily lives.
__label__machine_learning_for_other_sciences_and_fields To address these issues, we propose FlexPlanner, a flexible learning-based method in hybrid action space with multi-modality representation to simultaneously handle position, aspect ratio, and alignment of blocks.
__label__diffusion_based_models Based on these properties, our method enables the generation of 3D deformable objects with diversity in both identities and poses, using variations of a single object.
__label__reinforcement_learning Through extensive experiments and analysis, we demonstrate the effectiveness of our approach.
__label__natural_language_processing However, such steering vectors are directly extracted from the activations of human preference data and thus often lead to suboptimal results and occasional failures, especially in alignment-related scenarios.
__label__learning_theory Previously established Universal Approximation Theorems for PQCs are either nonconstructive or assisted with parameterized classical data processing, making it hard to justify whether the expressive power comes from the classical or quantum parts.
__label__generative_models We introduce a radiance representation that is both structured and fully explicit and thus greatly facilitates 3D generative modeling.
__label__learning_theory The reduction in sample complexity depends directly on the predictor’s quality, measured by its total variation distance from $p$.
__label__optimization We explore two specific variants of \textbf{JOBCD}: one based on a Gauss-Seidel strategy (\textbf{GS-JOBCD}), the other on a variance-reduced and Jacobi strategy (\textbf{VR-J-JOBCD}).
__label__interpretability_and_explainability Large Language Models (LLMs) have revolutionised natural language processing, exhibiting impressive human-like capabilities.
"__label__other Federated learning (FL) is an appealing paradigm that allows a group of machines
(a.k.a."
__label__machine_learning_for_healthcare We release the source code at https://github.com/DL4mHealth/Medformer.
__label__optimization_for_deep_networks Although efficient for training, the model biases induced by such growing approaches are largely unexplored.
__label__machine_learning_for_other_sciences_and_fields For multipliers, compared to RL-MUL, our method enhances speed and reduces size by as much as 49% and 45%.
__label__natural_language_processing When applied to open-source LLMs including Llama and Mistral, MoICE surpasses prior methods across multiple tasks on long context understanding and generation, all while maintaining commendable inference efficiency.
__label__human-AI_interaction Our experimental results demonstrate that this unified framework significantly enhances the alignment between AI and human players across a diverse range of expertise levels, paving the way for deeper insights into human decision-making and AI-guided teaching tools.
__label__reinforcement_learning Motivated by critical applications, we move beyond point estimators.
__label__machine_learning_for_physical_sciences In addition to improving predictive power, these priors make the model indentifiable, thus the identified features can be linked to comprehensible scientific properties of the system.
__label__machine_learning_for_physical_sciences In particular, how to effectively incorporate hard constraints in VQAs remains a critical and open question.
__label__online_learning In this paper we show that, perhaps surprisingly, we can achieve the asymptotically optimal switching regret on every possible segmentation simultaneously.
__label__graph_neural_networks Specifically, our proposed Information-aware Unsupervised Multiplex Graph Fusion framework (InfoMGF) uses graph structure refinement to eliminate irrelevant noise and simultaneously maximizes view-shared and view-unique task-relevant information, thereby tackling the frontier of non-redundant multiplex graph.
__label__machine_vision Despite recent advances, there remains considerable potential for improvement, particularly in addressing the notable redundancy within the color space of distilled images.
__label__machine_vision To address these challenges, this study proposes a novel interactive multi-modal image fusion framework based on the text-modulated diffusion model, called Text-DiFuse.
__label__bandits The most common structure in the literature is sparsity.
__label__machine_vision They are the de facto building block for Large Multimodal Models (LMMs), yet, we still lack a proper understanding of their success.
__label__causal_inference Second, when primary outcomes are not fully observed, surrogate outcomes can guide the adaptive treatment allocation rule.
__label__neuroscience_and_cognitive_science This novel hybrid learning framework can effectively improve the relatively poor performance of converted SNNs under low time latency.
__label__machine_vision This makes the nature of the training data a significant factor in the efficacy of CLIP for downstream tasks.
__label__deep_learning_architectures While existing deep learning models have shown promising results, few have fully embedded Retinex theory into their architecture, highlighting a gap in current methodologies.
__label__natural_language_processing In this paper, we investigate if self-recognition capability contributes to self-preference.
__label__learning_theory Transformers have achieved extraordinary success in modern machine learning due to their excellent ability to handle sequential data, especially in next-token prediction (NTP) tasks.
__label__natural_language_processing Compared to base LLMs, chat LLMs are further optimized for advanced abilities, e.g.
__label__learning_theory Specifically, we first perform clustering over features of factual triplets instead of entities, where cluster number is naturally set as number of relation types since each fact can be characterized by its relation in KGs.
__label__machine_learning_for_physical_sciences In previous works, the polynomial scaling in $d$ was addressed by amortizing the computation over the optimization process via randomization.
__label__reinforcement_learning Data augmentation creates new data points by transforming the original ones for an reinforcement learning (RL) agent to learn from, which has been shown to be effective for the objective of improving data efficiency of RL for continuous control.
__label__machine_vision The source code is publicly available at https://github.com/KPeng9510/EBiL-HaDS.
__label__learning_theory All of this is achieved by a novel set of analysis tools that characterize the algorithmic dynamics in a more deterministic manner.
__label__generative_models Furthermore, we release the first Chinese writing recognition dataset based on inertial sensors in GitHub.
__label__bandits MaxMinLCB consistently outperforms algorithms in the literature and satisfies an anytime-valid rate-optimal regret guarantee.
__label__speech_and_audio Large Language models (LLMs) have demonstrated supreme capabilities in textual understanding and generation, but cannot be directly applied to cross-modal tasks without fine-tuning.
__label__machine_vision LSM simultaneously estimates geometry, appearance, and semantics in a single feed-forward pass and can synthesize versatile label maps by interacting through language at novel views.
__label__deep_learning_architectures Our experiments demonstrate that this architecture not only outperforms traditional attention-based architectures such as BERT and Vision Transformers with smaller model sizes, but also extends the feasible sequence length beyond the limitations of the dense attention layers.
__label__machine_learning_for_other_sciences_and_fields The source code is available at https://anonymous.4open.science/r/ViusalGeoLocalization-F8F5/ and the dataset will also be released after the paper is accepted.
__label__diffusion_based_models Diffusion models have shown promise as potent generative tools for solving inverse problems due to their superior reconstruction quality and their compatibility with iterative solvers.
__label__learning_theory In fact, one cannot hope to achieve $\alpha < 3$ even when there are only two candidate hypotheses, unless the number of samples is proportional to the domain size of $P$ [Bousquet, Kane, Moran '19].
__label__generative_models We hope these encouraging observations will attract the community's attention to the fundamental problem of unconditional generation.
__label__probabilistic_methods Methods based on functional causal models can identify a unique graph, but either suffer from the curse of dimensionality or impose strong parametric assumptions.
__label__generative_models We further show that our system achieves superior coherence, structure, and overall arrangement quality compared to existing baselines.
__label__bandits More broadly, this work aims to provide insight into the intersection of online learning and mechanism design.
__label__probabilistic_methods We also propose a novel exploration strategy for off-policy methods, based on local search in the target space with the use of a replay buffer, and show that it improves the quality of samples on a variety of target distributions.
__label__machine_learning_for_other_sciences_and_fields Accurate modeling of the diverse and dynamic interests of users remains a significant challenge in the design of personalized recommender systems.
__label__robotics To ensure robust feature presentation and 3D point-level understanding, we first employ SAM masks without cross-frame associations to train instance features with 3D consistency.
__label__machine_vision Defining a noise concept that comprehensively considers both intra-identity consistency and inter-identity discrimination, CION seeks the identity correlation from cross-video images by modeling it as a progressive multi-level denoising problem.
__label__optimization_for_deep_networks Training deep neural networks (DNNs) is costly.
__label__deep_learning_architectures Softmax attention is the principle backbone of foundation models for various artificial intelligence applications, yet its quadratic complexity in sequence length can limit its inference throughput in long-context settings.
__label__safety_in_machine_learning Our research shows that EOT-based attacks face gradient dilemmas due to global gradient averaging, resulting in ineffective evaluations.
__label__learning_theory Our algorithm constitutes a fully polynomial-time approximation scheme (FPTAS) for any time-space recursive (TSR) cost criteria.
__label__deep_learning_architectures We supplement our theoretical results with experiments that highlight the differences in the performance of these architectures on practical-size sequences.
__label__machine_vision Our key idea is to more effectively constrain the SDF inference with the multi-view consistency.
__label__probabilistic_methods In real tasks, however, the environments are usually non-stationary, where the influence relations may be *dynamic*, leading to the failure of AUF by the existing method.
__label__learning_theory We prove that the risk of prospective ERM converges to the Bayes risk under certain assumptions on the stochastic process  generating the data.
__label__safety_in_machine_learning Extensive experiments across three aspects of human values--stereotypes, morality, and legality--demonstrate that ALI-Agent, as a general evaluation framework, effectively identifies model misalignment.
__label__machine_learning_for_other_sciences_and_fields Despite these advancements, existing methods struggle with the ineffective fine-tuning of pre-trained encoders.
__label__optimization This general framework encompasses existing models and enables novel performative gradient estimation methods, leading to more efficient and scalable learning strategies.
__label__diffusion_based_models Text-to-image generative models have recently attracted considerable interest, enabling the synthesis of high-quality images from textual prompts.
__label__diffusion_based_models The project page is at https://seqml.github.io/tse.
__label__machine_learning_for_physical_sciences Our extensive experiments showcase Q$^2$VMC's superior performance, achieving faster convergence and lower ground state energies in wavefunction optimization across various molecular systems, without additional computational cost.
__label__interpretability_and_explainability These estimates provide lower and upper bounds on the expected outcomes of the policy execution across all modeled states in the state space.
__label__machine_vision Previous works perform a two-stage paradigm, first conducting language-agnostic instance segmentation then matching with given text query.
__label__machine_vision Thanks to the additional external knowledge, our ReFIR can well handle the hallucination challenge and facilitate faithfully results.
__label__machine_vision We find **the CLS token naturally absorbs domain information** due to the inherent structure of the ViT, which is represented as the low-frequency component in the Fourier frequency space of images.
__label__safety_in_machine_learning Recently, Anil et al.
"__label__natural_language_processing To fill this gap, this work aims to investigate the research question: ""_What factors affect the performance of MM-ICL?_"" To this end, we investigate extensive experiments on the three core steps of MM-ICL including demonstration retrieval, demonstration ordering, and prompt construction using 6 vision large language models and 20 strategies."
__label__interpretability_and_explainability Notably, we find an algorithmic shift in deeper models, as we go from few to many in-context examples.
__label__deep_learning_architectures Additionally, the balance between high performance and efficiency remains an under-explored problem for exposure correction task.
__label__bandits Variants of Prod can obtain optimal regret for adversarial multi-armed bandits.
__label__graph_neural_networks Then, a graph QFormer encoder adaptively encodes the graph nodes into an auxiliary set of graph prompts to guide the denoising process of diffusion.
__label__neuroscience_and_cognitive_science Current models often use a cortical-like combination of bottom-up (BU) and top-down (TD) processing, where the TD part carries feedback signals for learning.
__label__safety_in_machine_learning For some algorithms like Tree-Ring watermarks, the extracted pattern can also forge convincing watermarks on clean images.
__label__deep_learning_architectures Foundation models are applied in a broad spectrum of settings with different inference constraints, from massive multi-accelerator clusters to resource-constrained standalone mobile devices.
__label__neuroscience_and_cognitive_science In summary, LDNS simultaneously enables inference of low-dimensional latents and realistic conditional generation of neural spiking datasets, opening up further possibilities for simulating experimentally testable hypotheses.
__label__fairness In this work we offer an alternative approach towards fairness in image restoration, by considering the *Group Perceptual Index* (GPI), which we define as the statistical distance between the distribution of the group's ground truth images and the distribution of their reconstructions.
__label__machine_learning_for_other_sciences_and_fields Detailed proofs of intermediate conjectures within the sketch are temporarily replaced by a placeholder tactic called sorry, deferring their proofs to subsequent levels.
__label__bandits In contrast to prior work, which required prior knowledge of the maximal delay $d_{\max}$ and had a linear dependence of the regret on it, our algorithm can tolerate arbitrary excessive delays up to order $T$ (where $T$ is the time horizon).
__label__learning_theory This paper delivers a non-asymptotic consistency analysis of the adversarial training procedure under $\ell_\infty$-perturbation in high-dimensional linear regression.
__label__machine_learning_for_healthcare To mitigate the tug-of-war problem of multi-modal multi-task optimization in MLLMs, recent advances primarily focus on improving the LLM components, while neglecting the connector that bridges the gap between modalities.
__label__natural_language_processing Our trained LINE module excels in capturing critical information from clinical notes, leveraging highly de-identified data.
__label__generative_models In contrast, our approach adopts the **feature reconstruction** objective, where tokenizers are trained by distilling knowledge from pretrained IU encoders.
__label__speech_and_audio Code for AVR and AcoustiX are available at https://zitonglan.github.io/avr.
__label__diffusion_based_models We hypothesize that the reciprocating denoising process in diffusion models may inherently enhance the robustness of the watermark when faced with strong attacks and validate the hypothesis.
__label__machine_vision With such a distinct framework, we flexibly collaborate with 11 heterogeneous agents to empower vision-language foundation models, without further cost in the inference phase.
__label__machine_vision We achieve this by extending implicit maximum likelihood estimation (IMLE) to functional space with an optimizable objective.
__label__machine_vision In this work, we introduce a simulator-conditioned scene generation framework called SimGen that can learn to generate diverse driving scenes by mixing data from the simulator and the real world.
__label__optimization_for_deep_networks These behaviors -- associated with generalization and robustness -- would manifest under specific conditions: models are trained towards zero loss, with noise-free labels belonging to balanced classes, which do not outnumber the model's hidden dimension.
__label__fairness Today's online platforms heavily lean on algorithmic recommendations for bolstering user engagement and driving revenue.
__label__machine_vision The proposed MSFA method significantly enhances the performance of SAR object detection models while demonstrating exceptional generalizability and flexibility across diverse models.
__label__evaluation To reveal the predictive power of SRR on generalization, we collect a set of model variants induced by varied implementations and hyperparameters and evaluate SRR as a complexity measure based on its correlation with generalization.
__label__neuroscience_and_cognitive_science Additionally, we perform analysis experiments to elucidate the mechanism through which SNNs encode positional information and to explore the function of CPGs in the human brain.
__label__optimization To address scalability issues commonly associated with kernel methods, we propose the Sequential Selection Optimization (SSO) algorithm to efficiently train the proposed Kernel Inverse Optimization (KIO) model.
__label__machine_learning_for_healthcare Phenotype imputation plays a crucial role in improving comprehensive and accurate medical evaluation, which in turn can optimize patient treatment and bolster the reliability of clinical research.
__label__diffusion_based_models To do so, we use an infinite-dimensional version of Girsanov's theorem to condition a function-valued stochastic process, leading to a stochastic differential equation (SDE) for the conditioned process involving the score.
__label__optimization_for_deep_networks The redundant parameters can be pruned by our proposed importance score evaluation function, Alternative Evaluation (AlterEva), which is based on the observation of the loss changes when certain modality parameters are activated and deactivated.
__label__privacy To address this issue, we propose an efficient graph property inference attack by leveraging model approximation techniques.
__label__robotics Consequently, ENP learns to globally align with the expert policy by maximizing the likelihood of the actions and modeling the dynamics of the navigation states in a collaborative manner.
__label__deep_learning_architectures Code and datasets are available at https://github.com/mims-harvard/UniTS.
__label__generative_models Comprehensive experiments on real-world generative tasks ranging from image, text to biological domains further demonstrate that SFM achieves higher sampling quality and likelihood than other discrete diffusion or flow-based models.
__label__machine_vision In addition, to support the diverse range of tasks, we carefully collected and combed training data from hundreds of public vision and vision-language tasks.
__label__natural_language_processing This approach mitigates the risk of overlooking essential contextual information.
__label__safety_in_machine_learning Unfortunately, inevitable misspecification of the distribution map can lead to a poor approximation of the true PO.
__label__generative_models Extensive experiments demonstrate superior performance of our method in terms of both rendering quality and spatial-temporal consistency.
__label__diffusion_based_models In addition, as the ID embeddings are integrated in a normal text prompt, it is highly compatible with existing pipelines and can be used without modification to generate authentic videos.
__label__interpretability_and_explainability Although some penalty-based methods have been developed to penalize the spurious features (e.g., invariance penalty, intervention penalty, etc) to help MMI work better, these are merely remedial measures.
__label__interpretability_and_explainability Finally, we experimentally prove the wide applicability of DETAIL by showing our attribution scores obtained on white-box models are transferable to black-box models in improving model performance.
__label__generative_models Additionally, we develop a high-quality image caption dataset, DetailedTextCaps-100K, synthesized with a sophisticated closed-source MLLM to enhance visual text generation capabilities further.
__label__evaluation We estimate augmented scaling laws, which enable us to quantify algorithmic progress and determine the relative contributions of scaling models versus innovations in training algorithms.
__label__machine_vision Codes are available in https://github.com/BeierZhu/VRF.
__label__learning_theory Perhaps the best known example of such scaling laws are for transformer-based large language models (**LLMs**), where networks with billions of parameters are trained on trillions of tokens of text.
__label__machine_vision We further highlight MoNE's adaptability by showcasing its ability to maintain strong performance across different inference-time compute budgets on videos, using only a single trained model.
__label__algorithmic_game_theory Algorithms for playing in Stackelberg games have been deployed in real-world domains including airport security, anti-poaching efforts, and cyber-crime prevention.
__label__machine_vision Our evaluation with four models and four datasets on tiny AI accelerators demonstrates that this simple idea improves accuracy on average by 3.5%p while keeping the inference latency the same on the AI accelerator.
__label__privacy Without privacy constraints, the standard estimators for this task are U-statistics, which commonly arise in a wide range of problems, including nonparametric signed rank tests, symmetry testing, uniformity testing, and subgraph counts in random networks, and are the unique minimum variance unbiased estimators under mild conditions.
__label__deep_learning_architectures Large Transformer networks are increasingly used in settings where low inference latency is necessary to enable new applications and improve the end-user experience.
__label__probabilistic_methods Importantly, our analysis extends beyond sparse regression to data-driven predictors like neural networks, enhancing the reliability of model-based deep learning.
__label__optimization_for_deep_networks AdaLoRA conducts adaptation based on singular value decomposition (SVD), dynamically allocating intrinsic ranks according to importance.
__label__other Extensive experiments demonstrate the effectiveness and efficiency of our proposed method, outperforming state-of-the-art approaches on five visual grounding benchmarks.
"__label__interpretability_and_explainability (2) For hinge-like loss functions, a comprehensive analysis on the computational complexity of \textsc{Debuggable} is provided;
(3) If the loss function is a linear function, \textsc{Debuggable} can be solved in linear time, that is, data debugging can be solved easily in this case."
__label__interpretability_and_explainability Parametric dimensionality reduction methods have gained prominence for their ability to generalize to unseen datasets, an advantage that traditional non-parametric approaches typically lack.
__label__diffusion_based_models Leveraging our theory, we establish that solving the optimal control problem with a specific objective function choice is equivalent to learning diffusion-based generative models.
__label__other This paper presents a case study to reveal this critical vulnerability in KD-based FL systems.
__label__reinforcement_learning Moreover, as often occurs in practice, when expert datasets are collected from an arbitrary state distribution instead of a stationary one, these shifts become more pronounced, potentially leading to substantial failures in existing IL methods.
__label__machine_vision Extensive experiments on both outdoor LiDAR point cloud datasets and indoor RGBD point cloud datasets demonstrate that our method achieves state-of-the-art accuracy, efficiency, and robustness.
__label__robotics Specifically, in the first stage, VidMan is pre-trained on the Open X-Embodiment dataset (OXE) for predicting future visual trajectories in a video denoising diffusion manner, enabling the model to develop a long horizontal awareness of the environment's dynamics.
__label__algorithmic_game_theory committees) through a sequence of minor yet impactful modifications, called reconfiguration path.
__label__machine_vision This approach, however, introduces strong inductive biases, which can render the representations fragile in downstream tasks that do not conform to these symmetries.
__label__generative_models In this work, we propose improved techniques for training rectified flows, allowing them to compete with knowledge distillation methods even in the low NFE setting.
__label__privacy Homomorphic encryption (HE)-based deep neural network (DNN) inference protects data and model privacy but suffers from significant computation overhead.
__label__generative_models To this end, we propose a novel diffusion-based pipeline that generates high-quality multi-view videos centered around a dynamic 3D object from text.
__label__reinforcement_learning In particular, it first rehearses decision-making trajectories based on the derived knowledge after understanding the books, then introspects in the imaginary dataset to distill a policy network.
__label__machine_vision However, the diffusion model, as an external prior that can directly provide visual supervision, has always underperformed in sparse-view 3D reconstruction using Score Distillation Sampling (SDS) due to the low information entropy of sparse views compared to text, leading to optimization challenges caused by mode deviation.
__label__natural_language_processing We found we can distinguish between benign and toxic prompts from the distribution of the first response token's logits.
__label__optimization However, there is a significant gap between the theory and practice regarding high-probability complexity guarantees for these methods on stochastic nonconvex minimax problems.
__label__causal_inference We consider the challenging problem of estimating causal effects from purely observational data in the bi-directional Mendelian randomization (MR), where some invalid instruments, as well as unmeasured confounding, usually exist.
__label__privacy This asymmetry is inherent to a range of important problems including fundamental statistics such as variance, as well as commonly used machine learning performance metrics for both classification and regression tasks.
__label__machine_learning_for_physical_sciences Specifically, we outperform PbGMR-GMUS Transformer-RealNVP and GMR-GMUS Transformer, with a reduction in error of 88% and 91%, respectively.
__label__machine_learning_for_other_sciences_and_fields Global routing plays a critical role in modern chip design.
__label__safety_in_machine_learning This work uncovers fundamental limitations of generative models in achieving both high perceptual quality and reliable predictions for image restoration.
__label__optimization_for_deep_networks However, not all pre-trained features are robust and those methods are largely indifferent to which ones to preserve.
__label__robotics Additionally, we also design an asynchronous hierarchical executor (AHE) for PIVOT-R, which can use different execution frequencies for different modules of the model, thereby helping the model reduce computational redundancy and improve model execution efficiency.
__label__deep_learning_architectures Widely adopted in modern Vision Transformer designs, Softmax attention can effectively capture long-range visual information; however, it incurs excessive computational cost when dealing with high-resolution inputs.
__label__machine_vision Specifically, a simple mapping network first maps image information into language space and forms a target description with a manipulation description.
__label__machine_vision Furthermore, to capture the complicated interplay between subjects and objects, we propose a new lightweight module called mutual visual adapter.
__label__interpretability_and_explainability Empirically, Source outperforms existing TDA techniques in counterfactual prediction, especially in settings where implicit-differentiation-based approaches fall short.
__label__diffusion_based_models In this paper, we propose the Meta-Diffu$B$ framework—a novel scheduler-exploiter S2S-Diffusion paradigm designed to overcome the limitations of existing S2S-Diffusion models.
__label__generative_models We propose a method that aims to automate this process by harnessing the power of state-of-the-art generative models to produce a diversity of challenging yet solvable problems, here in the context of Python programming puzzles.
__label__probabilistic_methods We overcome this limitation using annealing, which enhances the exploration of the hypothesis space during training.
__label__probabilistic_methods The idea is to learn a conditional diffusion model where the score function is estimated using gradient-boosted trees.
__label__machine_learning_for_other_sciences_and_fields Recent advances in large language models (LLMs) for code applications have demonstrated remarkable zero-shot fluency and instruction following on challenging code related tasks ranging from test case generation to self-repair.
__label__probabilistic_methods Additionally, we introduce a deep kernel variant by hierarchically stacking multiple spectral feature mappings, further enhancing the model's expressiveness to capture complex patterns in data.
__label__natural_language_processing These findings underscore the need to create more resilient LLMs that can maintain high performance across diverse prompts.
__label__natural_language_processing We train using a modified DPO loss with an additional negative log-likelihood term, which we find to be crucial.
__label__neuroscience_and_cognitive_science However, convolutional neural networks (CNNs) remain the preferred architecture in vision neuroscience due to their ability to efficiently process visual information, which comes at the cost of the biological realism provided by RNNs.
__label__machine_vision To meet the requirements of traditional explicit neural networks for output form, existing heatmap-based methods discretize the originally continuous heatmap representation into 2D pixel arrays, which leads to performance degradation due to the introduction of quantization errors.
__label__natural_language_processing Sketchpad substantially improves performance on all tasks over strong base models with no sketching, yielding an average gain of 12.7% on math tasks, and 8.6% on vision tasks.
__label__machine_vision We further combine this representation with a multi-resolution grid backbone that is trained in a coarse-to-fine manner, enabling faster reconstructions than prior methods.
__label__evaluation We also find that despite their significant computational cost, pretrained LLMs do no better than models trained from scratch, do not represent the sequential dependencies in time series, and do not assist in few-shot settings.
__label__algorithmic_game_theory In multiplayer games, self-interested behavior among the players can harm the social welfare.
__label__machine_learning_for_healthcare Notably, the combination of moderator review and external medical knowledge in group collaboration resulted in an average accuracy **improvement of 11.8\%**.
__label__safety_in_machine_learning Surprisingly, we find that the original backdoors still exist in defense models derived from existing post-training defense strategies, and the backdoor existence is measured by a novel metric called backdoor existence coefficient.
__label__machine_vision Building upon Stable Video Diffusion, we introduce GenRec, the first unified framework trained with a random-frame conditioning process so as to learn generalized spatial-temporal representations.
__label__causal_inference Furthermore, we establish that the sample complexity required to disentangle mixed data inversely correlates with the extent of change induced by an intervention in the equations governing the affected variable values.
__label__reinforcement_learning However, the belief state depends on the system model and is therefore not viable in reinforcement learning (RL) settings.
__label__other We further introduce client-specific adaptive thresholding and learning status-aware aggregation to adjust the training process based on the learning progress of each client.
__label__machine_vision To tackle this, we introduce a novel paradigm of clip-level vectorized HD map construction, MapUnveiler, which explicitly unveils the occluded map elements within a clip input by relating dense image representations with efficient clip tokens.
__label__other Code is available at https://github.com/msgwak/LAST.
__label__machine_vision Depth refinement aims to infer high-resolution depth with fine-grained edges and details, refining low-resolution results of depth estimation models.
__label__generative_models Leveraging a meticulously curated dynamic 3D dataset, we develop a 4D-aware video diffusion model capable of synthesizing orbital views of dynamic 3D assets.
__label__natural_language_processing Nonetheless, data selection and labeling are still a bottleneck for these systems, particularly at large scale.
__label__graph_neural_networks GRDL treats each graph's latent node embeddings given by GNN layers as a discrete distribution, enabling direct classification without global pooling, based on maximum mean discrepancy to adaptively learned reference distributions.
__label__natural_language_processing To boost the token acceptance rate while minimizing the latency of the self-drafting model, we introduce an additional \emph{early exiting} mechanism for both single-sequence and the tree decoding scenarios.
__label__optimization_for_deep_networks Specifically, GNM-PT seeks the gradient descent direction within a random parameter neighborhood, independent of input samples, during each gradient update.
__label__reinforcement_learning Recent efforts like GLAM and TWOSOME manually constrain the action space to a restricted subset and employ reinforcement learning to align agents' knowledge with specific environments.
__label__reinforcement_learning While doing so is sufficient to drive the *value gap* between the learner and the expert to zero under the assumption that agents are non-strategic, it does not guarantee robustness to deviations by strategic agents.
__label__learning_theory In particular, we show that distributions that approximately match at least $\mathrm{poly}(1/\varepsilon)$ moments of the standard Gaussian fool constant-degree PTFs (up to error $\varepsilon$).
__label__machine_vision For many real-world scenes this leads to their heavy dependence on good initializations.
__label__optimization_for_deep_networks Empirically, Spry reduces the memory footprint during training by 1.4-7.1$\times$ in contrast to backpropagation, while reaching comparable accuracy, across a wide range of language tasks, models, and FL settings.
__label__probabilistic_methods Denoting the factor by which the regret bound of A-GP-UCB was away from oracle as $g(T)$,  we show that LB is only $\log g(T)$ away from oracle regret.
__label__optimization A first answer is no: as we show, the distance-preserving objective of JL has a non-convex landscape over the space of projection matrices, with many bad stationary points.
__label__natural_language_processing Pretraining data selection has the potential to improve language model pretraining efficiency by utilizing higher-quality data from massive web data corpora.
__label__interpretability_and_explainability As an attempt to bridge this gap, we provide other authors with five recommendations to make future solutions easier to adapt to their potential real-world applications.
__label__natural_language_processing The former adds an advantage of real-world grounding but lacks theoretically backed-up analysis/validation, whereas the latter is far from real-world grounding.
__label__active_learning Additionally, the proposed method is applicable to both regression and classification tasks on graphs.
__label__machine_vision Many few-shot segmentation (FSS) methods use cross attention to fuse support foreground (FG) into query features, regardless of the quadratic complexity.
__label__evaluation Specifically, during training, we present to generate scaled-up comparative instructions by comparing images from the same IQA dataset, allowing for more flexible integration of diverse IQA datasets.
__label__optimization We design and analyze practical algorithms which only use few clean queries w.r.t.
__label__interpretability_and_explainability Unlike previous generative label-noise learning methods, we consider causal relations between latent causal variables and model them with a learnable graphical model.
__label__machine_vision Our numerical and visual comparisons with the stat-of-the-art methods show our superiority over these methods in surface reconstruction and point cloud denoising on widely used shape and scene benchmarks.
__label__reinforcement_learning This way, **Cinderella** is shown to achieve state-of-the-art regret bounds for all previously known (and some new) continuous MDPs for which RL is learnable and feasible.
__label__graph_neural_networks Sum-based aggregators have solid theoretical foundations regarding their separation capabilities.
__label__evaluation We provide extensive meta-evaluation and analysis for our and existing LLM benchmarks to deepen the community’s understanding of LLM evaluation and guide future research directions.
__label__machine_vision We implement this method by distilling and optimizing generalizable interpolated representations and assigning distinct weights for each modality considering their divergent generalization capabilities.
__label__reinforcement_learning To address this limitation, we introduce the notion of Cross-Embodiment Unsupervised RL (CEURL), which leverages unsupervised learning to enable agents to acquire embodiment-aware and task-agnostic knowledge through online interactions within reward-free environments.
__label__diffusion_based_models To address this, we introduce a conservative fine-tuning approach, BRAID, by optimizing a conservative reward model, which includes additional penalization outside of offline data distributions.
__label__neuroscience_and_cognitive_science Furthermore, they have the ability to remap, wherein the firing fields and rates of cells change in response to changes in the environment.
__label__generative_models In the past decade, deep conditional generative models have revolutionized the generation of realistic images, extending their application from entertainment to scientific domains.
__label__probabilistic_methods In this work, we propose to optimize expected scores under a probabilistic model over hierarchies.
__label__machine_vision Then, the derived model is directly transferred to the forgery detection task.
__label__natural_language_processing In this paper, we perceive the LLMs' knowledge boundary with semi-open-ended questions by discovering more ambiguous answers.
__label__optimization_for_deep_networks This means that the user does not need to compute optimizer-specific scale factors in order to scale training.
__label__fairness Our method is supported by theoretical results, notably a proof of the NBS for gradient aggregation free from linear independence assumptions, a proof of Pareto improvement, and a proof of monotonic improvement in validation loss.
__label__learning_theory A user, presented with a subset of $\\{1,\ldots,n\\}$, will select the item of the subset with the highest utility, according to a utility vector drawn from the specified distribution.
"__label__machine_vision Since OOD detection aims to correctly classify input images into ID/OOD class groups, we can ""make up"" OOD label candidates which are not standard class names but beneficial for the process."
__label__interpretability_and_explainability Our work shows that entropy neurons operate by writing onto an \textit{unembedding null space}, allowing them to impact the residual stream norm with minimal direct effect on the logits themselves.
__label__learning_theory bandwidth or privacy constraints that each server needs to satisfy.
__label__learning_theory But for other simple loss functions, including the $\ell_1$ loss, $\mathsf{DLQ}$ always achieves the same complexity as $\mathsf{SQ}$.
__label__probabilistic_methods In this paper, we view such approaches as decomposing the entire denoising diffusion process into several segments, each corresponding to a reverse transition kernel (RTK) sampling subproblem.
__label__optimization While inference on analog accelerators has been studied recently, the training perspective is underexplored.
__label__active_learning In batched BO, when multiple points are acquired in parallel, commonly used acquisition functions are often high-dimensional and intractable, leading to the use of sampling-based alternatives.
__label__causal_inference Recent text-based causal methods attempt to mitigate confounding bias by estimating proxies of confounding variables that are partially or imperfectly measured from unstructured text data.
__label__machine_vision Secondly, we design a selective mechanism based on the local feature discrepancy to prioritize the critical information contributing to prediction tasks during inter-drone interactions.
__label__machine_vision Dataset condensation, a concept within data-centric learning, efficiently transfers critical attributes from an original dataset to a synthetic version, maintaining both diversity and realism.
__label__deep_learning_architectures Replacement training enhances the student model's ability to replicate the teacher model's behavior.
__label__machine_vision Additionally, instead of measuring the similarity of two samples by only computing their distance, a novel triplet assignment loss is further proposed, in which the whole data distribution also contributes to optimizing the inter/intra-class distances.
__label__diffusion_based_models The challenge of score matching is that it includes a computationally expensive Jacobian trace.
"__label__deep_learning_architectures We propose a neural network weight encoding method for network property prediction that utilizes set-to-set and set-to-vector functions
to efficiently encode neural network parameters."
__label__robotics This offers a flexible and cost-efficient approach for scaling and also provides a robust infrastructure for robot-environment-algorithm communication.
__label__other Existing MIPL algorithms often overlook the margins for attention scores and predicted probabilities, leading to suboptimal generalization performance.
__label__reinforcement_learning Empirically, DMG achieves state-of-the-art performance across Gym-MuJoCo locomotion tasks and challenging AntMaze tasks.
__label__online_learning Continual test-time adaptation methods have shown promising results by using reliable pseudo-labels, but they still fall short in exploring representation alignment with the source domain in non-stationary environments.
__label__generative_models Furthermore, we show the application of FKEA's proxy eigenvectors to reveal the method's identified modes in evaluating the diversity of produced samples.
__label__safety_in_machine_learning In this paper, we pioneer the exploration of an object-aware backdoored VLN, achieved by implanting object-aware backdoors during the training phase.
__label__graph_neural_networks However, obtaining abundant labels is often challenging in practice, which makes graph few-shot incremental learning necessary.
__label__machine_learning_for_other_sciences_and_fields To overcome this limitation, we introduce MSA-Generator, a self-supervised generative protein language model.
__label__learning_theory The rate-optimal estimator proposed in this article achieves the optimal rate by estimating $O\left(\frac{\log n}{\log\log n}\right)$ cumulants and leveraging a variational representation of the noise variance in terms of the cumulants of the data distribution.
__label__diffusion_based_models Specifically, under mild data assumptions, we derive an approximation error bound for the score network of latent DiTs, which is sub-linear in the latent space dimension.
__label__natural_language_processing These models, which feature architectures of 54 billion and 140 billion parameters, respectively, are based on the Mixtral architecture.
__label__natural_language_processing We attribute this discrepancy to the DPO objective, which is empirically and theoretically ill-suited to correct even mild ranking errors in the reference model, and derive a simple and efficient formula for quantifying the difficulty of learning a given preference datapoint.
__label__learning_theory Moreover, through the lens of the pruning ratio threshold,  we can provide rigorous interpretations on several heuristics in existing pruning algorithms.
__label__natural_language_processing More generally, we find that the gains from LAIF vary substantially across base model families, test-time evaluation protocols, and critic models.
__label__machine_learning_for_physical_sciences Typically, FNOs employ separate parameters for different frequency modes to specify tunable kernel integrals in Fourier space, which, yet, results in an undesirably large number of parameters when solving high-dimensional PDEs.
__label__other While the robustness of EOT couplings/maps makes them a go-to choice in practical applications, EOT solvers remain difficult to tune because of a small but influential set of hyperparameters, notably the omnipresent entropic regularization strength $\varepsilon$.
__label__algorithmic_game_theory We contribute a learning algorithm that utilizes MARL policy gradient methods with iteration and sample complexity that is polynomial in the approximation error $\epsilon$ and the natural parameters of the ATMG, resolving the main caveats of the solution by (Kalogiannis et al., 2023).
__label__generative_models This task is challenging due to the need to adapt to various photo poses, the sensitivity of hairstyles, and the lack of objective metrics.
__label__bandits Our analysis relies on identifying a limiting fluid dynamics of allocations that satisfy a series of ordinary differential equations pasted together and that describe the asymptotic path followed by our algorithm.
__label__reinforcement_learning Our objective is to design an algorithm that incurs only finite regret over infinite episodes with high probability.
__label__infrastructure The results demonstrate that pFedClub outperforms baseline approaches, achieving state-of-the-art performance.
__label__reinforcement_learning Distributional Reinforcement Learning (RL) addresses these challenges by extending the traditional Bellman equation to consider value distributions instead of a single mean value, showing promising results in Deep Q Learning.
__label__online_learning Code is available at: https://github.com/jihoontack/MAC.
__label__interpretability_and_explainability Pre-trained large language models based on Transformers have demonstrated remarkable in-context learning (ICL) abilities.
__label__machine_vision Specifically, we design a Point-Word Cross-Modal Alignment module for aligning the fine-grained features of points and textual embedding.
__label__natural_language_processing We further design an expert-wise training strategy to alleviate the impact of unhelpful demonstrations when optimizing the retriever model.
__label__other Neighbor embedding (NE) is an essential technique for visualizing complex high-dimensional data, but collaboratively learning a joint NE model is difficult.
__label__interpretability_and_explainability As a result, our models retain the accuracy of black-box deep networks while offering free-lunch explainability for tabular data by design.
__label__natural_language_processing Additionally, we introduce a target reward margin to the Bradley-Terry objective to encourage a larger margin between the winning and losing responses, further improving the algorithm's performance.
__label__optimization_for_deep_networks To achieve this, we design an adaptive group risk minimization strategy, comprising two techniques in implementation: (i) dynamical group assignment, which clusters similar tasks based on task interactions; (ii) risk-guided group indicators, which exploit consistent task correlations with risk information from previous iterations.
__label__other Theoretical analysis provides a generalization bound for FL w.r.t.
__label__human-AI_interaction We attribute the success of IDA to preserving human autonomy while simultaneously offering assistance to prevent the human from entering universally bad states.
__label__machine_vision In this paper, we introduce SELMA: Skill-Specific Expert Learning and Merging with Auto-Generated Data, a novel paradigm to improve the faithfulness of T2I models by fine-tuning models on automatically generated, multi-skill image-text datasets, with skill-specific expert learning and merging.
__label__algorithmic_game_theory We introduce the first algorithmic framework for Blackwell approachability on the sequence-form polytope, the class of convex polytopes capturing the strategies of players in extensive-form games (EFGs).
__label__machine_vision We present DC-Gaussian, a new method for generating novel views from in-vehicle dash cam videos.
__label__learning_theory They thus extend well beyond the standard Gaussian assumptions commonly made in the literature.
__label__bandits As a byproduct, relaxing this assumption leads to the first near-optimal regret result for heavy-tailed bandits with Huber contamination in the adversarial regime, in contrast to all previous works focused on the (easier) stochastic regime.
__label__privacy We propose Adaptive Randomized Smoothing (ARS) to certify the predictions of our test-time adaptive models against adversarial examples.
__label__active_learning Drawing inspiration from existing mixture of Bernoulli models, which efficiently compress the label space into a more manageable weight coefficient space by learning correlated Bernoulli components, we propose a novel model called Evidential Mixture Machines (EMM).
__label__reinforcement_learning Offline reinforcement learning (RL) aims to learn policies from pre-existing datasets without further interactions, making it a challenging task.
__label__graph_neural_networks We hope that our work paves the way for an era where foundational GNNs drive pharmaceutical drug discovery.
__label__reinforcement_learning To evaluate the proposed mechanism, we consider the multi-agent autonomous driving scenario in which autonomous vehicles with different driving traits are on the road.
__label__diffusion_based_models The resulting approach also applies to the original parameterization of the concrete score.
__label__evaluation How should we assess this possibility?
__label__natural_language_processing Panacea trains a single model capable of adapting online and Pareto-optimally to diverse sets of preferences without the need for further tuning.
__label__generative_models We present Meta 3D AssetGen (AssetGen), a significant advancement in text-to-3D generation which produces faithful, high-quality meshes with texture and material control.
__label__other LAST scores are evaluated using $\mathcal{H}_{\infty}$ norms of subsystems for each state and layer-wise energy normalization.
__label__diffusion_based_models the data dimension $d$}.
__label__machine_vision We thus conduct a systematic study to explore the performance benefits of using more samples of non-English origins with respect to English vision tasks.
__label__machine_vision They are particularly relevant for self-labeled clustering methods, where latent pseudo-labels $y$ are jointly estimated with the model parameters and uncertainty is prevalent.
__label__machine_vision The strategy of utilizing NODE to leverage continuous dynamics in iterative methods enhances unsupervised learning and aids in achieving better convergence compared to discrete-space approaches.
__label__interpretability_and_explainability These findings are clear evidence of learned look-ahead in neural networks and might be a step towards a better understanding of their capabilities.
__label__graph_neural_networks CITRUS leverages the separability of continuous heat kernels from Cartesian graph products to efficiently implement graph spectral decomposition.
__label__machine_learning_for_healthcare To address these challenges in cross-domain disease grading, we propose a Severity-aware Recurrent Modeling (Samba) method in this paper.
__label__robotics Our dataset construction is cost-efficient, with the carefully-design hand-object interaction retargeting strategy, and the LLM-assisted language guidance annotation system.
__label__machine_learning_for_other_sciences_and_fields Code is available at https://github.com/qiaoqiaoLF/MxDNA.
__label__bandits We apply IGL to learning from image feedback and learning from text feedback, which are reward-free settings that arise in practice.
__label__learning_theory A prevailing hypothesis to explain these phenomena suggests that adversarial perturbations appear as random noise but contain class-specific features.
__label__graph_neural_networks This task poses significant challenges due to the explosion in graph size, dependencies among graph entities, and the need for controllability in graph conditions.
__label__learning_theory For this natural class of algorithms, we prove a nearly matching sample complexity lower bound for replicable uniformity testing.
__label__machine_learning_for_other_sciences_and_fields Our experimental results demonstrate that, at a 1% bit-error rate, NeuralFuse can reduce SRAM access energy by up to 24% while recovering accuracy by up to 57%.
__label__algorithmic_game_theory Moreover, the optimal allocation once more involves subsidization in favor of one group, where the extent of subsidization depends on the difference in future utilities for both the seller and buyers when allocating the item to one group versus the other.
__label__machine_vision However, their limited data memory often necessitates downsampling input images, resulting in accuracy degradation.
__label__neuroscience_and_cognitive_science The feature fusion \& classifier module aims to aggregate temporal patterns and dependencies from different levels and obtain the final classification results.
__label__machine_learning_for_healthcare To recover these false zeros, propagation-based imputation methods have been proposed using $k$-NN graphs.
__label__graph_neural_networks Our method minimizes the norms of intermediate variables involved in the different ways of computing the predicted tensor.
__label__optimization_for_deep_networks This paper empirically investigates the impact of scaling the architectures and training of causal language models (CLMs) on their progression towards $\mathcal{NC}$.
__label__natural_language_processing Furthermore, when texts exceed the context window, we analyze the change of positional vectors in two settings, i.e., direct extrapolation and context window extension.
__label__deep_learning_architectures Our work addresses these challenges by proposing the Calibrated Self-Rewarding (CSR) approach, which enables the model to self-improve by iteratively generating candidate responses, evaluating the reward for each response, and curating preference data for fine-tuning.
__label__neuroscience_and_cognitive_science We find that it takes surprisingly little information from the brain to produce reconstructions with high fidelity.
__label__machine_vision We propose Text2CAD, the first AI framework for generating text-to-parametric CAD models using designer-friendly instructions for all skill levels.
__label__neuroscience_and_cognitive_science We provide model analysis of scaling, similarity preservation and convergence behavior as well as experiments demonstrating noise robustness, sub-integer resolution in representing position, and path integration.
__label__machine_learning_for_other_sciences_and_fields The heterogeneity issue in federated learning (FL) has attracted increasing attention, which is attempted to be addressed by most existing methods.
__label__deep_learning_architectures Mamba, GLA, Hawk/Griffin, HGRN2), then the resulting architecture can surpass attention-powered foundation models trained on text in both accuracy and efficiency, at scales of billion parameters.
__label__learning_theory We introduce a type of Statistical Queries ($\mathsf{SQ}$), which we call Differentiable Learning Queries ($\mathsf{DLQ}$), to model gradient queries on a specified loss with respect to an arbitrary model.
__label__deep_learning_architectures MatrixNet achieves higher sample efficiency and generalization over several standard baselines in prediction tasks over the several finite groups and the Artin braid group.
__label__online_learning Given the fundamental role of NSW in the fairness literature, it is more than natural to ask whether no-regret fair learning with NSW as the objective is possible.
__label__probabilistic_methods Our pretrained model is available online.
__label__safety_in_machine_learning Recent research has shown that pre-trained vision-language models are effective at identifying out-of-distribution (OOD) samples by using negative labels as guidance.
__label__diffusion_based_models Our method can also achieve sampling with variable NFEs using a single distilled model.
__label__robotics In robotic tasks, the ability to predict future images and generate actions is highly correlated since they share the same underlying dynamics of the physical world.
__label__natural_language_processing Based on these ideas, we have developed a new framework that outperforms state-of-the-art techniques in zero-shot text-to-SQL on complex benchmarks
__label__privacy Specifically, FreqMark embeds the watermark by optimizing the latent frequency space of the images and then extracts the watermark through a pre-trained image encoder.
__label__safety_in_machine_learning We design a logit pairing loss to improve the union accuracy by analyzing the tradeoffs from the lens of distribution shifts.
__label__machine_vision **Videos are available at https://meshformer3d.github.io/**
__label__machine_vision To overcome this, we develop a two-stage approach that decomposes the problem into temporal completion and spatial completion.
__label__natural_language_processing Besides, we explore the limitations of layer-wise pruning from the perspective of error accumulation and propose Incremental Pruning Ratio, a non-uniform pruning strategy to reduce performance degradation.
__label__bandits Then, we leverage this convex reformulation of the lower bound to design the Track and Stop with Preferences (TSwP) algorithm that identifies the most preferred policy.
__label__evaluation To begin, the concept of self-awareness is clearly demarcated from related concepts like consciousness, agency, and free will.
__label__machine_learning_for_other_sciences_and_fields In this work, we learn a joint latent space between molecular structures and microscopy phenomic experiments, aligning paired samples with contrastive learning.
__label__optimization This transformation naturally implements a parameterization for the relaxation of permutation matrices, allowing for gradient-based optimization of problems involving permutations.
__label__natural_language_processing Parameter-efficient finetuning (PEFT) methods seek to adapt large neural models via updates to a small number of *weights*.
__label__machine_vision Most incremental learners excessively prioritize object classes while neglecting various kinds of states (e.g.
__label__deep_learning_architectures DOFEN is a novel DNN architecture inspired by oblivious decision trees and achieves on-off sparse selection of columns.
__label__graph_neural_networks Extensive experiments across diverse datasets and GNN architectures demonstrate significant improvement over existing global explainers in mapping GNN predictions to faithful logical formulae.
__label__machine_vision However, the noisy and discrete nature of 3D Gaussian primitives hinders accurate surface estimation.
__label__reinforcement_learning We give a model-based agent that builds a Python program representing its knowledge of the world based on its interactions with the environment.
__label__safety_in_machine_learning In our work, we observe that benign heterogeneous distributions and malicious triggered distributions exhibit distinct parameter importance degrees.
__label__deep_learning_architectures To further forcing student model to emulate teacher model, we incorporate prediction guidance and stage-wise feature mimicking to provide additional supervision during the teacher model's compression process.
__label__probabilistic_methods Current methods typically employ kernel-based approaches, using kernel functions directly for kernel density estimation or as basis functions in linear models.
__label__reinforcement_learning Moreover, this design supports the modeling of multi-modal action distributions while facilitating efficient action sampling.
__label__safety_in_machine_learning Extensive evaluations on ImageNet1K and Pascal-VOC benchmarks show SeTAR's superior performance, reducing the relatively false positive rate by up to 18.95\% and 36.80\% compared to zero-shot and fine-tuning baselines.
__label__deep_learning_architectures Computational intensiveness of deep learning has motivated low-precision arithmetic designs.
__label__privacy In this work, we define the fundamental PUT of private sampling in the minimax sense, using the $f$-divergence between original and sampling distributions as the utility measure.
__label__machine_learning_for_physical_sciences To address these issues, we propose *Codomain Attention Neural Operator* (CoDA-NO), which tokenizes functions along the codomain or channel space, enabling self-supervised learning or pretraining of multiple PDE systems.
__label__machine_vision Training models with longer in-context lengths is a significant challenge for multimodal machine learning due to substantial GPU memory and computational costs.
__label__optimization The theoretical results are supported by numerical simulations.
__label__neuroscience_and_cognitive_science Recent work suggests that PC can converge in fewer learning steps than backpropagation thanks to its inference procedure.
__label__other By adopting a modified Frank-Wolfe algorithm to minimise a novel criterion $\alpha$-Maximum Mean Discrepancy ($\alpha$-MMD), RDSS samples a representative and diverse subset for annotation from the unlabeled data.
__label__causal_inference Estimating individualized treatment rules (ITRs) is fundamental in causal inference, particularly for precision medicine applications.
__label__reinforcement_learning Our proposed framework outperforms programmatic RL and deep RL baselines on various tasks.
__label__generative_models Through experiments on various datasets, we demonstrate the effectiveness of our active viewpoint selection strategy, significantly enhancing segmentation and reconstruction performance compared to random viewpoint selection.
__label__natural_language_processing To achieve robust speculative decoding, Sequoia uses a novel sampling and verification method that outperforms prior work across different decoding temperatures.
__label__machine_vision Inspired by the observations that the final prediction in vision transformers (ViTs) is only based on a subset of most informative tokens, we take the novel step of enhancing the efficiency of SSM-based vision models through token-based pruning.
__label__interpretability_and_explainability Our work sheds new light on the behavior of pre-trained language models, challenging assumptions about their intrinsic dimensionality and Hessian properties.
__label__neuroscience_and_cognitive_science Here, we propose to fit stochastic low-rank RNNs with variational sequential Monte Carlo methods.
__label__optimization_for_deep_networks This structure is locally optimal near the point of convergence, so RAMDA is guaranteed to obtain the best structure possible among all methods converging to the same point, making it the first regularized adaptive method to output models that possess outstanding predictive performance while being (locally) optimally structured.
__label__machine_vision The second is lightweight query completion that works with the depth-aware masking to learn to reconstruct and complete the masked object queries.
__label__generative_models Energy-based models (EBMs) offer a flexible framework for probabilistic modelling across various data domains.
__label__machine_vision Extensive experimental results demonstrate that BAU effectively exploits the advantages of data augmentation, which previous studies could not fully utilize, and achieves state-of-the-art performance without requiring complex training procedures.
__label__natural_language_processing Demos and source code are available at https://github.com/qinghua-zhou/stealth-edits.
__label__machine_vision However, training TAs is expensive, as training these models is a knowledge transfer task in itself.
__label__natural_language_processing This theoretical framework could help to more effectively predict and guide the evolution of LLMs in desired directions.
__label__neuroscience_and_cognitive_science Both our paradigms and modelling approach offer a novel way to quantify alignment between neural networks and humans and extend cognitive science into more naturalistic domains.
__label__diffusion_based_models 1) From the data aspect, we carefully collect a human-centric dataset comprising over one million high-quality human-in-the-scene images and two specific sets of close-up images of faces and hands.
__label__bandits For the latter problem, contextual linear bandits, we provide an algorithm that achieves $O ( \sqrt{d T \log ( K \min\{ 1, \frac{S}{d} \} )} )$ together with a matching lower bound, where $d$ and $S$ represent the dimensionality of feature vectors and the size of the context space, respectively.
__label__other Large Language Models (LLMs) have shown promise in code generation but face difficulties in resolving Github issues, particularly at the repository level.
__label__safety_in_machine_learning Their certified robustness has gained increasing research attention due to security concerns.
__label__reinforcement_learning In response, we take a step back and ask what a *minimalist* RL algorithm for the era of generative models would look like.
__label__learning_theory In this paper, we investigate the estimation error between the learner obtained by the SCLS method and the actual learner.
__label__machine_vision Image relighting is the task of showing what a scene from a source image would look like if illuminated differently.
__label__generative_models To enhance the compression ratio, we propose normalized attention score as an effective metric for identifying salient tokens by considering the lower triangle characteristics of the attention matrix.
__label__other Our results hint that, contrary to previous claims in the literature, CoT's performance improvements do not stem from the model learning general algorithmic procedures via demonstrations but depend on carefully engineering highly problem specific prompts.
__label__neuroscience_and_cognitive_science This work demonstrates that Bayesian computation based on spiking neural networks can decouple event streams of different motions.
__label__neuroscience_and_cognitive_science This work represents a promising new foundation for understanding and making predictions about perturbations to visual processing in the brain.
__label__causal_inference Predicting potential outcomes of interventions from observational data is crucial for decision-making in medicine, but the task is challenging due to the fundamental problem of causal inference.
__label__graph_neural_networks Inspired by these findings, we conjecture that the consistency in the similarities of graph representations across GNN layers is crucial in capturing relational structures and enhancing graph classification performance.
__label__speech_and_audio Speech dysfluency modeling is the core module for spoken language learning, and speech therapy.
__label__generative_models By utilizing structure tokens, our model learns a unified representation, enhancing the ability to process diverse data without requiring extra modules or models.
__label__privacy Invisible watermarking is essential for safeguarding digital content, enabling copyright protection and content authentication.
__label__natural_language_processing We exploit a common form among a family of $f$-divergence regularized alignment approaches (such as PPO, DPO, and their variants) to identify a closed-form solution by Legendre transform, and derive an efficient decoding strategy.
__label__interpretability_and_explainability We demonstrate that the proposed fine-tuning improves intervention effectiveness and often yields better-calibrated predictions.
__label__neuroscience_and_cognitive_science Based on this investigation, we find that while for cross-modal models, their brain alignment is partially attributed to the video modality; for jointly pretrained models, it is partially attributed to both the video and audio modalities.
__label__reinforcement_learning Numerical results show that our method significantly reduces underestimation bias and improves performance in various offline control tasks compared to other offline RL methods.
__label__machine_vision The source code is available at https://github.com/Nokia-Bell-Labs/data-channel-extension.
__label__optimization_for_deep_networks Long-tailed visual recognition has received increasing attention recently.
__label__causal_inference We propose a novel neural network model for graph structure inference, which aims to learn a mapping from observational data to the corresponding underlying dependence structures.
__label__machine_vision KptLLM underscores the initial discernment of semantics in keypoints, followed by the precise determination of their positions through a chain-of-thought process.
__label__natural_language_processing Our theory points to simple solutions towards ameliorating these issues.
__label__generative_models Existing radiance representations either require an implicit feature decoder, which significantly degrades the modeling power of the representation, or are spatially unstructured, making them difficult to integrate with mainstream 3D diffusion methods.
__label__reinforcement_learning We then identify a condition of the partially observable environment, the deterministic filter condition, under which expert distillation achieves sample and computational complexities that are *both* polynomial.
__label__generative_models Evaluations on FID, HPSv2 benchmarks, and human feedback demonstrate the competitive performance of Fantasy against state-of-the-art diffusion and autoregressive models.
__label__natural_language_processing For generation, we compare our model with many strong baselines, including ChatQA-1.5, an open-sourced model with the state-of-the-art performance on RAG benchmarks.
"__label__safety_in_machine_learning Finally, addressing
methodological deficiencies observed in previous studies, we propose new ways of
robust benchmarking for future model extraction attacks."
__label__machine_learning_for_healthcare At the core of these challenges is a large matrix that is commonly sparsified for computational efficiency by neglecting small elements.
__label__reinforcement_learning Existing work attempts to address this issue by data augmentation with the learned policy or adding extra constraints with the value-based RL algorithm.
__label__natural_language_processing In this paper, we present a simple yet effective approach, called MiniCache, to compress the KV cache across layers from a novel depth perspective, significantly reducing the memory footprint for LLM inference.
__label__generative_models We discover two primary quantization challenges inherent in DiTs, notably the presence of salient channels with extreme magnitudes and the temporal variability in distributions of salient activation over multiple timesteps.
__label__machine_vision We integrate pose embedding to encapsulate information such as multi-view camera poses, providing implicit geometric constraints for multi-view disparity feature fusion dominated by attention.
__label__machine_vision To prevent excessive growth of the prompt sets, we utilize the maximum angular coverage (MAC) of the semantic space as a criterion for early termination.
__label__privacy Furthermore, we realize our upper bounds using a practical algorithm and demonstrate its advantage in high-dimensional regimes compared to prior approaches.
__label__interpretability_and_explainability 2022), the current consensus is that it is a necessary initial step for designing state-of-the-art performant language models.
__label__machine_vision In this paper, we propose a novel FedMVC framework, which concurrently addresses two challenges associated with heterogeneous hybrid views, i.e., client gap and view gap.
__label__neuroscience_and_cognitive_science Due to the incompatibility of self-attention with spikes, however, existing transformer-based SNNs limit themselves by either restructuring self-attention architecture or conforming to non-spike computations.
__label__generative_models However, these models often exhibit slow inference speeds and produce non-fluent texts.
__label__safety_in_machine_learning To reduce the sensitivity and improve the corruption robustness, we propose Frequency Adversarial Training (FAT) that adopts frequency-domain adversarial examples as data augmentation to train robust point cloud recognition models against corruptions.
__label__safety_in_machine_learning KLE defines positive semidefinite unit trace kernels to encode the semantic similarities of LLM outputs and quantifies uncertainty using the von Neumann entropy.
__label__machine_learning_for_other_sciences_and_fields We employ a hybrid optimization strategy and demonstrate the effectiveness of our approach on synthetic datasets composed of mixtures of proteins with large degrees of conformational variability.
__label__fairness On the positive side, we give an algorithm for fair low-rank approximation that, for a constant number of groups and constant-factor accuracy, runs in $2^{\text{poly}(k)}$ rather than the naive $n^{\text{poly}(k)}$, which is a substantial improvement when the dataset has a large number $n$ of observations.
__label__infrastructure However, a common challenge in this setting is the continuous adaptation of models necessary to accommodate changing environments, i.e., data distribution shifts.
__label__natural_language_processing CSE is also a lightweight structure that only needs to be incorporated in several deep layers.
__label__other On top of two different unsupervised feature extractors, we obtain significant accuracy improvements on image recognition datasets such as CIFAR-10 and CIFAR-100.
__label__speech_and_audio To train it, we discard the dynamic programming of RNN-T in favor of the frame-wise cross-entropy loss of AED, while the decoder employs the lighter text-only recurrence of RNN-T without learned cross-attention---it simply scans embedding frames in order from the beginning, producing one token each until predicting the end-of-message.
__label__online_learning We investigate the problem of sequentially adapting a model to non-stationary environments, where the data distribution is continuously shifting and only a small amount of unlabeled data are available each time.
__label__robotics We propose an LM-based Long-Horizon Planner for Multi-Agent Robotics (LLaMAR), a cognitive architecture for planning that achieves state-of-the-art results in long-horizon tasks within partially observable environments.
__label__machine_vision To deal with it, augmentation techniques are commonly used, such as copying ground truth LiDAR points and pasting them into scenes.
__label__deep_learning_architectures Finally, we study how ViT architectural attributes impact OoD generalization.
__label__natural_language_processing Prior approaches have thus explored how engineers might select an LLM to use for each sample (i.e.
__label__generative_models Our model is end-to-end trainable and supports rendering arbitrary views with as few as 5 sparse input views.
__label__neuroscience_and_cognitive_science EEG data were recorded from watermelons to remove stimulus-driven neural responses.
__label__natural_language_processing Our work pioneers new directions in retrieval-augmented generation from the perspective of multimodality fusion, and we hope it lays the foundation for future efficient and scalable retrieval-augmented systems.
__label__probabilistic_methods Here, we address these challenges by showing how diffusion-based models (DBMs), which have recently produced state-of-the-art performance in generative modeling tasks, can be repurposed for performing calibrated, identifiable Bayesian inference.
__label__machine_learning_for_other_sciences_and_fields Codes are available at https://github.com/OnlyLoveKFC/Neural_P3M.
__label__other - $\ell_p$ for $p \ge 1$: $d = O(m)$ is sufficient and $d = \tilde \Omega(\sqrt{m})$ is necessary.
__label__generative_models Going beyond mimicking limited human experiences, recent studies show initial evidence that, like humans, large language models (LLMs) are capable of improving their abilities purely by self-correction, i.e., correcting previous responses through self-examination, as seen in models like OpenAI o1.
__label__reinforcement_learning Based on our analysis, we develop a method that directly trains on scenarios with high learnability.
"__label__diffusion_based_models This lets us train the student model on real data, thus mitigating the imperfect ""real"" score estimation from the teacher model, and thereby enhancing quality."
__label__online_learning Traditional online algorithms typically assume a small bid setting, where the maximum bid-to-budget ratio ($\kappa$) is infinitesimally small.
__label__safety_in_machine_learning We study how to perform unlearning, i.e.
"__label__machine_learning_for_physical_sciences Code is available
https://github.com/shenoynikhil/ETFlow."
__label__graph_neural_networks Despite its success, the current literature has overlooked the efficiency of label smoothing in node classification with graph-structured data.
__label__interpretability_and_explainability While this coarse explanation is helpful for guiding root cause analyses, it provides limited details and can only suggest coarse fixes involving all variables in an ML system.
__label__machine_learning_for_social_sciences Our results show that inferred latent features align well with ground truth labels and significantly enhance the downstream classifier.
__label__other Unsupervised domain adaptation excels in transferring knowledge from a labeled source domain to an unlabeled target domain, playing a critical role in time series applications.
__label__deep_learning_architectures However, each hidden layer was formalized as an abstract group action, so it was not possible to cover real deep networks defined by composites of nonlinear activation function.
__label__privacy We study the limits and capability of public-data  assisted differentially private (PA-DP) algorithms.
__label__neuroscience_and_cognitive_science Additionally, we introduce an extended latent dynamics model to improve robustness against system nonlinearities.
__label__natural_language_processing To enhance this capability, existing work either collects large-scale math-related texts for pre-training, or relies on stronger LLMs (\eg GPT-4) to synthesize massive math problems.
__label__safety_in_machine_learning We develop a mechanism that assigns influence scores to neurons in each path to allow the strengthening of these cuts.
__label__machine_learning_for_physical_sciences First, we used simulated data from a toy model to tune the hyperparameters.
__label__causal_inference Subsequently, we propose a sound algorithm for the s-ID problem with latent variables.
__label__machine_vision To address these challenges and enable real-time action understanding in open-world scenarios, we propose OV-OAD, a zero-shot online action detector that leverages vision-language models and learns solely from text supervision.
__label__machine_vision However, effectively applying linear group RNN to 3D object detection in highly sparse point clouds is not trivial due to its limitation in handling spatial modeling.
__label__optimization_for_deep_networks Large Language Models (LLMs) are typically trained in two phases: pre-training on large internet-scale datasets, and fine-tuning for downstream tasks.
__label__diffusion_based_models Recent advancements in Automatic Prompt Optimization (APO) for text-to-image generation have streamlined user input while ensuring high-quality image output.
__label__online_learning Additionally, the proposed method can be seamlessly integrated into existing continual learning methods yielding significant performance improvement.
__label__learning_theory Experiments using deep learning models show that our bounds are nonvacuous thanks to this information-theoretic generalization analysis approach.
__label__online_learning We show that this approach guarantees a convergence rate of $\tilde{\mathcal{O}}(T^{-1/2})$ with high probability and has a near-optimal dependence on the game parameters when applied with the best theoretical choices of learning rates and sampling policies.
__label__robotics Codes and models will be public.
__label__deep_learning_architectures The code is publicly available at https://github.com/MathematicalAI-NUS/Monomial-NFN.
__label__deep_learning_architectures We evaluate rule extrapolation in formal languages with varying complexity in linear and recurrent architectures, the Transformer, and state space models to understand the architectures' influence on rule extrapolation.
__label__optimization_for_deep_networks It also improves LoRA in text classification (GLUE) and mathematical reasoning (GSM-8K).
__label__generative_models This allows us to inform the choice of perturbation from the structure of the modelled discrete variable, while the continuous time parameter enables fine-grained control of the perturbation.
__label__graph_neural_networks We also introduce novel node marking strategies and provide a theoretical analysis of their expressive power and other key aspects of our approach.
__label__online_learning The trichotomy is then determined by a combination of the Level-constrained Littlestone and Branching dimensions.
__label__generative_models Through comprehensive experiments, we observe that RCG significantly improves unconditional generation quality: e.g., it achieves a new state-of-the-art FID of 2.15 on ImageNet 256x256, largely reducing the previous best of 5.91 by a relative 64%.
__label__other Using the distortion-rate function as the baseline, we study the performance of existing compression schemes on a synthetic dataset consisting of prompts generated from a Markov chain, natural language queries, and their respective answers.
__label__graph_neural_networks However, despite its effectiveness, BP has limitations that challenge its biological plausibility and affect the efficiency, scalability and parallelism of training neural networks for graph-based tasks.
__label__causal_inference However, variances and generalization bound of these methods are generally unbounded when the propensity scores tend to zero, compromising their stability and robustness.
__label__optimization_for_deep_networks We report empirical results on various models and benchmarks to verify GmP's advantages of optimization stability, convergence speed and generalization performance.
__label__machine_vision First, we introduce a scale factor and an injection step to balance text and image features in cross-attention and to preserve image information in self-attention during the text-image inversion diffusion process, respectively.
__label__graph_neural_networks The adversarial robustness of Graph Neural Networks (GNNs) has been questioned due to the false sense of security uncovered by strong adaptive attacks despite the existence of numerous defenses.
__label__graph_neural_networks A unified instruction is designed for various graph tasks at different levels, such as node classification (node-level) and link prediction (edge-level).
__label__safety_in_machine_learning To establish a foundation for more realistic Multimodal OOD Detection, we introduce the first-of-its-kind benchmark, MultiOOD, characterized by diverse dataset sizes and varying modality combinations.
__label__machine_vision We validate the effectiveness of our method across benchmarks featuring both semantic and domain shifts.
__label__probabilistic_methods (3) We evaluate EPH on synthetic and real-world datasets including vector and graph datasets.
__label__online_learning Through the incorporation of non-linear activation and residual connection, NoRGa enhances continual learning performance while preserving parameter efficiency.
__label__machine_vision Through this annotation-free test-time adaptation, MfH achieves superior zero-shot performance in MMDE, demonstrating its strong generalization ability.
__label__machine_vision Current state-of-the-art LTSSL approaches heavily rely on high-quality pseudo-labels for large-scale unlabeled data.
__label__optimization Finally, we provide empirical results that demonstrate that our method offers improved robustness to outliers and is computationally less demanding for regression and classification tasks.
__label__machine_vision Here, we investigate how aligning vision model representations to human perceptual judgments impacts their usability in standard computer vision tasks.
__label__natural_language_processing To address this challenge, \textsc{DeePEn} maps the probability distribution of each model from its own probability space to a universal \textit{relative space} based on the relative representation theory, and performs aggregation.
__label__neuroscience_and_cognitive_science Our methods improve the accuracy by 4.05\% on ImageNet compared to baseline and achieve state-of-the-art performance of 87.80\% on CIFAR10-DVS and 87.86\% on N-Caltech101.
__label__safety_in_machine_learning Improving out-of-distribution (OOD) generalization during in-distribution (ID) adaptation is a primary goal of robust fine-tuning of zero-shot models beyond naive fine-tuning.
__label__privacy We design adaptivity benchmarks, based on CIFAR-10 and CelebA, and show that ARS improves standard test accuracy by  1 to 15\% points.
__label__other This enables the training of more distinctive representations, enhancing the ability to select clean samples.
__label__neuroscience_and_cognitive_science Our dataset and code will be released soon.
__label__probabilistic_methods Treeffuser learns well-calibrated predictive distributions and can handle a wide range of regression tasks---including those with multivariate, multimodal, and skewed responses.
__label__diffusion_based_models We further design a spherical epipolar-aware multi-view diffusion model to ensure the multi-view consistency of the generated panoramic images.
__label__algorithmic_game_theory We study, through this lens, several well-studied mechanism design paradigms, devising new mechanisms, but also providing refined analysis for existing ones, using as a metric the quality of recommendation.
__label__algorithmic_game_theory We argue that an approach informed by social choice theory is especially suitable.
__label__graph_neural_networks Graph Neural Networks (GNNs) excel across various applications but remain vulnerable to adversarial attacks, particularly Graph Injection Attacks (GIAs), which inject malicious nodes into the original graph and pose realistic threats.
__label__online_learning Our results show (i) collaboration is unnecessary in the absence of computational constraints on clients; (ii) collaboration is necessary if the computational cost on each client is limited to $o(K)$, where $K$ is the number of candidate hypothesis spaces.
__label__graph_neural_networks Graph CIL (GCIL) follows the same setting but needs to deal with graph tasks (e.g., node classification in a graph).
__label__machine_vision Codes and pre-trained models are released on the  https://github.com/yangtiming/ImOV3D.
__label__generative_models Our approach offers an alternative to conventional supervised fine-tuning and RL strategies, significantly improving both model performance and alignment.
"__label__generative_models ), traditional RLHF approaches adopt a ""one-size-fits-all"" approach, i.e., they indiscriminately assume and optimize a single preference model, thus not being robust to unique characteristics and needs of the various groups."
__label__online_learning Our algorithm relies on three new techniques including an improved Bernstein's inequality for martingale, a federated online mirror descent framework, and decoupling model selection and prediction, which might be of independent interest.
__label__interpretability_and_explainability Identifying these subsets leads to gaining insights, domain intuition,and a better understanding of the data-generating mechanism; it is often the first step in causal modeling.
__label__machine_vision Advanced retrieval systems usually adopt a cascade of aesthetic models as re-rankers or filters, which are limited to low-level features like saturation and perform poorly when stylistic, cultural or knowledge contexts are involved.
__label__machine_vision We demonstrate that these two pre-trained uni-modal models can be well-aligned by fine-tuning on the modest multi-modal dataset and avoid intricate training configurations.
__label__machine_learning_for_other_sciences_and_fields For samples where some annotators disagree, a comparative strategy is proposed to filter noise.
__label__machine_learning_for_physical_sciences The integration of the PDE into a loss function endows PINNs with a distinctive feature to require computing derivatives of model up to the PDE order.
__label__diffusion_based_models We propose a novel method for learning representations of poses for 3D deformable objects, which specializes in 1) disentangling pose information from the object's identity, 2) facilitating the learning of pose variations, and 3) transferring pose information to other object identities.
__label__machine_vision Conversely, batch-to-global matching typifies decoupled methods, which are particularly advantageous for large-scale datasets.
__label__diffusion_based_models Our code will be publicly released.
__label__machine_learning_for_healthcare Additionally, we deploy our method in an online diagnosis system to illustrate its effectiveness in real clinical applications.
__label__machine_vision First, a dynamic vision module that enables a variable and learnable number of box proposals.
__label__machine_learning_for_other_sciences_and_fields CoupleNet integrates multiple levels and scales of features in proteins, encompassing residue identities and positions for sequences, as well as geometric representations for tertiary structures from both local and global perspectives.
__label__machine_vision We provide a  closed form solution of the procedure and adjust it for robust stochastic training while computing everything efficiently.
__label__causal_inference It is shown to be bounded by the *cyclic complexity number* of the mixture model, defined as the size of the minimal intervention that can break the cycles in the mixture, which is upper bounded by the number of cycles among the ancestors of a node.
__label__machine_learning_for_healthcare The $\mathcal{S}$-Router then specializes in handling fewer modality combinations by assigning the top-1 gate to the expert corresponding to the observed modality combination.
__label__machine_vision Our extensive experiments demonstrate that MapUnveiler achieves state-of-the-art performance on both the nuScenes and Argoverse2 benchmark datasets.
__label__natural_language_processing This study explores how LLMs handle multilingualism.
__label__interpretability_and_explainability From a well-trained sparse and shallow neural network, one can interpret each layer and neuron through the language of logic rules, and a global explanatory rule set can be directly extracted.
__label__diffusion_based_models That is, the control is learned via a least squares problem by trying to fit a matching vector field.
__label__machine_learning_for_healthcare The GPTrack enhances motion tracking by employing the sequential Gaussian Process in the latent space and encoding statistics by spatial information at each time stamp, which robustly promotes temporal consistency and spatial variability of cardiac dynamics.
__label__causal_inference We propose plug-in estimators which are simple and readily implementable using off-the-shelf algorithms.
__label__machine_vision Many of our improvements are architecture agnostic and can be incorporated into new architectures as they are developed.
__label__other The code is available at \url{https://anonymous.4open.science/r/SSCNN-321D/}.
__label__natural_language_processing We assess various approaches, including training-free and fine-tuning methods for LLMs, the impact of model architecture, and the effectiveness of active prompting techniques for smoothing decision boundaries in a data-efficient manner.
__label__machine_vision For the *prediction bias*, we first introduce clustering-guided initialization to provide robust features.
__label__machine_vision This is due to the lack of intelligent systems that can quickly generate simpler intermediate parts.
__label__learning_theory All our results are under the Gaussian universality ansatz and the (non-rigorous) risk predictions in terms of the kernel eigenstructure.
__label__deep_learning_architectures Meanwhile, a domain discriminator is designed to transfer the domain invariant knowledge.
__label__generative_models While diffusion models have shown impressive performance in 2D image/video generation, diffusion-based Text-to-Multi-view-Video (T2MVid) generation remains underexplored.
__label__infrastructure We present a framework for distributed computation over a quantum network in which data is encoded into specialized quantum states.
__label__learning_theory However, when non-negativity constraints are imposed on the function's outputs, the literature usually takes the kernel method-based approximators as offering linear representations at the expense of limited model flexibility or good representation power by allowing for their nonlinear forms.
__label__probabilistic_methods Experimental results indicate that our estimator accurately estimates model parameters on synthetic data and yields results consistent with MLE on real data.
__label__generative_models However, its performance is still limited as it lacks high-quality annotated datasets for training.
__label__graph_neural_networks Theoretically, we demonstrate that IPR-MPNNs surpass the expressiveness of traditional MPNNs.
__label__deep_learning_architectures However, DNNs still lag behind Gradient Boosting Decision Trees (GBDT) on tabular data, a format extensively utilized across various domains.
__label__privacy In the important honest-but-curious setting, existing attacks enable exact reconstruction only for batch size of $b=1$, with larger batches permitting only approximate reconstruction.
__label__reinforcement_learning However, this compression into a compact discrete representation may ignore visual details that are important for reinforcement learning.
__label__optimization This is further exacerbated when considering the stochastic nature of human feedback.
__label__bandits At the conceptual level, we demonstrate that complexity of best-of-both-worlds bandits with delayed feedback is characterized by the amount of information missing at the time of decision making (measured by the number of outstanding observations) rather than the time that the information is missing (measured by the delays).
__label__generative_models These representations can be used to condition the image generator.
__label__bandits While binary choice feedback is simple and widely used, it offers limited information about preference strength.
__label__online_learning In this work, we design a universal approach with the *optimal* gradient-variation regret simultaneously for strongly convex, exp-concave, and convex functions, thus addressing an open problem highlighted by [Yan et al.
__label__learning_theory To close the gap, in this work we'll take a first-principles approach, i.e.
__label__neuroscience_and_cognitive_science While previous studies typically curtail variability to allow for high task performance in neural networks, our approach takes the reversed perspective.
__label__causal_inference However, the selected proxies may scramble both latent confounders and latent post-treatment variables in practice, where existing methods risk biasing the estimation by unintentionally controlling for variables affected by the treatment.
__label__deep_learning_architectures Consequently, TS-forecasting has a more rigorous evaluation methodology compared to classification.
__label__natural_language_processing At the lower layers, we aggregate tokens into fixed size blocks to apply attention across the entire sequence at coarse-grained detail, to capture the global context while minimizing KV cache overhead.
__label__interpretability_and_explainability These methods require efficient approximations, and although amortizing the process by learning a network to directly predict the desired output is a promising solution, training such models with exact labels is often infeasible.
__label__generative_models In this work, we introduce Unique3D, a novel image-to-3D framework for efficiently generating high-quality 3D meshes from single-view images, featuring state-of-the-art generation fidelity and strong generalizability.
__label__machine_vision Code and models are available at https://github.com/ZhangDailing8/CPDTrack.
__label__learning_theory Additionally, we show that when the validity region belongs to a VC-class, a limited number of validity queries are often sufficient.
__label__machine_learning_for_physical_sciences To address this, data-driven approaches learn parametric PDEs by sampling a very large variety of trajectories with varying PDE parameters.
__label__online_learning We study a data pricing problem, where a seller has access to $N$ homogeneous data points (e.g.
__label__other They assume that MP helps CF methods in a manner akin to its benefits for graph-based learning tasks in general (e.g., node classification).
__label__machine_vision \emph{The question is how to align the PV features with the generative models to facilitate the map estimation}.
__label__optimization_for_deep_networks In this work, we study the landscape through the lens of the Hessian, with a focus on its largest eigenvalue (i.e.
__label__machine_vision However, PTMs are often ''black-box,'' where information on such models is unavailable for commercial reasons or social responsibilities.
__label__probabilistic_methods Our numerical experiments on both hypothesis testing and generative modelling validate the out-performance of our approach compared with several state-of-the-art methods, highlighting its potential in broad applications of synthetic time series generation and in addressing classic financial and economic challenges, such as optimal stopping or utility maximisation problems.
__label__diffusion_based_models In this paper, we present **Make-An-Agent**, a novel policy parameter generator that leverages the power of conditional diffusion models for behavior-to-policy generation.
__label__bandits While these findings can be interpreted positively, they suggest that external summarization—which may not be possible in more complex settings—is essential for desirable LLM behavior.
__label__machine_learning_for_healthcare Despite the remarkable progress achieved in medical image segmentation, there is still an absence of a 3D foundation segmentation model that can segment a wide range of anatomical categories with easy user interaction.
__label__machine_learning_for_other_sciences_and_fields Machine learning (ML) is playing an increasingly important role in scientific research.
__label__natural_language_processing We also find NCA significantly outperforms DPO in complex reasoning tasks like math and coding.
__label__deep_learning_architectures However, the pose estimation model trained by the existing daily human pose datasets has poor robustness under specific poses such as pedestrian pre-collision pose, and it is difficult to obtain human pose datasets in the wild scenes, especially lacking scarce data such as pedestrian pre-collision pose in traffic scenes.
__label__machine_learning_for_healthcare This helps the model learn the semantics of time series to generalize across activities.
__label__natural_language_processing Mixture of Experts (MoE) framework has become a popular architecture for large language models due to its superior performance compared to dense models.
__label__diffusion_based_models We emphasize that this random mixture of noise-data mapping complicates the optimization of the denoising function in diffusion models.
__label__natural_language_processing Extensive experiments demonstrate that our idea improves comparison prompt optimization methods by 1.42% for soft prompt generalization and 2.16% for hard prompt generalization in accuracy on the multi-source domain generalization setting, while maintaining satisfying in-domain performance.
__label__probabilistic_methods Recently, through a unified gradient flow perspective of Markov chain Monte Carlo (MCMC) and variational inference (VI), particle-based variational inference methods (ParVIs) have been proposed that tend to combine the best of both worlds.
__label__learning_theory This paper performs an empirical evaluation of these persistent homology-based generalization measures, with an in-depth statistical analysis.
__label__safety_in_machine_learning Adversarial robustness and privacy of deep learning (DL) models are two widely studied topics in AI security.
__label__machine_learning_for_healthcare Under our framework, the model individualization stems from an optimal internal relation map within the samples themselves.
"__label__algorithmic_game_theory Our paper seeks to partially fill this gap by focusing on the full class of (generalized) harmonic games and examining the convergence properties of ""follow-the-regularized-leader"" (FTRL), the most widely studied class of no-regret learning schemes."
__label__causal_inference We discuss when these assumptions are appropriate.
__label__learning_theory We prove that learning is hard under input manifolds of bounded curvature by extending proofs of hardness in the SQ and cryptographic settings for boolean data inputs to the geometric setting.
__label__reinforcement_learning Somewhat surprisingly, our proposals also outperform baselines that get to see the task during training.
__label__safety_in_machine_learning However, many existing watermarking methods, particularly content-agnostic approaches that embed fixed patterns regardless of image content, are vulnerable to steganalysis attacks that can extract and remove the watermark with minimal perceptual distortion.
__label__reinforcement_learning We provide simulation results to demonstrate the performance of our algorithm.
__label__learning_theory Finally, we demonstrate that our approach provides a steep reduction in computation time (as much as several thousand times faster, depending on the algorithm for deeper networks) while yielding Lipschitz bounds that are very close to or even better than those achieved by state-of-the-art approaches in a broad range of experiments.
__label__optimization At each layer, given a learned graph, the target interpolated signal is simply a low-pass filtered output derived from the minimization of an assumed graph smoothness prior, leading to a dramatic reduction in parameter count.
__label__speech_and_audio SOI leverages the continuity and seasonality of time-series data and model predictions, enabling extrapolation for processing speed improvements, particularly in deeper layers.
__label__machine_learning_for_social_sciences Instantiated in our setting, performative power quantifies the ability of a search engine to steer web traffic by rearranging results.
__label__robotics Specifically, VDD leverages a decompositional upper bound of the variational objective that allows the training of each expert separately, resulting in a robust optimization scheme for MoEs.
__label__online_learning Away from the lazy regime, networks can be seen as ensembles of adaptive experts which improve over time.
__label__machine_learning_for_other_sciences_and_fields Furthermore, as a novel plug-and-play technique, the RCF can also significantly improve the prediction accuracy of existing models, including PatchTST and iTransformer.
__label__natural_language_processing As a cost-effective alternative, learning-free PTQ schemes have been proposed.
__label__safety_in_machine_learning Through extensive experiments, we carefully demonstrate the effectiveness of NoiseGPT on detecting and cleansing dataset noise, especially on ILSVRC12, the AUROC of NoiseGPT reached over 0.92.
__label__reinforcement_learning Recent works have shown the remarkable superiority of transformer models in reinforcement learning (RL), where the decision-making problem is formulated as sequential generation.
__label__reinforcement_learning Existing unsupervised skill discovery methods learn skills by encouraging distinguishable behaviors that cover diverse states.
__label__diffusion_based_models LOCO Edit identified editing directions with nice properties: homogeneity, transferability, composability, and linearity.
__label__graph_neural_networks Here, we develop GraphMETRO, a Graph Neural Network architecture that models natural diversity and captures complex distributional shifts.
__label__neuroscience_and_cognitive_science Recent advances in calcium imaging enable simultaneous recordings of up to a million neurons in behaving animals, producing datasets of unprecedented scales.
__label__machine_learning_for_other_sciences_and_fields Our results show that CodeRosetta outperforms state-of-the-art baselines in C++ to CUDA translation by 2.9 BLEU and 1.72 CodeBLUE points while improving compilation accuracy by 6.05%.
"__label__machine_learning_for_healthcare Specifically, 
we introduce extensive subgraph selection and encoding spaces that account for the diverse contexts of drug interactions in DDI prediction."
__label__learning_theory subsampling of the training data.
__label__optimization_for_deep_networks ADOPT addresses the non-convergence issue of Adam by removing the current gradient from the second moment estimate and changing the order of the momentum update and the normalization by the second moment estimate.
__label__machine_vision Consequently, the reconstructed videos are often plagued by artifacts and regional blur, primarily caused by the ambiguous semantics of event data.
__label__causal_inference Concretely, we construct non-linear features from the moments of unit outcomes and treatments and then learn a function that maps these features to future mean and variance of unit outcomes.
"__label__deep_learning_architectures To address this challenge, we propose a novel approach called ""amortized eigendecomposition"" that relaxes the exact eigendecomposition by introducing an additional loss term called eigen loss."
__label__machine_vision Additionally, for IR, it is commonly noted that small segments of a degraded image, particularly those closely aligned semantically, provide particularly relevant information to aid in the restoration process, as they contribute essential contextual cues crucial for accurate reconstruction.
__label__natural_language_processing Self-evaluation using large language models (LLMs) has proven valuable not only in benchmarking but also methods like reward modeling, constitutional AI, and self-refinement.
__label__optimization In this work, we propose a semi-definite programming (SDP) relaxation of the GW distance.
__label__privacy We show how to mount a near-perfect attack on the deleted data point from linear regression models.
__label__bandits Previous work focused on the platform perspective, with the goal of setting prices maximizing the *gain from trade* (the sum of sellers' and buyers' utilities).
__label__machine_learning_for_other_sciences_and_fields Secondly, various noise and missing peaks in mass spectra reduce the reliability of training data (Peptide-Spectrum Matches, PSMs).
__label__reinforcement_learning To alleviate this, we introduce DiffuserLite, a super fast and lightweight diffusion planning framework, which employs a planning refinement process (PRP) to generate coarse-to-fine-grained trajectories, significantly reducing the modeling of redundant information and leading to notable increases in decision-making frequency.
__label__machine_vision Our proposed interpolation mechanism draws inspiration from classic image-processing techniques, offering a more interpretable, stable, and faster approach tailored specifically for the object tracking task.
__label__optimization The best practical algorithms for the problem only guarantee $1/e$-approximation.
__label__probabilistic_methods Under this paradigm, we provide asymptotically honest group inference procedures based on the idea of orthogonalization, which enjoys the feature that it does not require the zero and nonzero coefficients to be well-separated.
__label__other We demonstrated its effectiveness and efficiency across diverse widely-used FL datasets.
__label__machine_vision Without additional training, we connect these two generalized models with attention maps as the prompts.
__label__interpretability_and_explainability Moreover, the structured nature of NCB's concept representations allows for intuitive inspection and the straightforward integration of external knowledge, such as human input or insights from other AI models like GPT-4.
__label__online_learning This approach simultaneously achieves high accuracy and low resource consumption.
__label__diffusion_based_models This method employs a domain-agnostic diffusion model, conditioned on the LiDAR points surrounding a coarse bounding box, to simultaneously refine the box's location, size, and orientation.
__label__machine_learning_for_social_sciences LLMs overcome the limitations of previous models by effectively processing semantic data and offering versatility in modeling various tasks.
__label__natural_language_processing Recent approaches primarily focus on improving the reasoning process to yield a more precise final answer.
__label__deep_learning_architectures Specifically, we reformulate the selective state space model and linear attention within a unified formulation, rephrasing Mamba as a variant of linear attention Transformer with six major distinctions: input gate, forget gate, shortcut, no attention normalization, single-head, and modified block design.
__label__diffusion_based_models Hence, we propose to suppress content shift to enhance the overall quality of diffusion features.
__label__interpretability_and_explainability Methods based on implicit differentiation, such as influence functions, can be made computationally efficient, but fail to account for underspecification, the implicit bias of the optimization algorithm, or multi-stage training pipelines.
__label__safety_in_machine_learning Existing attacks work either in the white-box setting (with full access to the model weights), or through _transferability_: the phenomenon that adversarial examples crafted on one model often remain effective on other models.
__label__other An NCG is a group of neurons mapped to a specific class, implementing intra-class WTA and a novel competition regulation mechanism based on two-compartment thresholds.
__label__learning_theory Apart from the independence of $X^*$, we only require a small fraction entries of $\eta$ to have magnitude at most $1$.
__label__interpretability_and_explainability Overall, our findings show that while steering can work well in the right circumstances, there remain many technical difficulties of applying steering vectors to guide models' behaviour at scale.
__label__natural_language_processing We discuss how the method we propose could be used to help researchers be more specific in the claims they make about large language models.
__label__machine_vision Human reasoning, in contrast, typically operates within the context of specific subjects in our surroundings.
__label__natural_language_processing Knowledge Graph Embedding (KGE) techniques are crucial in learning compact representations of entities and relations within a knowledge graph, facilitating efficient reasoning and knowledge discovery.
__label__machine_vision One contributing factor is their direct comparison of a query image's features with those of few-shot normal images.
__label__probabilistic_methods Our code for the sampling methods and benchmarks studied is made public at [this link](https://github.com/GFNOrg/gfn-diffusion) as a base for future work on diffusion models for amortized inference.
__label__reinforcement_learning Theoretical connections are established between SI2E and classical information-theoretic methodologies, highlighting our framework’s rationality and advantage.
__label__privacy As an application, we analyze how subsampling affects the privacy of groups of multiple users.
__label__generative_models [2023] utilizes the depth-first search-based decision tree (DFSDT) mechanism for multi-step reasoning with $16000+$ real-world APIs, effectively enhancing the performance of tool-augmented LLMs compared to traditional chain reasoning mechanisms.
__label__neuroscience_and_cognitive_science We show vastly superior performance compared to Adam on a number of RNN tasks, including a difficult double-reaching motor task and the learning of an adaptive Kalman filter algorithm trained over a long horizon.
__label__machine_vision Adapting VLMs with prompt learning offers an alternative to direct alignment.
__label__deep_learning_architectures In this paper, we utilize the Neural Collapse ($\mathcal{NC}$) as a tool to systematically analyze the representation of DEQ under both balanced and imbalanced conditions.
__label__causal_inference A middle ground between these two approaches is to estimate the causal effect of interest through proxy experiments, which are  conducted on variables with a lower cost to intervene on compared to the main target.
__label__machine_vision Moreover, our unified optimized projection space exhibits encouraging robustness performance for unseen data (degraded and depth images).
__label__causal_inference Instead, it involves a quantitative joint optimization of bias and variance.
__label__natural_language_processing In controlled-sentiment generation and summarization, we use tuned and untuned $\texttt{gpt2}$s to improve the alignment of large models without additional training.
__label__interpretability_and_explainability Notably, we showcase the versatility and usability of SCBMs by examining a setting with CLIP-inferred concepts, alleviating the need for manual concept annotations.
__label__generative_models We propose a novel encoding method called ``Structure Token'' to unify the processing and generation of both graphs and texts with a single transformer-based model.
__label__optimization Furthermore, we demonstrate that a stochastic variant of the generalized Frank-Wolfe algorithm for MVI problems also converges in a last-iterate sense, albeit at a slower $\mathcal{O}(T^{-1/6})$ convergence rate.
__label__privacy We then generalize our attack to other loss functions and architectures,  and empirically demonstrate the effectiveness of our attacks across a wide range of datasets (capturing both tabular and image data).
__label__safety_in_machine_learning Our approach strategically labels examples within a novel maximum disambiguation region, where the number of semantic and covariate OOD data roughly equalizes.
__label__generative_models At every inference step we combine the $k$ drafts with the top-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options, using an n-gram interpolation with minimal compute overhead to filter out incoherent generations.
__label__deep_learning_architectures By exploring the similarities and disparities between the effective Mamba and subpar linear attention Transformer, we provide comprehensive analyses to demystify the key factors behind Mamba’s success.
__label__diffusion_based_models However, current video diffusion models exhibit demanding computational requirements and high peak memory usage, especially for generating longer and higher-resolution videos.
__label__diffusion_based_models Project website: https://boyuan.space/diffusion-forcing/
__label__other Experiments verify that CoLA is simple but effective, which boosts the efficiency of TTA and demonstrates remarkable superiority in collaborative, lifelong, and single-domain TTA scenarios, e.g., on follower agents, we enhance accuracy by over 30\% on ImageNet-C while maintaining nearly the same efficiency as standard inference.
__label__machine_vision Furthermore, we design a Gaussian pooling layer to aggregate various Gaussian groups for efficient representations.
__label__reinforcement_learning We present Preference Flow Matching (PFM), a new framework for preference alignment that streamlines the integration of preferences into an arbitrary class of pre-trained models.
__label__optimization In this paper, we propose a particular case of the UGW problem, termed Partial Gromov-Wasserstein (PGW).
__label__privacy Therefore, ensuring the security and fidelity of the original 3D scene while embedding information into the 3DGS point cloud files is an extremely challenging task.
__label__probabilistic_methods In this work, we propose a framework where the advantages of the attention-based architecture are maintained and the limitation of the thinning algorithm is circumvented.
__label__reinforcement_learning Existing works mainly focus on arranging the levels to explicitly form a curriculum.
__label__deep_learning_architectures Empirically, AutoTimes achieves state-of-the-art with 0.1% trainable parameters and over $5\times$ training/inference speedup compared to advanced LLM-based forecasters.
__label__machine_vision RegionSpot outperforms GLIP-L by 2.9 in mAP on LVIS val set,  with an even larger margin of 13.1 AP for more challenging and rare categories, and a 2.5 AP increase on ODinW.
__label__machine_vision Our experiments, including comparisons to state-of-the-art approaches and user studies, demonstrate our approach's ability to interpret and execute complex instructions for camera operation, showing promising applications in real-world production settings.
__label__machine_vision LOFF-TA involves training a compact classifier on cached feature embeddings  from a frozen foundation model, resulting in up to  $37\times$ faster training and up to $26\times$ reduced GPU memory usage.
__label__other Our empirical results demonstrate that FedSD2C consistently outperforms other one-shot FL methods with more complex and real datasets, achieving up to 2.6 $\times$ the performance of the best baseline.
__label__diffusion_based_models During the dual framework, we then propose to represent the 3D spatial scene features with a novel 3D scene graph (3DSG) representation that can be shared and beneficial to both tasks.
__label__interpretability_and_explainability In this paper, we propose HyperLogic: a novel framework leveraging hypernetworks to generate weights of the main network.
__label__diffusion_based_models The inversion of diffusion model sampling, which aims to find the corresponding initial noise of a sample, plays a critical role in various tasks.
__label__causal_inference We conduct experiments on a Colored MNIST dataset having both the treatment ($X$) and the target variables ($Y$) as images and sample from $P(y|do(x))$.
__label__bandits 3.
__label__machine_vision Open-vocabulary part segmentation (OVPS) is an emerging research area focused on segmenting fine-grained entities using diverse and previously unseen vocabularies.
__label__interpretability_and_explainability In the later setting, while PCA is known to be effective at subspace recovery and is proven to aid clustering algorithms in some specific settings, its improvement of noisy data is still not well quantified in general.
__label__optimization Traditional \emph{IM} approaches consider each user/node independently as a potential target customer.
__label__machine_learning_for_other_sciences_and_fields We show that our model, dubbed IsoFormer, is able to accurately predict differential transcript expression, outperforming existing methods and leveraging the use of multiple modalities.
__label__safety_in_machine_learning This indicates an urgent need for further research on the development of targeted defenses against backdoor attacks on LLM-based agents.
__label__speech_and_audio This cycle of improvement is carried out iteratively to steadily convert weak models to strong ones.
__label__causal_inference Building on the theory of partial identification and transportability, this paper introduces new results for bounding the value of a functional of the target distribution, such as the generalization error of a classifiers, given data from source domains and assumptions about the data generating mechanisms, encoded in causal diagrams.
__label__generative_models Vector quantization (VQ) is a key technique in high-resolution and high-fidelity image synthesis, which aims to learn a codebook to encode an image with a sequence of discrete codes and then generate an image in an auto-regression manner.
"__label__learning_theory We study this
question in the spiked cumulant model, where the statistician needs to recover a
privileged direction or ""spike'' from the order-$p\ge 4$ cumulants
of $d$-dimensional inputs."
__label__machine_vision Thanks to this representation, SpelsNet learns from both spatial and topological domains to enable accurate and topologically consistent surface primitive element segmentation.
__label__machine_vision Our approach innovatively incorporates the Fast Fourier Transform into prompt embeddings and harmoniously considers both spatial and frequency domain information.
__label__machine_learning_for_healthcare Thus, our proposed decentralized expert system architecture presents a pioneering step towards revolutionizing disease diagnostics.
__label__reinforcement_learning $(ii)$ WSAC achieves the optimal statistical convergence rate of $1/\sqrt{N}$ to the reference policy, where $N$ is the size of the offline dataset.
__label__machine_vision Despite huge progress in skeleton-based action recognition, its generalizability to different domains remains a challenging issue.
__label__machine_learning_for_healthcare Protein language models (PLMs) have demonstrated strong capabilities in broad biological applications.
__label__generative_models Specifically, given an input textual query, our scheme consists of four stages: 1) we leverage the LLMs as the director to first decompose the complex query into several sub-queries, where each sub-query describes each element of the generated video; 2) to generate each element, pre-trained models are invoked by the LLMs to obtain the corresponding 3D representation; 3) to composite the generated 3D representations, we prompt multi-modal LLMs to produce coarse guidance on the scale, location, and trajectory of different objects; 4) to make the results adhere to natural distribution, we further leverage 2D diffusion priors and use score distillation sampling to refine the composition.
__label__optimization_for_deep_networks Code is available at https://github.com/jincan333/LoT.
__label__reinforcement_learning Imbalanced saliency is a phenomenon where an RL agent disproportionately identifies salient features across consecutive frames in a frame stack.
__label__learning_theory In particular, natural learning processes rely on mechanisms where data representation and learning are intertwined in such a way to respect spatiotemporal locality.
__label__machine_vision Our independent expert fine-tuning specializes multiple models for different skills, and expert merging helps build a joint multi-skill T2I model that can generate faithful images given diverse text prompts, while mitigating the knowledge conflict from different datasets.
__label__safety_in_machine_learning Given the effectiveness and efficiency of our new attacks, we argue that they should become the minimal test for any new defense or robust architectures in tabular machine learning.
__label__machine_vision In ImageNet, our method achieves speedups of 4$\times$ in MoCo, 3.3$\times$ in SimCLR, and 2.5$\times$ in DINO, demonstrating substantial efficiency gains.
__label__infrastructure In this paper, we reexamine this assumption -- we show that, although predicting the exact generation length of each request is infeasible, it is possible to predict the relative ranks of output lengths in a batch of requests, using learning to rank.
__label__generative_models This fact inspired the development of solvers that deal with the *unbalanced* EOT (UEOT) problem $-$ the generalization of EOT allowing for mitigating the mentioned issues by relaxing the marginal constraints.
__label__human-AI_interaction In this paper, we introduce gaze information, feasibly collected by ubiquitous wearable devices such as MR glasses, as a proxy for human attention to guide VLMs.
__label__generative_models Finally, we provide a latency- and memory-efficient SHiRA implementation based on Parameter-Efficient Finetuning (PEFT) Library which trains at nearly the same speed as LoRA while consuming up to 16% lower peak GPU memory, thus making SHiRA easy to adopt for practical use cases.
__label__reinforcement_learning Empirical results show that BSQ constraints provide a computationally feasible approach for user-aligned planning in partially observable settings.
__label__generative_models Code is available at [https://github.com/LTH14/mar](https://github.com/LTH14/mar).
__label__graph_neural_networks To bridge this gap, we introduce a new approach named CMGNN, which operates within the HTMP mechanism to explicitly leverage and improve the compatibility matrix.
"__label__probabilistic_methods The key insight is that,
  in the online setting,
  we do not need to add the KL term to regularize to the prior (which comes from the posterior at the previous timestep);
  instead we can optimize just the expected log-likelihood,
  performing a single step of natural gradient descent
  starting at the prior predictive."
__label__reinforcement_learning iVideoGPT features a novel compressive tokenization technique that efficiently discretizes high-dimensional visual observations.
__label__optimization We provide theoretical support in the form of bounds on the excess risk, and conduct numerical experiments showcasing our method's superior stability and competitive performance relative to current state-of-the-art alternatives.
__label__machine_learning_for_healthcare increasing secondary structures diversity, by introducing a Reinforced Finetuning (ReFT) objective.
__label__safety_in_machine_learning In this work, we study the stability and generalization of adversarial training for two-layer networks **without any data distribution assumptions** and **beyond the NTK regime**.
__label__bandits We propose a novel stochastic bandit algorithm that employs reward estimates using a tree ensemble model.
__label__other To address this cross-domain challenge, we present a novel adaptive domain learning (ADL) scheme for cross-domain RAW image denoising by utilizing existing data from different sensors (source domain) plus a small amount of data from the new sensor (target domain).
__label__natural_language_processing Additionally, we release base, instruct and aligned versions on top of SaulLM-medium and SaulLM-large under the MIT License to facilitate reuse and collaborative research.
__label__deep_learning_architectures Parameter-efficient finetuning (PEFT) methods effectively adapt large language models (LLMs) to diverse downstream tasks, reducing storage and GPU memory demands.
__label__deep_learning_architectures Additionally, ElasTST employs a multi-scale patch design, effectively integrating both fine-grained and coarse-grained information.
__label__deep_learning_architectures This is often done using a single network for the entire domain, imposing many global constraints on a single function.
__label__machine_vision Key-Grid achieves the state-of-the-art performance on the semantic consistency and position accuracy of keypoints.
__label__deep_learning_architectures We conducted extensive experiments on multiple mainstream long-tailed learning benchmarks.
__label__privacy Additionally, our method shows superior resistance to overfitting mitigation strategies, such as early stopping and data augmentation.
__label__diffusion_based_models While prompt compliance can be enhanced by addition of loss functions at inference, this is time consuming and does not scale to complex scenes.
__label__machine_vision Extensive experiments show that our method outperforms SOTA methods on three common benchmarks.
__label__generative_models This paper investigates a straightforward and training-free approach to extend an existing short video diffusion model (e.g.
__label__machine_vision Based on the CRM, we then propose a Region-aware Shrinking (RaS) loss to constrain the visual localization to be conducted progressively in a coarse-to-fine manner across different stages.
__label__machine_vision Specifically, a cross-classification loss with feature alignment is proposed to circumvent catastrophic forgetting.
__label__machine_learning_for_physical_sciences We present AROMA (Attentive Reduced Order Model with Attention), a framework designed to enhance the modeling of partial differential equations (PDEs) using local neural fields.
__label__causal_inference Standard black-box approaches mapping sequences of categorical variables to outputs are applicable, but they rely on poorly understood assumptions on how reliable generalization can be obtained, and may underperform under sparse sequences, temporal variability, and large action spaces.
__label__neuroscience_and_cognitive_science Project Page: https://xjay18.github.io/projects/neuma.html.
__label__reinforcement_learning While the statistical implications of distributional robustness in RL have been explored in some specific cases, the generalizability of the existing findings remains unclear, especially in comparison to standard RL.
__label__speech_and_audio Moreover, multimodal representation learning often relies on contrastive learning, facing the challenge of the so-called modality gap which hinders smooth integration between modalities.
__label__algorithmic_game_theory In response, we develop novel rules for learning reward functions with strong axiomatic guarantees.
__label__reinforcement_learning And based on this, we present an algorithm that aims to find a policy that maps states to a \emph{partial} distribution of the best expert actions for each given state.
__label__machine_learning_for_physical_sciences In this paper, we introduce the Universal Mesh Movement Network (UM2N), which -- once trained -- can be applied in a non-intrusive, zero-shot manner to move meshes with different size distributions and structures, for solvers applicable to different PDE types and boundary geometries.
__label__machine_learning_for_other_sciences_and_fields To overcome these deficiencies, we introduce density-based user representations (DURs), a novel method that leverages Gaussian process regression (GPR) for effective multi-interest recommendation and retrieval.
__label__machine_vision We begin with the premise that token merging should not rely solely on the similarity of video tokens; the saliency of tokens should also be considered.
__label__optimization Minimizing these upper-bounds requires solving implicit equations to obtain a sequence of strongly adapted step-sizes; we show that these equations are straightforward to solve for convex quadratics and lead to new guarantees for two classical step-sizes.
__label__neuroscience_and_cognitive_science This may be due to visual features common among the preferred class also being present in other images.
__label__machine_vision Federated Learning (FL) is a form of distributed learning that allows multiple institutions or clients to collaboratively learn a global model to solve a task.
__label__speech_and_audio Additionally, we develop an acoustic simulation platform, AcoustiX, which provides more accurate and realistic IR simulations than existing simulators.
__label__machine_vision Additionally, we have developed a coarse-to-fine training strategy to improve learning efficiency and eliminate floaters caused by overfitting in real-world scenes.
__label__privacy In this study, we introduce the first membership inference attack (MIA) benchmark tailored for various VLLMs to facilitate training data detection.
__label__diffusion_based_models Such a design requires the system to automatically figure out what to expect from the reference to perform the editing.
__label__optimization_for_deep_networks Using two reduction strategies, the MOTE is generated based on a reduced architecture and a reduced dataset.
"__label__probabilistic_methods In this work we examine the theoretical
properties of the embeddings learned by node2vec."
__label__robotics Addressing this limitation, we present an end-to-end general-purpose multi-modal system named Any-to-Policy Embodied Agents.
__label__other Consequently, our method achieves superior ability in retaining old knowledge and achieving excellent new task performance simultaneously.
__label__other In general, observing every entry would be necessary to uniquely identify a graph, however if we know the graph has a certain property some entries can be omitted - for example, only half the entries would be required for a symmetric graph.
__label__machine_vision In this work, we introduce a novel Unified Projection Sharing algorithm(UPS) to decouple the feature extraction and similarity modeling, achieving notable performance.
__label__other Subsequently, a meticulously designed collaborative representation learning procedure captures collaborative signals.
__label__machine_learning_for_healthcare Extensive ablation experiments validate the effectiveness of introducing CMoE under any configuration, with up to an average 8% performance gains.
__label__neuroscience_and_cognitive_science Humans represent scenes and objects in rich feature spaces, carrying information that allows us to generalise about category memberships and abstract functions with few examples.
__label__optimization We compare the performance of different modeling choices numerically using a stochastic shortest path example and provide practical insights from the empirical results.
__label__machine_vision UniPHD extracts multimodal representations and employs a proposed pose-centric hierarchical decoder to process (text or positional) instance queries and keypoint queries, producing results specific to the referred person.
__label__machine_vision To this end, we propose a novel method to selectively incorporate the physics models with the kinematics observations in an online setting, inspired by a neural Kalman-filtering approach.
__label__probabilistic_methods Furthermore, our proposed opinion projection method effectively mitigates the vanishing gradient issue, ensuring classification accuracy without additional model complexity.
__label__reinforcement_learning In response, we analyze the training dynamics of GAIL in function space and design a novel controller that not only pushes GAIL to the desired equilibrium but also achieves asymptotic stability in a simplified “one-step” setting.
__label__natural_language_processing Furthermore, we introduce preference learning based on the DPO algorithm, which empowers the fine-grained evaluation model to explore and learn better branching strategies within budget-limited scenarios.
__label__machine_vision This paper introduces $\alpha$-NeuS, a new method for simultaneously reconstructing thin transparent objects and opaque objects based on neural implicit surfaces (NeuS).
__label__optimization_for_deep_networks Our code is available at https://github.com/vaynexie/DRM.
__label__causal_inference We compare our algorithm against various baseline methods on simulated datasets, demonstrating its superior accuracy measured by the structural Hamming distance between the learned DAG and the ground truth.
__label__deep_learning_architectures Critical in this context is the *prior knowledge* accumulated from related tasks.
"__label__machine_vision However, here we provide extensive empirical evidence that this strategy leads to forgetting how to ""segment anything"": these models lose the original generalization abilities of SAM, in the sense that they perform worse for segmentation tasks not represented in the annotated fine-tuning set."
__label__privacy Second, Poisson subsampling is sometimes assumed to have similar privacy guarantees compared to sampling without replacement.
__label__diffusion_based_models We deploy a diffusion model trained from scratch on only the long-tailed dataset to create this proxy and verify the effectiveness of the data produced.
__label__speech_and_audio Additionally, it is able to independently control the acoustic conditions of the vocals and accompaniment in the generated song through different audio prompts, exhibiting its potential applicability.
__label__other We also introduce a metric called Error Residual Ratio (ERR) to better account for varying degrees of label noise.
__label__other Empirically, it achieves substantial improvements over existing methods, which is as significant as the gains existing methods have over the baseline (i.e., without distribution shift adaptations).
__label__generative_models In this work, we introduce the Geometric Diffusion Bridge (GDB), a novel generative modeling framework that accurately bridges initial and target geometric states.
__label__natural_language_processing We propose Batched Greedy Pruning for rapid and near-optimal pruning, which enhances the accuracy of head-wise pruning error estimation through grouped Cholesky decomposition and improves the pruning efficiency of FFN via Dynamic Group Size, thereby achieving approximate local optimal pruning results within one hour.
__label__interpretability_and_explainability Despite their different natures, these strategies often lead to comparable performance gains.
__label__machine_vision Previous works rely on sparse sampling or frame compression to reduce tokens.
__label__neuroscience_and_cognitive_science Prominent examples are the blood oxygenation level dependent (BOLD) signal in functional magnetic resonance imaging (fMRI) or Ca$^{2+}$ imaging data.
__label__online_learning Our algorithm for doing so is very efficient: having a space and per-trial time complexity that is logarithmic in the time-horizon.
__label__other They utilize prompting techniques such as Exploration-of-Thought, Decomposition, and Refinement to effectively navigate and solve intricate tasks.
__label__machine_learning_for_physical_sciences We present extensive quantitative and qualitative analyses of the learned representations in the context of range estimation and other spatial tasks, demonstrating the effectiveness of our approach.
__label__probabilistic_methods We study the problem of training diffusion models to sample from a distribution with a given unnormalized density or energy function.
__label__reinforcement_learning This work lays the foundations for investigating explicit modeling of generalization, thereby enabling principled yet effective methods for contextual RL.
__label__privacy Current state-of-the-art methods for differentially private model training are based on matrix factorization techniques.
__label__neuroscience_and_cognitive_science We apply these models to simulated data as well as cortical neural activity across mice and monkeys, which allows us to automatically detect discrete states that lead to the identification of varying neural dynamics.
__label__natural_language_processing This inspires us to redefine emergent abilities as those that manifest in models with lower pre-training losses, highlighting that these abilities cannot be predicted by merely extrapolating the performance trends of models with higher pre-training losses.
__label__machine_vision Pre-trained vision-language models (VLMs) have shown impressive results in various visual classification tasks.
__label__neuroscience_and_cognitive_science To address these challenges, we propose MiSO (MicroStimulation Optimization), a closed-loop stimulation framework to drive neural population activity toward specified states by optimizing over a large stimulation parameter space.
__label__deep_learning_architectures Code is available at this repository: https://github.com/thuml/TimeXer.
__label__neuroscience_and_cognitive_science Furthermore, the underlying pitfalls behind overestimated decoding accuracy have not been fully explained due to a lack of appropriate formulation.
__label__graph_neural_networks To address this limitation, we propose a novel graph Transformer called GCFormer.
__label__evaluation To address this challenge, we also introduce an efficient evaluation framework: \textit{Sort \& Search (S\&S)}, which reuses previously evaluated models by leveraging dynamic programming algorithms to selectively rank and sub-select test samples, enabling cost-effective lifelong benchmarking.
__label__generative_models S${^2}$FT accomplishes this by selecting sparsely and computing densely.
__label__diffusion_based_models We demonstrate its utility in text-to-2D, text-to-3D, translating paintings to real images, optical illusion generation, and 3D sketch-to-real.
__label__deep_learning_architectures It leverages the prior knowledge acquired during training to generate structured TPR representations by mapping input data to pre-learned symbolic features within these dictionaries.
__label__optimization_for_deep_networks Second-order optimizers, maintaining a matrix termed a preconditioner, are superior to first-order optimizers in both theory and practice.
__label__neuroscience_and_cognitive_science However, existing methods for latent variable estimation are not robust to dynamical noise and system nonlinearity due to noise-sensitive inference procedures and limited model formulations.
__label__machine_vision Human visual search ability enables efficient and accurate tracking of an arbitrary moving target, which is a significant research interest in cognitive neuroscience.
__label__machine_learning_for_other_sciences_and_fields Specifically, we integrate two essential sub-tasks, i.e., traffic data imputation and decision-making, by leveraging a Partial Rewards Conditioned Diffusion (PRCD) model to prevent missing rewards from interfering with the learning process.
"__label__learning_theory However, the paper requires that the input distribution for all $k$ subgroups be isotropic Gaussian, and states that removing this assumption is an ``interesting and challenging problem""."
"__label__other And we also adopt a hierarchical structure to alleviate the efficiency issue
caused by the removal of patching."
__label__optimization Experiments on both simulation functions and real scenarios (black-box adversarial attacks neural architecture search, and parameter-efficient fine-tuning for large language models), show its efficacy and efficiency.
__label__natural_language_processing To address this, Panacea is designed to use singular value decomposition (SVD)-based low-rank adaptation, which allows the preference vector to be simply injected online as singular values.
__label__deep_learning_architectures Deep Canonical Correlation Analysis (DCCA) and its variants share simple formulations and demonstrate state-of-the-art performance.
__label__other TCQ uses a stateful decoder that separates the codebook size from the bitrate and effective dimension.
__label__reinforcement_learning The slow agent analyzes actions suitable for coding, while the fast agent executes coding tasks.
__label__learning_theory While these assumptions have facilitated theoretical understanding, they have precluded a detailed understanding of the roles of the nonlinearity and input-data distribution in determining the learning dynamics, limiting the applicability of the theories to real biological or artificial neural networks.
__label__other In this paper, we propose a multi-timescale gradient correction (MTGC) methodology to resolve this issue.
__label__safety_in_machine_learning Notably, without further fine-tuning, the ProTransformer consistently improves the performance of vanilla transformers by 19.5\%, 28.3\%, 16.1\%, and 11.4\% for BERT, ALBERT, DistilBERT, and RoBERTa, respectively, under the classical TextFooler attack.
__label__reinforcement_learning However, previous works have observed that networks trained under non-stationarity exhibit an inability to continue learning, termed loss of plasticity, and eventually a collapse in performance.
__label__generative_models We begin by collecting PyX, a clean Python corpus of fully executable code samples with functional descriptions and test cases.
__label__interpretability_and_explainability Given the widespread use of large language models (LLMs) in supporting decision-making processes, it is crucial to assess whether their behavior aligns with human norms and ethical expectations or exhibits potential biases.
__label__deep_learning_architectures We design two simple conditioning networks that maintain shift equivariance in our data-dependent convolution operation.
__label__machine_vision Furthermore, inspired by the phenomenon that human eyes pay more attention to objects that produce significant dynamic changes, we design a Spatiotemporal Sensitivity Module (SSM) and an adaptive Temporal Activation Controller (TAC).
__label__machine_vision choosing the best performing in-context examples from all alternatives for each query sample.
__label__learning_theory This paper studies the properties of solutions to multi-task shallow ReLU neural network learning problems, wherein the network is trained to fit a dataset with minimal sum of squared weights.
__label__diffusion_based_models It efficiently performs consistent subject generation solely driven by prompts via a learned semantic guidance to bypass the laborious backbone tuning.
"__label__machine_vision As an alternative loss, we propose the negative log of ``collision probability'' that
maximizes the chance of equality between two random variables, predicted class and unknown true class, 
whose distributions are $\sigma$ and $y$."
__label__probabilistic_methods This mechanism forges connections between multiple sampling instances via a selected leader, enhancing both the efficiency and effectiveness of the entire sampling process.
__label__causal_inference 2022, Balazadeh et al.
__label__graph_neural_networks The majority of current public datasets are either randomly generated or extremely limited, containing only a few examples from unrelated problem families.
__label__optimization_for_deep_networks Furthermore we discover an intriguing phenomenon: MIDAS is not only training-efficient but surprisingly also has an inductive bias towards improving downstream tasks, especially tasks that require reasoning abilities like reading comprehension and math problems, despite having similar or slightly worse perplexity compared to baseline training.
__label__machine_vision Recognizing that foreground objects only occupy a small portion of the scene, we introduce object-centric occupancy as a supplement to object bboxes.
__label__deep_learning_architectures Additionally, CSR shows compatibility with different vision-language models and the ability to incrementally improve performance through iterative fine-tuning.
__label__learning_theory We study transfer learning for estimation in latent variable network models.
__label__optimization_for_deep_networks By studying the local geometry of reached minima, we observe that using LRs from this optimal range allows for the optimization to locate a basin that only contains high-quality minima.
__label__deep_learning_architectures Our analysis reveals a wide variability in how well-trained, and thus relatedly how prunable, different layers of an LLM are.
__label__learning_theory In this study, we assess how different sampling strategies, such as *i.i.d.
__label__diffusion_based_models Comprehensive experiments show that our method outperforms a variety of baselines with satisfactory subject consistency, superior prompt conformity as well as high image quality.
__label__probabilistic_methods We lastly deduce a data-driven heuristic to discover the largest subspace of parameters supporting SI distributions for a problem, that could be used for designing EA with minimal generalization error.
__label__safety_in_machine_learning Current adversarial attacks to MDE models focus on attaching an optimized adversarial patch to a designated obstacle.
__label__natural_language_processing Combining both cascades, CS Drafting achieves greater speedup compared to the baselines in our experiments, while preserving the same output distribution as the target model.
__label__algorithmic_game_theory We begin by studying the static case ($T=1$) and establish that the optimal mechanism involves two types of subsidization: one that increases the overall probability of allocation to all buyers, and another that favors the group which otherwise has a lower probability of winning the item.
__label__optimization_for_deep_networks Our code is available at https://github.com/changwoolee/BLAST.
__label__machine_vision Gaussian Vector Quantization, based on each Gaussian’s global significance, further lowers bitwidth with minimal accuracy loss.
__label__generative_models Our MVSplat360 addresses this by effectively combining geometry-aware 3D reconstruction with temporally consistent video generation.
__label__optimization In particular, we propose gradient-free stochastic methods for finding the $(\delta, \epsilon)$-Goldstein stationary points of such problems with non-asymptotic convergence rates.
__label__optimization_for_deep_networks Modern machine learning heavily depends on the effectiveness of optimization techniques.
__label__machine_vision Additionally, we show improved performance on standard novel view synthesis and 3D reconstruction benchmarks.
__label__deep_learning_architectures Existing meta-learning approaches typically rely on preselected priors, such as a Gaussian probability density function (pdf).
__label__diffusion_based_models Our experimental results further demonstrate that diffusion models can attain compositionality with a small amount of compositional examples, suggesting a novel way to train DDPMs.
__label__safety_in_machine_learning The utilization of large foundational models has a dilemma: while fine-tuning downstream tasks from them holds promise for making use of the well-generalized knowledge in practical applications, their open accessibility also poses threats of adverse usage.
__label__diffusion_based_models Our approach builds on an intriguing property of DiT, which we refer to as semantic sharing.
__label__neuroscience_and_cognitive_science Despite their success, traditional VAEs rely on continuous latent variables, which significantly deviates from the discrete nature of biological neurons.
__label__generative_models Modern image generation (IG) models have been shown to capture rich semantics valuable for image understanding (IU) tasks.
__label__machine_learning_for_physical_sciences Exploiting symmetry inherent in data can significantly improve the sample efficiency of a learning procedure and the generalization of learned models.
__label__online_learning Additionally, in stochastic settings, it provides $\widetilde O(\sqrt{T})$ regret without Slater's condition.
__label__other Optimal transport (OT) is a general framework for finding a minimum-cost transport plan, or coupling, between probability distributions, and has many applications in machine learning.
__label__diffusion_based_models In this paper, we reveal that the concrete score in absorbing diffusion can be expressed as conditional probabilities of clean data, multiplied by a time-dependent scalar in an analytic form.
"__label__optimization_for_deep_networks With a label space of possibly millions of candidates,
the classification layer alone will consume several gigabytes of memory."
__label__machine_learning_for_physical_sciences In this work, we formulate end-to-end language-to-structure generation as a multi-objective optimization problem, and propose Generative Hierarchical Materials Search (GenMS) for controllable generation of crystal structures.
__label__safety_in_machine_learning The PI is formulated as additional features that explain the distribution shift, however, they are only available during training and absent at test time.
__label__machine_learning_for_healthcare Specifically, we introduce $\textbf{\textit{CAT}}$, an innovative model that $\textbf{C}$oordinates $\textbf{A}$natomical prompts derived from 3D cropped images with $\textbf{T}$extual prompts enriched by medical domain knowledge.
__label__robotics Existing research mainly focuses on leveraging single-frame cooperative information to enhance the limited perception capability of the ego vehicle, while underutilizing the motion and interaction context of traffic participants observed from cooperative devices.
__label__other Moreover, we introduce regression distribution alignment (RDA), a complementary technique that further enhances RankUp's performance by refining pseudo-labels through distribution alignment.
__label__safety_in_machine_learning Our code will be made publicly available.
__label__probabilistic_methods A main application of Wasserstein gradient boosting in this paper is tree-based evidential learning, which returns a distributional estimate of the response parameter for each input.
__label__graph_neural_networks However, these results lack reliable uncertainty estimates.
__label__machine_vision This paper introduces Chain-of-Sight, a vision-language bridge module that accelerates the pre-training of Multimodal Large Language Models (MLLMs).
__label__other Overcoming this challenge, existing object hallucination evaluation methods average the results obtained from a set of instructions.
__label__safety_in_machine_learning Finally, we extend our approach to AI agents, demonstrating considerable reductions in the rate of harmful actions when they are under attack.
__label__machine_vision Additionally, we introduce a domain-specific uniformity loss that promotes uniformity within each source domain, thereby enhancing the learning of domain-invariant features.
__label__machine_learning_for_other_sciences_and_fields Formally based on diffusion transition processes in discrete state spaces, which is unlike common diffusion models such as DDPM that operate in continuous domains.
__label__machine_learning_for_other_sciences_and_fields Automatic translation of programming languages has garnered renewed interest, driven by recent advancements in large language models (LLMs).
__label__learning_theory In particular, we show that the existence of an online learning algorithm with bounded regret (against a fixed statistical learning algorithm in a specially constructed game of online learning with delayed feedback) implies low generalization error of said statistical learning method even if the data sequence is sampled from a mixing time series.
__label__evaluation However, models with heterogeneous feature space can still be helpful.
__label__learning_theory We then check if partial consistency holds under a given embedding and low-noise assumption, providing insight into when to use a particular embedding into $\mathbb{R}^d$.
__label__interpretability_and_explainability Many training data attribution (TDA) methods aim to estimate how a model's behavior would change if one or more data points were removed from the training set.
__label__neuroscience_and_cognitive_science Here, we provide a generalization of the NTK, which we call the surrogate gradient NTK, that enables the analysis of SGL.
__label__deep_learning_architectures Unlike traditional Transformers, PointMamba employs a linear complexity algorithm, presenting global modeling capacity while significantly reducing computational costs.
__label__deep_learning_architectures Based on the similar sequential formulation of time series and natural language, increasing research demonstrates the feasibility of leveraging large language models (LLM) for time series.
__label__machine_vision Also, we propose a two-stage pipeline to generate 3D objects from X-Ray Diffusion Model and Upsampler.
__label__optimization Optimal Transport (OT, also known as the Wasserstein distance) is a popular metric for comparing probability distributions and has been successfully used in many machine-learning applications.
"__label__machine_vision Text, the most familiar and well-understood modality for LLMs, is utilized as a bridge to establish connections between different motion tasks, facilitating mutual 
reinforcement."
__label__deep_learning_architectures Yet, difficulties in adapting Mamba from language to vision tasks arise due to the distinct characteristics of visual data, such as the spatial locality and adjacency within images and large variations in information granularity across visual tokens.
__label__natural_language_processing Recently, memory reduction methods have been broadly adopted to resolve the hardware bottleneck by decomposing forward and backward or using a memory bank.
__label__machine_vision In this study, we explicitly consider data privacy and heterogeneity in cooperatively optimizing SCI systems by proposing a Federated Hardware-Prompt learning (FedHP) framework.
__label__machine_learning_for_healthcare To this end, we present **s**ingle **c**ell, **Cell**-**o**ntology guided TFM (scCello).
__label__machine_learning_for_physical_sciences Moreover, to improve the error margin in meteorological skill scores such as Critical Success Index (CSI) and Fractions Skill Score (FSS), we propose and adopt the Regional Histogram Divergence (RHD), a distance metric that considers the patch-wise similarity between signal-based imagery patterns with tolerance to local transforms.
__label__online_learning However, this approach faces difficulties when the dynamic test distribution is available only in small batches and without access to the original source data.
__label__probabilistic_methods Experimentally, we demonstrate that *LinUProp* is consistent with the sampling-based method but with linear scalability and fast convergence.
__label__machine_learning_for_other_sciences_and_fields Designing ligand-binding proteins, such as enzymes and biosensors, is essential in bioengineering and protein biology.
__label__reinforcement_learning Initially, we establish that sample-efficient learning without additional assumptions is unattainable owing to the curse of support shift; i.e., the potential disjointedness of the distributional supports between the training and testing environments.
__label__probabilistic_methods We identify a necessary condition for any algorithm to correctly match all nodes across all graphs, and propose two algorithms for which the same condition is also sufficient.
__label__learning_theory On the other hand, we show that additional assumptions on the volume of the data manifold alleviate these fundamental limitations and guarantee learnability via a simple interpolation argument.
__label__natural_language_processing To this end, we propose Coke, a novel cost-efficient strategy for KBQA with LLMs, modeled as a tailored multi-armed bandit problem to minimize calls to LLMs within limited budgets.
__label__generative_models The project page: https://linshan-bin.github.io/GeoLRM/.
__label__deep_learning_architectures During the training phase, ElasTST uses a horizon reweighting strategy that approximates the effect of random sampling across multiple horizons with a single fixed horizon setting.
__label__generative_models In this paper, instead of input space alignment, we propose a novel parameter space alignment paradigm that represents visual information as model weights.
__label__natural_language_processing Across VQA benchmarks such as ScienceQA that share similar scientific diagram images, GraphVis provides a notable gain of 4.32%.
__label__optimization_for_deep_networks Consequently, methods for adaptive rank allocation are proposed, among which AdaLoRA demonstrates excellent fine-tuning performance.
__label__natural_language_processing We train an 8k context-length 1B model at the same cost as a 2k context-length model trained with the baseline approach.
__label__causal_inference Accurately predicting conditional average treatment effects (CATEs) is crucial in personalized medicine and digital platform analytics.
__label__safety_in_machine_learning (3) Detecting and correcting NPs according to the PPLDiff obtained by the aligned surrogate LLM to obtain a denoised training dataset for robust alignment.
__label__safety_in_machine_learning However, this process inevitably introduces new safety threats, particularly against the Fine-tuning based Jailbreak Attack (FJAttack) under the setting of Language-Model-as-a-Service (LMaaS), where the model's safety has been significantly compromised by fine-tuning on users' uploaded examples that contain just a few harmful examples.
__label__neuroscience_and_cognitive_science To do so, we extend to neural activity the maximum occupancy principle (MOP) developed for behavior, and refer to this new neural principle as NeuroMOP.
__label__natural_language_processing Our approach achieves lossless (mostly improved) LoRA transfer between models within and across different base model families, and even between different PEFT methods, on a wide variety of tasks.
__label__algorithmic_game_theory While no-external-regret dynamics can be completely determined by the cumulative reward vector received by each player, we show there does not exist any general no-swap-regret dynamics defined on the same state space.
__label__deep_learning_architectures We demonstrate the applicability of MomentumSMoE to many types of SMoE models, including those in the Sparse MoE model for vision (V-MoE) and the Generalist Language Model (GLaM).
__label__machine_vision To address this problem, we propose TopoFR, a novel FR model that leverages a topological structure alignment strategy called PTSA and a hard sample mining strategy named SDE.
__label__neuroscience_and_cognitive_science Here, we propose a goal reduction mechanism for effectively deriving subgoals from arbitrary and distant original goals, using a novel loop-removal technique.
__label__generative_models Hence, we are curious about whether LLMs' reasoning ability can be further enhanced by Self-Playing this Adversarial language Game (SPAG).
__label__optimization The exact iEF and EF methods are experimentally evaluated using practical deep learning setups, including widely-used setups for parameter-efficient fine-tuning of pre-trained models (T5-base with LoRA and Prompt-Tuning on GLUE tasks, and ViT with LoRA for CIFAR100).
__label__natural_language_processing This assistant manages memory and knowledge through tool usage, action execution, memory building, and plan specification.
__label__other Deep learning models are now trained on increasingly larger datasets, making it crucial to reduce computational costs and improve data quality.
__label__generative_models Among various methods, recent works have found that simply manipulating attention modules by concatenating features from multiple reference images provides an efficient approach to enhancing consistency without fine-tuning.
__label__graph_neural_networks Graph Neural Networks (GNNs) have become essential in interpreting relational data across various domains, yet, they often struggle to generalize to unseen graph data that differs markedly from training instances.
__label__natural_language_processing Aligning language models (LMs) to human preferences has emerged as a critical pursuit, enabling these models to better serve diverse user needs.
__label__machine_learning_for_physical_sciences The success of machine-learned interatomic potentials arises from integrating inductive biases such as equivariance to group actions on an atomic system, e.g., equivariance to rotations and reflections.
__label__bandits content matching or online labor markets), the knowledge about preferences may not be readily available and must be learned, i.e., one side of the market (aka agents) may not know their preferences over the other side (aka arms).
__label__natural_language_processing By carefully adjusting the direction and magnitude of the steering vector, we enabled personalized control over the desired behavior across a spectrum of intensities.
__label__reinforcement_learning To overcome this, we propose SCAFFLSA a new variant of FedLSA that uses control variates to correct for client drift, and establish its sample and communication complexities.
__label__machine_vision A scene representation is either metric, such as landmark maps in 3D reconstruction, 3D bounding boxes in object detection, or voxel grids in occupancy prediction, or topological, such as pose graphs with loop closures in SLAM or visibility graphs in SfM.
__label__machine_learning_for_healthcare We then show that many important ECG downstream tasks can be formulated as conditional generation methods in a Bayesian inverse problem framework using $\texttt{BeatDiff}$ as priors.
__label__machine_vision Built on a general Transformer-based framework, LSM predicts global geometry via pixel-aligned point maps.
__label__machine_learning_for_other_sciences_and_fields However, high-quality sequential financial investment decision-making remains challenging.
__label__machine_vision We verify the resulting diversification of the ensemble on ImageNet and demonstrate the benefit of diversification on the OOD generalization and OOD detection tasks.
__label__optimization The requirement of $R = \Omega(\log d)$ for random order streams is nearly tight for Price's analysis as we obtain a simple instance with $R = \Omega(\log d/\log\log d)$ for which Price's algorithm, with any fixed learning rate, cannot output a vector approximating the top eigenvector $v_1$.
__label__neuroscience_and_cognitive_science Using an LLM-based data-driven approach, we identify two domains that LMs do not capture well: social/emotional intelligence and physical commonsense.
__label__graph_neural_networks Unlike image and text, defining such transferable patterns for graphs remains an open question.
__label__optimization_for_deep_networks Many existing algorithms either use the retraction operator to keep each iterate staying on the manifold, or solve an unconstrained quadratic penalized problem.
__label__neuroscience_and_cognitive_science How to decode human vision through neural signals has attracted a long-standing interest in neuroscience and machine learning.
__label__reinforcement_learning Monte-Carlo tree search (MCTS) is an influential sequential decision-making algorithm notably employed in AlphaZero.
__label__privacy This condition is often met by real datasets, as demonstrated by our results on CIFAR10, CIFAR100, and ImageNet.
__label__machine_vision Comprehensive experiments demonstrate the superiority of our mechanism across both existing public video understanding benchmarks and our proposed FineAction-CGR.
__label__other Accordingly, we fit an informative length-hallucination curve, upon which a fine-grained evaluation framework named LeHaCE is introduced for evaluating object hallucinations at any given image description length.
__label__other In this paper, we propose an algorithm named MIPLMA, i.e., Multi-Instance Partial-Label learning with Margin Adjustment, which adjusts the margins for attention scores and predicted probabilities.
__label__other For very large digraphs, however, this means many (most) entries may be unobserved during training.
__label__machine_learning_for_healthcare It aims to overcome the limitations of existing evaluation methods, which either focus solely on textual similarities, ignoring clinical aspects, or concentrate only on a single clinical aspect, the pathology, neglecting all other factors.
__label__evaluation Thus, we argue that accuracy and perplexity are necessary but not sufficient for evaluating compressed models, since these metrics hide large underlying changes that have not been observed by previous work.
__label__optimization We introduce two novel formulations of online learning problems rooted in the paradigm of Pure Exploration in Combinatorial Multi-Armed Bandits (PE-CMAB): fixed confidence and fixed budget settings.
__label__generative_models The recently introduced Consistency models pose an efficient alternative to diffusion algorithms, enabling rapid and good quality image synthesis.
__label__learning_theory Although this hypothesis and perturbation learning are effective in explaining intriguing properties of adversarial examples, their solid theoretical foundation is limited.
__label__deep_learning_architectures Multimodal contrastive learning (MCL) has recently demonstrated significant success across various tasks.
__label__human-AI_interaction Moreover, as the agent's library of examples grows, it becomes more efficient, relying less on human feedback and requiring fewer environment interactions per demonstration.
__label__interpretability_and_explainability In this work, we study the distribution of optimized DNNs as a family of functions by leveraging a pointwise approach.
__label__generative_models These wavelet-based outputs are then processed and seamlessly fused with the original Mamba outputs through a cross-attention fusion layer, combining both spatial and frequency information to optimize the order awareness of state-space models which is essential for the details and overall quality of image generation.
"__label__machine_learning_for_physical_sciences To reduce the need for training data with heavy simulation costs, we mine unlabeled PDE data without simulated solutions,
and we pretrain neural operators with physics-inspired reconstruction-based proxy tasks."
__label__learning_theory In this work, we prove the first rigorous learning guarantees for neural networks based on unrolling approximate message passing (AMP).
__label__diffusion_based_models Recently, several methods propose training a dedicated image encoder on a large variety of subject images.
__label__diffusion_based_models Current models often yield static or minimally dynamic outputs, failing to capture complex motions described by text.
__label__infrastructure This paper introduces FlashMask, a simple yet effective \emph{Exact} attention algorithm designed to substantially reduce both the computational complexity and memory requirements of attention computations.
__label__machine_learning_for_physical_sciences We present a novel method for symbolic regression (SR), the task of searching for compact programmatic hypotheses that best explain a dataset.
__label__natural_language_processing By transferring expertise from the 7B model to the 13B model, our method closes the performance gap by 96.4\% in single-task scenarios and by 86.3\% in multi-task scenarios compared to full fine-tuning of the 13B model.
__label__optimization Example challenges arise in the physical sciences in the form of local movement constraints, required monotonicity in certain variables, and transitions influencing the accuracy of measurements.
__label__machine_learning_for_healthcare Impressively, our PSTUDA not only significantly reduces the floating-point computation by approximately 72% but also reduces the number of model parameters by about 50%, bringing higher efficiency and feasibility to practical clinical applications.
__label__generative_models We provide our code at https://github.com/vislearn/FFF.
__label__machine_vision Remarkably, as a single-stage approach, BaFormer significantly reduces the computational costs, utilizing only 6% of the running time compared to the state-of-the-art method DiffAct, while producing better or comparable accuracy over several popular benchmarks.
__label__machine_vision Our empirical results demonstrate state-of-the-art novel view synthesis peformances in both novel view rendered color maps quality and depth maps accuracy across different numbers of input views.
__label__machine_vision Besides, by leveraging the model parameter uncertainty estimated by our method, we can remove noisy Gaussians automatically, thereby obtaining a high-fidelity part of the reconstructed scene, which is of great help in improving the visual quality.
__label__interpretability_and_explainability Circuit analysis in this setting thus either yields intractably large circuits or fails to disentangle local and global behavior.
__label__safety_in_machine_learning Existing CLIP-based approaches perform OOD detection by devising novel scoring functions or sophisticated fine-tuning methods.
__label__machine_vision Code is available at https://github.com/w2kun/DCDepth.
__label__generative_models In this work, we show that, on text generation tasks, noisy annotations significantly hurt the performance of in-context learning.
__label__neuroscience_and_cognitive_science Substantial sensory changes, produced, e.g., by moving between environments, cause large subpopulations of place cells to change their tuning entirely.
__label__causal_inference Our method achieves state-of-the-art counterfactual estimation results using both synthetic and real-world data, marking the pioneering incorporation of Contrastive Predictive Encoding in causal inference.
__label__other To the best of our knowledge, ZOPP represents a pioneering effort in the domain of multi-modal panoptic perception and auto labeling for autonomous driving scenes.
__label__machine_learning_for_healthcare Furthermore, the interpretability of association patterns is elucidated in detail, thus revealing the intrinsic biological mechanisms and promoting it to be deployed in real-world scenarios.
__label__machine_learning_for_other_sciences_and_fields Computational methodsprimarily rely on evolutionary information (EI) encoded by protein languagemodels (PLMs) to predict fitness landscape for optimization.
__label__diffusion_based_models Our approach integrates three core components: Feature Slicer, Operator Grouping, and Step Rehash.
__label__machine_vision Moreover, our model integrates multi-scale CLIP features by utilizing a self-attention fusion module, technically implemented through one Transformer layer.
__label__diffusion_based_models The approach is based on our observation that diffusion models implicitly define a log-likelihood ratio that distinguishes distributions with different amounts of noise, and this expression depends on denoiser performance outside the standard training distribution.
__label__graph_neural_networks In this paper, we aim to bridge this gap by rethinking the transferable patterns on graphs as computation trees -- i.e., tree structures derived from the message-passing process.
__label__machine_vision In this paper, we introduce a novel binarized diffusion model, BI-DiffSR, for image SR. First, for the model structure, we design a UNet architecture optimized for binarization.
__label__other As with many other problems, real-world regression is plagued by the presence of noisy labels, an inevitable issue that demands our attention.
__label__diffusion_based_models Given an unconditional diffusion model and a predictor for a target property of interest (e.g., a classifier), the goal of training-free guidance is to generate samples with desirable target properties without additional training.
__label__diffusion_based_models Applying DiffAug to a given example consists of one forward-diffusion step followed by one reverse-diffusion step.
__label__machine_vision Furthermore, to fill the vacancy of the real-world STE evaluation benchmark, we create the first real-world image-pair dataset termed ScenePair for fair comparisons.
__label__natural_language_processing Further analysis demonstrate the superior generalization ability and model robustness of our BoT, while requiring only 12\% of the cost of multi-query prompting methods (e.g., tree/graph of thoughts) on average.
__label__infrastructure A gradient coding solution introduces redundancy within the assignment of chunks to the workers and uses coding theoretic ideas to allow the PS to recover $\nabla L$ (exactly or approximately), even in the presence of stragglers.
__label__machine_learning_for_physical_sciences In this work, we boost the prediction fidelity to an unprecedented level for simulating complex photonic devices with a novel operator design driven by the above challenges.
__label__safety_in_machine_learning Empirical experiments on both real and synthetic datasets indicate that our approach achieves a valid coverage rate and constructs more informative predictions compared to existing methods, which are not supported by theoretical guarantees.
__label__safety_in_machine_learning and Wang et al.
__label__causal_inference MC-EIF automates efficient statistical estimation for a broad class of models and functionals that previously required rigorous custom analysis.
__label__natural_language_processing However, the performance is somewhat limited because they cannot consider the inter-layer dependency within the attention module, which is a significant feature of Transformers.
__label__optimization_for_deep_networks We investigate this phenomenon in two-layer networks that satisfy a near-homogeneity condition.
__label__machine_learning_for_physical_sciences In climate science and meteorology, high-resolution local precipitation (rain and snowfall) predictions are limited by the computational costs of simulation-based methods.
__label__safety_in_machine_learning To address this challenge, we propose $\textsf{Safe LoRA}$, a simple one-liner patch to the original LoRA implementation by introducing the projection of LoRA weights from selected layers to the safety-aligned subspace, effectively reducing the safety risks in LLM fine-tuning while maintaining utility.
__label__natural_language_processing The resulting recipe, which we obtain through extensive experiments, can be used by practitioners to make informed design choices for their embedding models.
__label__machine_vision The training time is reduced by scene decomposition, and the training convergence and stability are guaranteed through the consensus on the shared 3D Gaussians.
"__label__safety_in_machine_learning However, the problem of provably editing a DNN to
satisfy a property remains challenging."
__label__fairness Extensive experiments show that data augmentation based on a balanced concept distribution augmented by ConBias improves generalization performance across multiple datasets compared to state-of-the-art methods.
__label__natural_language_processing Our method is compatible with a range of text encoders.
__label__optimization In dynamic submodular maximization, the goal is to maintain a high-value solution over a sequence of element insertions and deletions with a fast update time.
__label__neuroscience_and_cognitive_science These models are able to describe and generate a diverse array of complex, naturalistic images, yet they exhibit surprising failures on basic multi-object reasoning tasks -- such as counting, localization, and simple forms of visual analogy -- that humans perform with near perfect accuracy.
__label__deep_learning_architectures When fine-tuning the Llama2-13B model, VB-LoRA only uses 0.4% of LoRA's stored parameters, yet achieves superior results.
__label__interpretability_and_explainability Additionally, we leverage the parameterization to derive an effective intervention strategy based on the confidence region.
__label__optimization_for_deep_networks However, this family of approaches often requires an initial full-model warm-up phase, prior knowledge of a feasible rank, and it is sensitive to parameter initialization.
__label__machine_vision Code is available at https://github.com/gaozhitong/MultiShiftSeg.
__label__neuroscience_and_cognitive_science Our data and code are available at https://github.com/ZhangLab-DeepNeuroCogLab/MotionPerceiver.
__label__optimization This paper provides a comprehensive non-asymptotic analysis of SGD with biased gradients and adaptive steps for non-convex smooth functions.
__label__interpretability_and_explainability The projection is effected by a sparse oblique tree, having hard, hyperplane splits using few features and linear leaves.
__label__natural_language_processing Our data achieves an average score of 51.92, accompanied by a variance of 10.06.
__label__deep_learning_architectures Code is available at our project page https://sitzikbs.github.io/neural-experts-projectpage/ .
__label__diffusion_based_models Experiments demonstrate that our method can achieve up to 3x faster training for unconditional Consistency Models on the CIFAR dataset, as well as for DDIM and Stable Diffusion on CelebA and ImageNet dataset, and in class-conditional training and fine-tuning.
__label__neuroscience_and_cognitive_science This finding reconciles computational analyses of brain activity using large language models with the classic observation from aphasic patients showing left hemisphere dominance for language.
__label__probabilistic_methods Learning the unique directed acyclic graph corresponding to an unknown causal model is a challenging task.
__label__deep_learning_architectures Targeting improved expressiveness, this contribution introduces a *data-driven* prior that optimally fits the provided tasks using a novel non-injective change-of-variable (NCoV) model.
__label__machine_learning_for_healthcare ii.
__label__speech_and_audio Demo page: https://samsunglabs.github.io/FINALLY-page/
__label__graph_neural_networks Finally, we conduct experiments to verify the effectiveness of our regularization technique as well as the reliability of our theoretical analysis.
__label__machine_vision We demonstrate the superior performance of our model over its counterparts across a variety of tasks, including image reconstruction, image classification, auto-regressive image generation using GPT, and image creation with diffusion- and flow-based generative models.
__label__machine_learning_for_other_sciences_and_fields To bootstrap LMs for time series reasoning, we propose a prompt adaption module to determine domain-customized prompts dynamically instead of artificially.
__label__generative_models One of the main advantages of LoRA is its ability to be fused with  pretrained models, adding no overhead during inference.
__label__safety_in_machine_learning Starting from empirical evidence supporting our theoretical statements, we provide extensive experimental results on ImageNet distribution shift benchmarks that demonstrate the effectiveness of our theorem and its practical implementation.
__label__machine_learning_for_other_sciences_and_fields However, the former relies on strong assumptions of hidden confounding strength, whereas the latter relies on the expensive RCT data, thereby limiting their applicability in real-world scenarios.
__label__optimization Finally, we show the applications of our algorithmic framework on discounted Markov Decision Processes problem and Markov games, which bring new insights on the landscape analysis of reinforcement learning.
__label__natural_language_processing Experimental results show that COPPER possesses stronger reflection capabilities and exhibits excellent generalization performance across different actor models.
__label__learning_theory While finiteness of the Daniely-Shalev-Shwartz (DS) dimension has been shown to characterize the PAC learnability of a concept class [Brukhim, Carmon, Dinur, Moran, and Yehudayoff, 2022], there exist polylog factor gaps in the leading term of the sample complexity.
__label__natural_language_processing Real-world data deviating from the independent and identically distributed (\textit{i.i.d.})
__label__machine_vision Incorporating inductive bias by embedding geometric entities (such as rays) as input has proven successful in multi-view learning.
__label__deep_learning_architectures Image-feature regression methods heavily depend on matching results and do not utilize positional prior, while the autoregressive approach can only be trained using bounding boxes available in the training set, potentially resulting in suboptimal performance during testing with unseen data.
__label__natural_language_processing In this work, we propose a novel method called RankRAG, which instruction-tunes a single LLM for both context ranking and answer generation in RAG.
__label__probabilistic_methods Scientific modeling applications often require estimating a distribution of parameters consistent with a dataset of observations - an inference task also known as source distribution estimation.
__label__graph_neural_networks Equivariant Graph Neural Networks (GNNs) that incorporate E(3) symmetry have achieved significant success in various scientific applications.
__label__deep_learning_architectures We note that certain architectural changes cause degraded training efficiency/ICL accuracy by converging to suboptimal predictors or converging slower.
__label__diffusion_based_models Watermarking images is critical for tracking image provenance and proving ownership.
__label__machine_learning_for_other_sciences_and_fields The code is available at https://github.com/XLearning-SCU/2024-NeurIPS-CANDY.
__label__privacy Executing the data pruning of encrypted data on the server side is not trivial since the knowledge calculation of data pruning needs complex and expensive executions on encrypted data.
__label__machine_vision More information can be found on our project page: https://convrt-2024.github.io/
__label__algorithmic_game_theory Furthermore, we develop a decentralized stochastic primal-dual algorithm for efficiently computing the PSE point.
__label__natural_language_processing This paper introduces AutoSurvey, a speedy and well-organized methodology for automating the creation of comprehensive literature surveys in rapidly evolving fields like artificial intelligence.
__label__interpretability_and_explainability We investigate cases where the belief state geometry is represented in the final residual stream or distributed across the residual streams of multiple layers, providing a framework to explain these observations.
__label__speech_and_audio By employing a non-autoregressive vector field estimator based on a feed-forward transformer and channel-level cross-modal feature fusion with strong temporal alignment, our model generates audio that is highly synchronized with the input video.
__label__bandits The softmax function is ubiquitous in machine learning and optimization applications.
__label__evaluation Nevertheless, honesty and helpfulness, which ensure safe and useful real-world deployments, have been considered as the longstanding cornerstones in practice.
__label__diffusion_based_models Specifically, instead of directly measuring the divergence with paired images, we train a reward model with the dataset we construct, consisting of nearly 51,000 images annotated with human preferences.
__label__bandits Adaptivity to the problem's structure is essential in order to obtain optimal regret upper bounds.
__label__interpretability_and_explainability Our findings show that even linear transformers possess the surprising ability to discover sophisticated optimization strategies.
__label__causal_inference We then explore imitation learning in partially identifiable settings --- either transition distribution or reward function is non-identifiable from the available data and knowledge.
__label__other The expectation that stronger techniques would enhance performance has resulted in prominent PLL methods becoming not only highly complicated but also quite different from one another, making it challenging to choose the best direction for future algorithm design.
__label__machine_learning_for_healthcare Moreover, the function of certain proteins is highly dependent on the granular aspects of their surface topology, which have been overlooked by prior models.
__label__machine_vision Furthermore, we devise an interaction-aware attention module and a self-adaptive Gaussian refinement module.
__label__diffusion_based_models Central to our method is a rendering-based technique, which converts 3D semantic layouts into multi-view 2D proxy maps.
__label__learning_theory To answer this question, we establish novel statistical estimation and mathematical approximation theories for transformers when the input data are concentrated on a low-dimensional manifold.
__label__optimization_for_deep_networks To address these limitations, we propose Low-Rank Adapters for Quantized Training (LoQT), a method for efficiently training quantized models.
__label__algorithmic_game_theory by allowing either i) asymmetric initial conditions, or ii) an asymmetric game or iii) no-external regret dynamics suffices to destroy this result and lead to complex non-equilibrating or even chaotic behavior.
__label__reinforcement_learning Data selection is essential for training deep learning models.
__label__evaluation Surprisingly, we find that the state-of-the-art VLM-based metrics (e.g., TIFA, DSG, LLMScore, VIEScore) we tested fail to significantly outperform simple (and supposedly worse) feature-based metrics like CLIPScore, particularly on a hard subset of naturally-occurring T2I model errors.
__label__reinforcement_learning This analysis suggests that agents use a mixture of non-compositional and compositional messages to convey spatial relationships.
__label__machine_vision Diffusion-based Video Super-Resolution (VSR) is renowned for generating perceptually realistic videos, yet it grapples with maintaining detail consistency across frames due to stochastic fluctuations.
__label__machine_vision To address this issue, we present the first comprehensive study that probes various visual encoding models for 3D scene understanding, identifying the strengths and limitations of each model across different scenarios.
__label__machine_vision *, superior, inferior), and find that adopting multi-level preferences (*e.g.
__label__probabilistic_methods However, when modelling real-world data, learning problems are often not *exactly* equivariant, but only approximately.
__label__bandits Information theoretic lower bounds on sample complexity are well known for BAI problem and are matched asymptotically as $\delta\to 0$ by computationally demanding plug-in methods.
__label__online_learning Recent efforts in neural network optimization suggest a generalized smoothness condition, allowing smoothness to correlate with gradient norms.
__label__machine_learning_for_healthcare Unlike conventional methods, our approach introduces a learnable weight distribution via Bayesian modeling to automatically identify and mitigate false positive and negative pairs.
__label__other We then employ this algorithm in communication-efficient federated learning, in which model updates correspond to samples from a distribution, and achieve a 37% reduction in the communication load.
"__label__other We provide a theoretical analysis for our algorithm with an early stopping condition and show that if the batch is of size $\Omega((\gamma / \epsilon)^2\log (n\gamma/\epsilon))$, the algorithm must terminate within $O(\gamma^2/\epsilon)$ iterations with high probability, where $\gamma$ is the bound on the norm of points in the dataset in feature space,
and $\epsilon$ is a threshold parameter for termination."
__label__optimization_for_deep_networks Motivated by the above findings, we propose FedCCFA, a federated learning framework with classifier clustering and feature alignment.
__label__natural_language_processing Our further analysis reports that the user preference learned by CIPHER shows significant similarity to the ground truth latent preference.
__label__natural_language_processing Overall, our results make substantial strides toward detaching LMs from their tokenizer.
__label__optimization_for_deep_networks To this end, we study the infinite-width limit of neural networks trained with SAM, using the Tensor Programs framework.
__label__algorithmic_game_theory Our goal is to learn the expected core, that is, the set of allocations that are stable in expectation, given an oracle that returns a stochastic reward for an enquired coalition each round.
__label__natural_language_processing Extensive experiments conducted across a variety of models and tasks demonstrate that IRCAN not only achieves remarkable improvements in handling knowledge conflicts but also offers a scalable, plug-and-play solution that can be integrated seamlessly with existing models.
__label__natural_language_processing This method serves both as (1) a compute-efficient model up-scaling strategy that avoids directly tuning the large model and as (2) an instance of weak-to-strong generalization that enhances a strong model with weak test-time guidance.
__label__optimization To address these limitations, we utilize Bayesian Optimization (BO), a sample-efficient black-box solver, and propose a novel framework for combinatorial optimization on graphs.
__label__machine_learning_for_healthcare Motion time series collected from low-power, always-on mobile and wearable devices such as smartphones and smartwatches offer significant insights into human behavioral patterns, with wide applications in healthcare, automation, IoT, and AR/XR.
__label__privacy We propose DP scalable kernel empirical risk minimization (ERM) algorithms and a DP kernel mean embedding (KME) release algorithm suitable for general kernels.
__label__learning_theory Learning with reduced labeling standards, such as noisy label, partial label, and supplementary unlabeled data, which we generically refer to as imprecise label, is a commonplace challenge in machine learning tasks.
__label__safety_in_machine_learning We investigate a family of simple long-context attacks on large language models: prompting with hundreds of demonstrations of undesirable behavior.
"__label__machine_vision Project website:
https://open3dvlab.github.io/NeuRodin/"
__label__optimization A neural router is designed to adeptly distribute training instances among models, enhancing overall load balancing and collaborative efficacy.
__label__machine_learning_for_physical_sciences The benchmark features a total of 12,000 plate geometries with varying forms of beadings, material, boundary conditions, load position and sizes with associated numerical solutions.
__label__online_learning In particular, we construct a hypothesis class that is learnable in the iid batch setting under the PAC model but is not learnable under the smoothed online model.
__label__optimization Recent advances have employed a data-driven approach to select good cutting planes from a parameterized family, aimed at reducing the branch-and-bound tree size (in expectation) for a given distribution of integer programming instances.
__label__machine_vision Existing works based on the assumption of single and uniform illumination cannot correctly process these data.
__label__natural_language_processing Our results indicate that compositional learning in state-of-the-art Transformer language models is highly sample inefficient: LLaMA requires more data samples than relearning all sub-tasks from scratch to learn the compositional task; in-context prompting with few samples is unreliable and fails at executing the sub-tasks or correcting the errors in multi-round code generation.
__label__machine_vision Furthermore, 2D visibility maps and depth regularization are leveraged to mitigate the transient effects and constrain the geometry, respectively.
__label__learning_theory Under this manifold hypothesis, we show that convergence with respect to the $\mathrm{OT}_p$ metric happens exactly when $p>m$.
"__label__neuroscience_and_cognitive_science Additionally, EEGPT's hierarchical structure processes spatial and temporal information separately, 
  reducing computational complexity while increasing flexibility and adaptability for BCI applications."
__label__online_learning Exploiting the power of pre-trained models, prompt-based approaches stand out compared to other continual learning solutions in effectively preventing catastrophic forgetting, even with very few learnable parameters and without the need for a memory buffer.
__label__fairness Current approaches to mitigate these representational harms balance the number of retrieved items across population groups defined by a small number of (often binary) attributes.
__label__evaluation Experiments show that DEVIL evaluation metrics enjoy up to about 90\% consistency with human ratings, demonstrating the potential to advance T2V generation models.
__label__machine_vision Furthermore, to boost up both the adaptability and generality, we embed the learned features into hyperbolic space, which builds implicit hierarchical structures of 3D data from fewer examples.
__label__natural_language_processing Extensive evaluations of open-source and closed-source LLMs first validate the reliability of evaluation in CriticEval.
__label__machine_learning_for_healthcare However, ensuring data representations capture human-understandable concepts remains difficult, often requiring the incorporation of prior knowledge and decomposition of data into multiple subspaces.
__label__optimization_for_deep_networks We also prove that, in continuous time, gradient descent converges slowly on low-frequency classes while sign descent does not.
__label__causal_inference While identifiability has been well studied in scenarios where the system is fully observable, the conditions for identifiability remain unexplored when latent variables interact with the system.
__label__interpretability_and_explainability We further find that inferential (reasoning-based) solutions exhibit low complexity bias, which we hypothesize is a key factor enabling them to learn individual mappings for single anchors.
__label__machine_learning_for_other_sciences_and_fields Molecular learning is pivotal in many real-world applications, such as drug discovery.
__label__deep_learning_architectures The other branch employs patches to capture short-term information and aggregate the global representation of the series.
__label__optimization Our result is obtained via a novel analysis of the recursive regularization algorithm.
__label__diffusion_based_models We propose DiTFastAttn, a post-training compression method to alleviate the computational bottleneck of DiT.
__label__privacy We study the problems of differentially private federated online prediction from experts against both *stochastic adversaries* and *oblivious adversaries*.
__label__interpretability_and_explainability Previous work showed that despite claims of interpretability, humans are unable to use formal specifications presented in a variety of ways to validate even simple robot behaviors.
__label__machine_learning_for_healthcare In many settings, machine learning models may be used to inform decisions that impact individuals or entities who interact with the model.
__label__optimization_for_deep_networks Furthermore, as the rank allocation space expands, our method ensures fine-tuning efficiency, achieving a speed improvement of 94.5\% compared to AdaLoRA.
__label__online_learning A natural strategy for continual learning is to weigh a Bayesian ensemble of fixed functions.
__label__safety_in_machine_learning Our work provides useful insights into how to leverage foundation models in a data-efficient and computationally affordable manner to protect images against image segmentation models.
__label__machine_learning_for_other_sciences_and_fields Despite the impressive capabilities of large language models in biomedical text modelling, there remains a pressing need for a framework that seamlessly integrates these diverse modalities, particularly focusing on the three critical aspects of protein information: sequence, structure, and function.
__label__algorithmic_game_theory In a similar vein, we provide a novel input instance for deterministic divisible matching that demonstrates a nearly tight CEF approximation.
__label__other Extensive experiments on several large-scale vision benchmarks demonstrate the efficacy of HypStructure in reducing distortion and boosting generalization performance especially under low dimensional scenarios.
__label__robotics Experiments also demonstrate that as the memory bank expands, the Heuristic Process with only 1.8B parameters can inherit the knowledge from a GPT-4 powered Analytic Process and achieve continuous performance improvement.
__label__reinforcement_learning Subsequently, we introduce **B**ilin**E**ar **CAUS**al r**E**presentation (BECAUSE), an algorithm to capture causal representation for both states and actions to reduce the influence of the distribution shift, thus mitigating the objective mismatch problem.
__label__graph_neural_networks Additionally, we show that training in balance supports larger learning rates, which can improve generalization.
__label__reinforcement_learning Our proposed algorithm, ROIDICE, yields an efficient policy that offers a superior trade-off between return and accumulated cost compared to policies trained using existing frameworks.
__label__privacy For complete/labeled public data, we show that any $(\epsilon,\delta)$-PA-DP has excess risk $\tilde{\Omega}\big(\min(\frac{1}{\sqrt{n_{\text{pub}}}},\frac{1}{\sqrt{n}}+\frac{\sqrt{d}}{n\epsilon} ) \big)$, where $d$ is the dimension, ${n_{\text{pub}}}$ is the number of public samples, ${n_{\text{priv}}}$ is the number of private samples, and $n={n_{\text{pub}}}+{n_{\text{priv}}}$.
__label__machine_vision To enable prompt-conditioned estimation, we propose the first end-to-end promptable approach named UniPHD for R-HPM.
__label__generative_models This enables them to act as intelligent agents interacting with the real world.
__label__natural_language_processing Recent works show that assembling multiple off-the-shelf large language models (LLMs) can harness their complementary abilities.
__label__natural_language_processing Empirical results show consistent and significant performance gains afforded by a single-round structurization.
__label__optimization We also present an even faster approximation algorithm for quantizing large inputs on the fly.
__label__evaluation We also observe that GPT-4-Turbo can sustain much higher performance overall, but still cannot handle arithmetic API constraints well.
__label__machine_vision The augmented supervision guides the model to measure the completeness of label sets, thus facilitating the subsequent greedy tree search for label completion.
__label__deep_learning_architectures However, non-adjacent sections of real-world time-series may have strong dependencies.
__label__bandits Our methodology provides online services with an adaptive experimental design approach for learning the best-performing treatment for such encouragement designs.
__label__bandits In this study, we consider the infinitely many-armed bandit problems in a rested rotting setting, where the mean reward of an arm may decrease with each pull, while otherwise, it remains unchanged.
__label__bandits Interactive-Grounded Learning (IGL) [Xie et al., 2021] is a powerful framework in which a learner aims at maximizing unobservable rewards through interacting with an environment and observing reward-dependent feedback on the taken actions.
__label__infrastructure SGLang consists of a frontend language and a runtime.
__label__optimization However, given the distributional properties of the Gaussian processes applied on these methods, there may be potential to further exploit the information of the Gaussian processes to facilitate the BO search.
__label__machine_vision The code is provided in the appendix and will be open-source.
__label__natural_language_processing Moreover, theoretical analysis and experimental results confirm that ContAccum provides more stable dual-encoder training than current memory bank utilization methods.
__label__learning_theory Much of Bayesian inference centers around the design of estimators for inverse problems which are optimal assuming the data comes from a known prior.
__label__machine_learning_for_physical_sciences GenMS is able to generate complex structures such as double perovskites (or elpasolites), layered structures, and spinels, solely from natural language input.
__label__probabilistic_methods In this work, we present a comprehensive formulation of the binary decision making problem and provide a detailed characterization of the optimal solution.
__label__machine_learning_for_healthcare Consequently, when augmented with the commonly used negative log-likelihood loss, our approach significantly improves discrimination performance without directly manipulating the model outputs, thereby achieving better calibration.
__label__machine_vision Federated Learning (FL) allows multiple clients to collaboratively train models without directly sharing their private data.
__label__safety_in_machine_learning Our code is available at https://github.com/HongchaoZhang-HZ/SEEV.
__label__other In this work, we propose a new class of EOT solvers (ProgOT), that can estimate both plans and transport maps.
__label__neuroscience_and_cognitive_science We build a biologically motivated and trainable neural network model of dynamics in the visual pathway, incorporating local, lateral, and feedforward synaptic connections, excitatory and inhibitory neurons, and long-range top-down inputs conceptualized as low-rank modulations of the input-driven sensory responses by high-level areas.
__label__probabilistic_methods Previous work on LCNs has focused exclusively on marginal inference, namely computing posterior lower and upper probability bounds on a query formula.
__label__probabilistic_methods Here we leverage ideas from optimal transport to model disease progression as a latent permutation matrix of events belonging to the Birkhoff polytope, facilitating fast inference via optimisation of the variational lower bound.
__label__reinforcement_learning The proposed method’s effectiveness is verified on a collection of benchmark tasks, and the results support our theory that decentralized training with local interactions can still improve reward performance and satisfy safe constraints.
__label__machine_vision This scenario leads to misclassification of unseen classes as known ones, consequently undermining classification accuracy.
__label__diffusion_based_models We introduce DMD2, a set of techniques that lift this limitation and improve DMD training.
__label__natural_language_processing As a result, the model is provably unable to respond to these sequences in different ways---leading to errors in, e.g., tasks involving counting or copying.
__label__generative_models We provide the code, along with a blog post and video tutorial on the project page: https://s-sahoo.com/MuLAN
__label__machine_vision On the data side, we construct the first object-centric occupancy dataset from scratch using an automated pipeline.
__label__graph_neural_networks Specifically, the first and second ensembles are combinations of a set of base low-pass and high-pass filters, respectively, after which the third ensemble combines them with two learnable coefficients and yield a graph convolution (TFE-Conv).
__label__machine_learning_for_physical_sciences To address the pathology, we use the insights garnered to consider variable splitting that decomposes the high-order PDE into a system of lower-order PDEs.
__label__diffusion_based_models An intuitive and novel *balancer* is proposed to dynamically balance the strengths of the two models in denoising process, allowing plug-and-play use of any model without extra training.
__label__natural_language_processing Secondly, using gradient descent and a progressive learning curriculum, we find the optimal set of binary matrices and scaling vectors that minimize the $\ell_2$ distance between the produced approximation and original weights.
__label__graph_neural_networks \ourmethod first constructs a well-modeled graph embedding hypersphere and then samples \textit{representative, balanced, and unbiased subsets} from this embedding space, which achieves the goal we called {\fontfamily{lmtt}\selectfont \textbf{Graph Training Debugging}}.
__label__natural_language_processing This work identifies three critical $\underline{\textit{O}}$bstacles: ($\textit{O}$1) lack of comprehensive evaluation, ($\textit{O}$2) untested viability for scaling, and ($\textit{O}$3) lack of empirical guidelines.
__label__graph_neural_networks Graph Neural Networks (GNNs) have emerged as a dominant approach in graph representation learning, yet they often struggle to capture consistent similarity relationships among graphs.
__label__optimization In particular, the set of learnable targets is not dense in $\mathbb R^d$, and any subset of $\mathbb R^d$ homeomorphic to the $W$-dimensional sphere contains non-learnable targets.
__label__machine_learning_for_other_sciences_and_fields Second, we study two algorithmic approaches to signal prediction and inverse problems based on differentiable predictive modelling and diffusion models.
__label__optimization_for_deep_networks Extensive experiments on various detection datasets demonstrate the superiority of DCOD.
__label__interpretability_and_explainability We delineate the failure modes, including the errors of influence function and the non-additive structure of the collective influence.
__label__interpretability_and_explainability A bunch of heuristic approaches are proposed, however, none of them are guaranteed to solve this problem effectively.
__label__machine_vision This study pioneers the application of Mamba to multi-class unsupervised anomaly detection, presenting MambaAD, which consists of a pre-trained encoder and a Mamba decoder featuring (Locality-Enhanced State Space) LSS modules at multi-scales.
__label__safety_in_machine_learning It outperforms all the SotA techniques under the most complete benchmark suite.
__label__machine_learning_for_other_sciences_and_fields We consider a long-standing open problem in mathematics: discovering a Lyapunov function that ensures the global stability of a dynamical system.
__label__machine_vision However, the performance of the ONN model is often diminished by the gap between the ideal simulated system and the actual physical system.
__label__machine_vision Cross-domain few-shot learning (CDFSL) is proposed to transfer knowledge from large-scale source-domain datasets to downstream target-domain datasets with only a few training samples.
__label__speech_and_audio Our code aims to open source to the research communities.
__label__other [Forrow et al.
__label__robotics Mainstream methods adopt a one-query-one-trajectory paradigm, where each query corresponds to a unique trajectory for predicting multi-modal trajectories.
__label__learning_theory Multi-distribution or collaborative learning involves learning a single predictor that works well across multiple data distributions, using samples from each during training.
__label__online_learning We introduce the Strategic Littlestone Dimension, a new combinatorial measure that captures the joint complexity of the hypothesis class and the manipulation graph.
"__label__generative_models Despite the rise to dominance of deep learning in unstructured data domains, 
tree-based methods such as Random Forests (RF) and Gradient Boosted Decision Trees (GBDT) are still the workhorses for handling discriminative tasks on tabular data."
__label__safety_in_machine_learning Our approach achieves state-of-the-art deterministic robustness certificates on CIFAR-10 for the $\ell_2$ ($\epsilon = 36/255$) and $\ell_{\infty}$ ($\epsilon = 8/255$) threat models, outperforming the previous results by $+3.95$ and $+1.39$ percentage points, respectively.
__label__interpretability_and_explainability However, ensuring interpretability may impose constraints on the flexibility, depth, and width of neural networks.
__label__learning_theory kernel ridgeless regression), when the bandwidth or input dimension varies with the sample size.
__label__deep_learning_architectures It then re-attaches the shuffled segments back together and performs a learned weighted sum with the original input to capture both the newly shuffled sequence along with the original sequence.
__label__reinforcement_learning We propose a unified algorithm framework for randomized exploration in parallel Markov Decision Processes (MDPs), and two Thompson Sampling (TS)-type algorithms, CoopTS-PHE and CoopTS-LMC, incorporating the perturbed-history exploration (PHE) strategy and the Langevin Monte Carlo exploration (LMC) strategy respectively, which are flexible in design and easy to implement in practice.
__label__machine_learning_for_physical_sciences First, we derive a formula to systematically enforce hard boundary and initial conditions in Physics-Informed Neural Networks (PINNs), employing continuous functions within the PINN ansatz to ensure that these conditions are satisfied.
__label__optimization Frequently, the burgeoning field of black-box optimization encounters challenges due to a limited understanding of the mechanisms of the objective function.
__label__natural_language_processing The receiver decodes the message by ranking the (potentially corrupted) descriptions and selecting the one found to be most reliable.
__label__machine_learning_for_other_sciences_and_fields Comprehensive experiments demonstrate the superior performance of DeltaDock.
__label__interpretability_and_explainability Finally, extensive experiments on three real-world datasets demonstrate the effectiveness and efficiency of PoG.
__label__reinforcement_learning We employ two approaches: the inner state approach, which involves predicting based on the inner computations of the agents (e.g., plans or neuron activations), and a simulation-based approach, which involves unrolling the agent in a learned world model.
__label__machine_learning_for_other_sciences_and_fields However, these models often overlook the severe corruption in cryogenic electron microscopy (cryo-EM) images by high-level noises.
__label__optimization Taking CRONOS as a primitive, we then develop a new algorithm called CRONOS-AM, which combines CRONOS with alternating minimization, to obtain an algorithm capable of training multi-layer networks with arbitrary architectures.
__label__machine_vision We consider weakly supervised segmentation where only a fraction of pixels have ground truth labels (scribbles) and focus on a self-labeling approach where soft pseudo-labels on unlabeled pixels optimize some relaxation of the standard unsupervised CRF/Potts loss.
__label__natural_language_processing Common solutions often involve continual pre-training on longer sequences, which will introduce expensive computational overhead and uncontrollable change in model capabilities.
__label__speech_and_audio This paper introduces an extensive speech-text LLM framework, the Unified Spoken Dialog Model (USDM), designed to generate coherent spoken responses with naturally occurring prosodic features relevant to the given input speech without relying on explicit automatic speech recognition (ASR) or text-to-speech (TTS) systems.
__label__interpretability_and_explainability Moreover, MambaLRP facilitates a deeper inspection of Mamba architectures, uncovering various biases and evaluating their significance.
__label__fairness Debiasing approaches that fine-tune the VL model often suffer from catastrophic forgetting.
__label__machine_vision Extensive experiments on NuScenes and KITTI-360 datasets demonstrate the superiority of GeoNLF in both novel view synthesis and multi-view registration of low-frequency large-scale point clouds.
__label__algorithmic_game_theory No Free Lunch Analysis in adversarial (also called maximin) optimisation is a long-standing problem [45 , 46].
__label__machine_vision View-predictive generative models provide strong priors for lifting object-centric images and videos into 3D and 4D through rendering and score distillation objectives.
__label__machine_vision The proposed interface is adaptive to new tasks, and new models.
__label__human-AI_interaction Second, pretraining with global, unfiltered data before fine-tuning on English content can improve cultural understanding without sacrificing performance on said popular benchmarks.
__label__privacy Fixed size subsampling is appealing for its constant memory usage, unlike the variable sized minibatches in Poisson subsampling.
__label__fairness Via both theoretical analysis and a numerical case study on real-world data, we demonstrate the efficacy of our framework and method in maintaining platform revenue while ensuring desired levels of fairness for both items and users.
__label__machine_vision However, these methods are limited to small-scale, homogeneous data, i.e.
__label__causal_inference This allows for the estimation of the treatment effect over time.
__label__machine_vision However, they lack reasoning abilities and cannot be controlled via text instructions.
__label__diffusion_based_models Through our experiments, we make the intriguing finding that in many cases, single neurons are responsible for memorizing particular training samples.
__label__reinforcement_learning Additionally, the context encoder trained with SaNCE demonstrates greater robustness to a reduction in the number of available samples, thus possessing the potential to overcome the $\log$-$K$ curse.
__label__diffusion_based_models However, these models are limited by non-contextualized noise, which fails to fully consider the characteristics of Seq2Seq tasks.
__label__causal_inference To test the practical impact of these considerations, we recorded ISTAnt, the first real-world benchmark for causal inference downstream tasks on high-dimensional observations as an RCT studying how garden ants (Lasius neglectus) respond to microparticles applied onto their colony members by hygienic grooming.
__label__learning_theory We provide the first full classification of functions that transform Manhattan distances to Manhattan distances.
__label__generative_models To ensure cross-view consistency, MVInpainter is enhanced by video priors from motion components and appearance guidance from concatenated reference key\&value attention.
__label__other In such dynamic environments, employing pre-trained models may be inefficient, as they struggle to adapt to the constantly evolving data streams.
__label__learning_theory Minimizing these loss functions leads to new cardinality-aware algorithms that we describe in detail in the case of both top-$k$ and threshold-based classifiers.
__label__machine_vision Diffusion models have shown remarkable capabilities in visual generation, making them well-suited for addressing several requirements of the tracking problem.
__label__graph_neural_networks IntraMix is a theoretically grounded plug-in-play method that can be readily applied to all GNNs.
__label__deep_learning_architectures Current research into network-based methods primarily focuses on models' offline accuracy.
__label__diffusion_based_models Extensive experiments are given to validate the superior performance of DomainGallery on a variety of domain-driven generation scenarios.
__label__generative_models The efficacy of task-specific finetuning largely depends on the selection of appropriate training data.
__label__machine_learning_for_other_sciences_and_fields While recent advances enable the reconstruction of dynamic conformations of a single biomolecular complex, current methods do not adequately model samples with mixed conformational and compositional heterogeneity.
__label__machine_vision Codes are available in [https://github.com/zhuhsingyuu/Frolic](https://github.com/zhuhsingyuu/Frolic).
__label__interpretability_and_explainability However, evaluations produce scores better than what appears possible from the values in the explanation for a class of explanations, called encoding explanations.
__label__natural_language_processing To this end, we propose FunCoder, a code generation framework incorporating the divide-and-conquer strategy with functional consensus.
__label__learning_theory In the asymptotic limit of high-dimensional data and a comparably large number of training samples we provide a tight closed-form characterization of the global minimum of the non-convex empirical loss landscape.
__label__fairness Additionally, we do not require knowledge of the set of inputs a priori to inference time, making our method more appropriate for online tasks such as retrieval and text guided image generation.
__label__machine_vision Subsequently, we combine multi-view mask modeling with contrastive learning to obtain endoscopic video representations that possess fine-grained perception and holistic discriminative capabilities simultaneously.
__label__fairness Furthermore, we show that during training, some GNNs may adjust their loss on low-degree nodes more slowly than on high-degree nodes; however, with sufficiently many epochs of training, message-passing GNNs can achieve their maximum possible training accuracy, which is not significantly limited by their expressive power.
__label__machine_vision To tackle such a less-touched challenge, we propose an innovative method, termed as All-in-one VidEo Restoration Network (AverNet), which comprises two core modules, i.e., Prompt-Guided Alignment (PGA) module and Prompt-Conditioned Enhancement (PCE) module.
__label__machine_learning_for_healthcare However, existing molecular representation learning methods often introduce potential false positive and false negative pairs through conventional graph augmentations like node masking and subgraph removal.
__label__interpretability_and_explainability We present evidence of *learned look-ahead* in the policy and value network of Leela Chess Zero, the currently strongest deep neural chess engine.
__label__machine_learning_for_physical_sciences Our method outperforms DeepONets, Fourier Neural Operators and more traditional neural network architectures and can be used for design optimization.
__label__online_learning To address these challenges, we propose \mname, an efficient and effective online adaptation framework for LLMs with strong knowledge retention.
__label__probabilistic_methods Many critical decisions, such as personalized medical diagnoses and product pricing, are made based on insights gained from designing, observing, and analyzing a series of experiments.
__label__machine_learning_for_healthcare The model with 1.1 billion parameters also outperform over existing methods, achieving an average 27\% improvement on the QM9 and 14\% on COMPAS-1D dataset.
__label__reinforcement_learning We analyze SoftTreeMax and explain how tree expansion helps to reduce its gradient variance.
__label__safety_in_machine_learning Recent works have presented methods to identify if individual text sequences were members of the model's training data, known as membership inference attacks (MIAs).
__label__machine_vision Specifically, MfH generates humans on the input image with generative painting and estimates human dimensions with an off-the-shelf human mesh recovery (HMR) model.
__label__machine_vision Using this framework, we achieve equivalent performance as the baseline models, while reducing inference time compute by over two-fold.
__label__safety_in_machine_learning Our analysis also reveals that the iteration complexity to obtain an $\epsilon$-stationary point is bounded by O($\frac{1}{\epsilon^{2}}$).
__label__generative_models Additionally, the compression process introduces excessive overhead, substantially increasing memory burdens and the generation latency.
__label__fairness Thus, during a downstream task, we cannot debias such models by updating the weights of the feature encoder, as only the classifier can be finetuned.
__label__natural_language_processing The majority of language model training builds on imitation learning.
__label__machine_vision Besides, a point-to-point contrastive loss is proposed concentrating on distinguishing points with subtly similar features.
__label__machine_vision Simultaneously, CGFormer leverages multiple 3D representations (i.e., voxel and TPV) to boost the semantic and geometric representation abilities of the transformed 3D volume from both local and global perspectives.
__label__reinforcement_learning Learning safe policies has presented a longstanding challenge for the reinforcement learning (RL) community.
__label__machine_learning_for_other_sciences_and_fields We instantiate and evaluate our architecture in a premise selection setup, where it achieves promising initial results, surpassing strong baselines.
__label__natural_language_processing First, we apply an LLM-based approach to construct semi-open-ended questions and obtain answers from a target LLM.
__label__bandits Despite the success of RMABs, a key limiting assumption is the separability of rewards into a sum across arms.
__label__bandits The framework is useful in various real-world applications including ad placement, online retail, recommender systems, and fine-tuning language models, amongst many others.
__label__machine_vision Recently, vision model pre-training has evolved from relying on manually annotated datasets to leveraging large-scale, web-crawled image-text data.
__label__other These programs can be stored and applied locally, re-used and extended, and cost orders of magnitude less.
__label__probabilistic_methods At the same time, recent advancements in learning dynamical systems rely on modelling the underlying Hamiltonian to guarantee the conservation of energy.
__label__optimization The impressive performance of Large Language Models (LLMs) across various natural language processing tasks comes at the cost of vast computational resources and storage requirements.
__label__generative_models However, fine-tuning with only a handful, as little as one, of image-text paired data prevents fine-grained control of style attributes at generation.
__label__machine_vision Based on it, we further propose a simple but effective method for the CDFSL task to boost ViT's transferability by resisting the learning of query-key parameters and encouraging that of non-query-key ones.
__label__machine_vision However, these methods tend to lead to overfitting of the base classes seen during training and produce prompts that are no longer understandable by humans.
__label__graph_neural_networks Message passing plays a vital role in graph neural networks (GNNs) for effective feature learning.
__label__machine_learning_for_other_sciences_and_fields To reduce annotation costs, it is common in crowdsourcing to collect only a few noisy labels from different crowd workers for each instance.
__label__natural_language_processing We show the effectiveness of our approach using both LLama and Gemma model families.
__label__natural_language_processing In the final layers, LLMs generate responses aligned with the original language of the query.
__label__machine_vision However, the large number of Gaussians and their associated attributes require effective compression techniques.
__label__safety_in_machine_learning We approach this problem by introducing a novel generalization of weighted conformal prediction and support our method with theoretical coverage guarantees.
__label__machine_vision Codes and checkpoints are available at Github.
__label__natural_language_processing These varying requirements necessitate LLMs that can adapt their structures (depth and width) for optimal efficiency across different platforms and application specifications.
__label__graph_neural_networks In this fashion, message passing in the coarsened graph acts along persistent pooled topology, leading to improved performance.
__label__interpretability_and_explainability Code is available at https://github.com/fiveai/understanding_safety_finetuning.
__label__online_learning Our goal is to design best-of-both-worlds algorithms that perform optimally under both stochastic and adversarial constraints.
__label__learning_theory A long line of work established that uniformity testing has sample complexity $\Theta(\sqrt{n}\varepsilon^{-2})$.
__label__reinforcement_learning We also confirm that the effect of reward improvement remains valid across societies with different levels of character diversity.
__label__other How can the restricted model be useful to the full model?
__label__graph_neural_networks Graph Neural Networks (GNNs) have demonstrated remarkable performance across a spectrum of graph-related tasks, however concerns persist regarding their vulnerability to adversarial perturbations.
__label__safety_in_machine_learning To address this issue, we transform the problem of calibrating a multiclass classifier into calibrating a single surrogate binary classifier.
__label__natural_language_processing Our method enables two options, the **knowledge-preserved adaptation** and the **instruction-previewed adaptation**.
__label__optimization_for_deep_networks We find that exact GN generalizes poorly.
__label__optimization_for_deep_networks Our theoretical results imply that when training self-attention-only Transformers for fundamental regression tasks, the models learned by gradient-descent and Transformers show great similarity.
__label__natural_language_processing Secondly, the necessity to design distinct prompts for individual problems hampers their generalizability.
__label__deep_learning_architectures The proposed method capitalizes on the property of quadratic expansion to achieve superior performance while employing linear approximation for fast inference.
__label__optimization To minimize the objective function, we consider a generalized oracle termed as the internal function that includes the standard gradient oracle as a special case.
__label__machine_learning_for_physical_sciences As a result, these algorithms may search an unnecessarily large space and discover less accurate or overly complex equations.
__label__privacy Public pretraining is a promising approach to improve differentially private model training.
__label__diffusion_based_models Second, we design a time-dependent noise distribution (TimeNoise) for the conditional image during training, applying higher noise levels at larger time steps to disrupt it and reduce the model's dependency on it.
__label__graph_neural_networks Graph incremental learning has gained widespread attention for its ability to mitigate catastrophic forgetting for graph neural networks (GNN).
__label__graph_neural_networks Graph autoencoders (Graph-AEs) learn representations of given graphs by aiming to accurately reconstruct them.
__label__machine_vision It has been proven effective in numerous video processing tasks, including tracking, consistent video depth and feature refinement, motion and appearance editing, and stereoscopic video generation.
__label__machine_vision Despite with powerful generalization capability, the cloud model still cannot achieve error-free detection in a specific target domain.
__label__machine_vision However, direct applications of existing token pruning techniques designed for ViTs fail to deliver good performance, even with extensive fine-tuning.
__label__safety_in_machine_learning This attack is newly feasible with the larger context windows recently deployed by language model providers like Google DeepMind, OpenAI and Anthropic.
__label__interpretability_and_explainability Symmetry detection has been shown to improve various machine learning tasks.
__label__machine_learning_for_social_sciences Notably, we present a novel bias evaluation framework based on Masked Language Models to quantitatively assess social bias based on validated inventories of social cues/words, enabling a systematic analysis of the language used.
__label__fairness These models play a vital role in understanding complex relationships in high-dimensional data.
__label__natural_language_processing In practice, we build the APC scoring system by symbolically distilling small NLI and relevance discriminators (300M parameters) from GPT-4 for efficiency, and both show high consistency with GPT-4's discrimination.
__label__diffusion_based_models Furthermore, based on this, we propose a novel linear-nonlinear decoupling training strategy, significantly enhancing training effectiveness and surpassing consistency distillation on inference performance.
__label__machine_vision However, in real-world scenarios, the availability of target domain LR images is often limited, sometimes even to just one, which inevitably impairs the domain adaptation performance of SR networks.
__label__natural_language_processing In this work, we first conduct exploratory studies to demonstrate that increasing the number of activated experts does not necessarily improve and can even degrade the output quality.
__label__machine_vision By employing a retrieval-augmented generation strategy, our approach further enhances performance in some generative tasks within one model, offering a promising direction for future research in vision-language modeling.
__label__machine_vision Intuitively, as deepfakes also contain additional informative forgery clues ($\textit{e.g.,}$ deep generative artifacts), excluding all deepfake data in training deepfake detectors seems counter-intuitive.
__label__probabilistic_methods We provide an open implementation of DCPC in Pyro on Github.
__label__machine_vision The key idea of LuSh-NeRF is to sequentially model noise and blur in the images via multi-view feature consistency and frequency information of NeRF, respectively.
"__label__fairness SureMap's efficiency gains come from
(1) transforming the problem into structured simultaneous Gaussian mean estimation and (2) incorporating external data, e.g., from the AI system creator or from their other clients."
__label__active_learning In response to this challenge, we present Partially Observable Cost-Aware Active-Learning (POCA), a new learning approach aimed at improving model generalization in data-scarce and data-costly scenarios through label and/or feature acquisition.
__label__probabilistic_methods To address this, we explore integrating federated learning with a more effective prompt-tuning method, optimizing for a small set of input prefixes to reprogram the pre-trained model's behavior.
__label__other Finally, a conclusive reasoning scheme based on self-alignment is proposed for final result generation.
__label__fairness By translating principles of anti-discrimination law into a decision-theoretic framework, we formalise discrimination and propose a new, legally informed approach to developing systems for automated decision-making.
"__label__learning_theory different parameter groups and should not be neglected when
estimating the Fisher information."
__label__natural_language_processing Despite this, common approaches often rely on additional models or data, which increases costs and limits widespread adoption.
__label__generative_models We introduce Exo2Ego-V, a novel exocentric-to-egocentric diffusion-based video generation method for daily-life skilled human activities where sparse 4-view exocentric viewpoints are configured 360° around the scene.
"__label__natural_language_processing To surmount these limitations,
we propose a dynamic logit fusion approach that works with a series of task-specific small models, each specialized in a different task."
__label__infrastructure Previous efforts to improve efficiency include maximizing rematerialization and employing chunk-based tensor management to reduce host-device communication.
__label__other In this paper, we present the first systematic investigation of the effect of instructions on object hallucinations in LVLMs, with a specific focus on the role played by image description lengths.
__label__reinforcement_learning It innovates a synergistic strategy to meld the strengths of the oracle critic for efficiency improvement and the standard critic for variance reduction, featuring a novel mechanism for seamless transition and weighting between them.
__label__machine_learning_for_other_sciences_and_fields To this end, we develop an unsupervised learning approach based on the predicted pairwise contact map between a protein and a nucleic acid and demonstrate its effectiveness in protein-aptamer binding prediction.
__label__optimization_for_deep_networks Nonetheless, certain scenarios highlight potential pitfalls: training a Transformer using the Softmax attention kernel may sometimes lead to suboptimal local solutions.
__label__diffusion_based_models However, existing synthesis methods carry the risk of distribution deviations, potentially degrading model performance with out-of-distribution samples.
__label__other Our code is available at \href{https://github.com/xyang583/AMT}{https://github.com/xyang583/AMT}.
__label__learning_theory We prove our results through a novel application of the hemisphere transform.
__label__reinforcement_learning Specifically, we introduce the Q-weighted variational loss and its approximate implementation in practice.
__label__machine_learning_for_physical_sciences We propose Kronecker-factored approximate curvature (KFAC) for PINN losses that greatly reduces the computational cost and allows scaling to much larger networks.
__label__reinforcement_learning The bound for LS is provably tighter than its competitors, and naturally results in improved policy selection and learning strategies.
__label__diffusion_based_models We introduce the first continuous-time score-based generative model that leverages fractional diffusion processes for its underlying dynamics.
__label__neuroscience_and_cognitive_science However, existing DNNs are mostly designed to analyze neural responses to static images, relying on feedforward structures and lacking physiological neuronal mechanisms.
__label__machine_learning_for_other_sciences_and_fields We demonstrate the flexibility and effectiveness of our approach through multiple experiments with three benchmark models, namely Inception-V3, ResNet, and BERT.
__label__other We conduct comprehensive experiments to show that FuseFL supports high scalability of clients, heterogeneous model training, and low memory costs.
__label__online_learning Our results hold for sequential experts, beyond binary labels, which are settings rarely considered in prior work.
__label__machine_vision The reliability of driving perception systems under unprecedented conditions is crucial for practical usage.
__label__speech_and_audio Demos are available at https://0nutation.github.io/SpeechAlign.github.io/.
__label__diffusion_based_models Recent advances in diffusion models have demonstrated their strong capabilities in generating high-fidelity samples from complex distributions through an iterative refinement process.
__label__diffusion_based_models Through extensive experiments across various IP tasks, including two linear and three nonlinear IPs, we demonstrate that DMPlug consistently outperforms state-of-the-art methods, often by large margins especially for nonlinear IPs.
__label__natural_language_processing Typically, preference optimization is approached as an offline supervised learning task using manually crafted convex loss functions.
__label__natural_language_processing Our further observations provide a systematic pattern of activation spikes: 1) The activation spikes occur in the FFN of specific layers, particularly in the early and late layers, 2) The activation spikes are dedicated to a couple of tokens, rather than being shared across a sequence.
__label__generative_models To evaluate the performance of multi-objective controllable layout generation in natural scenes, we introduce the HiCo-7K benchmark, derived from the GRIT-20M dataset and manually cleaned.
__label__optimization The predominant approach is to alter the supervised learning pipeline by augmenting typical loss functions, letting model rejection incur a lower loss than an incorrect prediction.
__label__diffusion_based_models Diffusion models (DMs) have demonstrated remarkable proficiency in producing images based on textual prompts.
__label__speech_and_audio This alignment allows for the use of either audio or audiovisual input by combining or substituting the image encoder with the aligned audio encoder.
__label__machine_learning_for_healthcare Deep Learning in Image Registration (DLIR) methods have been tremendously successful in image registration due to their speed and ability to incorporate weak label supervision at training time.
__label__deep_learning_architectures Tested on 38 datasets across human activity sensors, healthcare, engineering, and finance, UniTS achieves superior performance compared to 12 forecasting models, 20 classification models, 18 anomaly detection models, and 16 imputation models, including adapted text-based LLMs.
__label__other 1) Knowledge reprogramming learning strategy jointly learns new domain-specific model parameters and a reweighting term to reprogram existing shared domain knowledge vectors, termed adaptation on _principal agents_.
__label__diffusion_based_models By defining the energy of self-attention, we introduce a method to reduce the curvature of the energy landscape of attention and use the output as the unconditional prediction.
__label__reinforcement_learning As such, we prevent policies from merely exploiting heuristic rewards without improving the task reward.
__label__learning_theory These pairings of online algorithms with corresponding learning rules yields improvements in the overall performance in comparison with previous work.
__label__reinforcement_learning In such MDPs, two measures of complexity have appeared in the literature: the diameter, $D$, and the optimal bias span, $H$, which satisfy $H\leq D$.
__label__neuroscience_and_cognitive_science Researchers have reported high decoding accuracy (>95%) using non-invasive Electroencephalogram (EEG) signals for brain-computer interface (BCI) decoding tasks like image decoding, emotion recognition, auditory spatial attention detection, etc.
__label__reinforcement_learning We further derive the implications of the proposed methods on the policy gradient.
__label__human-AI_interaction Rather than providing single-label predictions, these systems provide sets of label predictions constructed using conformal prediction, namely prediction sets, and ask human experts to predict label values from these sets.
__label__natural_language_processing Then, after incorporating corner tokens to aggregate diverse textual information, we manage to help the model catch up to its original level of short text understanding yet greatly enhance its capability of long text understanding.
__label__natural_language_processing The effectiveness of SimPO is attributed to a key design: using the _average_ log probability of a sequence as the implicit reward.
__label__machine_learning_for_other_sciences_and_fields To this end, we propose an optimal FL collaboration formation strategy -FedEgoists- which ensures that: (1) a FL-PT can benefit from FL if and only if it benefits the FL ecosystem, and (2) a FL-PT will not contribute to its competitors or their supporters.
__label__evaluation Instead of relying on high-variance REINFORCE policy gradient estimators that do not scale, our adaptive labeling policy is optimized using path-wise policy gradients computed by auto-differentiating through simulated roll-outs.
__label__machine_vision Building upon this finding, we present a unified training-time regularization technique to mitigate the bias and boost imbalanced OOD detectors across architecture designs.
__label__deep_learning_architectures These advances have spurred the vision of establishing a generalist robotic MLLM proficient in understanding complex human instructions and accomplishing various embodied tasks, whose feasibility has been recently verified~\cite{rt-2,rt-x}.
__label__learning_theory We study learning in a dynamically evolving environment modeled as a  Markov game between a learner and a strategic opponent that can adapt to the learner's strategies.
__label__probabilistic_methods In this work, we leverage information theory to connect conformal prediction to other notions of uncertainty.
__label__learning_theory In this paper, we introduce a new convolution method based on  $\ell_p$-norm.
__label__online_learning However, this approach struggles with determining optimal thresholds and adapting to complex scenarios with side information, where tracking accuracy is not the sole metric in the regression model.
__label__natural_language_processing To bolster the mathematical reasoning capabilities of LLMs, most existing efforts concentrate on seeking assistance from either domain experts or GPT-4 for high-quality process-supervised data, which is not only expensive but also labor-intensive.
__label__probabilistic_methods Expectation propagation (EP) is a family of algorithms for performing approximate inference in probabilistic models.
__label__diffusion_based_models Specifically, we first estimate grouped transition matrices through clustering.
__label__optimization_for_deep_networks Sharpness Aware Minimization (SAM) enhances performance across various neural architectures and datasets.
__label__optimization We also provide a probabilistic convergence result for Adam under a generalized smooth condition which allows unbounded smoothness parameters and has been illustrated empirically to capture the smooth property of many practical objective functions more accurately.
__label__machine_vision However, practical applications do not always require the classification of all kinds of objects, and leaving the model capable of recognizing unnecessary classes not only degrades overall accuracy but also leads to operational disadvantages.
__label__machine_vision In this paper, we delve deeper into the Kullback–Leibler (KL) Divergence loss and mathematically prove that it is equivalent to the Decoupled Kullback-Leibler (DKL) Divergence loss that consists of 1) a weighted Mean Square Error ($\mathbf{w}$MSE) loss and 2) a Cross-Entropy loss incorporating soft labels.
__label__privacy We first extend the definitions of Joint DP (JDP) and Local DP (LDP) to two-player zero-sum episodic Markov Games, where both definitions ensure trajectory-wise privacy protection.
__label__safety_in_machine_learning This seemingly counterintuitive strategy incorporates an intended distribution shift to encourage alignment preservation.
__label__machine_learning_for_other_sciences_and_fields The challenge of discovering new molecules with desired properties is crucial in domains like drug discovery and material design.
__label__generative_models Theoretically, we conduct a thorough examination to confirm our framework's ability to preserve joint distributions of geometric states and capability to completely model the underlying dynamics inducing trajectory distributions with negligible error.
__label__safety_in_machine_learning Diffusion-based purification has demonstrated impressive robustness as an adversarial defense.
__label__machine_vision LightGaussian achieves an average 15 times compression rate while boosting FPS from 144 to 237 within the 3D-GS framework, enabling efficient complex scene representation on the Mip-NeRF 360 and Tank & Temple datasets.
__label__safety_in_machine_learning Our work is the first to formalize and investigate secret collusion among frontier foundation models, identifying it as a critical area in AI Safety and outlining a comprehensive research agenda to mitigate future risks of collusion between generative AI systems.
__label__deep_learning_architectures Though this success is often attributed to the Transformer architecture, our systematic understanding is limited.
__label__machine_learning_for_healthcare The code is available at: https://github.com/xmed-lab/GPTrack.
__label__generative_models Here, we introduce an algorithm called energy rank alignment (ERA) that leverages an explicit reward function to produce a gradient-based objective that we use to optimize autoregressive policies.
__label__other The algorithm injects Laplace noise per diffusion iteration and adopts a degree-based thresholding function to mitigate the high sensitivity induced by low-degree nodes.
__label__machine_vision We verify our approach on three OSDG benchmarks, i.e., PACS, DigitsDG, and OfficeHome.
__label__graph_neural_networks Extensive experiments on ABIDE and ADNI datasets demonstrate that ALTER consistently outperforms generalized state-of-the-art graph learning methods (including SAN, Graphormer, GraphTrans, and LRGNN) and other graph learning based brain network analysis methods (including FBNETGEN, BrainNetGNN, BrainGNN, and BrainNETTF) in neurological disease diagnosis.
__label__fairness Code is provided at https://github.com/alex-oesterling/multigroup-proportional-representation.
__label__other The source code is available at https://github.com/Cascol-Chen/COLA.
__label__online_learning Finally, we identify a condition that ensures that the PAC learnability of a hypothesis class is sufficient for its smoothed online learnability.
__label__deep_learning_architectures Our approach achieves baseline full-precision accuracy in ImageNet classification and surpasses state-of-the-art results in semantic segmentation, with notable performance in image super-resolution, and natural language understanding with transformer-based models.
__label__privacy We then consider a special case of the oblivious adversaries setting, where there exists a low-loss expert.
__label__graph_neural_networks Here, we propose and study unitary group convolutions, which allow for deeper networks that are more stable during training.
__label__reinforcement_learning Our formulation results in an extended MDP that any standard RL algorithm can solve.
__label__learning_theory In contrast our bound implicitly controls all uncountably many weightings simultaneously.
__label__optimization Using an unbiassed compression technique, we develop a new method—Shadowheart SGD—that provably improves the time complexities of all previous centralized methods.
__label__reinforcement_learning Experimental results on retrosynthetic planning in organic chemistry, logic synthesis in integrated circuit design, and the classical Sokoban game empirically demonstrate the efficiency of SeeA$^*$, in comparison with the state-of-the-art heuristic search algorithms.
"__label__machine_vision Taken together, our study reveals an exponential need for training data which implies that the key to ""zero-shot"" generalization capabilities under large-scale training data and compute paradigms remains to be found."
__label__infrastructure Moreover, we develop the TinyTTA Engine, a first-of-its-kind MCU library that enables on-device TTA.
__label__optimization We analyze the sample complexity of AOT by considering the dual of the OT problem and show that it converges at the parametric rate.
__label__probabilistic_methods Ensembles can be constructed by training multiple CreNets, each associated with a different random seed, and averaging the outputted intervals.
__label__safety_in_machine_learning Code will be published after the anonymous period.
__label__graph_neural_networks A novel energy-based generative open-set node classification method, \textit{EGonc}, is proposed to achieve open-set graph learning.
__label__algorithmic_game_theory We study the proportional clustering problem of Chen et al.
__label__machine_learning_for_physical_sciences However, the naive estimation of this transform is infeasible, as it requires simulating sufficiently many forward trajectories to estimate rare event probabilities.
__label__machine_learning_for_healthcare However, most existing UDA methods are limited to one-to-one domain adaptation, which tends to be inefficient and resource-intensive when faced with multi-target domain transfer tasks.
__label__deep_learning_architectures }, attention, STAR employs a centralized strategy to improve efficiency and reduce reliance on the quality of each channel.
__label__natural_language_processing Additionally, our method incorporates $\beta$-guided data filtering to safeguard against the influence of outliers.
__label__machine_learning_for_healthcare We establish a high-probability regret bound that depends solely on the dimension of the differential-reward model, enabling us to achieve robust regret bounds even when the baseline reward is highly complex.
__label__optimization We consider a general noise model which governs affine variance noise, bounded noise, and sub-Gaussian noise.
__label__generative_models There is a rapidly growing interest in controlling consistency across multiple generated images using diffusion models.
"__label__deep_learning_architectures Our
codebase has been made available at https://rajatmodi62.github.io/apm_project_page/"
__label__infrastructure Compared to the state-of-the-art approach, our framework robustly achieves remarkable speedups from 3x to 30x in multiple distributed heterogeneous training setups and inference speedups of 1.5x to 2.35x without compromising arithmetic precision.
__label__deep_learning_architectures Large language models (LLMs) have acquired the ability to solve general tasks by utilizing instruction finetuning (IFT).
__label__reinforcement_learning We study episodic linear mixture MDPs with the unknown transition and adversarial rewards under full-information feedback, employing *dynamic regret* as the performance measure.
__label__other One key factor affecting the computational cost is the width of each layer.
__label__machine_vision Fine-grained visual classification (FGVC) involves classifying closely related subcategories.
__label__machine_vision In this work, we target adverse weather conditions and introduce an end-to-end domain adaptation strategy that leverages a fusion block, temporal-spatial teacher-student learning, and a temporal weather degradation augmentation approach.
__label__machine_vision We further provide an in-depth analysis of the gradient estimation error of various acceleration strategies as well as their impact on downstream tasks, offering valuable insights into the trade-offs between acceleration and performance.
__label__privacy Our DP pre-trained models are released in *fastDP* library (https://github.com/awslabs/fast-differential-privacy/releases/tag/v2.1)
__label__diffusion_based_models In this work, we propose a novel multi-stage generation paradigm that is designed for fine-grained control, flexibility and interactivity.
__label__graph_neural_networks To overcome the above limitations, this paper introduces a novel unified GA module dubbed UGA after reinterpreting the mechanism of GAs in GCLs from a message-passing perspective.
__label__probabilistic_methods Extensive experiments are conducted on various out-of-distributions (OOD) detection benchmarks (CIFAR10/100 vs SVHN/Tiny-ImageNet, CIFAR10 vs CIFAR10-C, ImageNet vs ImageNet-O) and using different network architectures (ResNet50, VGG16, and ViT Base).
__label__diffusion_based_models First, sampling a set of weights from this space results in a new model encoding a novel identity.
__label__diffusion_based_models A major challenge is making simultaneous edits across multiple objects or attributes.
__label__reinforcement_learning An emerging literature on opponent shaping has demonstrated the ability to reach prosocial outcomes by influencing the learning of other agents.
__label__generative_models 3D representation is essential to the significant advance of 3D generation with 2D diffusion priors.
__label__machine_vision SocialGPT introduces systematic design principles to adapt VFMs and LLMs separately and bridge their gaps.
__label__deep_learning_architectures However, fusing multiple modalities is challenging for SSMs due to its hardware-aware parallelism designs.
__label__natural_language_processing This paper introduces Sequoia, a scalable and robust algorithm for speculative decoding.
__label__machine_learning_for_healthcare We evaluate Flex-MoE on the ADNI dataset, which encompasses four modalities in the Alzheimer's Disease domain, as well as on the MIMIC-IV dataset.
__label__safety_in_machine_learning This results in a significant robustness to group shifts when equipped with a simple mechanism of last layer retraining.
__label__generative_models Motivated by this, we propose a novel solution named FreeLong to balance the frequency distribution of long video features during the denoising process.
__label__machine_learning_for_social_sciences We consider the problem of recovering the ground truth ordering (ranking, top-$k$, or others) over a large number of alternatives.
__label__machine_learning_for_other_sciences_and_fields For samples where all annotators agree, an aggregating strategy is designed to mitigate potential noise.
__label__learning_theory In contrast, to adopt the traditional theory of kernel regression, most recent works introduced a special mirrored architecture and a mirrored (random) initialization to ensure the network's output is identically zero at initialization.
__label__optimization The results indicate that KATE consistently outperforms AdaGrad and matches/surpasses the performance of Adam in all considered scenarios.
__label__deep_learning_architectures The chunkwise recurrent representation facilitates efficient long-sequence modeling with linear complexity, where each chunk is encoded parallelly while recurrently summarizing the chunks.
__label__graph_neural_networks It enumerates local subgraph patterns that match the predefined set of patterns $\mathcal{P}^\bullet$, applies non-linear transformations to node features, and aggregates them along with the patterns.
__label__machine_vision In this paper, we find an intriguing phenomenon neglected by previous works for the CDFSL task based on ViT: leaving the CLS token to random initialization, instead of loading source-domain trained parameters, could consistently improve target-domain performance.
__label__machine_vision Finally, the CYCLO method consistently achieves State-of-the-Art (SOTA) results on two in-the-wild scene graph generation benchmarks, i.e., PVSG and ASPIRe.
__label__natural_language_processing To address this, we introduce Self Logits Evolution Decoding (SLED), a novel decoding framework that enhances the truthfulness of LLMs without relying on external knowledge bases or requiring further fine-tuning.
__label__generative_models In this paper, we introduce an innovative technique called self-play fine-tuning for diffusion models (SPIN-Diffusion), where the diffusion model engages in competition with its earlier versions, facilitating an iterative self-improvement process.
__label__interpretability_and_explainability These methods can be computationally expensive for large ML models.
__label__reinforcement_learning To overcome these challenges, we introduce the **E**ffective **M**etric-based **E**xploration-bonus (EME).
__label__reinforcement_learning While the conditional sequence modeling with the transformer architecture has demonstrated its effectiveness in dealing with offline reinforcement learning (RL) tasks, it is struggle to handle out-of-distribution states and actions.
__label__learning_theory For PQ learning, we give efficient learning algorithms, while for TDS learning, our algorithms can tolerate moderate amounts of distribution shift.
__label__machine_learning_for_other_sciences_and_fields Specifically, FlexPlanner models 3D FP based on multi-modalities, including vision, graph, and sequence.
__label__optimization However, our current understanding of bias formation mostly focuses on the initial and final stages of learning, leaving a gap in knowledge regarding the transient dynamics.
__label__neuroscience_and_cognitive_science MP also demonstrates superior performance in these cases.
__label__algorithmic_game_theory Finally, we also study stronger notions of proportional representation, in which deviations do not only happen to single, but multiple candidate centers, and show that stronger proportionality notions of Brill and Peters imply approximations to these stronger guarantees.
__label__bandits We prove that single-parameter natural exponential families with subexponential tails are self-concordant with polynomial-sized parameters.
__label__machine_learning_for_healthcare From the biological view, the latent factors in biological data are discovered to model patient correlation.
__label__machine_learning_for_other_sciences_and_fields Besides,  lower-order exercise latent representations obtained in shallow layers are not well explored when learning the student representation.
__label__reinforcement_learning In this paper, we introduce a novel Advantage-Aware Policy Optimization (A2PO) method to explicitly construct advantage-aware policy constraints for offline learning under mixed-quality datasets.
__label__interpretability_and_explainability We justify the relevancy of the proposed framework via asymptotic and non-asymptotic theoretical guarantees and illustrate its benefits via an extensive set of numerical experiments.
__label__learning_theory For example, principal component analysis, Fisher's discriminant analysis, and canonical correlation analysis are specific instances of GEPs and are widely used in statistical data processing.
__label__machine_learning_for_other_sciences_and_fields We propose NeuralSteiner, an accurate approach to overflow-avoiding global routing in chip design.
__label__neuroscience_and_cognitive_science Localized receptive fields—neurons that are selective for certain contiguous spatiotemporal features of their input—populate early sensory regions of the mammalian brain.
__label__interpretability_and_explainability The code and pre-trained weights are available at https://github.com/YunshiWen/VQShape.
__label__graph_neural_networks Message-passing graph neural networks (MPNNs) have emerged as a powerful paradigm for graph-based machine learning.
__label__bandits We propose algorithms that achieve near-optimal regret with $O(T)$ time complexity and $O(1)$ arms stored, both of which are almost optimal and state-of-the-art.
__label__robotics To close this gap, we learn a controller that can pick up a large number (>1200) of objects and carry them to follow randomly generated trajectories.
__label__infrastructure We assess the performance of FlashMask in a variety of masking scenarios, including causal and customized attention masks, demonstrating its versatility and robustness across a wide range of attention patterns and models.
__label__safety_in_machine_learning However, these models are susceptible to adversarial examples, i.e., slightly perturbed examples that elicit a misprediction.
__label__deep_learning_architectures We prove that logarithmic depth is necessary and sufficient for tasks like graph connectivity, while single-layer transformers with small embedding dimensions can solve contextual retrieval tasks.
__label__generative_models However, this practice raises a pertinent question: Is it truly the optimal choice?
__label__reinforcement_learning Nevertheless, programmatic policies generated by these methods struggle to effectively solve long and repetitive RL tasks and cannot generalize to even longer horizons during testing.
__label__other Contrary to previous approaches, SHML autonomously diagnoses the reason for degradation and proposes diagnosis-based corrective actions.
__label__generative_models Generating novel views from a single image remains a challenging task due to the complexity of 3D scenes and the limited diversity in the existing multi-view datasets to train a model on.
__label__diffusion_based_models We lead the way to formalize the objective of consistent subject generation from a clustering perspective, and thus design a cluster-conditioned model.
__label__deep_learning_architectures A denoising block is responsible for removing the noise in a predicted bounding box, and multiple stacked denoising blocks cooperate to accomplish the whole denoising process.
"__label__learning_theory We examine two popular estimators whose accuracy and sample
complexity depend on their associated variances."
__label__reinforcement_learning In this paper, we propose a new algorithm that adaptively blends a set of OPE estimators given a dataset without relying on an explicit selection using a statistical procedure.
__label__neuroscience_and_cognitive_science A Ternary Self-Amplifying (TSA) neuron model with a silent period is proposed for supporting SWS-based computing, aimed at minimizing the residual error resulting from stepwise weighting in neural computation.
__label__safety_in_machine_learning In this work, we first investigate model unlearning from the perspective of weight changes and gradient norms, and find two interesting observations in the backdoored model: 1) the weight changes between poison and clean unlearning are positively correlated, making it possible for us to identify the backdoored-related neurons without using poisoned data; 2) the neurons of the backdoored model are more active (larger changes in gradient norm) than those in the clean model, suggesting the need to suppress the gradient norm during fine-tuning.
__label__machine_learning_for_physical_sciences Furthermore, we demonstrate that our proposed architecture enhances the efficiency of Boltzmann Generators trained on single molecular systems.
__label__reinforcement_learning (2024) to $poly(d, A, H)T^{2/3}$ for the full-information unknown transition setting, where $d$ is the rank of the transitions, $A$ is the number of actions, $H$ is the horizon length, and $T$ is the number of episodes.
__label__safety_in_machine_learning Certified adversarial robustness of large-scale deep networks has progressed substantially after the introduction of randomized smoothing.
__label__reinforcement_learning Learning the intents of an agent, defined by its goals or motion style, is often extremely challenging from just a few examples.
__label__natural_language_processing Large language models (LLMs) are commonly trained on datasets consisting of fixed-length token sequences.
__label__machine_learning_for_other_sciences_and_fields This paper extends the Agda ecosystem into machine learning territory, and, vice versa, makes Agda-related resources available to machine learning practitioners.
__label__machine_vision Based on MRDE predictions, it propagates the metric information from painted humans to the contexts, resulting in metric depth estimations for the original input.
__label__optimization We propose a generalized few-shot evolutionary optimization (FSEO) framework and focus on its performance on two common expensive optimization scenarios: multi-objective EOPs (EMOPs) and constrained EOPs (ECOPs).
__label__deep_learning_architectures Through analysis, we found the contribution ratio of Multi-Head Attention (a combination function) to pre-trained language modeling is a key factor affecting base capabilities.
__label__optimization In this work, we close this gap in the literature and provide the first analysis of methods with gradient compression and without-replacement sampling.
__label__online_learning To address the crucial need to keep models updated, online learning has emerged as a critical tool when utilizing LLMs for real-world applications.
__label__robotics Besides, a data reorganization strategy is introduced to narrow the gap between existing benchmarks and real-world applications, consistent with our network.
"__label__natural_language_processing Today's best language models still struggle with ""hallucinations"", factually incorrect generations, which impede their ability to reliably retrieve information seen during training."
__label__generative_models We demonstrate that RNNs, LSTMs, GRUs, Linear RNNs, and linear gated architectures such as Mamba and Hawk/Griffin can also serve be universal in-context approximators.
__label__other While the similarity between images is ambiguous, we argue that the spatial location of semantic objects does neither influence human perception nor deep learning classifiers.
__label__machine_vision We evaluate eFreeSplat on wide-baseline novel view synthesis tasks using the RealEstate10K and ACID datasets.
__label__neuroscience_and_cognitive_science We identify a fundamental trade-off between high resolution encoding of position and the number of storable contexts.
__label__reinforcement_learning We show that GAIL cannot converge to the desired equilibrium.
__label__reinforcement_learning In contrast, language can specify tasks in a more natural way.
__label__reinforcement_learning In SoftTreeMax, we extend the traditional logits with the multi-step discounted cumulative reward, topped with the logits of future states.
__label__optimization By applying our general solution to fairness and robustness, we delineate how and when heterogeneous data and spurious features can generate and amplify bias.
__label__machine_vision Furthermore, the theory tells us that approximating the reformulated loss should be improved by increasing the number of augmentations, and as such using multiple augmentations should lead to improved convergence.
__label__algorithmic_game_theory Our result applies to optimistic gradient and extra-gradient descent/ascent, as well as a certain iterative variant of Nesterov's smoothing technique.
__label__generative_models Our method is based on an analytical connection that we derive between the MSE-optimal denoiser for removing white Gaussian noise and the cross-entropy-optimal classifier for predicting the noise level.
__label__fairness Our method combines *maximum a posteriori* (MAP) estimation using a well-chosen prior together with cross-validation-free tuning via Stein's unbiased risk estimate (SURE).
__label__machine_learning_for_healthcare In pharmacology, for example, precise modeling of drug dynamics is vital to maximize therapeutic efficacy while minimizing patient harm, as in chemotherapy.
__label__natural_language_processing We also propose a simple training method to actively forget the learned co-occurrence statistics, which unblocks and enhances the learning of factual associations when training on plain narrative text.
__label__natural_language_processing These findings extend previous research on implicit bias in one-hot classification to the NTP setting, highlighting key differences and prompting further research into the optimization and generalization properties of NTP, irrespective of the specific architecture used to generate the context embeddings.
__label__machine_vision We find STRAINER to yield extremely powerful initialization for fitting images from the same domain and allow for a ≈ +10dB gain in signal quality early on compared to an untrained INR itself.
__label__causal_inference In this work, we investigate CSIvA \citep{ke2023learning}, a transformer-based model promising to train on synthetic data and transfer to real data.
__label__evaluation However, the absence of ground truth complicates model evaluation, as traditional metrics such as accuracy, precision, and recall cannot be directly calculated.
__label__interpretability_and_explainability In this study, we track how model mechanisms, operationalized as circuits, emerge and evolve across 300 billion tokens of training in decoder-only LLMs, in models ranging from 70 million to 2.8 billion parameters.
__label__causal_inference Another strength of our DiffPO method is that it is highly flexible (e.g., it can also be used to estimate different causal quantities such as CATE).
__label__machine_vision Given this powerful framework consisted of only Attentions and MLPs, we found four simple components that are essential to multi-class anomaly detection: (1) _Foundation_ _Transformers_ that extracts universal and discriminative features, (2) _Noisy_ _Bottleneck_ where pre-existing Dropouts do all the noise injection tricks, (3) _Linear_ _Attention_ that naturally cannot focus, and (4) _Loose_ _Reconstruction_ that does not force layer-to-layer and point-by-point reconstruction.
__label__machine_vision However, current VAD systems are often limited by their superficial semantic understanding of scenes and minimal user interaction.
__label__optimization_for_deep_networks Accumulated ray features along the feature volume provide a means to discount the coherence constraint amid erroneous ray matching.
__label__diffusion_based_models This paper introduces a novel pipeline for long-tail (LT) recognition that diverges from conventional strategies.
__label__natural_language_processing Finally, we use our automated metric to evaluate five recent LLMs with over 1000 questions from complex and ambiguous question answering tasks, which would cost $5k if evaluated by humans.
__label__machine_vision We demonstrate the approach’s effectiveness across diverse classification and detection tasks, outperforming existing methods in 7 out of 11 benchmarks and excelling in detection.
__label__machine_vision The deterministic nature of the existing finetuning methods makes them overlook the many possible interactions across the modalities and deems them unsafe for high-risk tasks requiring reliable uncertainty estimation.
__label__speech_and_audio We propose Frieren, a V2A model based on rectified flow matching.
__label__machine_vision To address MECD, we devise a novel framework inspired by the Granger Causality method, using an efficient mask-based event prediction model to perform an Event Granger Test, which estimates causality by comparing the predicted result event when premise events are masked versus unmasked.
__label__machine_vision Finally, we present the Spherical Frustum sparse Convolution Network (SFCNet) to adopt 2D CNNs for LiDAR point cloud semantic segmentation without quantized information loss.
__label__diffusion_based_models Due to the demanding memory, time, and data requirements, it is difficult to train a diffusion model directly on the entire volume of high-dimensional data to obtain an efficient 3D diffusion prior.
__label__other Unlike prior work in data valuation, which assumes centralized data access, we propose a federated approach to the data acquisition problem that is inspired by linear experimental design.
__label__neuroscience_and_cognitive_science Furthermore, it shows exceptional noise robustness and maintains high accuracy even at very low signal-to-noise ratios.
__label__deep_learning_architectures We demonstrate DT-L is capable of robustly learning algorithms which extrapolate to harder problems than in the training set.
__label__safety_in_machine_learning Due to the poor scalability of MIP solvers, large neural networks cannot benefit from these cutting planes.
__label__machine_vision Vision-Language Models (VLMs) have demonstrated their broad effectiveness thanks to extensive training in aligning visual instructions to responses.
__label__natural_language_processing tasks involving commonsense reasoning, moral decision-making, and sarcasm understanding.
__label__learning_theory By bridging our analysis to the realm of subspace learning, we systematically compare the efficacy of GAN-based methods against conventional approaches, both theoretically and empirically.
__label__machine_learning_for_healthcare Specifically, we employ a contrastive learning framework that aligns motion time series with text descriptions enriched by large language models.
__label__generative_models Code: https://www.github.com/brownvc/R3GAN
__label__learning_theory Given sample access to an unknown distribution $\mathbf{p}$ on $[n]$, one must decide if $\mathbf{p}$ is uniform or $\varepsilon$-far from uniform (in total variation distance).
__label__generative_models Being unable to learn interaction semantics through supervised training, we instead leverage pre-trained large models, synergizing knowledge from a large language model and a text-to-motion model.
__label__optimization Finally, we validate the generalization capabilities of the proposed KIO model and the effectiveness of the SSO algorithm through learning-from-demonstration tasks on the MuJoCo benchmark.
__label__natural_language_processing Based on these intriguing findings, our work not only presents a novel perspective for interpreting LLMs' generalization abilities from their intrinsic working mechanism but also provides new insights for the development of more effective learning methods for LLMs.
__label__natural_language_processing \textsc{AutoPSV} begins by training a verification model on the correctness of final answers, enabling it to generate automatic process annotations.
__label__natural_language_processing In this work, we propose a new mechanism to probe and understand in-context learning from the lens of decision boundaries for in-context binary classification.
__label__deep_learning_architectures As the adoption of large language models increases and the need for per-user or per-task model customization grows, the parameter-efficient fine-tuning (PEFT) methods, such as low-rank adaptation (LoRA) and its variants, incur substantial storage and transmission costs.
__label__safety_in_machine_learning The proposed attack method first adds random noise to an image to destroy the watermark and then reconstructs the image.
__label__machine_vision The code is released at https://github.com/linghan1997/Regression-based-Analytic-Incremental-Learning.
__label__deep_learning_architectures This novel methodology promises to integrate eigendecomposition efficiently into neural network training, overcoming existing computational challenges and unlocking new potential for advanced deep learning applications.
__label__reinforcement_learning E2GN2 demonstrates a significant improvement in sample efficiency, greater final reward convergence, and a 2x-5x gain in over standard GNNs in our generalization tests.
__label__machine_vision The domain adaptation method effectively mitigates the negative impact of domain gaps on the performance of super-resolution (SR) networks through the guidance of numerous target domain low-resolution (LR) images.
__label__natural_language_processing To address the lack of optimization, we propose three categories of RBs.
__label__machine_vision Due to the distinct imaging paradigm shift, a dominant line of research focuses on event-to-video (E2V) reconstruction to bridge event-based and standard computer vision.
__label__machine_vision Our method utilizes ray casting from the camera center to capture geometric and textured details, including depth, normal, and color, across all intersected surfaces.
__label__diffusion_based_models In the first stage, a large language model is used to rewrite the prompts.
__label__optimization_for_deep_networks Evaluations on Digit-5, Office-10, and DomainNet datasets demonstrate our method's superiority over existing approaches.
__label__reinforcement_learning In the outer optimization, it learns a policy on imaginary trajectories generated within the endogenous state space to maximize task-relevant uncertainty.
__label__deep_learning_architectures Numerous industrial sectors necessitate models capable of providing robust forecasts across various horizons.
__label__reinforcement_learning We achieve our results by building upon prior stochastic variance-reduced value iteration methods [Sidford, Wang, Wu, Yang, Ye 2018].
__label__optimization To this end, this work introduces algorithms tailored for communication-efficient Federated Group Distributionally Robust Optimization (FGDRO).
__label__machine_learning_for_other_sciences_and_fields The key to our methodology is introducing metrics based on the convergence dynamics of the formulas, rather than on the numerical value of the formula.
__label__deep_learning_architectures Domain Generalization (DG) aims to enable models to generalize to unseen target domains by learning from multiple source domains.
__label__learning_theory Further experimental results support our explanations.
__label__machine_learning_for_healthcare We further design a multi-level cross-branch temporal attention mechanism, which can facilitate the mining of interactive temporal context representations at both the intra-epoch and inter-epoch levels.
__label__machine_learning_for_other_sciences_and_fields In this paper, we propose GFNSeqEditor, a novel biological-sequence editing algorithm which builds on the recently proposed area of generative flow networks (GFlowNets).
__label__interpretability_and_explainability However, noise accumulated during this process can significantly distort the explanation.
__label__human-AI_interaction Both empirical results and user studies demonstrate CBPR's superior competitiveness compared to existing baselines.
__label__online_learning Then, we explore universal online learning, designing a single algorithm with the optimal gradient-variation regrets for convex and strongly convex functions simultaneously, without requiring prior knowledge of curvature.
__label__diffusion_based_models Our evaluations on real image datasets demonstrate that GFDM achieves greater pixel-wise diversity and enhanced image quality, as indicated by a lower FID, offering a promising alternative to traditional diffusion models
__label__graph_neural_networks However, most previous approaches have always used a recurrent architecture, where each iteration of the GNN matches an iteration of the algorithm.
__label__fairness However, these sensitive attribute annotations should be protected due to privacy and safety concerns.
__label__learning_theory Despite significant efforts to address each of these issues individually, a principled framework that reconciles these two objectives has been missing in the CP literature.
__label__privacy Model Inversion (MI) attacks pose a significant threat to the privacy of Deep Neural Networks by recovering training data distribution from well-trained models.
__label__natural_language_processing We demonstrate that within a limited search space, there exist better decision-making behaviors that facilitate the model in making reasonable and accurate judgments.
__label__natural_language_processing It not only resolves intricate mathematical problems but also demonstrates strong generalization capabilities across various mathematical contexts.
__label__machine_vision This work proposes a polarimetric imaging framework that can produce clean and clear polarized snapshots by complementarily fusing a degraded pair of noisy and blurry ones.
__label__machine_learning_for_healthcare Traditional linear methods fall short in modeling more than one space, while more expressive deep learning approaches lack interpretability.
__label__machine_vision Agent behavior simulation empowers robotics, gaming, movies, and VR applications, but building such simulators often requires laborious effort of manually crafting the agent's decision process and motion patterns.
__label__privacy Existing methods employ the influence function to achieve feature unlearning, which is impractical for FL as it necessitates the participation of other clients, if not all, in the unlearning process.
__label__deep_learning_architectures The transition matrix methods have garnered sustained attention as a class of techniques for label-noise learning due to their simplicity and statistical consistency.
__label__machine_vision VeXKD applies knowledge distillation exclusively to the Bird's Eye View (BEV) feature maps, enabling the transfer of cross-modal insights to single-modal students without additional inference time overhead.
__label__machine_learning_for_healthcare Mobile health leverages personalized and contextually tailored interventions optimized through bandit and reinforcement learning algorithms.
__label__neuroscience_and_cognitive_science 3.
__label__deep_learning_architectures In this paper, we propose Multi-Head Mixture-of-Experts (MH-MoE).
__label__machine_learning_for_other_sciences_and_fields For adders, our approach discovers designs of 128-bit adders that achieve Pareto optimality in theoretical metrics.
__label__diffusion_based_models Most existing solutions accelerate the sampling process by proposing fast ODE solvers.
__label__reinforcement_learning [2023a] proposed a PO-based algorithm with rate-optimal regret guarantees under the linear Markov Decision Process (MDP) model.
__label__diffusion_based_models Experiments on various diffusion models demonstrate the watermark remains verifiable even under significant image tampering and shows superior invisibility compared to other state-of-the-art robust watermarking methods.
__label__other Further, global thresholds are used to update model parameters by extracting aggregated parameter importance.
__label__reinforcement_learning To examine the achievability of ULI, we first provide two positive results for bandit problems with finite arms, showing that some elimination-based algorithms and high-probability adversarial algorithms with stronger analysis or additional designs, can attain near-optimal ULI guarantees.
__label__machine_vision Specifically, to enable GIMM as an effective motion modeling paradigm, we design a motion encoding pipeline to model spatiotemporal motion latent from bidirectional flows extracted from pre-trained flow estimators, effectively representing input-specific motion priors.
__label__machine_vision Existing Vision-Language Model (VLM)-based methods leverage VLM's rich knowledge to enhance additional explicit segmentation-specific networks, yielding competitive results, but at the cost of extensive training cost.
__label__machine_vision Thanks to the multitask capabilities of an encoder-decoder architecture, we show that an image captioner can effortlessly handle multiple tasks during pretraining.
__label__machine_vision Additionally, the general and diverse descriptions generated based on the class names may be wrong or absent in the specific given images.
__label__safety_in_machine_learning This strategy ensures that the calibration data and the selected test unit are exchangeable, allowing us to develop valid conformal p-values.
__label__diffusion_based_models We present *Conditional Conjugate Integrators*, which leverage the specific form of the inverse problem to project the respective conditional diffusion/flow dynamics into a more amenable space for sampling.
__label__optimization Adaptive methods are extremely popular in machine learning as they make learning rate tuning less expensive.
"__label__reinforcement_learning We then show that increases in gradient norm occur in RL in practice, and examine the differences between our 
theoretical model and the observed data."
__label__machine_vision Most previous approaches addressed these tasks separately, either by designing highly specialized social gaze models that do not generalize to other social gaze tasks or by considering social gaze inference as an ad-hoc post-processing of the gaze following task.
__label__fairness Using a training paradigm derived from Product of Experts, we create both biased and debiased branches with deep and shallow architectures and then distill knowledge to produce the target debiased model.
__label__online_learning A well-studied generalization of the standard online convex optimization (OCO) framework is constrained online convex optimization (COCO).
__label__machine_vision Experiments on benchmark datasets demonstrate that our method outperforms existing self-supervised and clean-image-free methods.
__label__machine_learning_for_physical_sciences In this paper, we introduce a new method that is **completely self-supervised and notably outperforms its fully-supervised counterparts while requiring only 1\% of the training samples (without labels) used by previous methods.
__label__machine_vision Extensive experiments across 6 IR tasks confirm the proposed SemanIR's state-of-the-art performance, quantitatively and qualitatively showcasing advancements.
__label__optimization_for_deep_networks We validate the effectiveness of our theory and the practical advantages of our proposed approach through comprehensive experiments.
__label__deep_learning_architectures We also show that such operators may be inverted locally via iteration provided that such inverse exists.
__label__natural_language_processing To mitigate this, we should be able to swap the original LM tokenizer with an arbitrary one, on the fly, without degrading performance.
__label__learning_theory One recent promising approach is to develop distillation-based algorithms that exploit unlabeled public data but the results are still unsatisfactory in both theory and practice.
__label__probabilistic_methods The estimator maps original data to the target distribution, for which MI is easier to estimate.
__label__machine_learning_for_other_sciences_and_fields We additionally demonstrate Hydra on an experimental dataset imaged of a cellular lysate containing a mixture of different protein complexes.
__label__machine_learning_for_healthcare We introduce a new functional representation for 3D molecules based on their continuous atomic density fields.
__label__natural_language_processing MEDIQ simulates clinical interactions consisting of a Patient System and an adaptive Expert System; with potentially incomplete initial information, the Expert refrains from making diagnostic decisions when unconfident, and instead elicits missing details via follow-up questions.
__label__safety_in_machine_learning Large Language Models (LLMs) are typically harmless but remain vulnerable to carefully crafted prompts known as ``jailbreaks'', which can bypass protective measures and induce harmful behavior.
__label__natural_language_processing Concretely, we first employ analysis on LLMs through the lens of information flow to detect the mechanism under zero-shot CoT reasoning, in which we discover that information flows from question to prompt and question to rationale jointly influence the reasoning results most.
__label__machine_vision All the existing methods assume ''white-box'' settings, where model information such as architectures, parameters, and gradients is available for training.
__label__active_learning Further testing on real-world datasets demonstrates improved performance compared to existing multi-label active learning methods.
__label__graph_neural_networks We construct hash codes using an optimization strategy and loss function to preserve the semantic information and compactness of the hash code.
__label__robotics Robo-Instruct further addresses this with InstAlign, an instruction-program alignment procedure that revises the task instruction to reflect the actual results of the generated program.
__label__reinforcement_learning Finally, we show that in an environment with explicit rewards, our algorithm recovers the same performance as a reward-based RL baseline, despite relying solely on a weaker preference signal.
__label__machine_vision Achieving both high rendering quality and accurate geometry is a challenge.
__label__reinforcement_learning Previous sim-to-real algorithms like Domain Randomization (DR) requires domain-specific expertise and suffers from issues such as reduced control performance and high training costs.
__label__optimization_for_deep_networks We propose a variant of gradual stacking called MIDAS that can speed up language model training by up to 40\%.
__label__optimization_for_deep_networks Additionally, we study the generalization result of $S^{2} - SAM$ and provide theoretical proof for convergence.
__label__optimization Under this computational budget, we consider the problem of sparse PCA, where the principal eigenvector of $\Sigma$ is $s$-sparse, and $r_{\mathsf{eff}}$ can be large.
__label__machine_vision Extensive experiments across public hallucination and general benchmarks, as well as our MRHal-Bench, demonstrate the effectiveness of our proposed method.
__label__learning_theory To conquer the challenges, our solution involves a cross-view reconstruction strategy that not only alleviate the anchor shift problem through a carefully designed cross-view learning process, but also reconstructs missing samples in a way that transcends the limitations imposed by convex combinations.
__label__generative_models These representations diversify and enrich the input conditions to the diffusion models, enabling more diverse outputs.
__label__machine_vision Recently, state space models (SSM) such as Mamba have gained great attention due to their linear complexity and long sequence modeling ability for language understanding.
__label__natural_language_processing However, training these LLMs typically involves substantial memory and computational costs during both forward and backward propagation.
__label__machine_learning_for_other_sciences_and_fields Recently, Large Language Models have shown great promise in solving certain biological tasks but current approaches are limited to a single sequence modality (DNA, RNA, or protein).
__label__machine_learning_for_other_sciences_and_fields We open-source our model, paving the way for new multi-modal gene expression approaches.
__label__machine_vision Specifically, we first employ an advanced LLM to automatically generate Textual Videos comprising continuous textual frames, along with corresponding annotations to simulate real video-text data.
__label__reinforcement_learning In this work, we show that DICE-based methods can be viewed as a transformation from the behavior distribution to the optimal policy distribution.
__label__interpretability_and_explainability While existing methods primarily concentrate on finding alternative paths to circumvent noise, they overlook a critical issue: intermediate-step images frequently diverge from the distribution of training data, further intensifying the impact of noise.
__label__probabilistic_methods At the core of our formulation lies the observation that the addition of two integer-valued random variables can be performed by adapting the fast Fourier transform to probabilities in the log-domain.
__label__reinforcement_learning Current approaches to learning cooperative multi-agent behaviors assume relatively restrictive settings.
__label__natural_language_processing This new annotator is adopted in the annotation pipeline for the next iteration.
__label__causal_inference The trained model effectively detects causal relationships and generalizes well across different functional forms of nonlinear dependencies.
__label__machine_vision Extensive experimental results show that we achieve better or comparable performances on the LVU, COIN, and Breakfast datasets.
__label__probabilistic_methods We demonstrate the versatility and effectiveness of our approach for a wide range of Bayesian inverse problems.
__label__machine_learning_for_other_sciences_and_fields It also achieves an average improvement of 22% on the OE62 dataset while integrating with various architectures.
__label__machine_learning_for_other_sciences_and_fields Semantic routing is already widely found in industry applications, especially navigational services like Google Maps; however, existing implementations only support limited route criteria and narrow query sets as they rely on repurposing classical route optimization algorithms.
__label__optimization_for_deep_networks Incorporating specific corruptions into the data augmentation pipeline can improve robustness to those corruptions but may harm performance on clean images and other types of distortion.
__label__bandits We are the first to achieve bounds on the expected cumulative reward for general confidence-rated predictors.
__label__optimization_for_deep_networks Firstly, during rank allocation, the salience measurement analyses the variation of singular value magnitudes across multiple time steps and establishes their inter-dependency relationships to assess the matrix importance.
__label__machine_vision This design is based on the observation that local attention is essentially the same as cost-volume, both of which are computed by dot-production between a query and its surrounding features.
__label__generative_models It selects a few heads and channels in the MHA and FFN modules for each Transformer Block, respectively.
__label__infrastructure In addition, we also demonstrate that \mavolion{} presents a more favorable performance-bandwidth balance compared to existing efficient distributed methods such as deep gradient compression and ternary gradients.
__label__human-AI_interaction In this paper, we explore whether we can quantitatively model how humans integrate both AI recommendations and explanations into their decision process, and whether this quantitative understanding of human behavior from the learned model can be utilized to manipulate AI explanations, thereby nudging individuals towards making targeted decisions.
__label__safety_in_machine_learning Implicit models such as Deep Equilibrium Models (DEQs) have emerged as promising alternative approaches for building deep neural networks.
__label__probabilistic_methods Our approach can be used with any decision-theoretic acquisition function and is readily compatible with trust region methods like TuRBO (Eriksson et al., 2019).
__label__machine_learning_for_other_sciences_and_fields We theoretically prove that the FL-PT coalitions formed are optimal since no coalitions can collaborate together to improve the utility of any of their members.
__label__generative_models We extensively evaluate PBP-GFN across eight benchmarks, including hyper-grid environment, bag generation, structured set generation, molecular generation, and four RNA sequence generation tasks.
__label__machine_learning_for_other_sciences_and_fields We also provide an extensive theoretical analysis of our methods.
__label__neuroscience_and_cognitive_science We validate LDNS on synthetic data, accurately recovering latent structure, firing rates, and spiking statistics.
"__label__machine_learning_for_physical_sciences In terms of runtime, 
PACE demonstrates 154-577x and 11.8-12x simulation speedup over numerical solver using scipy or highly-optimized pardiso solver, respectively."
__label__safety_in_machine_learning This environment enables the study of how ethical considerations, strategic planning, and negotiation skills impact cooperative outcomes.
__label__evaluation Experimental results demonstrate that this protocol not only ensures high-quality annotations but can also reduce evaluation costs by nearly 50\%.
__label__deep_learning_architectures Large pre-trained models excel in zero/few-shot learning for language and vision tasks but face challenges in multivariate time series (TS) forecasting due to diverse data characteristics.
__label__diffusion_based_models To address these issues, we introduce a novel diffusion model, called TEdit.
__label__diffusion_based_models We hope that our study provides new insights into understanding the data and pre-training processes of DMs.
__label__machine_vision It acts as a guide for effective sparsity learning and speeds up training.
__label__natural_language_processing In this paper, we introduce AlphaLLM for the self-improvements of LLMs, which integrates Monte Carlo Tree Search (MCTS) with LLMs to establish a self-improving loop, thereby enhancing the capabilities of LLMs without additional annotations.
__label__safety_in_machine_learning To further unveil the safety risks of LLMs, we introduce a Safety Concept Activation Vector (SCAV) framework, which effectively guides the attacks by accurately interpreting LLMs' safety mechanisms.
__label__learning_theory Then we show that there is a generic transformation from such codes over large alphabets to watermarking schemes for arbitrary language models.
__label__deep_learning_architectures Additionally, we develop novel algorithms that establish early-termination criteria for DeeR, conditioned on predefined demands such as average computational cost (\emph{i.e.
__label__machine_vision We also show that our renovated names improve evaluation by better measuring misclassification and enabling fine-grained model analysis.
__label__reinforcement_learning Moreover, under the assumption about the distribution of prediction errors, we have theoretically shown the superior efficiency of SeeA$^*$ over A$^*$ search, particularly when the accuracy of the guiding heuristic function is insufficient.
__label__diffusion_based_models Codes are available at https://github.com/OPTML-Group/AdvUnlearn.
__label__infrastructure This reduces static memory allocation by 1.47$\times$.
__label__evaluation It bridges (1) comprehensive and well-distributed real-world user queries and (2) efficient and fairly-graded ground-truth-based benchmarks, by matching queries mined from the web with similar queries from existing benchmarks.
__label__deep_learning_architectures While numerous forecasters have been proposed using different network architectures, the Transformer-based models have state-of-the-art performance in time series forecasting.
__label__generative_models In particular, when should one be preferred to the other?
__label__machine_learning_for_other_sciences_and_fields As a result, existing studies have clear limitations when scaled to a worldwide context.
__label__machine_vision Recent advancements in text-to-image diffusion models have introduced new possibilities for data augmentation in image classification.
__label__natural_language_processing Recent advancements in large language models (LLMs) focus on aligning to heterogeneous human expectations and values via multi-objective preference alignment.
__label__machine_vision In this paper, we improve prompt learning by distilling the textual knowledge from natural language prompts (either human- or LLM-generated) to provide rich priors for those under-represented concepts.
"__label__learning_theory Code is available at 
 https://github.com/kaotty/Understanding-ESSL."
__label__optimization Despite its ease of implementation, the EF approximation has its theoretical and practical limitations.
__label__probabilistic_methods Our code is available at https://github.com/michael-s-yao/gabo.
__label__other While traditional federated learning (FL) typically focuses on a star topology where clients are directly connected to a central server, real-world distributed systems often exhibit hierarchical architectures.
__label__reinforcement_learning The sample-based search makes it directly applicable to both discrete and continuous action spaces without modifications.
__label__diffusion_based_models Comprehensive experiments demonstrate our O-BELM sampler establishes the exact inversion property while achieving high-quality sampling.
__label__graph_neural_networks We conduct an extensive set of experiments across diverse datasets and tasks, demonstrating a consistent and superior performance of DiGRAF compared to traditional and graph-specific activation functions, highlighting its effectiveness as an activation function for GNNs.
__label__diffusion_based_models To tackle this issue, we propose a novel framework, MAS, to Mitigate Association-engendered Stereotypes.
"__label__interpretability_and_explainability Existing studies typically build the relation by explicitly defining the similarity between the estimated noise transition matrices of ""special"" instances and those of other instances."
__label__generative_models Specifically, we first perform a lightweight multimodal alignment on the encoding side to empower the MLLM to perceive visual details and language instructions.
__label__optimization_for_deep_networks In this work, we develop a stronger characterization of the optimization and generalization landscape of ICL through contributions on architectures, low-rank parameterization, and correlated designs: (1) We study the landscape of 1-layer linear attention and 1-layer H3, a state-space model.
__label__machine_learning_for_healthcare Our data and code are available at https://github.com/hry98kki/PatternBERP.
__label__natural_language_processing This approach results in an average improvement of $3.6\%$ for high-resource languages and $2.3\%$ for low-resource languages across all tasks with just $400$ documents.
__label__bandits In this work, we consider the same problem and provide the first provably efficient algorithms with sublinear regret under realizability.
"__label__natural_language_processing We more closely simulate finetuning workflows which train pretrained models on specialized knowledge by introducing
*WikiReversal*, a realistic testbed based on Wikipedia knowledge graphs."
__label__deep_learning_architectures Our method allows a wide flexibility in the encoder and decoder architectures and does not require group-specific layers.
__label__other Overall, we view our work as an important step in (i) deepening our scientific understanding of unlearning and (ii) revealing new pathways to improving the state-of-the-art.
__label__reinforcement_learning Upon on these findings, in this paper, we propose an Empirical MDP Iteration (EMIT) framework.
__label__online_learning Kleinberg et al.
__label__deep_learning_architectures Through extensive experimentation, we demonstrate that Cluster-Learngene not only is more efficient compared to other initialization methods but also customizes models of elastic scales according to downstream task resources.
__label__natural_language_processing Many defense strategies have been developed to enhance the safety of LLMs.
__label__optimization These are achieved by guiding the randomized greedy algorithm with a fast local search algorithm.
__label__online_learning Despite recent solutions to classical CURL, none address non-stationary MDPs.
__label__causal_inference We study the differences arising from merging predictors in the causal and anticausal directions using the same data.
__label__algorithmic_game_theory We introduce Team-Fictitious Play (Team-FP), a new variant of fictitious play where agents respond to the last actions of team members and the beliefs formed about other teams with some inertia in action updates.
__label__deep_learning_architectures Outlier detection in high-dimensional tabular data is an important task in data mining, essential for many downstream tasks and applications.
__label__generative_models Furthermore, our method is compatible with modern graphic pipelines, showcasing its potential in the 3D gaming and film industry.
__label__robotics Existing approaches focus on parts of this challenge, such as generating reward functions or task hyperparameters.
__label__machine_learning_for_other_sciences_and_fields This enables the model to analyze complex events, such as unexpected incidents and shifts in social behavior, and continuously refine the selection logic of news and the robustness of the agent's output.
__label__interpretability_and_explainability However, significant changes in the conditional distribution occur only when causal features are eliminated.
__label__neuroscience_and_cognitive_science High-level visual brain regions contain subareas in which neurons appear to respond more strongly to examples of a particular semantic category, like faces or bodies, rather than objects.
__label__reinforcement_learning In this setup, a knowledgeable teacher can demonstrate a dataset of state and action tuples and is required to teach an optimal policy to an entire family of BC learners using the smallest possible dataset.
__label__bandits Our approximations are motivated by posterior sampling with a Gaussian prior, and inherit its simplicity and efficiency.
__label__graph_neural_networks To figure out the missing part, in this paper, we disentangle the graph homophily into three aspects: label, structural, and feature homophily, which are derived from the three basic elements of graph data.
__label__deep_learning_architectures Project page is available at \url{https://slotssms.github.io/}
__label__machine_learning_for_physical_sciences For a very long time, computational approaches to the design of new materials have relied on an iterative process of finding a candidate material and modeling its properties.
__label__machine_vision Empowering 3D Gaussian Splatting with generalization ability is appealing.
__label__machine_vision Finally, we design a router fading strategy to integrate the learned parameters into the original Transformer, archiving efficient inference.
__label__machine_vision Our code, data and trained models are available at \url{https://github.com/chenkang455/S-SDM}.
__label__causal_inference Extensive robustness checks confirm that our findings are stable under variable misclassification.
__label__graph_neural_networks How to design a model that retains the ability of polynomial-based spectral GNNs to approximate filters while it possesses higher generalization and performance?
__label__reinforcement_learning These results pave the way for more reliable and effective solutions in complex multi-agent systems.
__label__interpretability_and_explainability These theoretical insights are then validated through experiments, which demonstrate that Transformer architectures indeed learn the adjacency and an incomplete reachability matrices, consistent with our theoretical predictions.
__label__diffusion_based_models We conduct extensive experiments on both unconditional and conditional generation in various scenarios, including physical simulation, molecular dynamics, and pedestrian motion.
__label__machine_vision Due to the low computational overhead and inherent flexibility of explicit representations, plane-based explicit methods are popular ways to predict deformations for Gaussian-based dynamic scene rendering models.
__label__generative_models We develop a new method that takes an ICL problem and estimates the probability that a CGM will generate a hallucination.
__label__machine_vision We verify AWT in multiple challenging scenarios, including zero-shot and few-shot image classification, zero-shot video action recognition, and out-of-distribution generalization.
__label__algorithmic_game_theory Currently, there are no hand abstraction algorithms that effectively integrate historical information.
__label__graph_neural_networks Extensive experiments across diverse heterogeneous datasets validate LMSPS's capability in discovering effective long-range meta-paths, surpassing state-of-the-art methods.
__label__machine_learning_for_physical_sciences Recently, AI-based data assimilation approaches have attracted increasing attention for their significant advantages over traditional techniques in terms of computational consumption.
__label__generative_models This shortcut eliminates the need for cumbersome primal-dual policy iterations, greatly reducing the computational burden and improving training stability.
__label__machine_vision It outperforms CLIP specialized distillation methods across five zero-shot classification datasets and two zero-shot image-text retrieval datasets.
__label__neuroscience_and_cognitive_science While recent advances in population-level recording technologies have allowed simultaneous recording of up to tens of thousands of neurons, this represents only a tiny fraction of most cortical circuits.
__label__graph_neural_networks Several recent works have shown that extending the message passing paradigm to subgraphs communicating with other subgraphs, especially via higher order messages, can boost the expressivity of graph neural networks.
__label__natural_language_processing Second, leveraging these insights, we propose algorithms to improve BPD inference speed by refining the block drafts using task-independent \ngram and neural language models as lightweight rescorers.
__label__other Notably, HiCS-FL drastically reduces computation cost compared to existing selection schemes and is adaptable to different heterogeneity scenarios.
__label__graph_neural_networks In this paper, we propose a prompt-based KG foundation model via in-context learning, namely KG-ICL, to achieve a universal reasoning ability.
__label__machine_vision The Event-Slots branch is engineered to learn event-centric slots from high temporal sample rate but low spatial resolution features.
__label__machine_vision MonoMAE consists of two novel designs.
__label__machine_vision These gradients are projected to a lower dimension and then concatenated with the model's output embedding.
__label__deep_learning_architectures This approach significantly reduces the memory required for training and enables rapid adaptation to the target domain with minimal parameter updates.
__label__learning_theory Information constraint distributed goodness-of-fit testing is a problem that has received considerable attention recently.
__label__bandits Concretely, our combinatorial HierTS algorithm attains comparable Bayes regret bound $O(m\sqrt{n}\log{n})$ with respect to the latest one.
__label__machine_vision However, applying PET directly to downstream data cannot fully explore the inherent knowledge in PTMs.
__label__interpretability_and_explainability As Large Language Models (LLMs) demonstrate impressive capabilities, demystifying their internal mechanisms becomes increasingly vital.
__label__machine_vision Concretely, we connect a multi-scale visual feature extractor and a large language model (LLM) by developing an object abstractor and an object-to-text abstractor.
__label__machine_vision Over multiple domains, we experimentally compare our method against the alternative of using only the one-shot model, and find that even under equal search-time budgets, our editing-based paradigm provides significant advantages.
__label__generative_models Our experiments demonstrate that HydraLoRA outperforms other PEFT approaches, even those that rely on domain knowledge during the training and inference phases.
__label__machine_vision However, it encounters two main challenges in multi-drone collaboration settings.
__label__graph_neural_networks As a result, we propose a simple mixture of experts model Link-MoE for link prediction.
__label__optimization The rising footprint of machine learning has led to a focus on imposing model sparsity as a means of reducing computational and memory costs.
__label__interpretability_and_explainability We propose end-to-end (e2e) sparse dictionary learning, a method for training SAEs that ensures the features learned are functionally important by minimizing the KL divergence between the output distributions of the original model and the model with SAE activations inserted.
__label__interpretability_and_explainability Despite diverse modeling techniques, existing models are black boxes and fail to provide insights and explanations about their representations.
__label__reinforcement_learning Code is available at https://github.com/jhoon-cho/MBTL/.
__label__other A distribution divergence minimization-based loss is proposed, under which a suite of sufficient conditions ensuring identifiability of the shared components are derived.
__label__optimization_for_deep_networks To validate (i) and (ii), we check various Transformers, CNNs, MLPs, and quadratic problems, and find that SGD can perform on par with Adam on problems without block heterogeneity, but performs worse than Adam when the heterogeneity exists.
__label__safety_in_machine_learning CT-SSF utilizes the inductive bias in deep representation learning to dynamically adjust weights, prioritizing semantic features relevant to the current prediction.
__label__generative_models In this paper, we introduce **Era3D**, a novel multiview diffusion method that generates high-resolution multiview images from a single-view image.
__label__probabilistic_methods However, there are many ways a user can transform a model that make inference more or less efficient.
__label__generative_models Diffusion Transformers (DiT) have attracted significant attention in research.
__label__machine_vision To investigate the proposed method, we establish a challenging benchmark called \textbf{C}ontinual \textbf{L}earning of \textbf{M}odality (MCL), which consists of high-quality QA data from five distinct modalities: image, video, \textcolor{black}{audio, depth} and point cloud.
__label__machine_vision We propose a pipeline consisting of three stages.
__label__optimization_for_deep_networks Neural network pruning is a key technique towards engineering large yet scalable, interpretable, and generalizable models.
__label__probabilistic_methods Our reported results on a variety of computer vision datasets confirm that the proposed method is most effective to combat extreme data heterogeneity in federated learning.
__label__bandits Our result also implies a high-probability BOBW regret guarantee when the bounded true losses are protected with pure Local Differential Privacy (LDP), while the existing work ensures the (weaker) \emph{approximate} LDP with the regret bounds in expectation only.
__label__probabilistic_methods This limitation leads to a diminished estimation of uncertainty and a subsequent decline in OOD detection performance.Additionally, EDL encounters the vanishing gradient problem within its fully-connected layers, further degrading classification accuracy.To address these issues, we introduce hyper-domain and propose Hyper-opinion Evidential Deep Learning (HEDL).
__label__safety_in_machine_learning Nevertheless, this guarantee is violated when the data is subjected to adversarial attacks, which often result in a significant loss of coverage.
__label__diffusion_based_models Although some pioneering methods have achieved automatic text-to-3D generation, they are generally limited to small-scale scenes with restricted control over the shape and texture.
__label__machine_vision Our proposed Dinomaly achieves impressive image AUROC of 99.6\%, 98.7\%, and 89.3\% on the three datasets respectively, which is not only superior to state-of-the-art multi-class UAD methods, but also surpasses the most advanced class-separated UAD records.
__label__natural_language_processing Central to AutoMix are two key technical contributions.
__label__optimization Numerical experiments across various tasks validate the effectiveness of our method.
__label__machine_learning_for_healthcare In this paper we seek to use trial data to draw valid inferences about the outcome of a policy on the target population.
__label__privacy Our results show that different ways of measuring memorization yield very similar aggregate results.
__label__machine_vision We explore DiffGS for various tasks, including unconditional generation, conditional generation from text, image, and partial 3DGS, as well as Point-to-Gaussian generation.
__label__interpretability_and_explainability We also uncover a surprising connection between group testing and the Möbius transform.
__label__natural_language_processing This paper investigates this challenge by focusing on comics with contradictory narratives, where each comic consists of two panels that create a humorous contradiction.
__label__natural_language_processing Our evaluation spans a wide variety of tasks, including multi-choice, open-generation, and adaptations to chain-of-thought reasoning tasks.
__label__machine_vision We conduct experiments on the large-scale RealEstate10K and ACID datasets to demonstrate the efficiency and generalization of our method.
__label__diffusion_based_models Molecular representation learning has shown great success in advancing AI-based drug discovery.
__label__machine_learning_for_other_sciences_and_fields Such cell-type-specific promoters are difficult to discover using existing methods, requiring either manual curation or access to large datasets of promoter-driven expression from both targeted and untargeted cells.
__label__reinforcement_learning The code to replicate these results can be found at \href{https://github.com/shshnkreddy/RLSF}{https://github.com/shshnkreddy/RLSF}
__label__machine_vision We explore whether language descriptions can be used to transform relative depth predictions to those in metric scale.
__label__reinforcement_learning Perceiving the pre-eminence of image reconstruction in representation learning, we propose SMG (\blue{S}eparated \blue{M}odels for \blue{G}eneralization), a novel approach that exploits image reconstruction for generalization.
__label__deep_learning_architectures In this paper, we explore a novel perspective of enlightening signal processing for deep time series forecasting.
__label__human-AI_interaction Finally, a cross-task synergy module is advised to learn to maximize the task-invariant fine-grained visual features, enhancing the synergy between different visual tasks.
__label__learning_theory In the case of self-attention with LayerNorm, we first show that for certain classes of value matrices, collapse to a rank one subspace still happens exponentially.
"__label__speech_and_audio The content of visual and audio scenes is multi-faceted such that a video stream can
be paired with various audio streams and vice-versa."
__label__machine_learning_for_physical_sciences We then construct the first Lorentz-equivariant generative model: a continuous normalizing flow based on an L-GATr network, trained with Riemannian flow matching.
__label__natural_language_processing On the other hand, there is abundant knowledge that may indirectly assist task completion, such as online tutorials that were created for human consumption.
__label__learning_theory While replicability typically incurs a $\rho^{-2}$ factor overhead in sample complexity, we obtain a replicable uniformity tester using only $\tilde{O}(\sqrt{n} \varepsilon^{-2} \rho^{-1})$ samples.
__label__learning_theory The quantum approximate optimization algorithm (QAOA) is a general-purpose algorithm for combinatorial optimization that has been a promising avenue for near-term quantum advantage.
__label__machine_vision However, most existing vision-language trackers still overly rely on initial fixed multimodal prompts, which struggle to provide effective guidance for dynamically changing targets.
__label__graph_neural_networks In this work, we focus on the link prediction task and systematically analyze the impact of heterophily in node features on GNN performance.
__label__reinforcement_learning Policy evaluation is complicated in the rare event setting by the long timescale of the event and by the need for \emph{relative accuracy} in estimates of very small values.
__label__algorithmic_game_theory Consequently, we focus on this setting and refine the analyses of the competitive ratios, with upper and lower bounds expressed as increasing functions of $\gamma$.
__label__machine_vision Video understanding has witnessed significant progress with recent video foundation models demonstrating strong performance owing to self-supervised pre-training objectives;  Masked Autoencoders (MAE) being the design of choice.
__label__probabilistic_methods EPH outperforms all other approaches quantitatively and provides meaningful hierarchies in qualitative evaluations.
__label__graph_neural_networks Our extensive experiments demonstrate that HEGNN not only aligns with our theoretical analyses on toy datasets consisting of symmetric structures, but also shows substantial improvements on more complicated datasets such as $N$-body and MD17.
__label__safety_in_machine_learning Conformal Prediction (CP) is a popular uncertainty quantification method that provides distribution-free, statistically valid prediction sets, assuming that training and test data are exchangeable.
__label__natural_language_processing We first establish principles for designing high-quality samples by integrating symbolic logic theory and previous empirical insights.
__label__machine_learning_for_other_sciences_and_fields The new engine leverages the ability of LLMs to conduct analysis of unstructured data, while also allowing us to exploit advances in sampling and optimization techniques to achieve efficient and accurate query execution.
__label__natural_language_processing To tackle $\textit{O}$1, we summarize existing approaches into four atomic growth operators and systematically evaluate them in a standardized LLM pre-training setting.
__label__reinforcement_learning Current reinforcement learning (RL) models are often claimed to explain animal behavior.
__label__optimization_for_deep_networks We propose dual risk minimization (DRM), which combines empirical risk minimization with worst-case risk minimization, to better preserve the core features of downstream tasks.
__label__machine_learning_for_other_sciences_and_fields The search algorithm is guided by a goal-conditioned cost network learned offline from a partially observed hypergraph of valid chemical reactions.
__label__deep_learning_architectures However, there has been relatively limited theoretical analysis on the representation of DEQ.
__label__diffusion_based_models We introduce T2V-Turbo, which integrates feedback from a mixture of differentiable reward models into the consistency distillation (CD) process of a pre-trained T2V model.
__label__evaluation This hypothesis suggests that deeper DNN layers compress representations and hinder OOD generalization.
__label__machine_learning_for_other_sciences_and_fields Therefore, we propose to predict label distribution from ternary labels, allowing experts to annotate labels in a three-way annotation scheme.
__label__machine_learning_for_physical_sciences The dominant paradigm in this field is to incorporate numerous physical domain constraints into the model, such as symmetry constraints like rotational equivariance.
"__label__generative_models Large Language Models (LLMs) have seen an impressive wave of advances, with
models now excelling in a variety of tasks, such as mathematical reasoning and
program synthesis."
__label__reinforcement_learning Transformer-based agents could emerge with self-improvement in online environments by providing task contexts, such as multiple trajectories, called in-context RL.
__label__optimization_for_deep_networks Class incremental learning (CIL) trains a network on sequential tasks with separated categories in each task but suffers from catastrophic forgetting, where models quickly lose previously learned knowledge when acquiring new tasks.
__label__machine_vision We introduce two datasets, OmniSim and InterReal, featuring 28 scenes with multiple interactive objects.
__label__diffusion_based_models To save the usage of GPU memory and accelerate the editing process, we further introduce the temporal-dimensional token merging strategy, which can effectively reduce the redundancy.
__label__generative_models In this paper, we propose a novel language-guided codebook learning framework, called LG-VQ, which aims to learn a codebook that can be aligned with the text to improve the performance of multi-modal downstream tasks.
__label__other In both cases, the goal is to find the smallest dimension $d$ of an $\ell_p$-space in which a given dataset can be represented.
__label__reinforcement_learning However, designing or evaluating such a cost function can be prohibitively expensive.
__label__safety_in_machine_learning In recent years, DeepFake technology has achieved unprecedented success in high-quality video synthesis, but these methods also pose potential and severe security threats to humanity.
__label__learning_theory Our analysis provides bounds on the size, depth, and computational efficiency of ARDTs, highlighting their surprising computational power.
__label__learning_theory We illustrate our techniques by giving algorithms for pricing problems, linkage-based clustering and dynamic-programming based sequence alignment.
__label__diffusion_based_models DistDiff consistently enhances accuracy across a diverse range of datasets compared to models trained solely on original data.
__label__natural_language_processing Hence, we propose a trajectory-based method TV score, which uses trajectory volatility for OOD detection in mathematical reasoning.
__label__other However, these methods fail to provide consistent evaluation across instruction sets that generate image descriptions of significantly different lengths.
__label__machine_learning_for_healthcare Automated seizure detection (ASD) using intracranial electroencephalography (iEEG) is critical for effective epilepsy treatment.
__label__other To address the lack of inter-client repulsion which is crucial for the alignment in the global embedding space, we develop a surrogate loss function that each client learns and shares with each other.
__label__neuroscience_and_cognitive_science In this paper, we investigate how stable neuronal embeddings are with respect to changes in model architecture and initialization.
__label__machine_vision As metallic and dielectric materials exhibit different BRDFs, SfPUEL additionally predicts dielectric and metallic material segmentation to further boost performance.
__label__learning_theory However, MORL still misses two crucial theoretical analyses of the properties of utility functions: (1) a characterisation of the utility functions for which an associated optimal policy exists, and (2) a characterisation of the types of preferences that can be expressed as utility functions.
__label__safety_in_machine_learning We validate our attack on GPT-3.5 and OpenAI's safety classifier; we can cause GPT-3.5 to emit harmful strings that current transfer attacks fail at, and we can evade the OpenAI and Llama Guard safety classifiers with nearly 100% probability.
__label__learning_theory Although optimization problems in SPGs are often NP-hard, a notable special case involving the least squares loss (SPG-LS) has gained significant research attention recently, (Bishop et al.
__label__active_learning While weakly-supervised methods have been explored to reduce annotation burden, the fusion of AL with weak supervision remains unexplored, despite its potential to significantly reduce annotation costs.
__label__machine_learning_for_social_sciences To tackle these challenges, this paper reformulates link prediction as a sequential decision-making process, where each link prediction interaction occurs sequentially.
__label__safety_in_machine_learning Experiments show that training on the high-quality data selected by our method can often outperform other data selection methods for collaborative fine-tuning of LLMs, across diverse private domain datasets, in medical, multilingual and financial settings.
__label__probabilistic_methods Leveraging Dirichlet-Multinomial-Beta data augmentation techniques, a fully-conjugate and efficient Gibbs sampler is developed to perform posterior simulation.
__label__privacy Instead, we propose methods to directly calibrate the noise scale to a desired attack risk level, bypassing the step of choosing ε.
__label__probabilistic_methods The size-and-shape of $\mu$ is a geometric property invariant to a family of space-time unitary transformations, viewed as rotations of the Hilbert space, that jointly transform the $x$ and $y$ axes.
__label__human-AI_interaction Extensive testing on the ScanRefer benchmark has shown that RG-SAN not only establishes new performance benchmarks, with an mIoU increase of 5.1 points, but also exhibits significant improvements in robustness when processing descriptions with spatial ambiguity.
__label__safety_in_machine_learning In order to achieve high-quality detection of black-box models, we would like to extract deep intrinsic characteristics of the black-box model generated texts.
__label__natural_language_processing In order to thrive in hostile and ever-changing natural environments, mammalian brains evolved to store large amounts of knowledge about the world and continually integrate new information while avoiding catastrophic forgetting.
__label__causal_inference To this end, this paper presents Causal representatiOn AssistanT (COAT) that introduces large language models (LLMs) to bridge the gap.
__label__privacy Our findings highlight a critical privacy concern within the machine learning community and call for a re-evaluation of safety protocols in the use of open-source pre-trained models.
__label__privacy In this work, we propose DAGER, the first algorithm to recover whole batches of input text exactly.
__label__generative_models Large language models (LLMs) often hallucinate and lack the ability to provide attribution for their generations.
__label__optimization Scalarization is a general, parallizable technique that can be deployed in any multiobjective setting to reduce multiple objectives into one, yet some have dismissed this versatile approach because linear scalarizations cannot explore concave regions of the Pareto frontier.
__label__machine_vision Through experiments, we demonstrate that our renovated names help train stronger open-vocabulary models with up to 15% relative improvement and significantly enhance training efficiency with improved data quality.
__label__optimization_for_deep_networks Spry makes feasible previously impossible FL deployments on commodity mobile and edge devices.
__label__fairness We then present the DRO dual formulation as an efficient tool to convert the main problem into a more tractable and computationally efficient form.
__label__neuroscience_and_cognitive_science We performed empirical benchmarks on the cross-site generalization of age-prediction models with resting-state EEG data from a large multi-national dataset (HarMNqEEG), which included $14$ recording sites and more than $1500$ human participants.
__label__reinforcement_learning In this paper, we establish an upper bound on the return gap between the oracle expert policy and an optimal decision tree policy.
__label__machine_learning_for_healthcare In the realm of big data and digital healthcare, Electronic Health Records (EHR) have become a rich source of information with the potential to improve patient care and medical research.
__label__deep_learning_architectures Codes are available at \url{https://github.com/brain-intelligence-lab/STMixer_demo}.
__label__other We consider Graphlet Sampling in the semi-streaming setting, where we have a memory of $M = \Omega(n \log n)$ words, and $G$ can be only read through sequential passes over the edge list.
__label__fairness Our findings represent a dual contribution: a significant technical advancement in understanding the relationship between AUROC and AUPRC and a stark warning about unchecked assumptions in the ML community.
__label__learning_theory We investigate two approaches for trading off consistency and dimensionality in multiclass classification while using a convex surrogate loss.
__label__diffusion_based_models Our experiments also demonstrate AsyncDiff can be readily applied to video diffusion models with encouraging performances.
__label__graph_neural_networks We demonstrate the superior performance of NLSFs over existing spectral GNNs in node and graph classification benchmarks.
__label__machine_vision Specifically, this research advances the patch division paradigm by introducing a novel extension: dynamic resolution with automatic patch configuration.
__label__reinforcement_learning We introduce Recurrent Trace Units (RTUs), a small modification on LRUs that we nonetheless find to have significant performance benefits over LRUs when trained with RTRL.
__label__reinforcement_learning Previous studies in data sampling are mainly based on heuristic rules or learning through a huge amount of time-consuming trials.
__label__optimization Inverse Optimization (IO) is a framework for learning the unknown objective function of an expert decision-maker from a past dataset.
__label__other Using these two as case studies, we demonstrate that our technique outperforms baselines in attack settings while maintaining comparable performance in benign settings.
__label__machine_learning_for_physical_sciences Molecular dynamics (MD) is a powerful technique for studying microscopic phenomena, but its computational cost has driven significant interest in the development of deep learning-based surrogate models.
__label__generative_models As a flexible representation, NeRF has been first adopted for 3D representation.
"__label__machine_learning_for_other_sciences_and_fields Then, the student representation is learned from the interaction graph by a devised meta multigraph learning module; multiple learnable propagation paths in this module  enable current student latent representation to access  lower-order exercise latent representations,
which can lead to  more effective nad robust student representations learned; 
the exercise and concept representations are learned on the relation and dependency graphs by graph attention modules."
__label__learning_theory Notably, we provide generalization bounds specifically tailored for mixtures of experts, leveraging the one-out-of-$n$ gating mechanism rather than the more common $n$-out-of-$n$ mechanism.
__label__evaluation However, evaluating these reasoning abilities has become increasingly challenging.
__label__natural_language_processing In addition, we also study hallucination detection in scenarios where ground-truth references are also available, such as in the setting of Retrieval-Augmented Generation (RAG).
__label__interpretability_and_explainability To address this gap, we propose a novel transparent neural network model for time series called Generalized Additive Time Series Model (GATSM).
__label__reinforcement_learning However, prior work in this area has relied on complex tri-level optimizations in order to infer safe behavior (constraints).
__label__generative_models We first design a unified diffusion model tailored for multi-view video generation by incorporating a learnable motion module into a frozen 3D-aware diffusion model to capture multi-view spatial-temporal correlations.
__label__natural_language_processing This design greatly enhances both training and inference efficiency through GLA's hardware-efficient training algorithm and reduced state size.
__label__learning_theory The risk is usually computed with respect to the empirical data distribution, but this may result in poor and unstable out-of-sample performance due to distributional uncertainty.
__label__probabilistic_methods To address this challenge, we propose a Bayesian Domain Adaptation with Gaussian Mixture Domain-Indexing(GMDI) algorithm.
__label__diffusion_based_models However, prior training-free approaches often rely on updating the noisy image during the reverse diffusion process via backpropagation from custom loss functions, which frequently struggle to provide precise control over individual bounding boxes.
"__label__learning_theory Despite empirical evidence showcasing its efficacy 
compared to feedforward neural networks, a theoretical understanding for its separation and bias is still limited."
__label__robotics Specifically, we propose a novel skill chaining framework called Skill Chaining via Dual Regularization (SCaR).
__label__diffusion_based_models Furthermore, to induce an expressive trajectory distribution for conditional generation, we introduce a generalized learnable geometric prior into the forward diffusion process to enhance temporal conditioning.
__label__optimization We provide the first tight characterization for the rate of the minimax simple regret by developing matching upper and lower bounds.
"__label__learning_theory Moreover, we identify combinatorial complexity
  measures that give rise to each case of our tetrachotomic characterization."
__label__natural_language_processing However, developing prompting techniques that enable LLM agents to effectively use these tools and knowledge remains a heuristic and labor-intensive task.
__label__reinforcement_learning We show that the default state features used in exiting benchmark tasks that are based on joint configurations are not amenable to Euclidean transformations.
__label__deep_learning_architectures With this perspective, we give several positive results.
__label__diffusion_based_models We further investigate and visualize the impact of Meta-Diffu$B$'s noise scheduling on the generation of sentences with varying difficulties.
__label__deep_learning_architectures We name our new family of NFNs the Monomial Matrix Group Equivariant Neural Functional Networks (Monomial-NFN).
__label__online_learning We clarify the unnecessary nature of collaboration in previous federated algorithms for distributed online multi-kernel learning, and improve the regret bounds at a smaller computational and communication cost.
__label__privacy This novel insight fuels the development of a new black box membership inference attack utilizing input loss curvature.
__label__optimization Much more recently, the study of algorithms with predictions was introduced: The algorithm is equipped with a (possibly erroneous) additional piece of information upfront which can be used to improve the algorithm's performance.
__label__interpretability_and_explainability Thanks to its efficiency, we scale Edge Pruning to CodeLlama-13B, a model over 100x the size of GPT-2.
__label__optimization_for_deep_networks Notably, the models trained by our method with the precision as low as 8 bits are  comparable  to those from the  full precision training.
__label__optimization This paper addresses the fully dynamic version of the problem, where the point set undergoes continuous updates (insertions and deletions) over time.
__label__optimization_for_deep_networks Our method also shows $\sim 2\times$ speedup than standard pre-training on a BERT-like code-generation LLM while achieving $4.23\times$  compression ratio in pre-training.
__label__robotics Large language models (LLMs) have shown great promise at generating robot programs from natural language given domain-specific robot application programming interfaces (APIs).
__label__optimization Traditionally, it focuses on the setting where you can arbitrarily query the search space.
__label__machine_vision To this end, we introduce a new anticipative masking strategy during training in which a late part of the video frames is masked as invisible, and learnable tokens replace these frames to learn to predict the invisible future.
__label__machine_learning_for_other_sciences_and_fields Extensive experiments over public large-scale benchmarks reveal that, compared with the state-of-the-art deep generative methods, NeuralSteiner achieves up to a 99.8\% reduction in overflow while speeding up the generation and maintaining a slight wirelength loss within only 1.8\%.
__label__machine_vision Existing works in a coarse-to-fine manner either suffer from severe noisy correspondences caused by unreliable coarse matching or struggle to form outlier-free coarse-level correspondence sets.
__label__interpretability_and_explainability Previous explanation approaches primarily focus on analyzing image-based models and are not readily applicable to LiDAR-based 3D detectors.
__label__optimization_for_deep_networks An evaluation-free (EF) version of MOTE-NAS delivers high efficiency in only 5 minutes, delivering a model more accurate than KNAS.
__label__probabilistic_methods Current approximate posteriors in Bayesian neural networks (BNNs) exhibit a crucial limitation: they fail to maintain invariance under reparameterization, i.e.
__label__machine_vision The biggest bottleneck is the scarcity of annotated 3D data, whereas 2D image datasets are abundant and richly annotated.
__label__generative_models Despite being a fundamental building block, conditional paths have been designed principally under the assumption of $\textit{Euclidean geometry}$, resulting in straight interpolations.
__label__natural_language_processing We evaluate multiple LLMs on the created causal queries and find that causal reasoning is challenging even for activities trivial to humans.
__label__machine_vision This integration is designed to effectively learn the variance/covariance for preventing entire collapse and ensuring invariance in the mean of augmented views, thereby overcoming the identified limitations.
__label__natural_language_processing Notably, we achieve substantial improvements across five datasets in mathematics and commonsense reasoning.
__label__machine_vision Additionally, we showcase its flexibility on CrowdPose, a popular occlusion benchmark with keypoints within the bounding box.
__label__algorithmic_game_theory We consider two versions: sequential, where Bob observes Alice's cut point  before choosing left/right, and simultaneous, where he only observes her cut point after making his choice.
__label__probabilistic_methods We consider a setup where experts provide advice on the next query point through binary accept/reject recommendations (labels).
__label__natural_language_processing Lastly, we show that uncertainty estimates inform human use of LLMs in human-AI collaborative settings through a user study.
__label__learning_theory Given this limitation, there has been a surge of interest in equivariant self-supervised learning (E-SSL) that learns features to be augmentation-aware.
__label__machine_learning_for_physical_sciences Our approach significantly reduces the search space over trajectories and avoids expensive trajectory simulation and inefficient importance sampling estimators which are required in existing methods.
__label__interpretability_and_explainability Building on and sharpening such analysis, we 1) provide a sharper understanding of the asymptotic behavior of a wide class of EDL methods by unifying various objective functions; 2) reveal that the EDL methods can be better interpreted as an out-of-distribution detection algorithm based on energy-based-models; and  3) conduct extensive ablation studies to better assess their empirical effectiveness with real-world datasets.
__label__evaluation By combining a streamlined VLM focused on perception with a powerful LLM tailored for reasoning, Prism achieves superior results in general vision-language tasks while substantially cutting down on training and operational expenses.
__label__machine_vision Our method is of high pre-training efficiency compared to other alternatives and achieves great improvement over Point-MAE, particularly surpassing it by \textbf{5.50\% on OBJ-BG, 6.03\% on OBJ-ONLY, and 5.17\% on PB-T50-RS} for 3D object classification on the ScanObjectNN dataset.
__label__machine_learning_for_physical_sciences Instead of solely predicting the future based on Eulerian observations, we propose DeepLag to discover hidden Lagrangian dynamics within the fluid by tracking the movements of adaptively sampled key particles.
__label__machine_vision In this work, we ask: How is the performance of multimodal models on downstream concepts influenced by the frequency of these concepts in their pretraining datasets?
__label__optimization_for_deep_networks Although LoRA reduces the computational and memory requirements significantly at each iteration, extensive empirical evidence indicates that it converges at a considerably slower rate compared to full fine-tuning, ultimately leading to increased overall compute and often worse test performance.
__label__machine_learning_for_physical_sciences As existing adaptive conditioning methods do not scale well with respect to the number of parameters to adapt in the neural solver, we propose GEPS, a simple adaptation mechanism to boost GEneralization in Pde Solvers via a first-order optimization and low-rank rapid adaptation of a small set of context parameters.
__label__reinforcement_learning We consider offline imitation learning (IL), which aims to train an agent to imitate from the dataset of expert demonstrations without online interaction with the environment.
__label__safety_in_machine_learning We refer to the weight attribution-guided LLM unlearning method as WAGLE, which unveils the interconnections between 'influence' of weights and 'influence' of data to forget and retain in LLM generation.
__label__diffusion_based_models Through evaluations on real-world datasets, we demonstrate that DiffuPac achieves strong evasion capabilities against sophisticated NIDS, outperforming conventional methods by an average of 6.69 percentage points, while preserving the functionality and practicality of the generated adversarial packets.
__label__optimization The paper extends these results to multiple feedback settings, facilitating conversions between semi-bandit/first-order feedback and bandit/zeroth-order feedback, as well as between first/zeroth-order feedback and semi-bandit/bandit feedback.
__label__machine_learning_for_physical_sciences Here we introduce Stormer, a simple transformer model that achieves state-of-the art performance on weather forecasting with minimal changes to the standard transformer backbone.
__label__evaluation Large language models (LLMs) and vision-language models (VLMs) have demonstrated remarkable performance across a wide range of tasks and domains.
__label__bandits In this work, we obtain the first sparse regret bounds that hold when $S$ is unknown and the action sets are adversarially generated.
__label__interpretability_and_explainability We also provide a similar framework for discovering discrete symmetry.
__label__deep_learning_architectures However, due to the partially-observed nature of real-world applications, solely focusing on the target of interest, so-called endogenous variables, is usually insufficient to guarantee accurate forecasting.
__label__machine_learning_for_healthcare Magnetic resonance imaging (MRI) is a widely used non-invasive imaging modality.
__label__machine_vision Our code is publicly available at https://github.com/Necolizer/CHASE .
__label__probabilistic_methods The comparison with state-of-the-art neural estimators, through extensive experimentation within established reference scenarios, shows that our approach offers higher accuracy and lower complexity.
__label__machine_learning_for_other_sciences_and_fields Specifically, it treats each client as a task and splits the local model into a feature extractor and a prediction head.
__label__diffusion_based_models Text-to-Image (T2I) has witnessed significant advancements, demonstrating superior performance for various generative tasks.
__label__reinforcement_learning Instead of enforcing optimal policy invariance, we train a policy that combines task and heuristic rewards and ensures it outperforms the heuristic-trained policy.
__label__generative_models Code and more examples open-sourced at https://github.com/RAIVNLab/SuperposedDecoding.
__label__natural_language_processing The choice of the input domain, covering both the generation and the representation of prompts, affects the identification of well-performing local optima (**Insight II**).
__label__graph_neural_networks Based on the theoretical analysis of CSBM-3H, we derive a new composite metric, named Tri-Hom, that considers all three aspects and overcomes the limitations of conventional homophily metrics.
__label__diffusion_based_models Moreover, we theoretically deduce the upper bound on the error of the reward model, which illustrates the potential confidence of reward estimation throughout the reinforcement alignment process, thereby facilitating accurate regularization.
__label__learning_theory We revisit the apparent gap between offline and online IL from a learning-theoretic perspective, with a focus on general policy classes up to and including deep neural networks.
__label__learning_theory Our theory is consistent with the empirical neural scaling laws and verified by numerical simulation.
__label__optimization We consider smooth nonconvex finite-sum (empirical risk minimization) problems in this setup and introduce a new parallel method, Freya PAGE, designed to handle arbitrarily heterogeneous and asynchronous computations.
__label__natural_language_processing To address this challenge, we propose $\textit{Trans-LoRA}$ --- a novel method for lossless, nearly data-free transfer of LoRAs across base models.
__label__machine_vision Based on the statistical analysis, we reveal that queries and keys are mapped in completely different spaces while only a few keys are blended into the query region.
__label__machine_vision Inverse graphic schemes recover an explicit representation of geometry and a set of chosen intrinsics, then relight with some form of renderer.
__label__optimization Federated reinforcement learning (RL) enables collaborative decision making of multiple distributed agents without sharing local data trajectories.
__label__graph_neural_networks Additionally, our analysis of generated graphs allows us to better understand the properties of graph distances: depending on which diversity measure is used for optimization, the obtained graphs may possess very different structural properties which gives a better understanding of the graph distance underlying the diversity measure.
__label__optimization Leveraging this framework, new algorithms are derived using existing results as base algorithms for convex optimization, improving upon state-of-the-art results in various cases.
__label__machine_vision However, many SSL methods typically struggle in real-world scenarios, particularly when there is a large number of irrelevant instances in the unlabeled data that do not belong to any class in the labeled data.
__label__causal_inference Although several approaches have been developed for learning conditional independence structures for observed variables, those existing methods generally fail to work when the variables of interest can not be directly observed and only discretized values of those variables are available.
__label__evaluation This includes problems as diverse as simple logical reasoning, geographic navigation, game-playing, and chemistry.
__label__speech_and_audio However, the integration of preference optimization to align speech outputs to human preferences is often neglected.
__label__natural_language_processing Automatic prompt optimization (APO) methods are designed to automate this and can be broadly categorized into those targeting instructions (instruction optimization, IO) vs. those targeting exemplars (exemplar optimization, EO).
__label__machine_learning_for_physical_sciences The aim of this work is to learn models of population dynamics of physical systems that feature stochastic and mean-field effects and that depend on physics parameters.
__label__machine_vision In addition, HisTPT features an adaptive knowledge retrieval mechanism that regularizes the prediction of each test sample by adaptively retrieving the memorized knowledge.
__label__generative_models Despite the widespread use of statistical prior models in various fields, such models for neural network gradients have long been overlooked.
__label__reinforcement_learning Our experimental results demonstrate that the proposed method successfully generates an instructive curriculum of environments, outperforming UED baselines in zero-shot generalization across novel, out-of-distribution environments.
__label__other To address this issue, Ex-MCR proposes to extend one modality's space into the other's, rather than mapping both modalities onto a completely new space.
__label__optimization_for_deep_networks We show that a key factor in this performance gap is the heavy-tailed class imbalance found in language tasks.
__label__natural_language_processing Citation networks are critical infrastructures of modern science, serving as intricate webs of past literature and enabling researchers to navigate the knowledge production system.
__label__diffusion_based_models The project page is https://mvig-rhos.com/pa_diffusion.
__label__machine_vision We reveal that the inclusion of the hard prototype in contrastive loss helps to emphasize divergence.
__label__machine_learning_for_social_sciences This step facilitates multi-perspective representation learning and improves model learning capability.
__label__generative_models As a result, the generated scenes are often object-centric and lack photorealism.
__label__optimization_for_deep_networks Experiments demonstrate the effectiveness of GRE on various model architectures and graph datasets in terms of multiple editing situations.
__label__reinforcement_learning Policy gradient methods are notorious for having a large variance and high sample complexity.
__label__evaluation Existing evaluation protocols primarily focus on temporal consistency and content continuity, yet largely ignore dynamics of video content.
__label__privacy We also complement our results with a lower bound for DP-OPE, showing that these rates are optimal for a natural family of low-switching private algorithms.
__label__optimization_for_deep_networks In this paper, we propose a new exemplar-free GCIL technique named generalized analytic continual learning (GACL).
"__label__deep_learning_architectures We consider three structured linear
parameterizations of the FFN using efficient low-rank and block-diagonal matrices."
__label__machine_vision The challenges of this task arise from the dual need to generalize across diverse domains and accurately quantify category novelty, which is critical for applications in dynamic environments.
__label__graph_neural_networks Sparse attention variants such as Exphormer can help, but may require high-degree augmentations to the input graph for good performance, and do not attempt to sparsify an already-dense input graph.
__label__machine_vision However, existing efforts have several major limitations, with the most critical being their reliance on accurate supervision.
__label__optimization For code implementation, see https://github.com/ninja-wm/POM/.
__label__natural_language_processing SafeWorld encompasses 2,775 test user queries, each grounded in high-quality, human-verified cultural norms and legal policies from 50 countries and 493 regions/races.
__label__natural_language_processing GPT-4.
__label__learning_theory We design algorithms for this task and prove that they achieve optimal query complexity.
__label__interpretability_and_explainability Surprisingly, these points precisely correspond to the emergence of hidden capabilities, i.e., where latent interventions show the model possesses the capability to manipulate a concept, but these capabilities cannot yet be elicited via naive input prompting.
__label__interpretability_and_explainability Using eigendecomposition of the pullback of the distance metric defined on the output space through the Jacobian of the model, we are able to reconstruct equivalence classes in the input space and navigate across them.
__label__natural_language_processing To mitigate these biases, we introduce UniBias, an inference-only method that effectively identifies and eliminates biased FFN vectors and attention heads.
__label__natural_language_processing Existing works primarily focus on fully depicting entity features by designing various modality encoders or fusion approaches.
__label__optimization_for_deep_networks However, the performance of quantized training with fixed-point forward gradients remains unclear.
__label__natural_language_processing To validate that these skill labels are meaningful and relevant to the LLM's reasoning processes we perform the following experiments.
__label__safety_in_machine_learning AI model alignment is crucial due to inadvertent biases in training data and the underspecified machine learning pipeline, where models with excellent test metrics may not meet end-user requirements.
__label__machine_learning_for_physical_sciences Our approach is trained end-to-end to predict spatio-temporally extremes and spatio-temporally drivers in the physical input variables jointly.
__label__robotics Further results and videos are available at https://quest-model.github.io.
__label__graph_neural_networks Equipped with in-context learning, ARC can directly extract dataset-specific patterns from the target dataset using few-shot normal samples at the inference stage, without the need for retraining or fine-tuning on the target dataset.
__label__learning_theory For the harder case of agnostic noise, we show that it is impossible to beat $O(1/\epsilon)$ query complexity even for the much simpler problem of learning singleton functions (and thus for learning halfspaces) using a reduction from agnostic distributed learning.
__label__optimization_for_deep_networks AutoDiff frameworks, like PyTorch, enable efficient end-to-end optimization of differentiable systems.
__label__robotics The source code is available at our project page https://3d-aigc.github.io/OpenGaussian.
"__label__learning_theory More specifically, we derive a generalization bound that combines
a covering number argument for compositionality, and the $F_{1}$-norm
(or the related Barron norm) for large width adaptivity."
__label__machine_vision This phenomenon is prevalent across current benchmarks.
__label__machine_vision Next, we propose a persistence homology distillation loss in SSCL and design an acceleration algorithm to reduce the computational cost of persistence homology in our module.
__label__diffusion_based_models For this purpose, we propose a generative training framework, dubbed MimicBrush, which randomly selects two frames from a video clip, masks some regions of one frame, and learns to recover the masked regions using the information from the other frame.
__label__algorithmic_game_theory This phenomenon subtly shapes the volume and diversity of the content pool, which is crucial for the platform's sustainability.
"__label__natural_language_processing AMOR builds reasoning logic over a finite state machine (FSM)
that solves problems through autonomous executions and transitions over disentangled modules."
__label__infrastructure We demonstrate that our method, FlashAttention-3, achieves speedup on H100 GPUs by 1.5-2.0$\times$ with BF16 reaching up to 840 TFLOPs/s (85\% utilization), and with FP8 reaching 1.3 PFLOPs/s.
__label__machine_vision To our knowledge, this is the first study to utilize frequency domain features to enhance the performance and efficiency of CL training on edge devices.
__label__optimization_for_deep_networks Numerical experiments on the benchmark datasets demonstrate the efficiency of our proposed method.
__label__neuroscience_and_cognitive_science Two seemingly conflicting phenomena of specificity and transfer have been widely observed in perceptual learning.
__label__natural_language_processing In this paper, we address the problem of sampling a set of high-quality and diverse translations.
__label__generative_models Inspired by the neural mechanisms that may contribute to the brain’s associative power, specifically the cortical modularization and hippocampal pattern completion, here we propose a self-supervised controllable generation (SCG) framework.
__label__interpretability_and_explainability Moreover, we find that more faithful mechanistic understanding leads to tighter performance bounds.
__label__deep_learning_architectures We conduct a series of experiment to verify the effectiveness and generalizability of CompressTracker.
__label__reinforcement_learning However, this category of methods easily leads to inefficient exploration due to limited trajectory visitations.
__label__generative_models We also demonstrate the practical applicability of our model with 3D generation tasks, showcasing its versatility and potential for broader adoption in real-world applications.
__label__generative_models By leveraging Tweedie's formula, we show that we can perform gradient steps to sequentially optimize these objectives.
__label__optimization_for_deep_networks To address such issues, we propose PACE, marrying generalization of PArameter-efficient fine-tuning with Consistency rEgularization.
__label__probabilistic_methods Experiments on multiple datasets and neural networks of varying complexity showed that the two BMRS methods offer a competitive performance-efficiency trade-off compared to other pruning methods.
__label__neuroscience_and_cognitive_science We demonstrate that the dual-learning model can account for both the specificity and transfer phenomena observed in classical psychophysical experiments.
__label__algorithmic_game_theory Since we often cannot observe the agent's beliefs directly, we take an approach inspired by *information design*.
__label__natural_language_processing Moreover, we further consider a situation where the gradient is not visible to be compatible with any LLM.
__label__learning_theory We obtain upper bounds on the sample complexity in terms of the VC dimension of the class composing predictors with transformations, which we show in many cases is not much larger than the VC dimension of the class of predictors.
__label__machine_vision Additionally, we employ regularization and initialization techniques to increase the likelihood that new information is learned from each channel.
__label__learning_theory This paper puts forward the notion of 'globality degree' of a target distribution to capture when weak learning is efficiently achievable by regular Transformers.
__label__machine_vision Our first insight is to exploit per-scene optimized Neural Radiance Fields (NeRFs) by generating dense depth and virtual camera targets from them, which helps our model to learn enhanced 3D geometry from sparse non-overlapping image inputs.
"__label__fairness We tackle
this problem by designing a new fairness metric, mutual fairness, that captures
variability in outreach through optimal transport theory."
__label__evaluation We leverage a unique dataset that captures the detailed performance of over 5M students across 8 college-entrance exams given over a span of two years in Brazil.
__label__speech_and_audio This paper proposes a cross-modal in-context learning approach, empowering the frozen LLMs to achieve multiple audio tasks in a few-shot style without any parameter update.
__label__safety_in_machine_learning Our study reveals that simply applying randomized smoothing to certify DEQs provides certified robustness generalized to large-scale datasets but incurs extremely expensive computation costs.
__label__learning_theory Furthermore, this loss function fails to account for label correlations.
__label__natural_language_processing The capability of *in-context learning* (ICL) allows us to adapt an LLM to downstream tasks by including input-label exemplars in the prompt without model fine-tuning.
__label__bandits Our theoretical results in both settings are also corroborated by a set of systematic simulations.
__label__optimization We then investigate this notion in more details, and show that it naturally leads to strong convergence guarantees for stochastic mirror descent.
__label__machine_vision Finally, UPS also demonstrates promising results across various image restoration tasks, including real-world and classic SISR, image denoising, and image deblocking.
__label__machine_vision Our method decomposes the scene into an explicit surface represented as 3D Gaussians, with a spatially varying BRDF, and an implicit volumetric representation of the scattering component.
__label__machine_vision AWT comprises three key components: augmenting inputs with diverse visual perspectives and enriched class descriptions through image transformations and language models; dynamically weighting inputs based on the prediction entropy; and employing optimal transport to mine semantic correlations in the vision-language space.
__label__causal_inference Our results are corroborated with experiments.
__label__learning_theory Finally, our simulation results verify the theoretical results.
__label__machine_learning_for_other_sciences_and_fields Additionally, we enhance the MP-Adapters with contextual perceptiveness.
__label__probabilistic_methods Through the GP prior, one can express structured and interpretable inductive biases, such as regularity or periodicity, directly in function space, while still exploiting the implicit inductive biases that allow deep networks to generalize.
__label__safety_in_machine_learning Through formal proofs and extensive empirical evaluations, we demonstrate that pixel-level invisible watermarks are vulnerable to this regeneration attack.
__label__generative_models We then learn the canonical 3D representation of the video using a freeze-time video, delicately generated from the reference video.
__label__generative_models This debiasing strategy contributes to advancing the reliability and applicability of synthetic data in statistical inference.
__label__privacy Extensive experiments demonstrated that the proposed GS-Hider can effectively conceal multimodal messages without compromising rendering quality and possesses exceptional security, robustness, capacity, and flexibility.
__label__learning_theory We also provide an $\Omega(nk)$ lower bound, implying our result is tight up to an $\tilde{\mathrm{O}}(k)$ factor.
__label__machine_learning_for_other_sciences_and_fields The proposed model learns to generate well-designed pseudo-abnormal samples to mitigate the imputation bias and ensure the discrimination ability of both the imputation and detection processes.
__label__reinforcement_learning We present a theoretical result demonstrating the strong dependency of suboptimality on the number of Monte Carlo samples taken per Bellman target calculation.
__label__bandits We examine multi-armed bandit problems featuring strategic arms under debt-free reporting.
__label__bandits We consider contextual bandits with graph feedback, a class of interactive learning problems with richer structures than vanilla contextual bandits, where taking an action reveals the rewards for all neighboring actions in the feedback graph under all contexts.
__label__graph_neural_networks Firstly, GAs designed for specific scenarios may compromise the universality of models if mishandled.
__label__reinforcement_learning In this paper, we address this issue by developing novel methods for Reinforcement Learning (RL) from preference feedback between two full multi-turn conversations.
__label__diffusion_based_models In particular, we find that encoder features change minimally, whereas the decoder features exhibit substantial variations across different time-steps.
__label__machine_learning_for_healthcare Our approach assumes experimentally perturbed samples by treatments, and uses this to estimate a propensity score from each modality.
__label__diffusion_based_models Custom diffusion models (CDMs) have attracted widespread attention due to their astonishing generative ability for personalized concepts.
__label__machine_vision In open-world scenarios, where both novel classes and domains may exist, an ideal segmentation model should detect anomaly classes for safety and generalize to new domains.
__label__machine_vision Pre-trained models produce strong generic representations that can be adapted via fine-tuning on specialised datasets.
__label__diffusion_based_models This discovery leads us to investigate the linear counterparts of the nonlinear diffusion models, which are a series of linear models trained to match the function mappings of the nonlinear diffusion denoisers.
__label__learning_theory Unlike previous MI-based bounds, our proof strategy does not rely on upper bounding the cumulant-generating function (CGF) in the variational formula of MI.
"__label__learning_theory This allows us to prove an almost complete phase
diagram of training behavior as a function of the variance at initialization
and the width, for a MSE training task."
__label__machine_learning_for_healthcare Specifically, Flex-MoE first trains experts using samples with all modalities to inject generalized knowledge through the generalized router ($\mathcal{G}$-Router).
__label__deep_learning_architectures FeT innovatively encodes these identifiers into data representations and employs a transformer architecture distributed across different parties, incorporating three new techniques to enhance performance.
__label__machine_vision In addition, the fixed sinusoidal positional encoding is adopted instead of undertrained learnable one to reflect appropriate positional clues into the inputs of self-attention.
__label__bandits Specifically, our focus is on a soft tree model, a variant of the conventional decision tree that has undergone both practical and theoretical scrutiny in recent years.
__label__natural_language_processing In this work, we identify that naive epistemic uncertainty estimation leads to the acquisition of redundant samples.
__label__machine_learning_for_physical_sciences On the other hand, recent deep learning and reinforcement learning-based approaches often struggle to optimize long-term control sequences under the constraints of system dynamics.
__label__diffusion_based_models More importantly, the models fine-tuned with our method can be merged without interference, allowing us to generate custom subjects in a custom style by composing separately customized subject and style models.
__label__machine_vision A logical regularization termed L-Reg is derived which bridges a logical analysis framework to image classification.
__label__reinforcement_learning Offline reinforcement learning (RL) aims to train agents from pre-collected datasets.
__label__machine_vision To address these key challenges of the MVSS task, this paper presents two major contributions: the introduction of MVUAV, a new MVSS benchmark dataset, and the development of a dedicated semi-supervised MVSS baseline - SemiMV.
__label__deep_learning_architectures Unifying predictive and generative time series tasks within a single model remains challenging.
__label__machine_vision Our results prove the power of this methodology, uncovering semantic regions without prior definitions and scaling effectively across various datasets.
__label__probabilistic_methods This makes CDE particularly useful in critical application domains.
__label__natural_language_processing Furthermore, LACIE generalizes to another dataset, resulting in a large increase in truthfulness on TruthfulQA when trained on TriviaQA.
__label__natural_language_processing Human possess a remarkable ability to create mental images of unseen objects and actions through a process known as the Mind's Eye, enabling the imagination of the unseen world.
__label__machine_vision Gaussian Splatting became widely adopted due to its efficiency and ability to render novel views accurately.
__label__safety_in_machine_learning These mechanisms employ an input-dependent strategy, in which uninformative tokens are discarded from the computation pipeline, improving the model’s efficiency.
__label__online_learning In this paper, we make a number of contributions, ultimately establishing that the weight-one dilated entropy (DilEnt) distance-generating function is optimal up to logarithmic factors.
__label__machine_vision Given a video, Artemis receives a natural-language question with a bounding box in any video frame and describes the referred target in the entire video.
__label__safety_in_machine_learning In contrast, transfer AT-trained LM-VP achieves a good trade-off between transferred adversarial robustness and privacy, a finding that has been consistently validated across various pre-trained models.
__label__reinforcement_learning Recent work has attempted to alleviate the overestimation bias by encouraging conservative behaviors.
__label__deep_learning_architectures Our thorough empirical analysis demonstrates the effectiveness and reliability of the proposed approach across different modalities, model architectures and label shift intensities.
__label__graph_neural_networks Indeed, we prove that our approach generalizes classical feed-forward layers such as fully connected and convolutional layers by choosing appropriate rules.
__label__other Through a trajectory-based optimization analysis and generalization characterization on downstream tasks, we identify the critical factor, which is the signal-to-noise ratio (SNR), that impacts the generalizability in downstream tasks of both multi-modal and single-modal contrastive learning.
__label__natural_language_processing Moreover, we observe a synergy between EO and IO, with optimal combinations surpassing the individual contributions.
__label__machine_vision The proposed framework establishes a noisy label detector by learning positive and negative textual prompts for each class.
"__label__learning_theory Subsequently, we design an efficient
replicable learner for PAC learning parities when the marginal distribution is far from uniform, making progress on a
question posed by Impagliazzo et al."
__label__diffusion_based_models The proposed TEdit is trained using a novel bootstrap learning algorithm that effectively enhances the coverage of the original data.
__label__generative_models Extensive experiments show the effectiveness of FineStyle at following fine-grained text prompts and delivering visual quality faithful to the specified style, measured by CLIP scores and human raters.
__label__diffusion_based_models Our results indicate that the weight space of fine-tuned diffusion models can behave as an interpretable $\textit{meta}$-latent space producing new models.
__label__probabilistic_methods We propose adopting the approach of hyperspherical energy (HE) on top of CKA kernels to address this drawback and improve training stability.
__label__diffusion_based_models Our proposed method demonstrates significant performance in conditional 3D molecular generation and offers a promising approach towards inverse molecular design, potentially facilitating advancements in drug discovery, materials science, and other related fields.
__label__machine_learning_for_healthcare This study aims to bridge the gap by addressing issues regarding textual information loss in surgical lecture videos and the spatial-temporal challenges of surgical VLP.
"__label__learning_theory These ""snapshots"", or pieces of data captured from a data stream at adaptively chosen times, provide a glimpse of different animal movements unfolding through time."
__label__machine_vision Code can be found in project page https://silongyong.github.io/GL-NeRF_project_page/.
__label__reinforcement_learning The best STAR estimator outperforms baselines in all twelve cases studied, and even the median STAR estimator surpasses the baselines in seven out of the twelve cases.
__label__probabilistic_methods Our experiments demonstrate that, in comparison to existing interpretable CDE methods, CDTrees are both more accurate (as measured by the log-loss) and more robust against irrelevant features.
__label__machine_vision This is the first attempt to deeply and explicitly embed information fusion within the diffusion process, effectively addressing compound degradation in image fusion.
__label__machine_vision In this paper, a novel multimodal Probabilistic Conformal Distillation (PCD) method is proposed, which considers the inherent indeterminacy in this alignment.
__label__optimization_for_deep_networks The retraction operator in the former corresponds to orthonormalization of matrices and can be computationally costly for large-scale matrices.
__label__other Recent advancements in DETR-based visual grounding methods have attracted considerable attention, as they directly predict the coordinates of the target object without relying on additional efforts, such as pre-generated proposal candidates or pre-defined anchor boxes.
__label__machine_learning_for_healthcare In this work, we introduce a novel method that integrates data-driven and physics-based cost functions, akin to Physics-Informed Neural Networks (PINNs).
__label__reinforcement_learning Despite advances in robust offline reinforcement learning (RL), existing methods struggle to learn robust agents under high uncertainty caused by the diverse corrupted data (i.e., corrupted states, actions, rewards, and dynamics), leading to performance degradation in clean environments.
__label__fairness CultureLLM employs the World Value Survey (WVS) as seed data and generates semantically equivalent training data through the proposed semantic data augmentation.
__label__causal_inference As many scientific questions are inherently causal, this paper looks at the causal inference task of treatment effect estimation, where the outcome of interest is recorded in high-dimensional observations in a Randomized Controlled Trial (RCT).
__label__machine_vision The fusion block integrates temporal information from adjacent frames at the feature level, trained end-to-end, eliminating the need for pretrained optical flow, distinguishing our method from existing approaches.
__label__machine_vision We comprehensively investigate this question across 34 models and 5 standard pretraining datasets (CC-3M, CC-12M, YFCC-15M, LAION-400M, LAION-Aesthetics), generating over 300GB of data artifacts.
__label__speech_and_audio In this paper, we show that it is possible to synthesize spectrograms that simultaneously look like natural images and sound like natural audio.
__label__machine_learning_for_physical_sciences We introduce Poseidon, a foundation model for learning the solution operators of PDEs.
__label__machine_learning_for_physical_sciences We evaluate our approach on three newly created synthetic benchmarks, where two of them are based on remote sensing or reanalysis climate data, and on two real-world reanalysis datasets.
__label__other Federated learning is a distributed machine learning paradigm designed to protect user data privacy, which has been successfully implemented across various scenarios.
__label__generative_models We optimize a learnable latent variable based on an energy function, enhancing the strength of referring regions in the attention map.
__label__online_learning However, it remains unclear how task similarity in input features and readout patterns influences knowledge transfer and forgetting, as well as how they interact with common algorithms for continual learning.
__label__machine_learning_for_healthcare *Accurate* and *interpretable* modeling of these systems is essential for understanding complex temporal processes, optimizing interventions, and minimizing adverse effects.
__label__machine_learning_for_other_sciences_and_fields Specifically, CMCD integrates the monotonicity assumption, a fundamental educational principle in CD, to establish two constraints for data augmentation.
__label__deep_learning_architectures Next we show that bilipschitz neural operators may always be written via the repeated alternating composition of strongly monotone neural operators and invertible linear maps.
__label__machine_vision Moreover, a Snapshot Spectral Heterogeneous Dataset has been built upon multiple practical SCI systems.
__label__natural_language_processing Finally, we provide a mechanistic explanation for when SFT may outperform the full two-step LAIF pipeline as well as suggestions for making LAIF maximally useful in practice.
__label__optimization_for_deep_networks Quantization via decoupleQ is linear and uniform, making it hardware-friendlier than non-uniform counterpart, and enabling the idea to be migrated to high-bit quantization to enhance its robustness.
__label__deep_learning_architectures Multi-label image recognition aims to predict all objects present in an input image.
__label__machine_vision We deploy this model patch-wise at multiple scales, with guidance from inter-patch shape consistency constraints.
__label__neuroscience_and_cognitive_science However, task-optimized RNNs typically have a fixed weight matrix representing the synaptic connectivity between neurons.
__label__machine_learning_for_social_sciences To this end, we propose a novel intent learning method termed \underline{ELCRec}, by unifying behavior representation learning into an \underline{E}nd-to-end \underline{L}earnable \underline{C}lustering framework, for effective and efficient \underline{Rec}ommendation.
__label__graph_neural_networks Graph transformers need strong inductive biases to derive meaningful attention scores.
__label__learning_theory Finally, we compare the dot-product attention layer to a linear positional baseline, and show that it outperforms the latter using the semantic mechanism provided it has access to sufficient data.
__label__neuroscience_and_cognitive_science To understand how encoding spatial information of multiple agents shapes neural representations, we train a recurrent neural network (RNN) model that captures properties of MEC to path integrate trajectories of two agents simultaneously navigating the same environment.
__label__optimization Notably, unlike previous analyses of Adam, our analysis crucially relies on its core elements---momentum and discounting factors---as well as model EMA, motivating their wide applications in practice.
__label__graph_neural_networks Surprisingly, our unsupervised method even beats the sophisticated supervised approaches.
__label__learning_theory We consider the problem of estimating a structured multivariate density, subject to Markov conditions implied by an undirected graph.
__label__generative_models We will open-source the code for future research and applications.
__label__robotics Videos of PAD can be found at https://sites.google.com/view/pad-paper
__label__reinforcement_learning We evaluate our proposals across various datasets, domains and tasks, and show that conservative zero-shot RL algorithms outperform their non-conservative counterparts on low quality datasets, and perform no worse on high quality datasets.
__label__privacy We propose a new transformation that transforms lazy online learning algorithms into private algorithms.
__label__other We achieve robust performance on synthetic hierarchies and a larger real-world taxonomy, observing improved convergence rates in a resource-constrained setting while reducing the set of training examples by as much as 99%.
__label__machine_vision Our approach applies various transformations to extract semantic, structural, boundary, color, and frequency information from datasets and assess how much each type of information contributes to their bias.
__label__generative_models We first describe this process analytically in several controlled setups that allow us to fully monitor the training dynamics until convergence.
__label__probabilistic_methods Moreover, the hereby proposed model only needs $O(p \log\epsilon^{-1})$ time and $O(p)$ memory for converging to an $\epsilon$-accurate solution.
__label__optimization_for_deep_networks However, distributed concept drift with data heterogeneity, where clients may additionally experience different concept drifts, is a largely unexplored area.
__label__optimization_for_deep_networks Minimizers can have arbitrarily large sharpness, but not an arbitrarily small one.
__label__machine_learning_for_healthcare Generative models hold great promise for small molecule discovery, significantly increasing the size of search space compared to traditional in silico screening libraries.
__label__neuroscience_and_cognitive_science One of the major challenges is that the brain's extensive recurrent connectivity requires the propagation of error through both space and time, a problem that is notoriously difficult to solve in vanilla recurrent neural networks.
__label__deep_learning_architectures On public benchmark, our approach achieves state-of-the-art performance, and on synthetic shortcut datasets, we outperform existing baseline methods by an average of 97.95\% on the CLIP model.
__label__natural_language_processing State-of-the-art KD methods for LLMs mostly rely on minimizing explicit metrics measuring the divergence between teacher and student probability predictions.
__label__optimization Augmented Lagrangian Methods (ALMs) are widely employed in solving constrained optimizations, and some efficient solvers are developed based on this framework.
__label__probabilistic_methods A \emph{coreset} is a subset of a (large) training set, such that minimizing an empirical loss averaged over the coreset is a controlled replacement for the intractable minimization of the original empirical loss.
__label__bandits In this work, we establish the first minimax lower bound for this setting that scales like $\tilde{\Omega}(\min_{L \le k}(L^{1/3}n^{1/3}T^{2/3} + \sqrt{{n \choose k - L}T}))$.
__label__interpretability_and_explainability In this paper, we found that by learning the latent causal structure governing the generating process of noisy data, we can estimate noise transition matrices without the need for similarity-based assumptions.
__label__diffusion_based_models Experimental results demonstrate that our method significantly outperforms the previous SOTA models in multiple settings.
__label__machine_learning_for_other_sciences_and_fields On Nucleotide Transformer Benchmarks and Genomic Benchmarks, MxDNA demonstrates superior performance to existing methods with less pretraining data and time, highlighting its effectiveness.
__label__diffusion_based_models Moreover, we advance our method by transitioning the discrete shift process to a continuous formulation, termed as DoS-SDEs.
__label__natural_language_processing Our approach is theoretically supported by the universality theorem and the rank representation theorem to achieve efficient high-rank adaptations.
__label__fairness Our code is available at [github.com/UCSC-REAL/FairnessWithoutHarm](https://github.com/UCSC-REAL/FairnessWithoutHarm).
__label__machine_learning_for_physical_sciences Neural operators, serving as physics surrogate models, have recently gained increased interest.
__label__active_learning Ideally, learnability is reflected by ground truth consistency.
__label__bandits To address this, we leverage human response times, which inversely correlate with preference strength, as complementary information.
__label__learning_theory To be specific, our algorithm operates in the primal space and we resolve the primal LP for the CMDP problem at each period in an online manner, with \textit{adaptive} remaining resource capacities.
__label__other In this work, we investigate the efficiency properties of data from both optimization and generalization perspectives.
__label__natural_language_processing In this work, we show that localized fine-tuning serves as an effective alternative to such representation intervention methods.
__label__safety_in_machine_learning In this work, we propose the collaborative generation and editing for jailbreaking text-to-image deep generation (ColJailBreak), which comprises three key components: adaptive normal safe substitution, inpainting-driven injection of unsafe content, and contrastive language-image-guided collaborative optimization.
__label__natural_language_processing The former fine-tunes the LLM with examples automatically constructed from various public datasets, enabling AMOR to generalize across different knowledge environments, while the latter tailors AMOR to specific domains using process feedback.
__label__interpretability_and_explainability To address these challenges, we introduce a novel metric to assess the stability of top-$k$ salient features.
__label__neuroscience_and_cognitive_science We further drive the implications of our model from both theoretical and experimental points of view.
__label__evaluation We further evaluate compressed models qualitatively and quantitatively using MT-Bench and show that compressed models exhibiting high flips are worse than baseline models in this free-form generative task.
__label__generative_models Utilizing the Fisher information metric, we equip the manifold with a Riemannian structure whose intrinsic geometries are effectively leveraged by following the shortest paths of geodesics.
__label__optimization_for_deep_networks We show that the second phase begins once the empirical risk falls below a certain threshold, dependent on the stepsize.
__label__evaluation Rapid model validation via the train-test paradigm has been a key driver for the breathtaking progress in machine learning and AI.
__label__natural_language_processing On single-turn dialogue and summarization, a SAMI-trained mistral-7b outperforms the initial pretrained model, with win rates between 66% and 77%.
__label__learning_theory Learning a continuous-time process through snapshots, such as smart camera traps, is a central theme governing a wide array of online learning situations.
__label__learning_theory This measure shows a contrast with the expressivity results of Transformers captured by $TC^0/TC^1$ classes (further studied here), since the globality relates to correlations with the more limited $NC^0$ class.
__label__privacy For the key scenario of stochastic gradient descent with momentum and weight decay, we even derive analytical expressions for BSR that render the computational overhead negligible.
__label__machine_vision The code will be available for download at \url{https://github.com/OpenGVLab/OV-OAD}.
__label__reinforcement_learning Our code is available at: https://github.com/niklasdbs/paspo
__label__machine_learning_for_other_sciences_and_fields To address the long-tail item challenge, we design a dual-view modeling framework that combines semantics from LLMs and collaborative signals from conventional SRS.
__label__machine_learning_for_other_sciences_and_fields The framework consists of five steps, including graph coarsening, node representation learning and policy optimization.
"__label__online_learning To solve this online learning problem,
we first develop novel discretization schemes to approximate any pricing curve."
__label__algorithmic_game_theory We employ Yao’s Principle and our new NFL Theorem to provide general lower bounds for the query complexity of finding a Nash Equilibrium in adversarial optimisation.
__label__neuroscience_and_cognitive_science This enables us to cast the memorization problem in KHMs into a point arrangement problem on a hypersphere.
__label__deep_learning_architectures Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.
__label__reinforcement_learning First, a single humanoid character learns to interact with objects through imitation learning from human motion priors.
__label__machine_learning_for_healthcare This strong correlation hints to the fact that architectural designs in learning-based methods is unlikely to affect this correlation, and therefore, the performance of learning-based methods.
__label__probabilistic_methods We validate these results empirically on synthetic MDPs and tasks posed in the International Planning Competition.
__label__learning_theory random perturbations, both of which are considered as non-active-exploration inputs.
"__label__optimization_for_deep_networks Our findings highlight the critical role of understanding the joint
dynamics of the learning rate and curvature, beyond greedy minimization, to
diagnose failures and design effective adaptive learning rate tuners."
__label__generative_models In contrast, SCube leverages high-resolution sparse networks and produces sharp outputs from few views.
__label__human-AI_interaction Using a broad range of benchmark datasets and evaluation metrics, we bring to attention several important findings.
__label__deep_learning_architectures We focus on Knowledge Distillation (KD), where a compact student model is trained to mimic a larger teacher model, facilitating the transfer of knowledge of large models.
__label__algorithmic_game_theory We show that when $\Phi$ is finite, there exists an efficient uncoupled learning algorithm that approximates the corresponding $\Phi$-equilibria.
__label__probabilistic_methods Specifically, due to the fundamental stochasticity from the offline data-collection policies and the environment dynamics, highly non-trivial conditional/constrained generation is required to elicit rewarding actions.
__label__generative_models We find that training on only 20 digit numbers with a single GPU for one day, we can reach state-of-the-art performance, achieving up to 99% accuracy on 100 digit addition problems.
__label__machine_learning_for_other_sciences_and_fields Compared to general closed-source LLMs, our proposed bidirectional learning-based method improves C++ to CUDA translation by 22.08 BLEU and 14.39 CodeBLUE with 2.75% higher compilation accuracy.
__label__learning_theory However, spectral clustering is known to be non-robust to model mis-specification.
__label__interpretability_and_explainability This new method achieves state-of-the-art performance on local structure preservation for parametric methods without sacrificing the fidelity of global structural representation.
__label__optimization To fill this gap, we propose a general and rigorous method, namely boundary decomposition for nadir objective vector estimation (BDNE).
__label__machine_vision Within the Text2CAD framework, we propose an end-to-end transformer-based auto-regressive network to generate parametric CAD models from input texts.
__label__natural_language_processing This thus motivates us to re-think the necessity of finding a global optimum in prompt optimization.
__label__machine_learning_for_healthcare Conditional 3D structure-based drug design (3D-SBDD) models, which take into account complex three-dimensional interactions and molecular geometries, are particularly promising.
__label__safety_in_machine_learning Second, there exists a conflict between adversarial losses at different distances, which causes difficulties in optimization.
__label__natural_language_processing Global human evaluators also note a nearly 20% higher winning rate in helpfulness and harmfulness evaluation.
__label__optimization_for_deep_networks However, the increased memory and computational costs associated with these models pose significant challenges for deployment on resource-limited devices.
__label__online_learning Finally, we also show that logarithmic regret is possible whenever there exists one agent who is indifferent about different arms.
__label__generative_models For instance, many interfaces of popular text-to-image generative models, such as Stable Diffusion or Midjourney, produce several variations of an image for a given query which can eventually be curated by the users.
__label__optimization We perform rigorous analysis and experimental evaluations to demonstrate the effectiveness of adaptive, mirror-mapping DML.
__label__machine_vision Multimodal Large Language Models (MLLMs) have demonstrated an excellent understanding of images and 3D data.
__label__reinforcement_learning To enable future thinking functionality, we first develop a *multi-character policy* that captures diverse characters with an ensemble of heterogeneous policies.
__label__infrastructure Nonetheless, we observe a dynamic shift in token importance across different decoding steps.
__label__machine_learning_for_other_sciences_and_fields Antibody design, a crucial task with significant implications across various disciplines such as therapeutics and biology, presents considerable challenges due to its intricate nature.
__label__optimization_for_deep_networks The ability of learning useful features is one of the major advantages of neural networks.
__label__online_learning The primary idea is to decouple the joint effect of the delays and the bandit feedback on the regret by carefully incorporating the delayed bandit feedback with a blocking update mechanism.
__label__fairness Experiments on both synthetic and semi-synthetic datasets demonstrate the validity of our analysis and methods.
__label__learning_theory In this paper, using a d-dimensional generalization to the fundamental lemma of Neyman and Pearson (d-GNP), we obtain the Bayes optimal solution for learn-to-defer systems under various constraints.
__label__machine_vision In this work, we present SpatialPIN, a framework designed to enhance the spatial reasoning capabilities of VLMs through prompting and interacting with priors from multiple 3D foundation models in a zero-shot, training-free manner.
__label__fairness We evaluate SureMap on disaggregated evaluation tasks in multiple domains, observing significant accuracy improvements over several strong competitors.
__label__natural_language_processing Our code is publicly available at https://github.com/lfsszd/CS-Drafting.
__label__generative_models To tackle these challenges, we propose Channel-wise Salience Balancing (CSB) and Spearmen's $\rho$-guided Salience Calibration (SSC).
__label__machine_learning_for_other_sciences_and_fields Moreover, addressing the inherent data scale differences among these modalities is essential.
__label__reinforcement_learning The best-known result of Hwang and Oh [2023] has achieved an $\widetilde{\mathcal{O}}(\kappa^{-1}dH^2\sqrt{K})$ regret, where $\kappa$ is a problem-dependent quantity, $d$ is the feature dimension, $H$ is the episode length, and $K$ is the number of episodes.
__label__other RLGSSL incorporates a carefully designed reward function that balances the use of labeled and unlabeled data to enhance generalization performance.
__label__optimization_for_deep_networks This mechanism enables matrics to set a higher initial rank, thus expanding the allocation space for ranks.
__label__bandits We propose MaxMinLCB, which emulates this trade-off as a zero-sum Stackelberg game and chooses action pairs that are informative and have favorable reward values.
__label__machine_vision Additionally, we propose Pattern Observer (PrObe) to address these challenges.
__label__machine_learning_for_healthcare Additionally, we employ a density-aware mechanism to dynamically adjust the variable graph at different timestamps, adapting to the time-varying correlations among variables in ISMTS.
__label__reinforcement_learning This paper introduces a novel approach, KALM (Knowledgeable Agents from Language Model Rollouts), to learn knowledgeable agents by bridging this gap.
__label__other Furthermore, we demonstrate that CQ can preserve model quality reasonably with KV cache quantized down to 1 bit.
__label__probabilistic_methods However, the information gathered through these methods is suboptimal for down-the-line decision-making, as the experiments are not inherently designed with downstream objectives in mind.
__label__machine_vision Our approach addresses two primary challenges: generalizable knowledge of unseen scene configurations and strong adaptation to arbitrary depth sensors with various specifications.
__label__reinforcement_learning In the bilevel optimization structure, the outer problem involves optimizing dual variables derived from the risk measures, while the inner problem involves finding an optimal policy given these dual variables.
__label__speech_and_audio This approach reduces the reliance on the input audio features while preserving the integrity of the base SSLR.
__label__optimization To solve this problem, convergent algorithms are developed with both single-loop and stochastic variants.
__label__reinforcement_learning Then, to capture such uncertainty, it uses all offline data as the observations to approximate the posterior distribution of the action-value function under a Bayesian inference framework.
__label__diffusion_based_models Empirically, $\lambda$-Harmonic proves to be a reliable approach for model selection in subject-driven generation tasks.
__label__graph_neural_networks Self-supervised heterogeneous graph learning (SHGL) has shown promising potential in diverse scenarios.
__label__probabilistic_methods Theoretical analysis confirms that the proposed algorithm achieves sub-linear regret in relation to the number of rounds and arms.
__label__deep_learning_architectures To remedy these deficiencies, this paper presents a Dual-Stream Interactive Transformer (DSIT) design.
__label__bandits Both our analytical and numerical results corroborate that the efficacy   of PS$\varepsilon$BAI$^+$ is due to   the delicate change detection and context alignment procedures embedded in PS$\varepsilon$BAI.
__label__machine_learning_for_physical_sciences We consider the problem of learning the dynamics in the topology of time-evolving point clouds, the prevalent spatiotemporal model for systems exhibiting collective behavior, such as swarms of insects and birds or particles in physics.
__label__machine_vision Volume rendering in neural radiance fields is inherently time-consuming due to the large number of MLP calls on the points sampled per ray.
__label__reinforcement_learning However, in several problems of interest it is possible to observe the behavior of multiple experts with different degree of optimality (e.g., racing drivers whose skills ranges from amateurs to professionals).
__label__learning_theory Extensive experimental results on datasets including MNIST → MNIST-M and CIFAR10 → SVHN suggest that FedGTST significantly outperforms other relevant baselines, such as FedSR.
__label__robotics You can watch the videos in supplementary files to learn more about the details of our work.
__label__natural_language_processing This approach involves scoring training tokens using a reference model, and then training the language model with a focused loss on tokens with higher scores.
__label__robotics However, advanced self-supervised pre-trained visual representations (PVRs) in robotic motor control, leveraging large-scale egocentric videos, often focus solely on learning the static content features of sampled image frames.
__label__diffusion_based_models In this paper, we introduce SymmetricDiffusers, a novel discrete diffusion model that simplifies the task of learning a complicated distribution over $S_n$ by decomposing it into learning simpler transitions of the reverse diffusion using deep neural networks.
__label__deep_learning_architectures Neural compression models learn to encode an image into a quantized latent representation that can be efficiently sent to the decoder, which decodes the quantized latent into a reconstructed image.
__label__reinforcement_learning Beginning with the simplification of flattening all actions, we theoretically explore the discrepancies between action-level optimization and this naive token-level optimization.
__label__interpretability_and_explainability Typically, sparsity is measured in terms of the size of a model globally, such as the number of variables it uses.
__label__diffusion_based_models Our approach is derived through the lens of Expectation-Maximization (EM), where the generator parameters are updated using samples from the joint distribution of the diffusion teacher prior and inferred generator latents.
__label__natural_language_processing In the domain of code generation, self-debugging is crucial.
__label__deep_learning_architectures Despite using less than 1% of the training data typically used to train models from scratch, Phi-Mamba boasts substantially stronger performance compared to all past open-source non-Transformer models.
__label__machine_learning_for_physical_sciences However, oftentimes, such approaches underperform in the long-range propagation of information and lack explainability.
__label__infrastructure For exact gradient reconstruction, our protocol is around $2\times$ faster than the original class of protocols and for approximate gradient reconstruction, the mean-squared-error of our reconstructed gradient is several orders of magnitude better.
__label__machine_learning_for_other_sciences_and_fields Beyond spatial autoregressive entropy models, more efficient backward adaptation-based entropy models have been recently developed.
__label__robotics Our innovative approach involves training a versatile modality network that adapts to various inputs and connects with policy networks for effective control.
__label__natural_language_processing Large language models (LLMs) have demonstrated impressive capabilities in various reasoning tasks, aided by techniques like chain-of-thought prompting that elicits verbalized reasoning.
__label__learning_theory However, it has been shown empirically and theoretically, that too many graph convolutions can degrade performance significantly, a phenomenon known as oversmoothing.
__label__reinforcement_learning Unsupervised Environment Design (UED) is a paradigm that automatically generates a curriculum of training environments, enabling agents trained in these environments to develop general capabilities, i.e., achieving good zero-shot transfer performance.
__label__generative_models For instance, when evaluating Mistral-7B model on GSM8k dataset, ZipCache is capable of compressing the KV cache by $4.98\times$, with only a 0.38% drop in accuracy.
__label__causal_inference Unlike k-means clustering, which requires the margin condition, our proposed estimators do not rely on strong structural assumptions on the outcome process.
__label__neuroscience_and_cognitive_science Thirdly, we propose a novel baseline EEG2Video for video reconstruction from EEG signals that better aligns dynamic movements with high temporal resolution brain signals by Seq2Seq architecture.
__label__natural_language_processing Addressing the challenge of effectively processing long contexts has become a critical issue for Large Language Models (LLMs).
__label__bandits The Gaussian prior is computationally efficient but it cannot describe complex distributions.
__label__machine_vision Extensive experiments on three commonly-used datasets consistently demonstrate the superiority of our method compared to the previous approaches upon different missing scenarios.
__label__machine_learning_for_other_sciences_and_fields This is because any extension of these approaches to new, more sophisticated statistical tasks requires task-specific algebraic derivations and software implementations, which ignores the massive library of existing software tools already developed for the same scientific problem given observed data.
__label__diffusion_based_models We identified two main obstacles behind this issue.
__label__neuroscience_and_cognitive_science Compared to state-of-the-art methods, our results showed that GOPSA achieved significantly higher performance on three regression metrics ($R^2$, MAE, and Spearman's $\rho$) for several source-target site combinations, highlighting its effectiveness in tackling multi-source DA with predictive shifts in EEG data analysis.
__label__diffusion_based_models This can often complicate the reverse process’ task in learning generative trajectories, and results in costly inference for diffusion models.
__label__machine_learning_for_other_sciences_and_fields Active learning (AL) automatically queries labels for most informative samples, thereby remarkably alleviating the annotation hurdle.
__label__other LoD-Loc mainly achieves this goal by aligning the wireframe derived from the LoD projected model with that predicted by the neural network.
__label__reinforcement_learning To seamlessly integrate both modalities, we introduce a two-level hierarchical framework, RL-GPT, comprising a slow agent and a fast agent.
__label__natural_language_processing The base model maintains shared knowledge across users, while the multiple personal heads capture user-specific preferences.
__label__machine_vision Finally, we design an event-based 3D reconstruction loss to optimize the parameters of our method for better reconstruction quality.
__label__diffusion_based_models This advancement leads to the fast and customized solvers that further enhance sampling efficiency.
__label__machine_learning_for_healthcare The functional nature of deep networks do not guarantee that the predicted transformation is a local minima of the registration objective, the representation of the transformation (displacement/velocity field/affine) is fixed, and the networks are not robust to domain shift.
__label__learning_theory Our work highlights the provable benefits of combining labeled and unlabeled data for classification and feature selection in high dimensions.
__label__generative_models In this paper, we introduce a novel system that leverages prior modelling over disentangled style factors to address these challenges.
__label__machine_vision To address this issue, we propose a Cross-video Identity-cOrrelating pre-traiNing (CION) framework.
__label__machine_vision Comprehensive experiments on COCO and ODinW-13 datasets demonstrate that ZiRa effectively safeguards the zero-shot generalization ability of VLODMs while continuously adapting to new tasks.
__label__natural_language_processing While this diversity offers a broad spectrum of choices, effectively leveraging the options to optimize computational cost and performance remains challenging.
__label__machine_learning_for_other_sciences_and_fields Our dataset and code is available at https://github.com/Ljyustc/SocraticLM.
__label__optimization It can achieve near-optimal solutions in milliseconds with an average optimality gap of just 0.362\% on benchmarks with up to 2500 variables.
__label__deep_learning_architectures Parallelism introduces collective communication that is both expensive and represents a phase when hardware resources are underutilized.
__label__machine_vision For instance, by integrating our fragment pruning technique with state-of-the-art Gaussian pruning methods, we achieve up to a 1.71$\times$ speedup on an edge GPU device, the Jetson Orin NX, and enhance rendering quality by an average of 0.16 PSNR on the Tanks\&Temples dataset.
__label__machine_vision Techniques that explicitly focus on reflective surfaces can model complex and detailed reflections by exploiting better reflection parameterizations.
__label__learning_theory The spectacular results achieved in machine learning, including the recent advances in generative AI, rely on large data collections.
__label__evaluation We hope this paper will accelerate the research community in better evaluating their models and encourage future advancements in the consistency domain.
__label__machine_vision The Coordinate Field is optimizable and is used to transform the local shapes from the world coordinate frame to the aligned shape coordinate frame.
__label__infrastructure We validate TinyTTA on a Raspberry Pi Zero 2W and an STM32H747 MCU.
__label__machine_learning_for_healthcare Extensive experiments show that DMNet significantly outperforms previous SOTAs while maintaining high efficiency on a real-world clinical dataset collected by us and two public datasets for subject-independent seizure detection.
__label__privacy This challenge presents an important practical case of vertical federated learning, where the two parties perform parameter-efficient fine-tuning (PEFT) of a large model.
__label__optimization Across five heterogeneous algorithmic types, six different COPs, and both white-box and black-box views of COPs, ReEvo yields state-of-the-art and competitive meta-heuristics, evolutionary algorithms, heuristics, and neural solvers, while being more sample-efficient than prior LHHs.
__label__safety_in_machine_learning Additionally, 1-evaluation underestimates resubmit risks in stochastic defenses.
__label__fairness In this paper, we propose a general min-max optimization framework that can achieve interventional fairness with promising prediction accuracy and can be extended to maximally oriented PDAGs (MPDAGs) with added background knowledge.
__label__generative_models Then, for reconstructions on the ground, to deal with the feature extraction ability degradation due to simplifying encoders, we propose a diffusion-based model to compensate image details when decoding.
__label__graph_neural_networks Finally, our theoretical findings are validated experimentally on both synthetic and real-world datasets.
__label__diffusion_based_models Hence, model unlearning and data cleaning are the most essential methods for maintaining the safety of models, given their impact on model parameters.
__label__diffusion_based_models Current data expansion techniques include image transformation and image synthesis methods.
__label__generative_models GDB leverages a probabilistic approach to evolve geometric state distributions, employing an equivariant diffusion bridge derived by a modified version of Doob's $h$-transform for connecting geometric states.
__label__machine_vision To realize efficient action segmentation, we introduce BaFormer, a boundary-aware Transformer network.
__label__learning_theory However, optimising generalisation bounds might not always be viable for tractable or computational reasons, or both.
__label__machine_vision However, fine-tuning on task-specific datasets often leads to overfitting to seen classes and suboptimal performance on unseen classes, due to the absence of unseen class labels.
__label__diffusion_based_models Our approach is shown to combine the strengths of next-token prediction models, such as variable-length generation, with the strengths of full-sequence diffusion models, such as the ability to guide sampling to desirable trajectories.
__label__generative_models SSC extends this approach by dynamically adjusting the balanced salience to capture the temporal variations in activation.
__label__bandits We provide a unified framework to analyze these corruptions.
__label__reinforcement_learning Some recent work investigates how to learn robot policies from only a single/few expert video demonstrations.
__label__machine_learning_for_other_sciences_and_fields Our method is provably at least as expressive as the Geometric Weisfeiler-Lehman (GWL) test.
__label__generative_models In the context of aligning large language models (LLM), this objective fuses the direct preference optimization (DPO) loss with the supervised fune-tuning (SFT) loss to help mitigate the overoptimization towards undesired responses, for which we name the algorithm Regularized Preference Optimization (RPO).
__label__machine_vision To this end, existing methods can directly leverage the aligned AV data via our agentic workflow to improve AV joint representations.
__label__deep_learning_architectures This is because the former paradigm only utilizes limited downstream data to fit the multi-modal feature fusion.
__label__evaluation A variety of T2I faithfulness metrics have been proposed, leveraging advances in cross-modal embeddings and vision-language models (VLMs).
__label__bandits Our performance guarantee is based on a dynamic benchmark, distinguishing our work from existing works on adversarial Bwk who compare with the static benchmark.
__label__machine_vision AWT can be seamlessly integrated into various VLMs, enhancing their zero-shot capabilities without additional training and facilitating few-shot learning through an integrated multimodal adapter module.
__label__online_learning However, the prevalence of AI-generated images may have side effects for the machine learning community that are not clearly identified.
__label__natural_language_processing Mobile device operation tasks are increasingly becoming a popular multi-modal AI application scenario.
"__label__speech_and_audio Thereby, in video-to-audio
generation task, it is imperative to introduce steering approaches for controlling the
generated audio."
__label__online_learning While various machine learning problems can be written as CURL, its non-linearity invalidates traditional Bellman equations.
__label__machine_vision Through extensive evaluations, we show that GOMAA-Geo outperforms alternative learnable approaches and that it generalizes across datasets -- e.g., to disaster-hit areas without seeing a single disaster scenario during training -- and goal modalities -- e.g., to ground-level imagery or textual descriptions, despite only being trained with goals specified as aerial views.
__label__machine_vision Existing unified methods typically treat multi-degradation image restoration as a multi-task learning problem.
__label__machine_vision Collectively, Cambrian-1 not only achieves state-of-the-art performances but also serves as a comprehensive, open cookbook for instruction-tuned MLLMs.
__label__other Addressing this challenge necessitates external supervision that should be accessible in practice.
__label__natural_language_processing We introduce Buffer of Thoughts (BoT), a novel and versatile thought-augmented reasoning approach for enhancing accuracy, efficiency and robustness of large language models (LLMs).
__label__natural_language_processing Their limited context awareness can lead to overlooking critical information and subsequent task failures.
__label__other We present our research as the first to apply LLMs to a broad range of CPs and demonstrate that SGE outperforms existing prompting strategies by over 27.84% in CP optimization performance.
__label__reinforcement_learning Communication is a fundamental aspect of human society, facilitating the exchange of information and beliefs among people.
__label__interpretability_and_explainability While our algorithms are conceptualized in an idealized setting, they indicate that the Möbius transform is a potent tool for interpreting deep learning models.
__label__bandits This is achieved through a novel procedure that we design for checking whether the best arm is eliminated, which is of independent interest.
__label__machine_learning_for_physical_sciences Extensive empirical evaluations on a diverse set of PDEs demonstrate that our method is highly data-efficient, more generalizable, and even outperforms conventional vision-pretrained models.
__label__speech_and_audio Our method achieves significantly improved performances in zero-shot audio captioning, compared to existing approaches.
__label__privacy We further design novel schemes to generate a surface trajectory that involves a series of fixed-length trajectories with dynamically adjusted step sizes.
__label__natural_language_processing Experiments indicate that our model can reduce inference costs by 90\% compared to conducting searches across the entire evaluation tree, thereby significantly enhancing efficiency.
__label__machine_vision Furthermore, an identity-guided self-distillation loss is proposed to implement better large-scale pre-training by mining the identity-invariance within person images.
__label__online_learning Along the way, we show that the trichotomy of possible minimax rates established by Hanneke et al.
__label__machine_vision Extensive experiments in open-world object recognition show that our  RegionSpot achieves significant performance gain over prior alternatives, along with substantial computational savings (e.g., training our model with 3 million data in a single day using 8 V100 GPUs).
__label__machine_learning_for_healthcare Based on uC, we further present uC 3DU-Net, an enhanced 3D U-Net backbone that integrates the uC approach to facilitate optimal axial-slice plane feature utilization.
__label__generative_models Through theoretical analysis and empirical results, our method prevents overfitting and forgetting, delivers SOTA performance on both commonsense and arithmetic reasoning with 4.6% and 1.3% average improvements compared to LoRA, and outperforms full FT by 11.5% when generalize to various domains after instruction tuning.
__label__natural_language_processing In this work, we identify a goal-agnostic prerequisite to meaningful communication, which we term semantic consistency, based on the idea that messages should have similar meanings across instances.
__label__machine_vision Our approach is conceptually simple yet effective, it partitions predefined IPC samples into smaller subtasks and employs local optimizations to distill each subset into distributions from distinct phases, reducing the uniformity induced by the unified optimization process.
__label__machine_learning_for_other_sciences_and_fields Specifically, HORSE enables the partitioning of the input ground set into manageable chunks that can be processed independently and then aggregated, ensuring consistent outcomes across different partitions.
__label__probabilistic_methods Moreover, we establish the global and local optimality theory of our model.
__label__neuroscience_and_cognitive_science However, there is still a lack of dimensionality reduction methods that can effectively align low-dimensional latent dynamics with movements.
__label__learning_theory In this work, we establish algebraic relationships between logical definitions and quantitative metrics to derive theoretically grounded disentanglement metrics.
__label__robotics At the coarse level, we consider the positional information of 3D points to achieve location-based clustering, which is then refined at the fine level.
__label__machine_vision MMStar benchmarks 6 core capabilities and 18 detailed axes, aiming to evaluate LVLMs' multi-modal capacities with carefully balanced and purified samples.
__label__machine_vision Experiments conducted in multiple different settings have consistently demonstrated the effectiveness of CDC.
__label__machine_learning_for_healthcare At the crossroads between URL for time series and shape analysis, the proposed algorithm handles irregularly sampled multivariate time series of variable lengths and provides shape-based representations of temporal data.
__label__privacy Hence, in this paper, we propose PrivCirNet, a protocol/network co-optimization framework based on block circulant transformation.
__label__natural_language_processing The development of SaulLM-54B and SaulLM-140B is guided by large-scale domain adaptation,  divided into strategies: (1) the exploitation of continued pretaining involving a legal corpus that includes over $400$ billion tokens, (2) the implementation of a specialized legal instruction-following protocol, and (3) the alignment of model outputs with human preferences in legal interpretations.
__label__online_learning Online Class Incremental Learning (OCIL) aims to train models incrementally, where data arrive in mini-batches, and previous data are not accessible.
__label__reinforcement_learning Experimental results show that our approach achieves superior performance compared to state-of-the-art MARL algorithms.
__label__reinforcement_learning While it may not be feasible to obtain numerous expert demonstrations, it is often possible to gather a larger set of sub-optimal demonstrations.
__label__natural_language_processing These findings significantly broaden the practicality and versatility of our proposed method.
__label__machine_vision In our work, we delve into why traditional teacher-student designs falter in generalized category discovery as compared to their success in closed-world semi-supervised learning.
__label__probabilistic_methods But while Bayesian coresets and methods for construction are applicable in a wide range of models, existing theoretical analysis of the posterior inferential error incurred by coreset approximations only apply in restrictive settings---i.e., exponential family models, or models with strong log-concavity and smoothness assumptions.
__label__reinforcement_learning In this work, we introduce the first comprehensive formalization of Feint behaviors at both action-level and strategy-level, and provide concrete implementation and quantitative evaluation of them in multi-player games.
__label__machine_vision We provide theoretical insights that the CLIP objective cannot offer additional robustness.
__label__bandits As a result, it is necessary to improve the robustness of neural bandit models against potential reward corruptions.
__label__causal_inference % To achieve this, we design the bridge equations to estimate the underlying conditional independence.
__label__reinforcement_learning Reinforcement Learning from Human Feedback (RLHF) has become the standard approach for aligning Large Language Models (LLMs) with human preferences, allowing LLMs to demonstrate remarkable abilities in various tasks.
__label__optimization_for_deep_networks Indeed, we show a lower bound on the sharpness of minimizers, which grows linearly with depth.
__label__privacy We aim to minimize the average regret on $m$ clients working in parallel over time horizon $T$ with explicit differential privacy (DP) guarantees.
__label__safety_in_machine_learning In contrast, our proposed definition yields certification guarantees that depend only on the loss function and the intermediate learned metric spaces of the neural network.
__label__machine_vision The generalization ability of deepfake detectors is vital for their applications in real-world scenarios.
__label__algorithmic_game_theory As our main theoretical contribution, we characterize optimal cost-robust contracts through a direct correspondence to optimal composite hypothesis tests from statistics, generalizing a result of Saig et al.
__label__reinforcement_learning Notably, this loss is shown to be a tight lower bound of the policy objective.
__label__safety_in_machine_learning The experiments conducted on OPT-1.3B and LLaMA-7B demonstrate the effectiveness of Bileve in defeating spoofing attacks with enhanced detectability.
__label__human-AI_interaction Additionally, we introduce a new model VOILA-A that integrate gaze information into VLMs while maintain pretrained knowledge from webscale dataset.
__label__neuroscience_and_cognitive_science Here, we analyze a linear gated network where the weights and gates are jointly optimized via gradient descent, but with neuron-like constraints on the gates including a faster timescale, non-negativity, and bounded activity.
__label__machine_vision Then, a multi-granularity relevance discriminator is exploited to produce precise video-query relevance feedback and a relation-aware segment grounding module is employed to selectively conduct the grounding process, dynamically adapting to the presence or absence of query-related segments in videos.
__label__diffusion_based_models To address this challenge, we introduce StepbaQ, a method that calibrates the sampling trajectory and counteracts the adverse effects of accumulated quantization error through a sampling step correction mechanism.
__label__interpretability_and_explainability Here, we present evidence that this structure is given by the meta-dynamics of belief updating over hidden states of the data-generating process.
__label__diffusion_based_models Our cascading method uses the image generated at the lowest resolution as baseline to sample at higher resolutions.
__label__natural_language_processing As these technologies become integral to various applications, establishing an objective measure for the quality of information extraction becomes imperative.
__label__deep_learning_architectures Our approach involves training personalized Bayesian models at each client tailored to the unique complexities of the clients' datasets and efficiently collaborating across these clients.
__label__machine_learning_for_other_sciences_and_fields Furthermore, we employ expert iteration whereby model-generated proofs progressively replace longer teacher proofs as the new ground truth.
__label__optimization_for_deep_networks Empirical experiments validate the efficacy of our PID-based adaptive gradient decay rate approach, ensuring consistent optimization of model calibration and model accuracy.
__label__machine_learning_for_physical_sciences We evaluate CoDA-NO's potential as a backbone for learning multiphysics PDEs over multiple systems by considering few-shot learning settings.
__label__robotics Extensive experiments on three benchmark datasets, Argoverse 1, nuScenes and Argoverse 2, demonstrate the effectiveness of our approach.
__label__other Capitalizing on the complementary advantages of generative and discriminative models has always been a compelling vision in machine learning, backed by a growing body of research.
__label__probabilistic_methods This work addresses the fundamental linear inverse problem in compressive sensing (CS) by introducing a new type of regularizing generative prior.
__label__robotics We guarantee that we will open-source our code as soon as possible.
__label__probabilistic_methods The variational perspective shows that the previous types of inference for planning are only adequate in environments with low stochasticity, and allows us to characterize each type by its own merits, disentangling the type of inference from the additional approximations that its practical use requires.
__label__machine_learning_for_social_sciences Our approach addresses three research questions: aligning LLMs with real-world urban mobility data, developing reliable activity generation strategies, and exploring LLM applications in urban mobility.
__label__reinforcement_learning Specifically, we establish a novel off-policy evaluation lemma and introduce a new coverage coefficient for LMDPs.
__label__natural_language_processing Using this idea, we build a robust detector of toxic prompts using a sparse logistic regression model on the first response token logits.
__label__interpretability_and_explainability The source code is available at https://anonymous.4open.science/r/GATSM-78F4/.
__label__fairness It generates high-quality cross-cultural dialogues encapsulating human beliefs, norms, and customs.
__label__reinforcement_learning Usually, rewards are observed only _after_ acting, and so the goal is to maximize the _expected_ cumulative reward.
__label__probabilistic_methods Specifically, DDPM uses a Gaussian approximation for the RTK, resulting in low per-subproblem complexity but requiring a large number of segments (i.e., subproblems), which is conjectured to be inefficient.
__label__machine_learning_for_other_sciences_and_fields The framework was implemented in a formal synthetic domain, demonstrating that it is transparent and systematic.
__label__optimization_for_deep_networks We in fact arrive at SAMPa by treating this convergence guarantee as a hard requirement---an approach we believe is promising for developing SAM-based methods in general.
__label__diffusion_based_models Diffusion-based text-to-video (T2V) models have achieved significant success but continue to be hampered by the slow sampling speed of their iterative sampling processes.
__label__fairness Data distillation and coresets have emerged as popular approaches to generate a smaller representative set of samples for downstream learning tasks to handle large-scale datasets.
__label__safety_in_machine_learning In this paper, we focus on in-training backdoor defense, aiming to train a clean model even when the dataset may be potentially poisoned.
__label__robotics Constructed in real-time as the camera moves, the NGF effectively models the grasp distribution in 3D space by rendering graspness predictions from each view.
__label__natural_language_processing We conduct a comprehensive evaluation of MiniCache utilizing various models including LLaMA-2, LLaMA-3, Phi-3, Mistral, and Mixtral across multiple benchmarks, demonstrating its exceptional performance in achieving superior compression ratios and high throughput.
__label__natural_language_processing word concatenation).
__label__machine_vision Therefore, we design a Detection Adaptation Score (DAS) approach to approximately measure the flat minima without using target labels.
__label__optimization Yet, the massive scale of LLMs often forces current pruning approaches to rely on heuristics instead of optimization-based techniques, potentially resulting in suboptimal compression.
__label__generative_models This is achieved by jointly optimizing the relevance and distinctness of docids through a combination of docid generation and autoencoder models.
__label__online_learning Moreover, compared to other TTA methods, our approach can operate effectively with just a single image.
__label__natural_language_processing Preference learning algorithms (e.g., RLHF and DPO) are frequently used to steer LLMs to produce generations that are more preferred by humans, but our understanding of their inner workings is still limited.
__label__reinforcement_learning Offline-to-online (O2O) reinforcement learning (RL) provides an effective means of leveraging an offline pre-trained policy as initialization to improve performance rapidly with limited online interactions.
__label__bandits (2) Strong adversary: The adversary sets the reward variance after observing the learner's action.
__label__fairness We also provide the upper bound of generalization error and risk disparity as well as the corresponding connections.
__label__optimization In this paper, we contribute a non-linear class aggregation framework HyperPrism that leverages distributed mirror descent with averaging done in the mirror descent dual space and adapts the degree of Weighted Power Mean (WPM) used in each round.
__label__human-AI_interaction In this paper, we introduce a Rule-Guided Spatial Awareness Network (RG-SAN) by utilizing solely the spatial information of the target instance for supervision.
__label__deep_learning_architectures Our approach relies on an additional averaging step after each transformer block, which computes a weighted average of current and past representations---we refer to this operation as Depth-Weighted-Average (DWA).
__label__safety_in_machine_learning Recent advancements in text-to-image (T2I) models have greatly benefited from large-scale datasets, but they also pose significant risks due to the potential generation of unsafe content.
__label__reinforcement_learning Our method, Learned **O**ptimization for **P**lasticity, **E**xploration and **N**on-stationarity (*OPEN*), meta-learns an update rule whose input features and output structure are informed by previously proposed solutions to these difficulties.
__label__machine_vision These scenarios often involve Spatio-Temporal Discontinuities (i.e., STDChallenge), prevalent in long-term tracking and global instance tracking.
__label__optimization_for_deep_networks Moreover, in practice, camera-LiDAR fusion models utilize pre-trained backbones for efficient training.
__label__machine_vision As a result, we demonstrate that our method, AltO, can be trained on multimodal datasets without any ground-truth data.
__label__machine_vision The Segmentation Anything Model (SAM) requires labor-intensive data labeling.
__label__machine_vision We conduct comprehensive testing across 25 datasets and the result indicates that CVPT significantly improves VPT’s performance and efficiency in visual tasks.
"__label__fairness On the other hand, fine-tuning-free methods typically utilize a ``one-size-fits-all"" approach that assumes that correlation with the spurious attribute can be explained using a single linear direction across all possible inputs."
__label__safety_in_machine_learning Source code is available at: https://github.com/Yuxin104/FedGMark.
__label__deep_learning_architectures Experiments on widely used datasets demonstrate the superiority of our method compared with state-of-the-art (SOTA) multimodal learning approaches.
"__label__interpretability_and_explainability We release our
code and pre-trained model weights at https://github.com/shrebox/B-cosification."
__label__evaluation Moreover, we analyze the relevance of existing metrics to dynamics metrics, improving them from the perspective of dynamics.
__label__natural_language_processing Besides, we show the feasibility of distilling advanced LLMs’ language processing abilities to a smaller yet effective StruXGPT-7B to execute structurization, addressing the practicality of our approach.
__label__machine_learning_for_other_sciences_and_fields Within this framework, the timestamps are modeled individually to capture the global dependencies.
__label__diffusion_based_models In DKI, we adopt positive (real) domain finetuning and negative (rendered) domain embedding to inject knowledge into a pretrained Text-to-image (T2I) diffusion model.
__label__graph_neural_networks However, most existing studies primarily concentrate on aligning feature distributions directly to extract domain-invariant features, while ignoring the utilization of the intrinsic structure information in graphs.
__label__machine_vision The code is available at https://github.com/ZhangYushan3/DiffSF.
__label__reinforcement_learning However, these approaches often face shortcomings such as the effort on manual design and the absence of theoretical groundings.
__label__machine_vision Despite the substantial progress of novel view synthesis, existing methods, either based on the Neural Radiance Fields (NeRF) or more recently 3D Gaussian Splatting (3DGS), suffer significant degradation when the input becomes sparse.
__label__infrastructure In this work, we redesign the data flow of heterogeneous hardware and sharded model training to minimize the excessive communication overhead.
__label__learning_theory Under appropriate conditions, we show that any optimal solution to the corresponding optimization problems attains the optimal statistical rate.
__label__diffusion_based_models We systematically benchmark across 7 diffusion models on 16 tasks with 40 targets, and improve performance by 8.5% on average.
__label__graph_neural_networks Given a graph with textual attributes, we enable users to `chat with their graph': that is, to ask questions about the graph using a conversational interface.
__label__learning_theory The running time of our approach scales with the actual number of pieces that appear as opposed to worst case upper bounds on the number of pieces.
__label__learning_theory We develop a technique to design efficiently computable estimators for sparse linear regression in the simultaneous presence of two adversaries: oblivious and adaptive.
__label__machine_vision We perform extensive experiments to confirm the effectiveness of AnyChange for zero-shot change detection.
__label__deep_learning_architectures However, this approach necessitates selecting elements that need to be kept fixed, as well as centroids that should be adjusted throughout editing.
__label__robotics For robots to be more generalizable embodied agents, they should be capable of following instructions and perceiving the world with adaptation to diverse modalities.
__label__neuroscience_and_cognitive_science While humans effortlessly discern intrinsic dynamics and adapt to new scenarios, modern AI systems often struggle.
__label__algorithmic_game_theory In this work, we demonstrate, both theoretically and empirically, that a purely relevance-driven policy with low exploration strength boosts short-term user satisfaction but undermines the long-term richness of the content pool.
__label__machine_vision Given the characteristics of the LiDAR sensor, an observation of an object at a certain location confirms both 1) the occupation of that location and 2) the absence of obstacles along the line of sight from the LiDAR to that point.
__label__reinforcement_learning Motivated by this paradigm shift, we introduce DIAMOND (DIffusion As a Model Of eNvironment Dreams), a reinforcement learning agent trained in a diffusion world model.
__label__machine_vision We close this gap with ProvNeRF, an approach that models the provenance for each point -- i.e., the locations where it is likely visible -- of NeRFs as a stochastic field.
__label__causal_inference We focus on exchangeable data satisfying an assumption of independent causal mechanisms.
__label__evaluation We perform experiments on multiple datasets with these metrics, validating the effectiveness of the proposed approach.
__label__speech_and_audio We conduct extensive experiments and find that under the same setting, REBORN outperforms all prior unsupervised ASR models on LibriSpeech, TIMIT, and five non-English languages in Multilingual LibriSpeech.
__label__diffusion_based_models Based on these findings, we build two types of efficient text-to-image models, called KOALA-Turbo & -Lightning, with two compact U-Nets (1B & 700M), reducing the model size up to 54% and 69% of the SDXL U-Net.
__label__learning_theory In this work, we study the dynamics of training a shallow transformer on a task of recognizing co-occurrence of two designated words.
__label__generative_models This paper aims to bridge the gap between Code LLMs' reliance on static text data and the need for semantic understanding for complex tasks like debugging and program repair.
__label__active_learning These training methods are particularly relevant for active learning and data subset selection problems.
__label__learning_theory Models of hyperbolic geometry have been successfully used in ML for two main tasks: embedding *models* in unsupervised learning (*e.g.
__label__deep_learning_architectures Recent advancements using transformer-based in-context learning have shown promise on smaller and less complex tabular datasets, but have struggled to scale to larger and more complex ones.
__label__causal_inference Convergent cross mapping (CCM) and related methods have been proposed to study time series that are generated by dynamical systems, where traditional approaches like Granger causality are unreliable.
__label__machine_vision Extensive experimental results on 15 benchmark datasets demonstrate that our proposed DPE consistently outperforms previous state-of-the-art methods while also exhibiting competitive computational efficiency.
__label__causal_inference In this paper, we study the problem of discovering the underlying causal structure from event sequences.
__label__reinforcement_learning Consequently, we introduce a Transformer-like DeMa as opposed to an RNN-like DeMa.
__label__reinforcement_learning Sequential decision-making can be formulated as a conditional generation process, with targets for alignment with human intents and versatility across various tasks.
__label__optimization To tackle these issues, we adapt the Byzantine framework to asynchronous dynamics by introducing a novel weighted robust aggregation framework.
__label__infrastructure For example, on up to 10 fine-tuned models in the GPT-NeoX-20B family, FM-Delta reduces the original storage requirement from 423GB to 205GB, significantly saving cloud storage costs.
__label__deep_learning_architectures Empirical results on two chemical process datasets clearly show that AOPU outperforms other models in achieving stable convergence, marking a significant advancement in soft sensor field.
__label__learning_theory Our result builds upon the recent short-flat decomposition framework of [KLLST23a, KLLST23b] and leverages fast algorithms for flow problems on graphs to solve adaptive reweighting subproblems efficiently.
__label__privacy Our approach for $\mathbb{R}^2$ extends to arbitrary metric spaces as it goes via hierarchically separated trees.
__label__optimization_for_deep_networks This coefficient exhibits a strong correlation with the overall confidence level.
__label__machine_learning_for_physical_sciences This reintroduces the need for expensive numerical PDE solutions, partially undermining the original goal of avoiding these expensive simulations.
"__label__learning_theory Prior works (DGT19, CKMY20)  came with worse sample complexity
 guarantees (in both $\epsilon$ and $\gamma$) or could only
 handle random classification noise (DDKWZ23,KITBMV23)--- a much milder noise assumption."
__label__natural_language_processing Using a suite of five tasks, we demonstrate that frontier LLMs can perform inductive OOCR.
__label__other Experiment results on 15 benchmark datasets under three different missing patterns show the effectiveness of M$^3$-Impute by achieving 13 best and 2 second-best MAE scores on average.
__label__causal_inference Experimental results demonstrate that our proposed solutions outperform state-of-the-art baselines on synthetic data with varying ratios of linear and nonlinear relations.
__label__machine_vision Talking face generation (TFG) aims to animate a target identity's face to create realistic talking videos.
__label__deep_learning_architectures This is achieved by using *low displacement rank matrices* (LDRMs), which hasn't been used in this context before.
__label__generative_models Experiments demonstrate that LPT not only effectively discovers useful molecules across single-objective, multi-objective, and structure-constrained optimization tasks, but also exhibits strong sample efficiency.
__label__diffusion_based_models This paper proposes a method to learn an efficient data prior for the entire image by training diffusion models only on patches of images.
__label__machine_vision StreamDSGN is an end-to-end framework that directly predicts the 3D properties of objects in the next moment by leveraging historical information, thereby alleviating the accuracy degradation of streaming perception.
__label__privacy In this work, we show empirically across three tasks that even in settings with large distribution shift, where both zero-shot performance from public data and training from scratch with private data give unusably weak results, public features can in fact improve private training accuracy by up to 67\% over private training from scratch.
__label__learning_theory In an attempt to overcome this challenge and make up for the gap in the generalization theory of LSRL, we develop a novel vector-contraction inequality and derive the generalization bound for general function class of LSRL with a weaker dependency on the number of labels than the state of the art.
__label__diffusion_based_models We reveal an intriguing connection between the conditional variance of their training loss and the discretization error of the ODEs.
__label__safety_in_machine_learning This paper introduces VRCP (Verifiably Robust Conformal Prediction), a new framework that leverages recent neural network verification methods to recover coverage guarantees under adversarial attacks.
__label__learning_theory It is well-known that natural nonconvex formulations of phase retrieval do not have spurious local optima.
__label__machine_vision In this work, we propose a simple but effective framework (called ResAD) that can be directly applied to detect anomalies in new classes.
__label__machine_vision Then the task-specific decoding is carried out by flexibly routing the shared representation to lightweight task decoders with negligible training efforts.
__label__machine_learning_for_social_sciences It is largely agreed that social network links are formed due to either homophily or social influence.
__label__machine_vision Our model can acquire strong robustness to the noises, achieving significant improvements in accuracy, edge quality, efficiency, and generalizability on five different benchmarks.
__label__robotics However, the realistic human demand may involve multiple objects.
__label__diffusion_based_models To address this, we explore the potential of leveraging vision priors embedded in pre-trained latent diffusion models (LDM) for estimating foreground RGBA values in challenging scenarios and rare objects.
__label__neuroscience_and_cognitive_science Electroencephalography (EEG) data is often collected from diverse contexts involving different populations and EEG devices.
__label__graph_neural_networks Guided by this principle, we design novel frames for eigenvectors that are strictly superior to existing methods --- some are even optimal --- both theoretically and empirically.
__label__reinforcement_learning Among the research topics in multi-agent learning, mixed-motive cooperation is one of the most prominent challenges, primarily due to the mismatch between individual and collective goals.
__label__bandits This significantly generalizes other works on this topic which impose strict conditions on the strength of interference on a *known* network, and also compare regret to a markedly weaker optimal action.
__label__machine_vision We overcome these challenges thanks to three main insights: (i) synthetic pre-training with diverse enough data enables to learn reasonable characters localization in any script; (ii) modern transformer-based detectors can jointly detect a large number of instances and, if trained with an adequate masking strategy, leverage consistency between the different detections; (iii) once a pre-trained detection model with approximate character localization is available, it is possible to fine-tune it with line-level annotation on real data, even with a different alphabet.
__label__reinforcement_learning We compare NeoRL to other baselines on several deep RL environments and empirically demonstrate that NeoRL achieves the optimal average cost while incurring the least regret.
__label__diffusion_based_models We revisit the key design principles of hybrid architectures and propose a simple and effective \emph{asymmetric} architecture, where the distribution of convolutional and transformer blocks is \emph{asymmetric}, containing more convolutional blocks in the earlier stages, followed by more transformer blocks in later stages.
__label__generative_models Unlike previous approaches, our work achieves the first non-vacuous bounds for models that are deployed in practice and generate high-quality text.
__label__deep_learning_architectures In this paper, we present a novel data-free method for merging neural networks in weight space.
__label__machine_learning_for_other_sciences_and_fields To this end, we propose a **D**eep **E**volutionary **C**lustering jointed temporal knowledge graph **R**epresentation **L**earning approach (**DECRL**).
__label__natural_language_processing Existing alignment methods, such as Direct Preference Optimization (DPO), are mainly tailored for pairwise preference data where rewards are implicitly defined rather than explicitly given.
__label__generative_models The simulator is a world model that separately learns dynamics and reward, where the dynamics model comprises an image tokenizer as well as a causal transformer to generate interaction transitions autoregressively, and the reward model is a bidirectional transformer learned by maximizing the likelihood of trajectories in the expert demonstrations under language guidance.
__label__natural_language_processing In particular, we demonstrate that similar model capabilities, or similar model responses, can result in static debate dynamics where the debate procedure simply converges to the majority opinion.
__label__machine_learning_for_other_sciences_and_fields Leveraging a pre-trained conditional diffusion model that jointly models sequences and structures of antibodies with equivariant neural networks, we propose direct energy-based preference optimization to guide the generation of antibodies with both rational structures and considerable binding affinities to given antigens.
__label__reinforcement_learning We further introduce a new adversarial evaluation procedure for directly measuring robustness, closely mirroring the conditional value at risk (CVaR).
__label__machine_learning_for_other_sciences_and_fields Finally, a novel diagnostic function is devised to handle three disentangled representations for prediction.
__label__optimization Existing models to learn the diffusion terms from observational data rely on complex bilevel optimization problems and model only the drift of the system.
__label__reinforcement_learning Our analysis highlights two key stability properties, relating to how changes in value functions and/or policies affect the Bellman operator and occupation measures.
__label__machine_vision Guided by such insight, we propose a novel training paradigm named MC-DiT for fully learning contextual information via diffusion denoising at different noise variances with clean-to-clean mask-reconstruction.
__label__evaluation Our code is available at https://github.com/NinhPham/sDbscan.
__label__other Topological Data Analysis (TDA) allows us to extract powerful topological, and higher-order information on the global shape of a data set or point cloud.
__label__machine_vision We design the regulation framework as a plug-and-play module to embed into existing representative large 3D models.
__label__natural_language_processing This perspective enables us to detect *any* model with inflated performance, i.e., performance that does not generalize to rephrased samples, synthetic samples from the same distribution, or different benchmarks for the same task.
__label__machine_learning_for_other_sciences_and_fields We demonstrate this methodology on Polynomial Continued Fraction formulas, which are ubiquitous in their intrinsic connections to mathematical constants, and generalize many mathematical functions and structures.
__label__neuroscience_and_cognitive_science Although modern imaging technologies allow us to study connectivity between two distinct brain regions $\textit{in-vivo}$, an in-depth understanding of how anatomical structure supports brain function and how spontaneous functional fluctuations emerge remarkable cognition is still elusive.
__label__interpretability_and_explainability Our new methods are easy to implement and ensure monotonic loss decrease and global convergence.
__label__diffusion_based_models At the core of our approach is a domain shift equation that integrates seamlessly with existing diffusion models.
__label__generative_models However, the preference datasets used in DAP methods are usually collected ahead of training and never updated, thus the feedback is purely offline.
__label__diffusion_based_models Our proposed model shows competitive empirical performance against other state-of-the-art graph generation solutions on various benchmarks while at the same time can flexibly trade off the generation quality and efficiency in the sampling phase.
"__label__safety_in_machine_learning Given a DNN $\mathcal{N}$
with parameters $\theta$, input polytope $P$, and output
polytope $Q$, PREPARED finds new parameters $\theta'$ such that $\forall
\mathrm{x} \in P
    ."
__label__learning_theory Similarly, in generative models, such as those trained on existing artworks or music, it is important to ensure that any generated content influenced by these works appropriately credits the original creators.
__label__optimization_for_deep_networks In this paper, we delve into the optimization challenges of RNNs and discover that, as the memory of a network increases, changes in its parameters result in increasingly large output variations, making gradient-based learning highly sensitive, even without exploding gradients.
__label__machine_learning_for_physical_sciences UPTs efficiently propagate dynamics in the latent space, emphasized by inverse encoding and decoding techniques.
__label__safety_in_machine_learning We empirically validate our insights on a range of vision and language tasks, demonstrating that risk control can produce substantial computational savings, all the while preserving user-specified performance goals.
__label__reinforcement_learning Real-world offline datasets are often subject to data corruptions (such as noise or adversarial attacks) due to sensor failures or malicious attacks.
__label__natural_language_processing Traditional model merging methods often show significant performance gaps compared to fine-tuned models due to these issues.
__label__probabilistic_methods Since our method inherits the coverage guarantees of conformal prediction, our conformal credal sets are guaranteed to be valid with high probability (without any assumptions on model or distribution).
__label__privacy Existing results are limited to the assumptions of bounded norm $ \|\mathbf{x}\|_2 \leq 1$, which becomes meaningless with increasing data dimensionality.
__label__generative_models This paper studies Generative Flow Networks (GFlowNets), which learn to sample objects proportionally to a given reward function through the trajectory of state transitions.
__label__generative_models Compared with SOTA training-free structured pruning works that can generate smaller networks, our method demonstrates superior performance across standard benchmarks.
__label__natural_language_processing It covers pretraining, supervised fine-tuning, and affects the starting conditions for reinforcement learning from human feedback (RLHF).
__label__machine_vision Specifically, we introduce an In-context Interaction module to complement in-context information and produce correlations between the target image and the in-context example and a Matching Transformer that uses fixed matching and a Hungarian algorithm to eliminate differences between different tasks.
__label__bandits We demonstrate that our algorithm achieves a smaller cumulative regret compared to the existing ReLU-based neural bandit algorithms.
__label__machine_learning_for_other_sciences_and_fields Anomalies exhibit association descending patterns, a key phenomenon we exclusively observe and attribute to the disruptive nature of anomalies detaching anomalous features from others.
__label__robotics To improve perception robustness, we leverage the recent advances in automotive radars and introduce a novel approach that utilizes 4D imaging radar sensors for 3D occupancy prediction.
__label__optimization The goal of the CSS problem is to output a submatrix S, consisting of k columns from an n×d input matrix A that minimizes the residual error ‖A-SS^\dagger A‖_F^2, where S^\dagger is the Moore-Penrose inverse matrix of S. Many previous approximation algorithms have non-linear running times in both n and d, while the existing linear-time algorithms have a relatively larger approximation ratios.
__label__generative_models In this work, we propose a training-free method to inject visual prompts into Multimodal Large Language Models (MLLMs) through learnable latent variable optimization.
__label__neuroscience_and_cognitive_science However, the research on fMRI-to-video reconstruction remains limited since decoding the spatiotemporal perception of continuous visual experiences is formidably challenging.
__label__natural_language_processing Experimental results show that our methods can effectively extend the context window length.
__label__diffusion_based_models These insights allow us to propose an unsupervised, single-step, training-free **LO**w-rank **CO**ntrollable image editing (LOCO Edit) method for precise local editing in diffusion models.
__label__natural_language_processing We perform supervised fine-tuning (SFT) and further reinforcement learning (RL) on both success and failure trajectories with a novel reward design considering code explanation and refinement quality.
__label__reinforcement_learning Extensive experiments are conducted in navigation, manipulation, and locomotion, verifying DRAIL’s effectiveness compared to prior imitation learning methods.
__label__natural_language_processing Second, we propose an exponential moving average-based coefficient learning method to strengthen our higher-order predictor.
__label__graph_neural_networks UGC integrates node attributes and adjacency information, leveraging the dataset's heterophily factor.
__label__graph_neural_networks However, it is the most vulnerable, often suffering significant performance degradation with imbalanced or biased data schema, thus raising concerns about its accuracy and reliability in on-device deployment.
__label__graph_neural_networks Graph neural networks (GNNs) with a rescale invariance, such as GATs, can be re-parameterized during optimization through dynamic rescaling of network parameters and gradients while keeping the loss invariant.
__label__machine_vision Our code is available at https://github.com/GATECH-EIC/Fragment-Pruning.
__label__generative_models DGS optimizes time-varying warping functions to transform Gaussian surfels (surface elements) from a static state to a dynamically warped state.
__label__causal_inference The main difficulty lies in adapting these constraints, typically suited for the space of total orderings, to the continuous optimization context of structure learning in the graph space.
__label__reinforcement_learning However, NLs descriptions are not always readily available and are expensive to collect.
__label__machine_vision This implicit mutual guidance ensures robustness and accuracy in both synthetic and real-world scenarios.
__label__machine_vision Extensive experiments demonstrate that our method achieves state-of-the-art results on multiple corruption benchmarks.
__label__causal_inference To bridge this gap, this paper formalizes a set of equivalent constraints that map partial orders onto graph spaces and introduces a plug-and-play module for their efficient application.
__label__deep_learning_architectures Existing approaches, which integrates knowledge distillation into domain adaptation frameworks to simultaneously address domain shift and model complexity, often neglect network capacity gap between teacher and student and just coarsely align their outputs over all source and target samples, resulting in poor distillation efficiency.
__label__human-AI_interaction Given these characteristics, we explore the problem of how developers of new generative AI software can release and price their technology.
__label__algorithmic_game_theory A key innovation from the standpoint of social choice is that our problem has a *linear* structure, which greatly restricts the space of feasible rules and leads to a new paradigm that we call *linear social choice*.
__label__reinforcement_learning Surprisingly, despite methods aiming to maximise regret in theory, the practical approximations do not correlate with regret but with success rate.
__label__bandits In this paper, we designed efficient algorithms for the problem of regret minimization in assortment selection with \emph{Plackett Luce} (PL) based user choices.
__label__interpretability_and_explainability As we experimentally show, the proposed construction not only outperforms recent CBM approaches, but also yields a principled framework towards interpetability.
__label__fairness Such discrimination is known to be exacerbated when data is equipped with pairwise relationships encoded in a graph.
__label__online_learning Experiments on both real and synthetic datasets corroborate that the proposed approach consistently yields more efficient prediction sets while maintaining valid coverage, outperforming alternative methods.
__label__interpretability_and_explainability Although it is known that transformer language models (LMs) pass features from early layers to later layers, it is not well understood how this information is represented and routed by the model.
__label__diffusion_based_models Previous work for stereotype mitigation mainly concentrated on mitigating stereotypes engendered with individual objects within images, which failed to address stereotypes engendered by the association of multiple objects, referred to as *Association-Engendered Stereotypes*.
__label__learning_theory In this paper, we explore prediction risk as well as estimation risk under more general regression error assumptions, highlighting the benefits of overparameterization in a more realistic setting that allows for clustered or serial dependence.
__label__causal_inference We also explore the impact of injecting structural prior knowledge (counterfactual Markov boundaries) on the results.
__label__reinforcement_learning Additionally, it is often infeasible to collect feedback for every trajectory generated by the agent, hence, two fundamental questions arise: (1) Which trajectories should be presented to the human?
__label__natural_language_processing We rely on a theoretical signal propagation analysis---specifically, we analyse the representations of the last token in the final layer of the Transformer, as this is the representation used for next-token prediction.
__label__machine_learning_for_other_sciences_and_fields In particular, G3 consists of three steps, i.e., **G**eo-alignment, **G**eo-diversification, and **G**eo-verification to optimize both retrieval and generation phases of worldwide geolocalization.
__label__optimization_for_deep_networks If the dataset is linearly separable and the derivative of the activation function is bounded away from zero, we show that the average empirical risk decreases, implying that the first phase must stop in finite steps.
__label__privacy Machine unlearning (MU) has emerged to enhance the privacy and trustworthiness of deep neural networks.
__label__machine_learning_for_social_sciences However, as the network size increases, the problem becomes computationally intractable.
__label__machine_vision In addition, we design language hierarchical prompt generation that introduces language hierarchy into prompt generation which helps bridge the vocabulary gaps between training and testing.
__label__safety_in_machine_learning In its essence, we need to solve a problem of  statistical estimation which is usually very time-consuming since we need to perform numerous (usually $10^5$) forward passes of the classifier for every point to be certified.
__label__safety_in_machine_learning Datasets and models can be found in https://github.com/GuanlinLee/ART.
__label__machine_learning_for_physical_sciences We demonstrate the ability of our method to find feasible transition paths on real-world molecular simulation and protein folding tasks.
__label__deep_learning_architectures Compared to the clock-driven synchronous chip, the event-driven asynchronous chip achieves much lower energy consumption but only supports some specific network operations.
__label__infrastructure We also delineate the space of models for which exponential communication advantages hold by showing that they cannot hold for linear classification.
__label__generative_models For validation, we consider several low-dimensional scenarios and image-space setups, including *non-Euclidean* cost functions.
__label__machine_vision Focused on enhancing expressiveness, our work makes three key contributions.
__label__machine_vision HisTPT introduces three types of knowledge banks, namely, local knowledge bank, hard-sample knowledge bank, and global knowledge bank, each of which works with different mechanisms for effective knowledge memorization and test-time prompt optimization.
__label__generative_models A user study conducted on the outcomes of YouDream demonstrates the preference of the animal models generated by our method over others.
__label__reinforcement_learning Addressing the limitations of traditional Chain of Thought (CoT) methods, we introduce the Chain of Summarization (CoS) method, enhancing LLMs' capabilities in rapid and effective decision-making.
__label__learning_theory While Tensor-based Multi-view Subspace Clustering (TMSC) has garnered significant attention for its capacity to effectively capture high-order correlations among multiple views, three notable limitations in current TMSC methods necessitate consideration: 1) high computational complexity and reliance on dictionary completeness resulting from using observed data as the dictionary, 2) inaccurate subspace representation stemming from the oversight of local geometric information and 3) under-penalization of noise-related singular values within tensor data caused by treating all singular values equally.
__label__machine_vision Evaluation on synthetic and real-world datasets demonstrates competitive or superior performance of our method in few-shot novel view synthesis compared to existing state-of-the-art methods.
__label__probabilistic_methods The model is regularized by appropriate priors on the unitary transformations, posterior summaries of which may then be suitably interpreted as optimal data-driven rotations of a fixed orthonormal basis for the Hilbert space.
__label__machine_learning_for_other_sciences_and_fields We empirically evaluate the performance of SPEAC in a case study for the UCLID5 formal verification language and find that, compared to existing retrieval and fine-tuning baselines, SPEAC produces syntactically correct programs more frequently and without sacrificing semantic correctness.
__label__natural_language_processing Additionally, to correct erroneous operations, the reflection agent observes the outcomes of each operation and handles any mistake accordingly.
__label__machine_vision In this paper, we push the task setup to its limits by exploring the potential of using solely 2D images to learn OV-3Det.
__label__probabilistic_methods However, optimizing a black-box which is also a function of time (*i.e.
__label__neuroscience_and_cognitive_science To address this, we introduce a hybrid architecture that integrates the continuous-time recurrent dynamics of RNNs with the spatial processing capabilities of CNNs.
__label__natural_language_processing We overcome a number of technical challenges to realize this idea, such as automating data curation and using LLMs to impute missing information.
__label__optimization_for_deep_networks This lower bound represents the expected value of the log posterior distribution of the latent variables under a scaled, factorized variational distribution.
__label__fairness In this study, we explore the potential for achieving fairness without compromising its utility when no prior demographics are provided to the training set, namely _harmless Rawlsian fairness_.
__label__learning_theory Our work reveals the intricate interplay between data connectivity, training dynamics, and implicit regularization in matrix factorization models.
__label__privacy To the best of our knowledge, this is the first work examining the differentially private online prediction from experts in the federated setting.
__label__machine_vision However, existing methods struggle with the lack of sample diversity for minority classes and the limitation of suitable placement.
__label__machine_learning_for_physical_sciences Data-driven deep learning models are transforming global weather forecasting.
__label__interpretability_and_explainability On classification tasks, we show that the representations of VQShape can be utilized to build interpretable classifiers, achieving comparable performance to specialist models.
__label__safety_in_machine_learning This approach enables FDCR to handle backdoor attacks in heterogeneous federated learning environments.
__label__safety_in_machine_learning Still, the limited certified robustness that is currently achievable has been a bottleneck for their practical adoption.
__label__machine_learning_for_other_sciences_and_fields Our experimental results show that in video SCI our proposed solution achieves state-of-the-art among UNN methods, and in the case of noisy measurements, it even outperforms supervised solutions.
__label__diffusion_based_models Unlike existing single-model uncertainty methods like Monte-Carlo dropout and Bayesian neural networks, HyperDM offers prediction accuracy on par with, and in some cases superior to, multi-model ensembles.
__label__interpretability_and_explainability 2024).
__label__probabilistic_methods To address such discontinuities, Aldous introduced the extended weak convergence, which can fully characterise all essential properties, including the filtration, of stochastic processes; however, it was considered to be hard to find efficient numerical implementations.
__label__interpretability_and_explainability Our code is available at https://github.com/hyhuang00/ParamRepulsor.
__label__other More relaxed conditions are also provided via adding reasonable structural constraints, motivated by available side information in various applications.
"__label__natural_language_processing Unlike prior work that relies on seed examples or existing datasets to construct instruction tuning data, 
GLAN exclusively utilizes a pre-curated taxonomy of human knowledge and capabilities as input and generates large-scale 
synthetic instruction data across all disciplines."
__label__machine_vision We observe that, with the cross-entropy loss, model predictions are optimized to align with the corresponding labels via increasing logit magnitude or refining logit direction.
__label__safety_in_machine_learning On more than 20 datasets, our empirical results show that RashomonGB outperforms existing baselines in terms of improving the estimation of predictive multiplicity metrics and model selection with group fairness constraints.
__label__machine_vision Designed in two stages, the first stage focuses on object swapping in a single frame with HOI awareness; the model learns to adjust the interaction patterns, such as the hand grasp, based on changes in the object's properties.
__label__machine_vision Moreover, we show the potential of aTLAS as a parameter-efficient fine-tuning method, particularly with less data, and demonstrate that it can be easily scaled up for higher performance.
__label__natural_language_processing While large language models (LLMs) offer promise in automating this process, challenges such as context window limitations, parametric knowledge constraints, and the lack of evaluation benchmarks remain.
__label__algorithmic_game_theory The collaboration is organized by a benevolent aggregator who gathers samples so as to maximize total welfare, but is unaware of data quality.
__label__neuroscience_and_cognitive_science Learning progress, typically measured as the observed change in performance, can provide a valuable signal for goal selection in both humans and artificial agents.
__label__generative_models We apply an effective training recipe that improves the performance of masked diffusion models and derive a simplified, Rao-Blackwellized objective that results in additional improvements.
__label__natural_language_processing Moreover, due to the scarcity of high-quality evaluation data, LLMs exhibit deficiencies in their evaluation capabilities.
__label__generative_models One interpretation of ICL assumes that the CGM computes the posterior predictive of an unknown Bayesian model, which implicitly defines a joint distribution over observable datasets and latent mechanisms.
__label__graph_neural_networks Empirical evaluation is conducted on the challenging CLRS Algorithmic Reasoning Benchmark, which consists of 30 diverse algorithmic tasks.
__label__optimization Our theoretical analysis proves that CRONOS converges to the global minimum of the convex reformulation under mild assumptions.
__label__evaluation Animals are an integral part of the natural world, and animal-centric video understanding is crucial for animal welfare and conservation efforts.
__label__machine_vision To overcome these challenges, we propose to construct consistent flat loss regions and enhance knowledge exploitation for each modality via cross-modal knowledge transfer.
__label__optimization The problem pertains to the performative prediction setting in which a trained model can affect the outcome estimated by the model.
__label__machine_learning_for_physical_sciences Nature Communications 2024], to $\mathcal{O}(\log 𝑛)$ samples when the geometry of the $n$-qubit system is known.
__label__diffusion_based_models Furthermore, our models vastly outperform previous discrete diffusion models on pixel-level image modeling, achieving 2.75 (CIFAR-10) and 3.40 (ImageNet 64x64) bits per dimension that are better than autoregressive models of similar sizes.
__label__robotics Skill chaining offers a feasible solution for these tasks by pre-training the skills for each sub-task and linking them sequentially.
__label__reinforcement_learning Offline reinforcement learning endeavors to leverage offline datasets to craft effective agent policy without online interaction, which imposes proper conservative constraints with the support of behavior policies to tackle the out-of-distribution problem.
__label__other Our analysis extends existing theories from bounding generalized prediction loss (on unseen data) with loss sharpness to bounding the worst-case generalized surrogate sharpness with its empirical estimate on training data, providing a new perspective on sharpness regularization.
__label__natural_language_processing However, existing LLMs, pre-trained on sequences with a restricted maximum length, cannot process longer sequences due to the out-of-domain and distraction issues.
__label__online_learning In the case that $V_T$ is unknown, we run multiple FTPL-D with different restarting parameters as experts and use a meta-algorithm to track the best one on the fly.
__label__machine_vision However, prevailing works typically assume that such datasets are primarily affected by closed-set noise (where the true/clean labels of noisy samples come from another known category), and ignore therefore the ubiquitous presence of open-set noise (where the true/clean labels of noisy samples may not belong to any known category).
__label__safety_in_machine_learning To further verify this finding, we empirically show that these dormant backdoors can be easily re-activated during inference stage, by manipulating the original trigger with well-designed tiny perturbation using universal adversarial attack.
__label__optimization We also evaluate the practical performance of our algorithm by comparing it to existing second-order algorithms for minimax optimization.
__label__machine_learning_for_physical_sciences GenMS additionally uses a graph neural network to predict properties (e.g., formation energy) from the generated crystal structures.
__label__generative_models Finally, we conduct illustrative experiments on both synthetic datasets and on CIFAR10 showing that such a procedure amplifies biases of the reward model.
__label__machine_vision Through extensive experiments, our model demonstrated a significant improvement over the state-of-the-art models on the Pascal-Part-116, ADE20K-Part-234, and PartImageNet datasets.
__label__reinforcement_learning These retrievable skills are incrementally learned on their corresponding adapters.
__label__privacy The code is publicly available at \href{https://github.com/UCF-Lou-Lab-PET/Private-Data-Prune}.
__label__natural_language_processing To this end, we propose a low-cost but effective method, namely Prompt Bias Calibration (PBC), to estimate the \emph{prompt-template bias} term during reward modeling, which can be utilized to calibrate reward scores in the following RL fine-tuning process.
__label__robotics Unlike previous work, we introduce FACTORSIM that generates full simulations in code from language input that can be used to train agents.
__label__diffusion_based_models DistDiff constructs hierarchical prototypes to approximate the real data distribution, optimizing latent data points within diffusion models with hierarchical energy guidance.
__label__diffusion_based_models We provide a solid theoretical basis to justify the linearity and low-rankness in the PMP.
__label__machine_learning_for_other_sciences_and_fields In this work, we seek to narrow this gap and present TABULA-8B, a language model for tabular prediction.
__label__machine_vision Recently, 3D Gaussian Splatting (3DGS) has emerged as a promising alternative to NeRF, offering superior training and inference efficiency along with better rendering quality.
__label__probabilistic_methods To bridge this gap, in this paper, we propose a novel probabilistic mixture model for anomaly detection to establish a theoretical connection among representation learning, clustering, and anomaly detection.
__label__machine_learning_for_other_sciences_and_fields However, we find that the residual term is more crucial for getting accurate fillings, since it is more related to the diverse changes of data and the biggest component of imputation errors.
__label__online_learning [2023] study similar problems in stochastic multi-agent multi-armed bandits and show that $\sqrt{T}$-regret is possible after $T$ rounds, their fairness measure is the product of all agents' rewards, instead of their NSW (that is, their geometric mean).
__label__generative_models However, these 2D tokenizations face challenges in managing the inherent redundancies present in images, where adjacent regions frequently display similarities.
__label__learning_theory (2022).
__label__learning_theory For independent and identically distributed observations, we provide a concentration result for the estimation error and an upper bound for its expectation.
__label__generative_models We represent the multimodal inconsistency query information to detect specific hallucinations in 3D content, using this as an enhanced prompt to re-consist the 2D renderings of 3D and jointly optimize the structure and appearance across different views.
__label__neuroscience_and_cognitive_science One widely-adopted example is task-optimized recurrent neural networks (RNNs), which have been used to generate hypotheses about how the brain’s neural dynamics may organize to accomplish tasks.
__label__optimization We further provide clear geometric interpretations and heuristics for the choice of parameters.
__label__machine_learning_for_other_sciences_and_fields FPS is evaluated via two important but challenging applications, intelligent tutoring systems and a healthcare application for sepsis treatment and intervention.
__label__machine_vision This decoder ensures linear complexity while maximizing the perception of point cloud geometry information from unmasked patches with higher information density.
__label__machine_learning_for_physical_sciences AL-RNNs can be efficiently trained with any SOTA algorithm for dynamical systems reconstruction (DSR), and naturally give rise to a symbolic encoding of the underlying DS that provably preserves important topological properties.
__label__optimization Second, the model is input-convex in the constraint bounds, which not only matches the structure of the GVF but also enables the trained model to be efficiently optimized over using LP.
__label__causal_inference In this paper, we focus on permutation-based methods for learning causal graphs in Linear Gaussian Acyclic Models (LiGAMs), where the permutation encodes a causal ordering of the variables.
__label__graph_neural_networks It consistently achieves the best separation of in-distribution and out-of-distribution data on 6 out of 7 anomaly types while having the best average rank over shifts on *all* datasets.
__label__learning_theory The current paper takes a step towards its resolution, by establishing formal gaps between real and complex diagonal SSMs.
__label__optimization_for_deep_networks developed influential scaling laws for the optimal model size as a function of the compute budget, but these laws yield substantially different predictions.
__label__reinforcement_learning STAR leverages state abstraction to distill complex, potentially continuous problems into compact, discrete models which we call *abstract reward processes* (ARPs).
__label__machine_vision The encoder predicts keypoints and the decoder utilizes the generated keypoints to reconstruct the objects.
__label__reinforcement_learning To the best of our knowledge, this is the **first** work that achieves **near-optimal** dynamic regret for adversarial linear mixture MDPs with the unknown transition without prior knowledge of the non-stationarity measure.
__label__machine_learning_for_healthcare This optimal warp is then used to minimize image and label alignment errors.
__label__machine_vision Extensive experiments on benchmarks from different DG tasks demonstrate that LFME is consistently beneficial to the baseline and can achieve comparable performance to existing arts.
__label__machine_vision Video causal reasoning aims to achieve a high-level understanding of video content from a causal perspective.
__label__machine_vision Night-to-Day translation (Night2Day) aims to achieve day-like nighttime vision.
__label__robotics These features exhibit both intra-object consistency and inter-object distinction.
__label__interpretability_and_explainability We use a constraint-based formulation which views a visual question as having a set of visual or textual constraints that the model's generated answer must satisfy to be correct (e.g.
__label__generative_models In this paper, we present Lumina-Next, an improved version of Lumina-T2X, showcasing stronger generation performance with increased training and inference efficiency.
__label__safety_in_machine_learning Recently, various new methods have been proposed to adapt closed LLMs to private data without leaking private information to third parties and/or the LLM provider.
__label__machine_vision The first component performs base rate adjustment to strengthen the prior belief corresponding to the knowledge gained through pre-training, making the model more confident in its predictions; the second component builds an evidential ensemble that leverages belief regularization to ensure diversity among different ensemble components.
__label__interpretability_and_explainability We show that, for data with *underlying community structure*, PCA significantly reduces the distance of data points belonging to the same community while reducing inter-community distance relatively mildly.
__label__deep_learning_architectures To solve this problem, we propose a dual-network architecture that unifies the training-time and test-time objectives.
__label__reinforcement_learning We evaluated RESeL in 18 POMDP tasks, including classic, meta-RL, and credit assignment scenarios, as well as five MDP locomotion tasks.
__label__machine_vision In this paper, we present PCoTTA, an innovative, pioneering framework for Continual Test-Time Adaptation (CoTTA) in multi-task point cloud understanding, enhancing the model's transferability towards the continually changing target domain.
__label__optimization This is confirmed in practice, with LoCoDL outperforming existing algorithms.
__label__machine_vision However, in practical domains such as medical applications, data availability is often limited.
__label__machine_vision It then uses this matrix to transfer useful statistics of VLMs from open-source datasets to the target dataset for bridging two gaps.
__label__machine_vision It comprises (i) a temporal, transformer-based architecture that, in addition to frame tokens, handles person-specific tokens capturing the gaze information related to each individual; (ii) a new dataset, VSGaze, built from multiple gaze following and social gaze datasets by extending and validating head detections and tracks, and unifying annotation types.
__label__natural_language_processing We also extend our method to non-linear editing using feature functions.
__label__graph_neural_networks This unification establishes a connection between relative encodings and temporal walk matrices, providing a more principled way for analyzing and designing relative encodings.
__label__algorithmic_game_theory This framework has been successfully applied recently to various mechanism design settings, where in most cases the mechanism is provided with a prediction about the types of the players.
__label__optimization_for_deep_networks Experiments on image classification and machine translation tasks show that our method is quite competitive compared to the state-of-the-art methods.
__label__reinforcement_learning Instead, a surrogate problem is solved where all objectives are combined with a weighted sum.
__label__optimization_for_deep_networks In this work, we propose a novel approach that relaxes the constraint imposed by regular structural pruning methods and eliminates the structural dependence along the embedding dimension.
__label__learning_theory We then show that the implicit bias of optimization in robust ERM can significantly affect the robustness of the model and identify two ways this can happen; either through the optimization algorithm or the architecture.
__label__natural_language_processing The code is available at [https://github.com/WooSunghyeon/dropbp](https://github.com/WooSunghyeon/dropbp).
__label__deep_learning_architectures While recent studies have experimentally demonstrated that the non-one-hot ECOCs with multi-bits error correction ability, could be a better solution, there is a notable absence of theoretical foundations that can elucidate the relationship between codeword design, weight-error magnitude, and network characteristics, so as to provide robustness guarantees.
__label__natural_language_processing While they deliver powerful performance, they also face a set of common optimization needs to meet specific requirements or standards, such as instruction following or avoiding the output of sensitive information from the real world.
__label__optimization In this paper, we model **client-selection** and **model-training** as two interconnected optimization problems, proposing a novel bilevel optimization problem for collaborative learning.
__label__machine_learning_for_other_sciences_and_fields To remedy hallucinations, we propose a feedback loop where validation is performed by providing the reasoning trace of vulnerabilities to the LLM for iterative self-reflection.
__label__machine_vision This task remains challenging due to the complexities in accurately modeling real haze distributions and the scarcity of paired real-world data.
__label__machine_learning_for_other_sciences_and_fields This necessitates FL-PT selection based on their data complementarity.
__label__reinforcement_learning We evaluate the effectiveness of our method through an extensive set of experiments covering diverse covariate shift scenarios.
__label__machine_vision This restricts the ability to access scenes from a variety of more diverse and potentially useful perspectives.
"__label__deep_learning_architectures Extensive experiments demonstrate that our proposed approach \ourmethodname achieves superior performance across multiple audio-visual tasks, 
including AVE, AVVP, AVS, and AVQA."
__label__deep_learning_architectures Finally, we empirically show that the dictionary outperforms the Max separation function, which had previously been argued to be optimal, and that performance can further be improved by replacing the Euclidean distance kernel by a Manhattan distance kernel.
__label__robotics Previous methods typically use a fixed-length and sufficiently long trajectory of an agent as observations to predict its future trajectory.
__label__machine_learning_for_other_sciences_and_fields Model-based optimization (MBO) has emerged as an effective method to design biological sequences in an automated manner, and has recently been used in promoter design methods.
__label__interpretability_and_explainability To achieve object-specific visual explanations, we refine the global visual explanation using the feature gradient of a target object.
__label__machine_vision Concretely, PTSA uses persistent homology to align the topological structures of the input and latent spaces, effectively preserving the structure information and improving the generalization performance of FR model.
__label__diffusion_based_models To address this problem, incorporating well-shot personal images as additional reference inputs may be a promising strategy.
__label__optimization_for_deep_networks Motivated by this, we propose **Direction-Aware SHrinking (DASH)**, a method aiming to mitigate plasticity loss by selectively forgetting memorized noise while preserving learned features.
__label__optimization_for_deep_networks To the best of our knowledge, we are the first to obtain generalization bound via minima stability in the non-interpolation case and the first to show ReLU NNs without regularization can achieve near-optimal rates in nonparametric regression.
__label__interpretability_and_explainability This transform is closely related to widely used game-theoretic notions of importance like the *Shapley* and *Bhanzaf value*, but it also captures crucial higher-order interactions.
__label__other Our findings reveal that high accuracy on benchmark-simulated datasets with PLs can misleadingly amplify the perceived effectiveness of some general techniques, which may improve representation learning but have limited impact on addressing the inherent challenges of PLs.
"__label__diffusion_based_models Third, we introduce a new training procedure that enables multi-step sampling in the student, and
addresses the training--inference input mismatch of previous work, by simulating inference-time generator samples during training."
__label__interpretability_and_explainability Our results reveal that LLMs generally exhibit patterns similar to humans, such as risk aversion and loss aversion, with a tendency to overweight small probabilities, but there are significant variations in the degree to which these behaviors are expressed across different LLMs.
__label__machine_vision Our results demonstrate consistent performance gains, underscoring the critical role of these additional tasks in fostering comprehensive intelligence in MLLMs.
__label__natural_language_processing Motivated by the observation that the adaptation of fully continuous methods has been an overarching trend in Deep Learning, we develop Mixture of Tokens (MoT), a simple, continuous architecture that is capable of scaling the number of parameters similarly to sparse MoE models.
__label__learning_theory First, algorithms are given to construct memorization networks for an i.i.d.
__label__machine_vision In summary, our research underscores the importance of human-like modeling and offers strategic insights for advancing intelligent visual target tracking.
__label__other In this work, we introduce a feature learning theory framework that provides a theoretical foundation for understanding the differences between multi-modal and single-modal contrastive learning.
__label__fairness We also show empirical effects across various fairness objectives in six key fairness datasets and two image classification tasks.
__label__deep_learning_architectures Our work focuses on the above questions, first identifying several quantitative metrics, such as the kurtosis over neuron activation norms, to measure OFs.
__label__machine_vision The project is publicly available at https://icdiff.github.io/.
__label__reinforcement_learning In particular, Unsupervised Environment Design (UED) methods have gained recent attention as their adaptive curricula promise to enable agents to be robust to in- and out-of-distribution tasks.
__label__deep_learning_architectures In complex real-world data sets, even defining what is out-of-distribution is not obvious.
"__label__diffusion_based_models The main contributions of our work are the following: 
we present systematic experimental study of these points, 
we propose a novel conditioning mechanism that disentangles semantic and low-level conditioning, 
we obtain state-of-the-art performance  on CC12M for text-to-image at 512 resolution."
__label__diffusion_based_models To numerically simulate the sampling process, a discretisation schedule from the reference back towards clean data must be chosen.
__label__natural_language_processing In this work, we argue that the SFT stage significantly benefits from learning a reward model as well.
__label__natural_language_processing It proves difficult to encode various semantic contents, like honesty and safety, into a singular semantic feature, restricting its practicality.
"__label__graph_neural_networks We argue that 
the graph heterogeneity reflected on node types, node attributes, and  neighborhood structures can 
impose significant challenges for generalizing 
LDL onto graphs."
__label__generative_models 128 frames).
__label__generative_models Code is available at: https://github.com/YangLing0818/VideoTetris
__label__machine_learning_for_physical_sciences On various complicated photonic device benchmarks, we demonstrate one sole PACE model is capable of achieving 73% lower error with 50% fewer parameters compared with various recent ML for PDE solvers.
__label__natural_language_processing Finally, we utilize knowledge circuits to analyze and interpret language model behaviors such as hallucinations and in-context learning.
__label__privacy However, when being applied to Transformers, existing approaches based on secure two-party computation (2PC) bring about efficiency limitations in two folds: (1) resource-intensive matrix multiplications in linear layers, and (2) complex non-linear activation functions like $\mathsf{GELU}$ and $\mathsf{Softmax}$.
__label__deep_learning_architectures In this study, we extend the method for *vector-valued joint-group-equivariant* feature maps, so to cover such real networks.
__label__bandits First, to quantify the impact of preferences, we derive a novel lower bound on the sample complexity for identifying the most preferred arm with confidence level $1-\delta$.
__label__causal_inference We show that even when the causal model and the mixing function are both linear, there exists a surrounded-node ambiguity (SNA) [Varici et al.
__label__machine_vision \!Then, to mitigate the negative impact of the domain-specific attributes, we devise a function to estimate and mitigate uncertainty in category prediction.
__label__algorithmic_game_theory Real-life contractual relations typically involve repeated interactions between the principal and agent, where, despite theoretical appeal, players rarely use complex dynamic strategies and instead manage uncertainty through learning algorithms.
__label__machine_learning_for_healthcare However, most existing machine learning methods for small molecule generation suffer from poor synthesizability of candidate compounds, making experimental validation difficult.
__label__machine_learning_for_other_sciences_and_fields Extensive evaluations in toy problems and scientific applications, such as therapeutic protein design and airfoil optimization, demonstrate PropEn's advantages over common baselines.
__label__diffusion_based_models Diffusion models have demonstrated significant promise in various generative tasks; however, they often struggle to satisfy challenging constraints.
__label__deep_learning_architectures Neural functional networks (NFNs) have recently gained significant attention due to their diverse applications, ranging from predicting network generalization and network editing to classifying implicit neural representation.
__label__natural_language_processing Personalization has emerged as a critical research area in modern intelligent systems, focusing on mining users' behavioral history and adapting to their preferences for delivering tailored experiences.
__label__generative_models While there exist encoders that achieve good results in 3D GAN inversion, they are predominantly built on EG3D, which specializes in synthesizing near-frontal views and is limiting in synthesizing comprehensive 3D scenes from diverse viewpoints.
__label__other Finally, we develop an efficient implementation that translates these computational savings into actual wall-clock speedup.
__label__machine_learning_for_healthcare Machine learning approaches struggle to estimate the complete tumor cell distribution due to a lack of appropriate training data.
__label__probabilistic_methods The framework depends on modeling the conditional distribution of inter-event times with a mixture of log-normals satisfying a Markov property and the conditional probability mass function for the marks with a Transformer-based architecture.
__label__deep_learning_architectures In this paper we explore the idea of editable INRs, and specifically focus on the widely used cropping operation.
__label__machine_learning_for_physical_sciences The task, however, is very challenging since there are time delays between extremes and their drivers, and the spatial response of such drivers is inhomogeneous.
__label__human-AI_interaction One way of improving the learning speed and performance of these agents is to leverage human guidance.
__label__generative_models Generating high-quality 3D assets from text and images has long been challenging, primarily due to the absence of scalable 3D representations capable of capturing intricate geometry distributions.
__label__learning_theory The second algorithm relaxes these sub-problems and provides a closed-form solution to each sub-problem for extremely fast estimation, altogether eliminating the need to solve SDPs.
__label__bandits In the first stage, we learn the induced subgraph on ancestors of the reward, along with a necessary and sufficient subset of latent confounders, to construct the set of possibly optimal arms.
__label__probabilistic_methods Previous work addresses this issue by employing approximate Bayesian estimation after the LLMs are trained, enabling them to quantify uncertainty.
__label__natural_language_processing These findings demonstrate an inherent advantage of distance-based communication goals, and contextualize previous empirical discoveries.
__label__machine_learning_for_healthcare Additional covariate data from the target population is used to model the sampling of individuals in the trial study.
__label__deep_learning_architectures To address this limitation, this paper proposes a new instance normalization solution, called frequency adaptive normalization (FAN), which extends instance normalization in handling both dynamic trend and seasonal patterns.
__label__online_learning Online continual learning, the process of training models on streaming data, has gained increasing attention in recent years.
__label__machine_vision Furthermore, we also re-evaluate strategies such as scaling up parameters and high-quality pre-trained data.
__label__learning_theory Regarding information structure, our regret results are obtained under two feedback models, respectively, where the algorithm accesses the external factor at the end of each round and at the end of a round only when a non-null action is executed.
__label__generative_models It uses a low-rank decomposition of the soft-prompt component encoded for each instance to achieve parameter efficiency.
__label__reinforcement_learning This generative model can be flexibly trained from any (human or neural policy) agent interaction data.
__label__safety_in_machine_learning Meanwhile, safeguards for these generative models are developed, to protect users' rights and safety, most of which are designed for large language models.
__label__safety_in_machine_learning However, in practice, it is difficult and costly to collect sufficiently diverse auxiliary outlier data.
__label__machine_learning_for_other_sciences_and_fields We significantly improve the alignment score from 0.474 to 0.940 and achieve an average reduction of 16% in wirelength.
__label__machine_vision Code and pre-trained models are available at: https://github.com/shallowdream204/DreamClear.
__label__machine_vision Extensive experiments demonstrate the superiority of our method, surpassing the state-of-the-art methods by up to 12.10% and 13.75% mIoU on S3DIS and ScanNet, respectively.
__label__neuroscience_and_cognitive_science The CTDS model defines separate latents for both cell types, and constrains the dynamics so that E (I) latents have a strictly positive (negative) effects on other latents.
"__label__human-AI_interaction Algorithmic indistinguishability yields a natural test for assessing whether experts incorporate this kind of ""side information"", and further provides a simple but principled method for selectively incorporating human feedback into algorithmic predictions."
__label__natural_language_processing Our experimental results show that only pretraining CSE-based LMs is enough to achieve superior performance than regularly pretrained-finetuned LMs on various downstream tasks, implying the prospects of domain-specific-pretraining-free language models.
__label__graph_neural_networks This paper pertains to an emerging machine learning paradigm: learning higher- order functions, i.e.
__label__machine_learning_for_healthcare Among them, MAIM can capture the interaction among modalities by learning the shared representation distribution of all modalities.
__label__machine_learning_for_other_sciences_and_fields Here, we explore the idea that underlying low-dimensional structure in high-dimensional data can be exploited to faithfully approximate MI in high-dimensional settings with realistic sample sizes.
__label__learning_theory In this paper, we focus on the PQC expressivity for general multivariate function classes.
__label__optimization_for_deep_networks As deep learning models continue to grow in size, the demand for scalable bi-level optimization has become increasingly critical.
__label__natural_language_processing Further training MAmmoTH2 on public instruction tuning datasets yields MAmmoTH2-Plus, achieving state-of-the-art performance on several reasoning and chatbot benchmarks.
__label__graph_neural_networks Additionally, we design a novel objective that aligns the representations from different expert models to ensure reliable optimization.
__label__machine_vision The proposed LSS module, integrating parallel cascaded (Hybrid State Space) HSS blocks and multi-kernel convolutions operations, effectively captures both long-range and local information.
__label__safety_in_machine_learning Both automatic and human evaluations demonstrate that our attack method significantly improves the attack success rate and response quality while requiring less training data.
__label__deep_learning_architectures In this paper, we reveal that the powerful Mamba model shares surprising similarities with linear attention Transformer, which typically underperform conventional Transformer in practice.
__label__other This high within-class similarity can be attributed to the fact that previous methods use samples from different classes to construct a single batch for batch normalization (BN) matching.
__label__probabilistic_methods However, unlike most state-of-the-art generative models, it is able to learn from a few compressed and noisy data samples and requires no optimization algorithm for solving the inverse problem.
__label__machine_vision Through extensive experimentation, we demonstrate that the proposed method significantly reduces computational costs by nearly two-thirds, while simultaneously outperforming baseline methods.
__label__deep_learning_architectures On clock-driven synchronous chips, employing shorter time steps can enhance energy efficiency but reduce SNN performance.
__label__probabilistic_methods Equivariant deep learning architectures exploit symmetries in learning problems to improve the sample efficiency of neural-network-based models and their ability to generalise.
__label__machine_vision Subsequently, semantic classes are adaptively assigned using nearest neighbor search based on the learned locations.
__label__diffusion_based_models For backward computation, we leverage the low-rank structure within the gradient computation of DiTs training for possible algorithmic speedup.
__label__machine_vision To this end, we leverage the implicit sewing patterns (ISP) model for garment modeling and extend it by adding a diffusion-based deformation prior to represent these shapes.
__label__machine_vision Extensive experiments on the nuScenes dataset demonstrate that our DH-Fusion method surpasses previous state-of-the-art methods w.r.t.
__label__natural_language_processing Similarly, steering the model towards complex generations improves complexity by $21.6$\% over the baseline.
__label__generative_models Recent research has attempted to interpret their behaviors through the lens of inner representation.
__label__natural_language_processing Recent advances in large language models (LLMs) have empowered AI agents capable of performing various sequential decision-making tasks.
__label__natural_language_processing Through a series of controlled experiments with increasing levels of realism, including non-reciprocal relations, we find that reliable information retrieval is an inherent failure of the next-token prediction objective used in popular large language models.
__label__graph_neural_networks Code is available at https://github.com/LUMIA-Group/Cluster-wise-Graph-Transformer.
__label__natural_language_processing We construct DHA models by transforming various scales of MHA checkpoints given target head budgets.
__label__safety_in_machine_learning Empirical results on various heterogeneous federated scenarios under backdoor attacks demonstrate the effectiveness of our method.
__label__machine_vision Moreover, to avoid model collapse, we design two complementary branches of DiT decoders for enhancing the use of noisy patches and mitigating excessive reliance on clean patches in reconstruction.
__label__reinforcement_learning The constrained Markov decision process (CMDP) framework emerges as an important reinforcement learning approach for imposing safety or other critical objectives while maximizing cumulative reward.
__label__machine_learning_for_physical_sciences On WeatherBench 2, Stormer performs competitively at short to medium-range forecasts and outperforms current methods beyond 7 days, while requiring orders-of-magnitude less training data and compute.
__label__causal_inference Existing methods frequently assume causal sufficiency, disregarding the presence of unobserved confounding variables.
__label__machine_learning_for_healthcare Multimodal physiological signals, such as EEG, EOG and EMG, provide rich and reliable physiological information for automated sleep staging (ASS).
__label__deep_learning_architectures Code is available at this repository: https://github.com/thuml/AutoTimes.
__label__robotics We show that FACTORSIM outperforms existing methods in generating simulations regarding prompt alignment (i.e., accuracy), zero-shot transfer abilities, and human evaluation.
__label__machine_learning_for_other_sciences_and_fields Our approach adapts models originally designed for able-bodied individuals to forecast joint motion in limb-impaired patients without altering model parameters.
__label__safety_in_machine_learning Experimental results on two aligned LLMs (LLaMA-2-7B-Chat and Vicuna-7B-V1.5) and six types of jailbreak attacks (GCG, AutoDAN, PAIR, TAP, Base64, and LRL) show that Gradient Cuff can significantly improve the LLM's rejection capability for malicious jailbreak queries, while maintaining the model's performance for benign user queries by adjusting the detection threshold.
__label__natural_language_processing The code is open-sourced at https://github.com/X-PLUG/MobileAgent.
__label__deep_learning_architectures TPGN incorporates two branches to comprehensively capture the semantic information of time series.
__label__reinforcement_learning Notably, HPGD uses stochastic hypergradient estimates, based on observations of the followers’ trajectories.
__label__natural_language_processing In this work, we propose Graph Uncertainty -- which represents the relationship between LLM generations and claims within them as a bipartite graph and estimates the claim-level uncertainty with a family of graph centrality metrics.
__label__deep_learning_architectures Finally, we show how each method can be extended to three- instead of two-class rounding.
__label__diffusion_based_models We employ an adversarial optimization algorithm to produce the optimal hiding prompt guiding signal for each watermark.
__label__optimization BAdam offers a memory efficient approach to the full parameter finetuning of large language models.
__label__optimization To our knowledge, this is the first learning-based model to reach such a performance.
__label__machine_vision To address the challenges of leveraging masks without semantic labels, we devise an online clustering algorithm using learnable class names to acquire general semantic concepts.
__label__diffusion_based_models Additionally, we estimate the state manifold of diffusion model to allow for early termination when the sample starts to wander away from the state manifold at each diffusion step.
__label__neuroscience_and_cognitive_science PAM is a streaming model that learns a sequence in an online, continuous manner by observing each input only once.
__label__interpretability_and_explainability Our approach divides the state space into safe and unsafe regions upon convergence, providing clear insights into the policy's weaknesses.
__label__robotics STP adheres to two key designs in a multi-task learning manner.
__label__optimization We can verify that all of above complexity (nearly) matches the corresponding lower bounds.
__label__reinforcement_learning Then, we design an enhanced algorithm that leverages local information to enhance statistical efficiency.
__label__machine_vision On the other hand, lexical representation, a vector whose element represents the similarity between the sample and a word from the vocabulary, is a natural sparse representation and interpretable, providing exact matches for individual words.
__label__machine_vision To this end, we propose an $\underline{O}$riented $\underline{P}$rogressive $\underline{R}$egularizor (OPR) to establish the constraints that compel the distribution of anchors to be discretely arranged.
__label__reinforcement_learning We conduct a novel theoretical analysis to demonstrate CODA's capability to solve CGO problems in the offline data setup.
__label__learning_theory We prove that it is NP-complete in general, and for random instances, we show that on the order of $N\ln(N)$ samples, implying very sparse interactions, suffice to identify the partition.
__label__machine_vision Based on the VSS blocks, we develop a family of VMamba architectures and accelerate them through a succession of architectural and implementation enhancements.
__label__machine_vision By greatly scaling up sizes of data and model parameters, the large models learn deep priors which lead to remarkable performance in various tasks.
__label__natural_language_processing Moreover, we tackle the issue of adapting D-LLMs to real-world applications, specifically concerning the missing KV-cache when layers are skipped.
__label__natural_language_processing While previous research has advocated for constraining policy optimization, our study introduces a novel approach to enhance the reward model's generalization ability against distribution shifts by regularizing the hidden states.
__label__optimization Further, we outline extensions to other matroid oracle types, non-free dirty oracles and other matroid problems.
__label__reinforcement_learning An effective data sampler assigns proper sampling probability for training data and helps the model converge to a good local minimum with high performance.
__label__machine_vision The proposed temporal radial basis function Gaussian residual utilizes Gaussian parameter interpolation over time, enabling smooth parameter transitions and capturing temporal residuals of Gaussian distributions.
__label__generative_models In this work, we introduce Spatial Region GPT (SpatialRGPT) to enhance VLMs’ spatial perception and reasoning capabilities.
__label__learning_theory In particular, the Vision Transformer (ViT) has brought revolutionary changes to the field of vision, achieving significant accomplishments on the experimental side.
__label__neuroscience_and_cognitive_science We also show that by training on multiple animals, we can improve the generalization ability of the model to unseen animals, paving the way for a foundation model of the brain at single-cell, single-spike resolution.
__label__optimization Finally, our learning method is unsupervised, meaning that training data generation does not require computing LP optimal values, which can be prohibitively expensive at large scales.
__label__diffusion_based_models Finally, extensive empirical experiments demonstrate the effectiveness and efficiency of LOCO Edit.
__label__deep_learning_architectures The key feature of ELCD is a parametrization of the extended linearization of the nonlinear vector field.
__label__other We conduct comprehensive experiments on both synthetic and real-world datasets.
__label__learning_theory As a result, we formally characterise the families of preferences and utility functions that MORL should focus on: those for which an optimal policy is guaranteed to exist.
__label__deep_learning_architectures DOFEN surpasses other DNNs on tabular data, achieving state-of-the-art performance on the well-recognized benchmark: Tabular Benchmark, which includes 73 total datasets spanning a wide array of domains.
__label__online_learning Thanks to localization, L-ARC is demonstrated via experiments to produce prediction sets with risk guarantees across different data subpopulations, significantly improving the fairness of the calibrated model for tasks such as image segmentation and beam selection in wireless networks.
__label__machine_vision The current trend in computer vision is to utilize one universal model to address all various tasks.
__label__machine_vision We release our code and models to foster downstream research.
__label__machine_vision Project page: https://livescenes.github.io.
__label__deep_learning_architectures However, one of the challenges with deep Transformer models is the oversmoothing problem, where representations across layers converge to indistinguishable values, leading to significant performance degradation.
__label__machine_learning_for_other_sciences_and_fields In this paper, we introduce CodeRosetta, an encoder-decoder transformer model explicitly designed for translating between programming languages and also their HPC extensions.
"__label__reinforcement_learning Our work shows that Embodied RL agents can leverage unsupervised emergent abstractions to greatly improve their exploration skills in sparse reward settings, thus opening new research avenues between Embodied AI and Emergent
Communication."
__label__neuroscience_and_cognitive_science Additionally, via theoretical analyses we show how neuromodulatory gain scaling endows networks with gating mechanisms commonly found in artificial RNNs.
__label__natural_language_processing In contrast to the concat-and-chunk baseline, which incurs a fixed attention cost at every step of training, our proposed method incurs a computational cost proportional to the actual document lengths at each step, resulting in significant savings in training time.
__label__algorithmic_game_theory We study a practical matching problem that involves assigning children to daycare centers.
__label__natural_language_processing This method employs a meta-learning framework that optimizes token importance predictions, facilitating targeted knowledge updates and minimizing forgetting.
__label__optimization_for_deep_networks We perturb features learned from the adapter with the multiplicative noise and ensure the fine-tuned model remains consistent for same sample under different perturbations.
__label__diffusion_based_models Spatial grounding with bounding boxes has gained attention for its simplicity and versatility, allowing for enhanced user control in image generation.
__label__human-AI_interaction Classically, such assistance is studied through the lens of inverse reinforcement learning, where an assistive agent (e.g., a chatbot, a robot) infers a human's intention and then selects actions to help the human reach that goal.
__label__deep_learning_architectures In this study, we bridge the gap between these two techniques, proposing a simple but effective adaptation method based on Householder reflections.
__label__interpretability_and_explainability Interpreting the decisions of deep learning models, including audio classifiers, is crucial for ensuring the transparency and trustworthiness of this technology.
__label__natural_language_processing We discover that each attention head in the model consistently focuses on specific prompt attention features during generation.
__label__optimization_for_deep_networks Through extensive computer vision benchmarks, we demonstrate that our method can adjust to complex distribution shifts with significant improvements over current state-of-the-art in data-scarce settings.
__label__interpretability_and_explainability This enables obtaining expressive, discrete representations from unlabeled images.
__label__online_learning Using these norm pairs, we recover the diameter-to-strong-convexity ratio that predicts the same performance as KOMWU.
__label__natural_language_processing To tackle this problem, we introduce DeTikZify, a novel multimodal language model that automatically synthesizes scientific figures as semantics-preserving TikZ graphics programs based on sketches and existing figures.
__label__diffusion_based_models Both qualitative and quantitative comparisons demonstrate the superiority of LucidDrag over previous methods.
__label__diffusion_based_models In this work, we introduce Model-Based Diffusion (MBD), an optimization approach using the diffusion process to solve trajectory optimization (TO) problems without data.
__label__causal_inference In this paper, we acknowledge this issue and study the problem of causal disentangled representation learning from a combination of data gathered from various heterogeneous domains and assumptions in the form of a latent causal graph.
__label__machine_learning_for_other_sciences_and_fields Finally, KFNN determines the optimal neighborhood size by the max-margin learning.
__label__machine_learning_for_other_sciences_and_fields Following a four-stage training scheme, UniMoT emerges as a multi-modal generalist capable of performing both molecule-to-text and text-to-molecule tasks.
__label__machine_learning_for_other_sciences_and_fields Their primary objective is to efficiently support the insertion of new elements with assigned priorities and the extraction of the highest priority element.
__label__reinforcement_learning GCPO consists of (1) Pre-training from Demonstrations, which pre-trains the policy to possess an initial goal-achieving capability, thereby diminishing the difficulty of subsequent online learning.
__label__natural_language_processing We show reasoning improves across repeated iterations of this scheme.
__label__robotics In this paper, we propose EfficientCAPER, an end-to-end Category-level Articulated object Pose EstimatoR, eliminating the need for optimization functions as post-processing and utilizing the kinematic structure for joint-centric pose modeling, thus enhancing the efficiency and applicability.
__label__deep_learning_architectures Key-value (KV) caching plays an essential role in accelerating decoding for transformer-based autoregressive large language models (LLMs).
"__label__learning_theory NCP can be used to build conditional confidence regions and extract key statistics such as 
conditional quantiles, mean, and covariance."
__label__machine_vision In addition, KGD can extract both visual and textual KG independently, providing complementary vision and language knowledge for object localization and object classification in detection tasks over various downstream domains.
__label__robotics The integration of TorchRL with the LEGO hubs, via Bluetooth bidirectional communication, enables state-of-the-art reinforcement learning training on GPUs for a wide variety of LEGO builds.
__label__infrastructure However, existing FL frameworks suffer from efficacy deterioration due to the system heterogeneity inherent in edge computing, especially in the presence of domain shifts across local data.
__label__other We further propose to adjust the width based on the identified pattern and show that conventional layer width settings for CNNs could be adjusted to reduce the number of parameters while boosting the performance.
__label__machine_vision When trained on multiple datasets, RSA can serve as a general alignment module in zero-shot settings.
__label__safety_in_machine_learning This score rarely predicts well the probability of making a correct prediction and requires a post-processing calibration step.
__label__machine_learning_for_healthcare Recent advancements in protein language models (PLMs), trained on extensive datasets of amino acid sequences, have significantly improved our understanding of proteins.
__label__generative_models In contrast, in the field of artificial intelligence, controllable generation methods like ControlNet heavily rely on annotated training datasets such as depth maps, semantic segmentation maps, and poses, which limits the method’s scalability.
__label__machine_vision At the other end, recent vision-language foundation models such as CLIP demonstrate superior open-vocabulary recognition capability.
__label__machine_learning_for_healthcare Most multimodal deep learning approaches use modality-specific architectures that are often trained separately and cannot capture the crucial cross-modal information that motivates the integration of different data sources.
__label__safety_in_machine_learning The code are available in https://github.com/skJack/DiffusionFake.git.
__label__machine_vision Smoke segmentation is of great importance in precisely identifying the smoke location, enabling timely fire rescue and gas leak detection.
__label__optimization In the server time cost model, predictions themselves require server processing time and are scheduled on the same server as the jobs.
__label__generative_models The distribution of pedestrian trajectories is highly complex and influenced by the scene, nearby pedestrians, and subjective intentions.
__label__safety_in_machine_learning In the spatial domain, we disrupt the semantics of both the foreground and background in the image to confuse SAM.
"__label__optimization For instance, the logistic loss is
associated with the soft argmax link function."
__label__machine_vision We find that a simple self-supervised objective—next frame prediction—trained on video data, results in a set of latent tokens which bind to specific scene structures and track them as they move.
__label__machine_vision To address these issues, we propose SA3DIP, a novel method for Segmenting Any 3D Instances via exploiting potential 3D Priors.
__label__generative_models This issue becomes even more pronounced in complex domains, highlighting the need for improved PEFT approaches that can achieve better performance.
__label__reinforcement_learning Project page: https://nturobotlearninglab.github.io/DRAIL/
__label__other Foundation models have significantly enhanced 2D task performance, and recent works like Bridge3D have successfully applied these models to improve 3D scene understanding through knowledge distillation, marking considerable advancements.
__label__deep_learning_architectures Our framework provides a flexible mapping from the algebraic specification of a domain to an interpretation as orthogonal operators.
__label__machine_learning_for_other_sciences_and_fields However, the performance of a single LVLM is still limited by its intrinsic knowledge and reasoning capabilities.
__label__evaluation To solve this problem, we propose a simple yet effective method, _contrastive prototype-image adaptation_ (CoPA), to adapt different transformations for prototypes and images similarly to CLIP by treating prototypes as text prompts.
__label__optimization_for_deep_networks Learning Rate Warmup is a popular heuristic for training neural networks, especially at larger batch sizes, despite limited understanding of its benefits.
__label__probabilistic_methods Beyond estimating parameters of interest from data, one of the key goals of statistical inference is to properly quantify uncertainty in these estimates.
__label__bandits Buyers will purchase products only if the prices are below their valuations.
__label__generative_models (2) We conduct numerous experiments and report over one hundred experimental results to empirically summarize a unified accelerating strategy from the perspective of PDF.
__label__learning_theory We consider general non-linear relationships and identify a novel notion of complexity measures to establish an explicit Jackson-type approximation rate estimate for the Transformer.
__label__diffusion_based_models In each optimization, we allow the sample to take multiple steps along the gradient of the proxy constraint function until we can no longer trust the proxy, according to the variance at each diffusion level.
__label__learning_theory Finally, we present numerical results demonstrating the practical performance on challenging problems including the parametric diffusion, Navier-Stokes-Brinkman and Boussinesq PDEs.
__label__graph_neural_networks However, due to the inherent complex characteristics in graphs, GNNs encoders pre-trained on one dataset struggle to directly adapt to others that have different node feature shapes.
__label__generative_models It paves the way for real-time engagements with lifelike avatars that emulate human conversational behaviors.
__label__machine_vision To maintain the effectiveness of sparse tuning with low-rank matrices, we extend the low-rank decomposition by applying nonlinear kernel functions to the whole-matrix merging.
__label__generative_models Our main idea is to utilize segmented subjects generated by a foundation model for segmentation (Segment Anything) for both training and inference, as a form of data augmentation for training and initialization for the generation process.
__label__other We introduce SuperClass, a super simple classification method for vision-language pre-training on image-text data.
__label__safety_in_machine_learning A common approach to mitigate distribution shift is to regularize internal representations or predictors learned from in-distribution (ID) data to be domain invariant.
__label__optimization However, for almost surely unstable policy-dependent dynamics, the existence of the PSC solution will necessitate a temporally backward decaying of the distributional sensitivities.
__label__machine_vision We propose ''collision cross-entropy'' as a robust alternative to Shannon's cross-entropy (CE) loss when class labels are represented by soft categorical distributions y.
__label__safety_in_machine_learning To bridge this gap, we propose a novel foundation for coded computing, integrating the principles of learning theory, and developing a framework that seamlessly adapts with machine learning applications.
__label__machine_vision Consequently, it is intuitive to leverage the wealth of annotations in 2D images to alleviate the inherent data scarcity in OV-3Det.
__label__machine_vision Recognizing the limitations of current methods for estimating SMPL-X parameters from in-the-wild videos, we introduce a reconstruction module that significantly improves the image-model alignment.
__label__interpretability_and_explainability This tree is dynamically pruned and grown by querying the LLM using language templates, tailoring the explanation to the model.
__label__probabilistic_methods Because sampling is tractable, DPPs are natural candidates for subsampling tasks, such as minibatch selection or coreset construction.
__label__machine_vision The performance of 3D object detection in large outdoor point clouds deteriorates significantly in an unseen environment due to the inter-domain gap.
__label__diffusion_based_models Furthermore, we study the transition from finite to infinite step sampling, offering new insights into bridging the gap between discrete and continuous-time processes for discrete diffusion models.
__label__deep_learning_architectures We implement the proposed idea by exploiting the multi-grade deep learning (MGDL) model, a recently introduced model that trains a DNN incrementally, grade by grade, a current grade learning from the residue of the previous grade only an SNN (with trainable parameters) composed with the SNNs (with fixed parameters) trained in the preceding grades as features.
__label__causal_inference Yet, CARA designs often assume that primary outcomes are immediately observable, which is not the case in many clinical scenarios where there is a delay in observing primary outcomes.
__label__reinforcement_learning The behaviors are learned zero-shot without ground-truth rewards or expert demonstrations, and are qualitatively more natural according to human evaluation.
__label__reinforcement_learning To address this, we introduce Adam-Rel.
__label__learning_theory We focus on two frameworks: *PQ learning* [GKKM'20], allowing abstention on adversarially generated parts of the test distribution, and *TDS learning* [KSV'23], permitting abstention on the entire test distribution if distribution shift is detected.
__label__learning_theory This research reveals the unique computational abilities of ARDTs, aiming to broaden the architectural diversity in language model development.
__label__reinforcement_learning ``Distribution shift'' is the primary obstacle to the success of offline reinforcement learning.
__label__machine_vision Our method first decomposes a scene into $K$ blocks and then introduces the Alternating Direction Method of Multipliers (ADMM) into the training procedure of 3DGS.
__label__optimization_for_deep_networks However, the existing results on the optimization landscape apply under stylized settings where task and feature vectors are assumed to be IID and the attention weights are fully parameterized.
__label__graph_neural_networks To this end, we follow the idea of estimating the underlying density of the training data to decide whether a given input is close to the in-distribution (IND) data and adopt Energy-based models (EBMs) as density estimators.
__label__optimization To verify our PIP designs, we conduct extensive experiments on the highly challenging Traveling Salesman Problem with Time Window (TSPTW), and TSP with Draft Limit (TSPDL) variants under different constraint hardness levels.
__label__machine_vision Our code locates at \url{https://github.com/JiazuoYu/PathWeave}.
__label__natural_language_processing The models were able to align with human preferences on issues of safety, factualness, and bias concurrently.
__label__generative_models What may be an effective approach to consutrct a hybrid-DGM with theoretically-proven identifiability?
__label__machine_vision In this work, we propose Ref-MC2 that introduces the multi-time Monte Carlo sampling which comprehensively computes the environmental illumination and meanwhile considers the reflective light from object surfaces.
__label__machine_vision Unlike previous methods that model reference features in image space, \textit{Wild-GS} explicitly aligns the pixel appearance features to the corresponding local Gaussians by sampling the triplane extracted from the reference image.
__label__online_learning This provides the first order-optimal barely random algorithms for metrical task systems, i.e.
__label__neuroscience_and_cognitive_science Labels were assigned to continuous EEG according to the experimental design for EEG recording of several typical datasets, and then the decoding methods were conducted.
__label__diffusion_based_models The code will be open sourced upon acceptance of the paper.
__label__generative_models To answer this question, this paper introduces a novel perspective to understand, localize, and manipulate motion-aware features in video diffusion models.
__label__optimization In this paper, we introduce faster accelerated primal-dual algorithms for minimizing a convex function subject to strongly convex function constraints.
__label__interpretability_and_explainability Our dataset, source code, and pretrained weights: https://github.com/deep-real/DCP
__label__active_learning This paper investigates ML systems serving a group of users, with multiple models/services, each aimed at specializing to a sub-group of users.
__label__neuroscience_and_cognitive_science Animals survive in dynamic environments changing at arbitrary timescales, but such data distribution shifts are a challenge to neural networks.
__label__machine_learning_for_physical_sciences and applies to a property of interest known beforehand.
__label__generative_models Without any extra features, PARD achieves state-of-the-art performance on molecular and non-molecular datasets, and scales to large datasets like MOSES containing 1.9M molecules.
__label__optimization In minimax optimization, the extragradient (EG) method has been extensively studied because it outperforms the gradient descent-ascent method in convex-concave (C-C) problems.
__label__probabilistic_methods In this paper, we introduce a plug-and-play method that learns the covariance structure of errors over multiple steps for autoregressive models with Gaussian-distributed errors.
__label__interpretability_and_explainability We qualitatively and quantitatively evaluate the results of the learnt concepts.
__label__learning_theory For exact classification, we show that the separability threshold can be improved exponentially up to $O({\log{n}}/{\log\log{n}})$ corrected convolutions.
__label__learning_theory More precisely, we consider a non-linear self-attention layer with trainable tied and low-rank query and key matrices.
__label__optimization We address the problem of optimizing over functions defined on node subsets in a graph.
__label__deep_learning_architectures Furthermore,  the discrepancy of those frequency components between inputs and outputs is explicitly modeled as a prediction task with a simple MLP model.
__label__algorithmic_game_theory In a dual type of result, we show that the power of no-swap-regret dynamics comes at a cost of imposing a time-asymmetry on its inputs.
__label__optimization Extensive experimentation demonstrates that FUGAL consistently surpasses state-of-the-art graph alignment methods in accuracy across all benchmark datasets without encumbering efficiency.
__label__generative_models Specifically, using the same generator framework, TiTok attains **1.97** gFID, outperforming MaskGIT baseline significantly by 4.21 at ImageNet 256 × 256 benchmark.
__label__optimization Yet, these results still lack a solid theoretical understanding, and it is unclear whether they can be improved by leveraging connections to the wealth of work on sparse recovery algorithms.
__label__optimization To address such problems, in this work we focus on the deterministic concept of Order Oracle, which only utilizes order access between function values (possibly with some bounded noise), but without assuming access to their values.
__label__safety_in_machine_learning Language models have shown vulnerability against backdoor attacks, threatening the security of services based on them.
__label__reinforcement_learning Visualized learned reward functions of GAIL and DRAIL suggest that DRAIL can produce more robust and smoother rewards.
__label__active_learning Building on graph signal recovery theories and the random spectral sparsification technique, the proposed method adopts a two-stage biased sampling strategy that takes both informativeness and representativeness into consideration for node querying.
__label__machine_vision Experiments on synthetic and real-world datasets demonstrate that our method outperforms state-of-the-art approaches in accuracy and efficiency.
__label__reinforcement_learning While emergent communication literature shows success in developing various language properties, no research has shown the emergence of such positional references.
__label__machine_vision the agent must effectively leverage its sequentially observed aerial views when searching for the goal.
__label__probabilistic_methods We tackle this problem by introducing a novel functional prior for the flow, motivated by a decision-theoretic argument, and show empirically that the belief density can be inferred as the function-space maximum a posteriori estimate.
__label__algorithmic_game_theory Multi-team games, prevalent in robotics and resource management, involve team members striving for a joint best response against other teams.
__label__deep_learning_architectures Plugging CondTSF into previous dataset condensation methods facilitates a reduction in the distance between the predictions of the model trained with the full dataset and the model trained with the synthetic dataset, thereby enhancing performance.
__label__fairness Further, very often they may be blackbox, either due to scale, or because of unavailability of model weights or architecture.
__label__learning_theory The main challenge is to take infimum over all possible learners with arbitrary model complexity.
__label__safety_in_machine_learning Common methods for enhancing robustness involve heavy adversarial training or leveraging learned knowledge from clean data, both necessitating substantial computational resources.
__label__optimization_for_deep_networks Experiments show that IRE consistently improves the generalization performance for image classification tasks across a variety of benchmark datasets (CIFAR-10/100, ImageNet) and models (ResNets and ViTs).
__label__learning_theory Our tight result indicates that uniform stability has reached its limit in stability analysis for the UD-based algorithm.
__label__privacy Researchers have provided a probabilistic notion of approximate unlearning under a similar definition of Differential Privacy (DP), where privacy is defined as statistical indistinguishability to retraining from scratch.
__label__machine_vision Besides, excessive interactions can not meet real-time prediction requirements within the constrained drone-based communication bandwidth.
__label__causal_inference This is in stark contrast to the CATE where it is possible to obtain high-quality estimates  which have less dependency upon the smoothness of the nuisance parameters when the CATE itself is smooth.
__label__robotics Motion forecasting for agents in autonomous driving is highly challenging due to the numerous possibilities for each agent's next action and their complex interactions in space and time.
__label__machine_learning_for_physical_sciences This approach allows us to compute multiple solutions in a single learning process while requiring fewer supervised data points than existing neural network methods.
__label__neuroscience_and_cognitive_science Comprehensive experiments on CIFAR-10, CIFAR-100, ImageNet, and CIFAR10-DVS validate that our method achieves comparable performance to BPTT counterparts, and surpasses state-of-the-art efficient training techniques.
__label__interpretability_and_explainability Unfortunately, the black-box nature of deep neural networks impedes the inclusion of domain experts for inspecting the model and revising suboptimal policies.
__label__interpretability_and_explainability Data valuation has emerged as a powerful framework for quantifying each datum's contribution to the training of a machine learning model.
__label__generative_models While diffusion models have demonstrated impressive performance, there is a growing need for generating samples tailored to specific user-defined concepts.
__label__probabilistic_methods So far, only tree-shaped PICs have been explored, and training them via numerical quadrature requires memory-intensive processing at scale.
__label__interpretability_and_explainability (2) Guaranteed Safety: Our approach can provide formal verification guarantees over the entire state space by sampling only a fraction of the policy.
__label__speech_and_audio Moreover, we show that the representation-space of ELSA is structured, enabling swapping of direction of audio via vector arithmetic of two directional text embeddings.
__label__speech_and_audio There is a rising interest and trend in research towards directly translating speech from one language to another, known as end-to-end speech-to-speech translation.
__label__deep_learning_architectures While these models have proven successful in practice, they lead to sub-optimal results due to imperfect optimization and limitations in the encoder and decoder capacity.
__label__diffusion_based_models Finally, we show that inverting a single image into this space encodes a realistic identity into a model, even if the input image is out of distribution (e.g., a painting).
__label__deep_learning_architectures The dynamic nature of the proposed convolution kernel grants Orchid high expressivity while maintaining quasilinear scalability for long sequences.
__label__reinforcement_learning In this work, we start by characterizing churn in a view of Generalized Policy Iteration with function approximation, and we discover a chain effect of churn that leads to a cycle where the churns in value estimation and policy improvement compound and bias the learning dynamics throughout the iteration.
__label__causal_inference One challenge is that soft intervention's effects are ambiguous, since parental relations remain intact.
__label__optimization The proposed UOT distance employs a soft penalization term instead of hard constraints, enabling the construction of an ambiguity set that is more resilient to outliers.
__label__natural_language_processing Reasoning capabilities are crucial for Large Language Models~(LLMs), yet a notable gap exists between English and non-English languages.
__label__machine_learning_for_physical_sciences Finally, UPTs allow for queries of the latent space representation at any point in space-time.
__label__machine_learning_for_healthcare Each voxel in a diffusion MRI (dMRI) image contains a spherical signal corresponding to the direction and strength of water diffusion in the brain.
__label__machine_vision Our comprehensive experiments highlight the superiority of ODGS by delivering the best reconstruction and perceptual quality across various datasets.
__label__machine_learning_for_physical_sciences In particular, the field has notably advanced with the emergence of equivariant message passing.
__label__interpretability_and_explainability This paper introduces MeLLoC(Mechanism Learning for Lossless Compression), a novel approach that combines high-order mechanism learning with classical encoding to enhance lossless compression for scientific data.
__label__deep_learning_architectures Our scheme can accommodate various structures, including sequences, grids and trees, as well as their compositions.
__label__machine_learning_for_healthcare ATAC-Diff is the first diffusion model for the scATAC-seq data generation and analysis, composed of auxiliary modules encoding the latent high-level variables to enable the model to learn the semantic information to sample high-quality data.
__label__natural_language_processing Some works argue that prompting high-performance LLMs is sufficient to produce calibrated uncertainties, while others introduce sampling methods that can be prohibitively expensive.
__label__deep_learning_architectures Furthermore, we empirically evaluate various models using a diverse set of time series forecasting datasets, which (1) verifies the validity of scaling law on dataset size and model complexity within the realm of time series forecasting, and (2) validates our theoretical framework, particularly regarding the influence of look back horizon.
__label__reinforcement_learning The traditional classifier-free guidance paradigm suffers from the inconsistency between the conditions and generated trajectories.
__label__neuroscience_and_cognitive_science In neuroscience, such methods promise to alleviate the need to handcraft models based on biophysical principles and allow to automatize the inference of inter-individual differences in brain dynamics.
__label__safety_in_machine_learning The final score for identifying OOD samples integrates static negative labels with our proposed adaptive proxies, effectively combining textual and visual knowledge for enhanced performance.
__label__diffusion_based_models For this issue, we propose a novel one-shot tuning paradigm, termed OneActor.
__label__optimization_for_deep_networks As soon as we go beyond two layers or two classes, DNC stops being optimal for the deep unconstrained features model (DUFM) -- the standard theoretical framework for the analysis of collapse.
__label__optimization Maintaining a pre-flow that saturates a cut, it enjoys better theoretical and empirical running time than other flow algorithms, such as Ford-Fulkerson.
__label__bandits Notably, in these bounds, we manage to eliminate the dependence on a key instance dependent parameter $\kappa$, that captures non-linearity of the underlying reward model.
__label__diffusion_based_models Despite their strong performances on many generative tasks, diffusion models require a large number of sampling steps in order to generate realistic samples.
__label__reinforcement_learning This work focuses on state-based control, where the RL agent can directly observe raw kinematic and task features, and considers an alternative data augmentation applied to these features based on Euclidean symmetries under transformations like rotations.
__label__deep_learning_architectures During training, we  optimize the parameters of multiple nested FFN blocks with varying sizes, enabling the extraction of hundreds of accurate smaller models without incurring additional computational costs.
__label__infrastructure Importantly, it provides the user with the ability to gracefully trade off accuracy for cost.
__label__generative_models Automatically generating novel and interesting games is a complex task.
__label__machine_vision Modeling the shape of garments has received much attention, but most existing approaches assume the garments to be worn by someone, which constrains the range of shapes they can assume.
__label__diffusion_based_models We promote the generation of AID samples during the training of a generative model by utilizing a feature extractor to guide the process and filter out detrimental samples during generation.
__label__other Large Language Model (LLM) inference on Central Processing Units (CPU) is challenging due to the vast quantities of  Multiply-Add (MAD) matrix operations in the attention computations.
__label__robotics These results provide compelling evidence that world models can significantly enhance the precision of robot action prediction.
"__label__generative_models As a result, we demonstrate discrete data itself can be continuously reparameterised to points on the positive orthant of the $d$-hypersphere $\mathbb{S}^d_+$, 
which allows us to define flows that map any source distribution to target in a principled manner by transporting mass along (closed-form) geodesics of $\mathbb{S}^d_+$."
__label__speech_and_audio We conclude that using a triplet loss function for contrastive regression improves generalisation for speech quality prediction models but also has potential utility across a wide range of applications using regression-based predictive models.
__label__evaluation Upon the text prompt benchmark, we assess the generation capacity of T2V models, characterized by metrics of dynamics ranges and T2V alignment.
__label__machine_vision 3) Centered around the theme of multi-condition multi-LiDAR perception, we collect a 280,000-frame dataset from both clean and adverse conditions.
__label__learning_theory We address both these challenges for natural function classes, including intersections of halfspaces and decision trees, and standard training distributions, including Gaussians.
__label__probabilistic_methods They are tractable if LVs can be analytically integrated out, otherwise they can be approximated by tractable probabilistic circuits (PC) encoding a hierarchical numerical quadrature process, called QPCs.
__label__other The analysis may be of independent interest.
__label__probabilistic_methods For regression, GMDI reduces MSE by up to 21% (from 3.160 to 2.493), achieving the lowest errors among all methods.
__label__machine_vision Experimental results demonstrate the superior reconstruction accuracy of our method compared to previous ones, especially when dealing with large non-rigid deformations arising from the manipulations.
__label__online_learning We then give an efficient gradient-based controller for these systems, with near-optimal regret bounds with respect to a broad class of linear policies.
__label__fairness By leveraging corpus samples from publicly accessible outputs of advanced models such as ChatGPT, GPT-4 and Claude-3, DALD fine-tunes surrogate models to synchronize with unknown source model distributions effectively.
__label__optimization_for_deep_networks This integration can be viewed as a novel sequential optimization process.
__label__deep_learning_architectures Experiments on five LLM families and eight tasks consistently validate the effectiveness of ShiftAddLLM, achieving average perplexity reductions of 5.6 and 22.7 points at comparable or lower latency compared to the most competitive quantized LLMs at 3- and 2-bit precision, respectively, and more than 80% memory and energy reductions over the original LLMs.
__label__evaluation Preliminary analyses suggest that these transforms induce reversal by suppressing the influence of mislabeled examples, exploiting differences in their learning dynamics from those of clean examples.
__label__reinforcement_learning We present a methodology for identifying sub-networks within a larger network in reinforcement learning (RL).
__label__deep_learning_architectures To further improve their compute efficiency, we develop a natural extension of these performant structures that convert them into a sparse Mixture-of-Experts layer.
__label__machine_vision Extensive experiments demonstrate that our method significantly improves retrieval performance on three expanded cross-modal re-identification datasets, paving the way for utilizing large foundation models in downstream data-demanding multi-modal retrieval tasks.
__label__machine_vision Extensive experiments on forward-facing, surrounding, and complex large scenes show the effectiveness of our approach with state-of-the-art performance and high efficiency.
__label__machine_vision We argue that existing stereo matching networks overlook the importance of extracting semantically and structurally meaningful features.
__label__safety_in_machine_learning It enhances the prospects of finding models that perform well in accuracy while adhering to ethical standards, such as fairness or interpretability.
__label__graph_neural_networks Unlike images, graphs do not have a natural notion of domain translation.
__label__machine_learning_for_healthcare On the other hand, textual descriptions of proteins, which could be annotated by human experts or a pretrained protein sequence-to-text model, provide meaningful context that could assist in the functional annotations, such as the localization of active sites.
__label__online_learning However, 3DGS can be inefficient in terms of storage and prone to overfitting by excessively growing Gaussians, particularly with limited views.
__label__safety_in_machine_learning To investigate this, we evaluate existing structural variants of SSMs with AT to assess their AR performance.
__label__learning_theory But what do these optimality guarantees mean if the prior is unknown?
__label__generative_models Our key design is the use of vector quantization and a unique multi-stream Transformer to model the long-term flow of the orchestration style, which enables flexible, controllable, and structured music generation.
__label__graph_neural_networks In addition to the theoretical analysis, we also perform extensive experiments, not only demonstrating the superiority of Temp-G$^3$NTK in the temporal graph classification task, but also showing that Temp-G^3NTK can achieve very competitive performance in node-level tasks like node classification compared with various SOTA graph kernel and representation learning baselines.
__label__other Built upon our observation, we propose a novel learning approach to endow OFL with superb performance and low communication and storage costs, termed as FuseFL.
__label__other The community has designed various techniques to tackle this issue, among which Knowledge Distillation (KD)-based techniques are common.
__label__reinforcement_learning Reinforcement learning (RL) is an actively growing field that is seeing increased usage in real-world, safety-critical applications -- making it paramount to ensure the robustness of RL algorithms against adversarial attacks.
__label__optimization_for_deep_networks Empirically, we conduct extensive experiments where, compared with existing GCIL methods, our GACL exhibits a consistently leading performance across various datasets and GCIL settings.
__label__causal_inference This randomness is referred to as aleatoric uncertainty and is necessary for understanding the probability of benefit from treatment or quantiles of the treatment effect.
__label__safety_in_machine_learning Recent advances in CLIP-based OOD detection have shown promising results via regularizing prompt tuning with OOD features extracted from ID data.
__label__optimization_for_deep_networks The adaptive drift tends to draw the parameters towards the initialisation distribution, so the approach can be understood as a form of soft parameter reset.
__label__other In multi-label learning, each training instance is associated with multiple labels simultaneously.
__label__fairness For Euclidean metrics, these are the first parameterized approximation schemes for the problems, improving upon the previously known $O(1)$-approximation ratios given by Thejaswi et al.
__label__probabilistic_methods Furthermore, our method also shows greater resilience against the detrimental impacts of corrupted gradients as intended.
__label__machine_vision However, their merging policies are directly dependent on intermediate features in ViTs, which prevents exploiting features tailored for merging and requires end-to-end training to improve token merging.
__label__machine_learning_for_other_sciences_and_fields Endowed by this, the flexible 1D MSA decoding framework facilitates zero- or few-shot learning.
__label__neuroscience_and_cognitive_science To the best of our knowledge, this work is the first to present general models that achieve emergent cultural accumulation in reinforcement learning, opening up new avenues towards more open-ended learning systems, as well as presenting new opportunities for modelling human culture.
__label__learning_theory Leave-one-out CV can have a smaller bias as compared to plug-in; however, this bias improvement is negligible compared to the variability of the evaluation, and in some important cases leave-one-out again does not outperform plug-in once this variability is taken into account.
__label__infrastructure While our unfused GEMM-based kernels only improve half precision performance compared to naive kernels by an average of 548% and 193% in 1-D and 2-D problems respectively, our fused kernels improve naive kernels by an average of 1759% and 958% in 1-D and 2-D problems respectively.
__label__reinforcement_learning The proposed QDQ demonstrates solid theoretical guarantees for the accuracy of Q-value distribution learning and uncertainty measurement, as well as the performance of the learning policy.
__label__robotics To achieve these operations, we additionally introduce combined Attention and Mamba techniques for global information aggregation and state sequence modeling, leveraging their respective strengths.
__label__probabilistic_methods We empirically demonstrate the competitive performance of the probabilistic prediction by Wasserstein gradient boosting in comparison with existing uncertainty quantification methods.
__label__causal_inference For one thing, we can capture functional dependencies.
__label__other In this paper, we introduce Terra, a novel Time-varying low-rank adapter that offers a fine-tuning framework specifically tailored for domain flow generation.
__label__deep_learning_architectures Given the focus on training large-scale Transformer models, we consider the challenge of converting these pretrained models for deployment.
__label__causal_inference Our setting is more general than that of prior research—we allow all variables, including both observed and latent ones, to be flexibly related, and we consider the coefficients of all edges, whereas most existing works focus only on the edges between observed variables.
__label__optimization We resolve this fundamental gap with the following contributions: (i) we establish the first lower bounds on the communication and subgradient computation complexities of solving non-smooth convex decentralized optimization problems over time-varying networks; (ii) we develop the first optimal algorithm that matches these lower bounds and offers substantially improved theoretical performance compared to the existing state of the art.
__label__neuroscience_and_cognitive_science For instance, while backpropagation is known to perform accurate credit assignment of error in artificial neural networks, how a similarly powerful process can be realized within the constraints of biological circuits remains largely unclear.
__label__optimization_for_deep_networks Indeed, the standard practice is to re-use the learning HPs originally crafted for dense models.
__label__causal_inference Despite the success of existing Ctrl methods, they require either directly observing the domain variables or assuming a Markov prior on them.
__label__machine_vision They overlooked that in real-world scenarios, e.g., in high-speed data stream environments, data do not pause to accommodate slow models.
__label__diffusion_based_models Our work introduces Stochastic Optimal Control Matching (SOCM), a novel Iterative Diffusion Optimization (IDO) technique for stochastic optimal control that stems from the same philosophy as the conditional score matching loss for diffusion models.
__label__generative_models The recent large-scale text-to-image generative models have attained unprecedented performance, while people established *adaptor* modules like LoRA and DreamBooth to extend this performance to even more unseen concept tokens.
__label__graph_neural_networks However, are these the sole symmetries present in NN parameterizations?
__label__machine_vision To tackle these challenges, we establish a new benchmark dataset and an open-source method for large-scale SAR object detection.
__label__reinforcement_learning Then, we propose a practical algorithm called Scalable MAPPO-Lagrangian (Scal-MAPPO-L).
__label__natural_language_processing This allows humans to provide direct feedback to the individual modules, and thus naturally forms process supervision.
__label__infrastructure However, fine-tuning LLMs, given their massive scale of parameters, poses challenges for clients with constrained and heterogeneous resources in FL.
__label__optimization_for_deep_networks We identify the set of parameterizations which admit well defined infinite width and depth limits that allow the attention layers to update throughout training, a relevant notion of feature learning in these models.
__label__optimization_for_deep_networks Teaching is a potentially effective approach for understanding interactions among multiple intelligences.
__label__machine_vision Meanwhile, we introduce optimization constraints from the perspective of spatial distance and normal consistency, which play a key role in point cloud reconstruction based on multi-scale optimization strategies.
__label__diffusion_based_models We synthetically corrupt ImageNet-1K and CC3M to pre-train and evaluate over $50$ conditional DMs.
__label__algorithmic_game_theory This paper investigates how many samples are needed from bidders' value distributions to find near-optimal posted prices, considering both independent and correlated bidder distributions, and welfare versus revenue maximization.
__label__deep_learning_architectures SURMs achieve: **5**-**7**% accuracy gains on various image classification tasks while replacing low-rank matrices in LoRA and: up to **12x** reduction of the number of parameters in adapters (with virtually no loss in quality) on the GLUE benchmark.
__label__infrastructure On the other hand, LoRA-Inlaid develops a novel multi-task scheduling algorithm guided by output length prediction and grouping among different tasks, which effectively shrinks the memory consumption and avoids frequent switching of LoRA adapters.
__label__online_learning This stands in contrast to the full information scenario, where adaptive and oblivious adversaries are equivalent, and the gap in mistake bounds between randomized and deterministic learners is a constant multiplicative factor of $2$.
__label__generative_models Benefiting from the latent space compatibility, video models can be trained seamlessly from pre-trained T2I or video models in a truly spatio-temporally compressed latent space, rather than simply sampling video frames at equal intervals.
__label__optimization Different from small balanced random initialization in the existing literature, we adopt an unbalanced initialization, where $\mathbf{X}_0$ is large and $\mathbf{Y}_0$ is $0$.
__label__machine_learning_for_other_sciences_and_fields DRACO demonstrates the best performance in denoising, micrograph curation, and particle picking tasks compared to state-of-the-art baselines.
__label__deep_learning_architectures Existing models generally address either multi-party VFL or fuzzy VFL between two parties.
__label__generative_models Code, dataset, and benchmark are released at https://www.anjiecheng.me/SpatialRGPT.
__label__safety_in_machine_learning We shed light on the mechanics of this phenomenon by showing that even when model generations are safe, harmful content can persist in hidden representations and can be extracted by decoding from earlier layers.
__label__probabilistic_methods However, the combinatorial nature of this problem poses a significant challenge,  as the possible number of ordered top-k recommendations from $n$ items grows exponentially with $k$.
__label__machine_learning_for_other_sciences_and_fields Existing methods typically assume a uniform time interval among consecutive user interactions and may not capture users' continuously evolving behavior in the short and long term.
__label__learning_theory We introduce the concept of \emph{relative signal strength} (RSS), a pointwise measure that quantifies the transferability from noisy to clean posterior.
__label__reinforcement_learning To explain this, we analyze the quality of the trained dynamics model.
__label__safety_in_machine_learning We find the failure to achieve satisfactory post-purification robustness stems from the insufficient deviation of purified models from the backdoored model along the backdoor-connected path.
__label__machine_learning_for_other_sciences_and_fields The results show that the integration of the Third Law significantly improves the performance of road segment representations in downstream tasks.
__label__diffusion_based_models In the visual spatial understanding (VSU) field, spatial image-to-text (SI2T) and spatial text-to-image (ST2I) are two fundamental tasks that appear in dual form.
__label__other Leveraging collaborative connections among such similar learners proves valuable in comprehending human learning.
__label__machine_vision Extensive experiments demonstrate that eFreeSplat surpasses state-of-the-art baselines that rely on epipolar priors, achieving superior geometry reconstruction and novel view synthesis quality.
__label__fairness We also design algorithms for auditing proportional fairness of a given clustering solution.
__label__interpretability_and_explainability We introduce the Gated Sparse Autoencoder (Gated SAE), which achieves a Pareto improvement over training with prevailing methods.
__label__generative_models To address this gap, we propose Low-Rank Prompt Adaptation (LoPA), a prompt-tuning-based approach that performs on par with state-of-the-art PEFT methods and full fine-tuning while being more parameter-efficient and not requiring a server-based adapter.
__label__machine_vision FlexCap is trained to produce length-conditioned captions for input boxes, enabling control over information density, with descriptions ranging from concise object labels to detailed captions.
__label__safety_in_machine_learning While large language models (LLMs) such as Llama-2 or GPT-4 have shown impressive zero-shot performance, fine-tuning is still necessary to enhance their performance for customized datasets, domain-specific tasks, or other private needs.
__label__probabilistic_methods Moreover, we develop a multiple testing procedure for determining if the individual coefficients are relevant simultaneously, and show that it is able to control the false discovery rate asymptotically.
__label__other We demonstrate the effectiveness of our approach over conventional approaches in simulations and empirical analyses for mean estimation, least absolute regression, and the fitting of option implied volatility surfaces.
__label__bandits However, similar to other neural network applications, neural bandit algorithms can be vulnerable to adversarial attacks or corruptions on the received labels (i.e., arm rewards), which can lead to unexpected performance degradation without proper treatments.
__label__optimization_for_deep_networks The optimized field, referred to as a feature volume, can be “probed” by the camera rays for novel view synthesis (NVS) and 3D geometry reconstruction.
__label__probabilistic_methods This paper focuses on iterative methods, which use linear system solvers, like conjugate gradients, alternating projections or stochastic gradient descent, to construct an estimate of the marginal likelihood gradient.
__label__machine_learning_for_other_sciences_and_fields In the Integrated Circuit (IC) design flow, floorplanning (FP) determines the position and shape of each block.
__label__learning_theory Our analysis provides results for various data batching schemes, including fully online and minibatch.
__label__machine_vision In this paper, we present VL-SAM, a training-free framework that combines the generalized object recognition model (i.e., Vision-Language Model) with the generalized object localization model (i.e., Segment-Anything Model), to address the open-ended object detection and segmentation task.
__label__bandits Moreover, we propose an algorithm Sub-UCB that achieves regret $\tilde{\mathcal{O}}(\min_{L \le k}(Ln^{1/3}T^{2/3} + \sqrt{{n \choose k - L}T}))$ capable of matching the lower bound on regret for the restricted class up to logarithmic factors.
__label__neuroscience_and_cognitive_science It accelerates performance in both discrete and continuous action space tasks, such as grid world navigation and robotic arm manipulation, relative to the corresponding standard RL models.
__label__graph_neural_networks However, a major barrier to the advancement of this field has been the scarcity of large, realistic datasets.
__label__other General-purpose learning systems should improve themselves in open-ended fashion in ever-changing environments.
__label__learning_theory To achieve this advance, we develop a new framework for analyzing CMDP problems.
__label__reinforcement_learning However, current RLHF techniques cannot account for the naturally occurring differences in individual human preferences across a diverse population.
__label__deep_learning_architectures Finally, the proposed method not only achieves improvements in efficiency and convergence speed but also attains new state-of-the-art performance on these benchmarks.
__label__machine_vision OMG-LLaVA achieves image-level, object-level, and pixel-level reasoning and understanding in a single model, matching or surpassing the performance of specialized methods on multiple benchmarks.
__label__graph_neural_networks Furthermore, to highlight the versatility of the DRAGON framework, we conduct empirical evaluations across a range of graph learning tasks.
__label__optimization The relaxation can be viewed as the Lagrangian dual of the GW distance augmented with constraints that relate to the linear and quadratic terms of transportation plans.
__label__reinforcement_learning The evaluation results demonstrate that our method achieves superior performance compared to widely-adopted representative baselines.
__label__reinforcement_learning Benefiting from its low computational expense, SS can be applied on large-scale data sets with high efficiency.
__label__natural_language_processing Then a Functional Causal Model is employed to encapsulate the discovered causal relations and predict the stock movements.
__label__diffusion_based_models However, directly combining these components leads to two significant challenges: a conflict in frame learning objectives, where video distillation learns from low-quality video frames while the image discriminator targets high-quality images, and training-inference discrepancies due to the differing quality of video samples used during training and inference.
__label__graph_neural_networks We then devise an efficient graph learning algorithm operating directly on ICG in linear memory and time with respect to the __number of nodes__ (rather than edges).
__label__optimization First, this dynamics is amenable to an annealing schedule, adapted from [Suzuki et al., 2023], that results in polynomial convergence rates to a fixed multiplicative accuracy.
__label__optimization Yet, stochastic EG (SEG) has seen limited success in C-C problems, especially for unconstrained cases.
__label__neuroscience_and_cognitive_science Furthermore, we analyzed the impacts of different time windows and brain regions on decoding and reconstruction.
__label__active_learning We perform comprehensive evaluations using both weak and full annotations across four datasets (medical and non-medical).
__label__safety_in_machine_learning We open source the full suite of our research results, including the simulation environment, agent prompts, and a comprehensive web interface.
__label__machine_learning_for_healthcare We demonstrate that with the proposed set of reactions and building blocks, it is possible to obtain a search space of molecules orders of magnitude larger than existing screening libraries coupled with low cost of synthesis.
__label__machine_vision Nonetheless, due to the model structure and the multi-step iterative attribute of DMs, existing binarization methods result in significant performance degradation.
__label__bandits Our algorithm utilizes a stochastic optimization technique to minimize a log-barrier potential based on Frank-Wolfe updates for computing a low-variance exploration distribution over the hypotheses, and is made computationally efficient provided access to an ERM oracle over $\mathcal{H}$.
__label__reinforcement_learning Diffusion policy has shown a strong ability to express complex action distributions in offline reinforcement learning (RL).
__label__evaluation We further use a code-augmented LLM to ensure the label correctness of newly generated data.
__label__natural_language_processing Further analysis substantiates our hypothesis that our improvement can be attributed to reduced overfitting to instruction tuning datasets.
__label__machine_learning_for_social_sciences We thus propose a *refined retraining process* to stabilize the dynamics.
__label__machine_learning_for_other_sciences_and_fields Inspired by fuzzy information processing theory, this paper introduces the DDSR model, which uses fuzzy sets of interaction sequences to overcome the limitations and better capture the evolution of users' real interests.
__label__machine_vision To explain this phenomenon, we introduce a generalized statistical framework, termed ImOOD, to formulate the OOD detection problem on imbalanced data distribution.
__label__deep_learning_architectures In this paper, we propose a mixture of experts (MoE) implicit neural representation approach that enables learning local piece-wise continuous functions that simultaneously learns to subdivide the domain and fit it locally.
__label__learning_theory We conduct a systematic study of the approximation properties of Transformer for sequence modeling with long, sparse and complicated memory.
__label__machine_vision They are crucial to build intelligent agents able to assist users effectively.
__label__machine_vision Our MVUAV dataset is captured via Unmanned Aerial Vehicles (UAV), which offers a unique oblique bird’s-eye view complementary to the existing MVSS datasets; it also encompasses a broad range of day/night lighting conditions and over 30 semantic categories.
__label__machine_vision This limitation hinders the advancement of Vid-LLMs towards fine-grained video understanding.
__label__reinforcement_learning A promising direction is to use linear recurrent architectures (LRUs), where dense recurrent weights are replaced with a complex-valued diagonal, making RTRL efficient.
__label__machine_vision The code is available at https://github.com/Sam1224/HMNet.
__label__machine_vision The Memory-Propagated Streaming Encoding architecture segments long videos into short clips and sequentially encodes each clip with a propagated memory.
__label__neuroscience_and_cognitive_science PCs act as first-order stage processors with heightened sensitivity and reduced latency to intricate contours, while adjacent iso-orientation domains serve as second-order stage processors that refine edge representations for clearer perception.
__label__learning_theory On the practical side, we explore two simple methods for learning projection matrices: PCA- and gradient-based methods.
__label__natural_language_processing For instance, if the suspect model is open-weight, we demonstrate that training on watermarked instructions can be detected with high confidence ($p$-value $< 10^{-5}$) even when as little as $5\%$ of training text is watermarked.
__label__diffusion_based_models Additionally, we introduce Time-Dependent Layer Normalization (TD-LN), a parameter-efficient approach that incorporates time-dependent parameters into layer normalization to inject time information and achieve superior performance.
__label__machine_vision Specifically, we present methods for predictive correlation analysis of Mamba's hidden states from both internal and external perspectives, along with corresponding definitions of correlation scores, aimed at understanding the workings of Mamba in visual recognition tasks and identifying flaws therein.
__label__machine_vision Our core idea is to use models with different parameter sizes to process different resolution levels of the image pyramid, thereby balancing computational efficiency and performance.
__label__reinforcement_learning Existing metrics for reinforcement learning (RL) such as regret, PAC bounds, or uniform-PAC (Dann et al., 2017), typically evaluate the cumulative performance, while allowing the play of an arbitrarily bad policy at any finite time t. Such a behavior can be highly detrimental in high-stakes applications.
__label__causal_inference Our proposed algorithm follows the recursive steps of the existing likelihood-based identification algorithms to train a set of feed-forward models, and connect them in a specific way to sample from the desired distribution.
__label__reinforcement_learning Existing solutions either work under very specific assumptions or achieve bounds that are vacuous in some regimes.
__label__natural_language_processing To retain focus content, we design a memory unit that updates with task progress by decision agent.
__label__reinforcement_learning In the pre-training stage, we employ a discrete diffusion model with a mask-and-replace diffusion strategy to predict future video tokens in the latent space.
__label__generative_models However, the potential of IU models to improve IG performance remains uncharted.
__label__reinforcement_learning Moreover, additional experimental results demonstrate the generalizability and data efficiency of DRAIL.
__label__natural_language_processing To address these shortcomings, we propose a novel CKL approach termed Train-Attention-Augmented Language Model (TAALM), which enhances learning efficiency by dynamically predicting and applying weights to tokens based on their usefulness.
__label__natural_language_processing The methodology presented is domain-agnostic,  even though this article applies it to math problems.
__label__machine_vision The swift advancement in Multimodal LLMs (MLLMs) also presents significant challenges for effective knowledge editing.
__label__learning_theory We characterize the exact asymptotic minimum mean square error, and design a novel spectral estimator with rigorous optimality guarantees: under a technical condition, it attains positive correlation with the signals whenever information-theoretically possible and, for one-sided heteroscedasticity, it also achieves the Bayes-optimal error.
__label__evaluation Despite the importance of the concept, the problem of measuring generalizability remains open.
__label__reinforcement_learning EMIT constructs a sequence of empirical MDPs using data from the growing replay memory.
__label__machine_vision (2) when dealing with wild data, some pixels belonging to the open-set class may exhibit high confidence and also appear speckled.
__label__learning_theory In particular, suppose that an adversary enumerates the strings of an unknown target language L that is known only to come from one of a possibly infinite list of candidates.
__label__interpretability_and_explainability While there has been a large body of research attempting to circumvent tokenization for language modeling (Clark et al.
__label__learning_theory In particular, we show that each of these settings admits a Braess' paradox-like phenomenon in which optimizing over less expressive model classes allows one to achieve strictly better equilibrium outcomes.
__label__online_learning First-order methods (FOMs) are arguably the most scalable algorithms for equilibrium computation in large extensive-form games.
__label__machine_vision Then, AVAgent reasons whether this paired data is aligned well and plans to edit the audio signal if needed (i.e., planning).
__label__machine_vision Open-vocabulary 3D object detection has recently attracted considerable attention due to its broad applications in autonomous driving and robotics, which aims to effectively recognize novel classes in previously unseen domains.
__label__machine_vision The pretext models the driving scene by predicting the angular-wise spatial objectness and temporal dynamics, without manual annotation.
__label__reinforcement_learning This algorithm conceptualizes the reverse process of the diffusion model as a novel policy function and leverages the capability of the diffusion model to fit multimodal distributions, thereby enhancing the representational capacity of the policy.
"__label__other Further, assuming that the conditional probability function $P(y=1|x)=\eta(x)$ is H\""{o}lder continuous and for optimal choice of $m$, we show that the convergence rate of this classifier is minimax-optimal."
__label__machine_vision However, using supervised learning methods is often challenging or costly due to the difficulty of collecting ground-truth data.
__label__learning_theory This paper focuses on a distribution shift setting where train and test distributions can be related by classes of (data) transformation maps.
__label__diffusion_based_models Conditional diffusion models can create unseen images in various settings, aiding image interpolation.
__label__deep_learning_architectures Therefore, we decompose the denoising learning process into every denoising block within a model, not by running the model multiple times, and thus we summarize the proposed paradigm as an in-model latent denoising learning process.
__label__generative_models Moreover, trajectory data can be seamlessly leveraged in our GDB framework by using a chain of equivariant diffusion bridges, providing a more detailed and accurate characterization of evolution dynamics.
__label__machine_vision We present a computational framework that transforms single images into 3D physical objects.
__label__machine_learning_for_physical_sciences Conventional sophisticated mesh movement methods are extremely expensive and struggle to handle scenarios with complex boundary geometries.
"__label__natural_language_processing However, the contrastive objective focuses mainly on the relative values of implicit rewards associated with two responses while ignoring
their actual values, resulting in suboptimal alignment with human preferences."
__label__natural_language_processing xRAG reinterprets document embeddings in dense retrieval--traditionally used solely for retrieval--as features from the retrieval modality.
"__label__generative_models However, in an adversarial framework, we observe that a na\""ive generator architecture suffers from training instability and lacks the capability to adjust the scale of Gaussians."
__label__diffusion_based_models The sampling for diffusion models is done by solving either the *probability flow* ODE or diffusion SDE wherein a neural network approximates the score function allowing a numerical ODE/SDE solver to be used.
__label__reinforcement_learning A key challenge in lifelong reinforcement learning (RL) is the loss of plasticity, where previous learning progress hinders an agent's adaptation to new tasks.
__label__machine_learning_for_other_sciences_and_fields At this point, this approach addresses the curse of dimensionality by segmenting high-dimensional data and ensures resilience to data drift and perturbations through flexible boundary fitting.
__label__learning_theory However, when the input distribution is neither uniform nor far from uniform, known algorithms may have highly non-replicable behavior.
__label__probabilistic_methods To enable streaming Bayesian inference over discrete parameter spaces, we propose streaming Bayes GFlowNets (abbreviated as SB-GFlowNets) by leveraging the recently proposed GFlowNets --- a powerful class of amortized samplers for discrete compositional objects.
__label__generative_models Realistic trajectory generation with natural language control is pivotal for advancing autonomous vehicle technology.
__label__interpretability_and_explainability Existing methods in functional data analysis are promising but often not expressive enough to account for all interactions and non-linearities and do not scale well to large datasets.
__label__diffusion_based_models However, these models require multiple denoising steps during sampling, resulting in high computational costs.
__label__neuroscience_and_cognitive_science However, place cells also respond to contextual cues, such as smell.
__label__graph_neural_networks In addition, we reveal novel permutation symmetries in the resulting node feature tensor, characterize associated linear equivariant layers, and integrate them into our Subgraph GNN.
__label__natural_language_processing We demonstrate that existing models exhibit a significant *alignment gap* -- *i.e.
__label__other Despite the overwhelming success of deep learning methods key questions about their internal workings still remain largely unanswered, due to their internal high dimensionality and complexity.
__label__machine_vision This discovery leads us to propose a simple visual pretraining framework: cross-attention masked autoencoders (CrossMAE).
__label__reinforcement_learning We approach this problem by first establishing a policy difference lemma for the episodic setting, which provides the theoretical foundation for the algorithm.
__label__machine_learning_for_healthcare The rapid advancement of single-cell ATAC sequencing (scATAC-seq) technologies holds great promise for investigating the heterogeneity of epigenetic landscapes at the cellular level.
__label__diffusion_based_models The prompt embedding is optimized to minimize artifacts in the generated image, while the watermark is optimized to achieve maximum strength.
__label__natural_language_processing Experiments on three benchmarks demonstrate that KnowGPT significantly outperforms all competitors.
__label__other In this work, we first showcase the potential of low precision ensembling, where ensemble members are derived from a single model within low precision number systems in a training-free manner.
__label__privacy Our final algorithms obtain regret which significantly improves the regret in the high privacy regime $\varepsilon \ll 1$, obtaining $\sqrt{T \log d} + T^{1/3} \log(d)/\varepsilon^{2/3}$ for DP-OPE and $\sqrt{T} + T^{1/3} \sqrt{d}/\varepsilon^{2/3}$ for DP-OCO.
__label__machine_vision Furthermore, leveraging the automated nature of our data creation process, we generate a large-scale training dataset, which we use to finetune CLIP (a foundational VLM) and Idefics2 (a multimodal large language model).
__label__machine_vision The input normal maps can be predicted by 2D diffusion models, significantly aiding in the guidance and refinement of the geometry's learning.
__label__deep_learning_architectures Our code will be available at https://github.com/Xnhyacinth/TAGI.
__label__diffusion_based_models Additionally, we present an AID variant called \textbf{Prompt-guided Attention Interpolation via Diffusion (PAID)}, which \textbf{3)} treats interpolation as a condition-dependent generative process.
__label__machine_vision Notably, our method consistently surpasses SAM by 3-6 points in mean IoU and 4-7 in mean boundary IoU across point-prompt interactive segmentation rounds.
__label__interpretability_and_explainability CLIP embeddings have demonstrated remarkable performance across a wide range of multimodal applications.
__label__machine_vision Treating superpoints as rigid parts, we can discover the underlying skeleton model through intuitive cues and optimize it using the kinematic model.
__label__machine_vision Extensive experiments demonstrate that the PIIP achieves superior performance in tasks such as object detection, segmentation, and image classification, compared to traditional image pyramid methods and single-branch networks, while reducing computational cost.
__label__machine_vision * The recent outstanding performance of MLLMs in multimodal understanding has garnered broad attention from both academia and industry.
__label__optimization To enhance robustness, we propose an ensemble-based *Collaborative Neural Framework (CNF)* w.r.t.
__label__privacy To overcome the challenge, we propose an efficient method, Single Image Unlearning (SIU), to unlearn the visual recognition of a concept by fine-tuning a single associated image for few steps.
__label__natural_language_processing Such behavior complicates the alignment of multiagent systems, emphasizing the need for benchmarks that can rigorously evaluate the degree of emotional alignment.
__label__other Extensive experiments on the GVLQA dataset and five real-world datasets show that GITA outperforms mainstream LLMs in terms of general graph reasoning capabilities.
__label__reinforcement_learning By combining ideas from periodic Markov chains and stochastic approximation, we rigorously establish that PASQL converges to a cyclic limit and characterize the approximation error of the converged periodic policy.
__label__machine_vision It significantly outperforms the counterpart Fuyu-8B with mysterious training procedures and undisclosed training data.
__label__interpretability_and_explainability To investigate these questions, we investigate what functions can be learned by randomly initialized transformers in which only the embedding layers are optimized, so that the only input--output mappings learnable from data are those already implemented (up to a choice of encoding scheme) by the randomly initialized model.
__label__deep_learning_architectures The objective of dataset condensation is to ensure that the model trained with the synthetic dataset can perform comparably to the model trained with full datasets.
__label__machine_vision Our edit network consumes a complete input program and a visual target.
__label__optimization To address this issue, we have developed novel techniques to progressively estimate the strong convexity of the Lagrangian function.
__label__reinforcement_learning Theoretically, we prove that OPT-AIL achieves polynomial expert sample complexity and interaction complexity for learning near-expert policies.
__label__diffusion_based_models Moreover, although MBD does not require external data, it can be naturally integrated with data of diverse qualities to steer the diffusion process.
__label__machine_vision We note that the input text description typically already contains detailed information on how to localize the target object, and we also observe that humans often follow a step-by-step comprehension process (\ie, progressively utilizing target-related attributes and relations as cues) to identify the target object.
__label__neuroscience_and_cognitive_science We end by analyzing the low-rank dynamics of trained NM-RNNs, to show how task computations are distributed.
__label__deep_learning_architectures Previous methods have employed linear attention to mitigate the complexity of the original self-attention mechanism at the expense of effectiveness.
__label__optimization Let $\overline{\operatorname{vol}}{ (\mathcal S_t)}$ and $\overline{\gamma_t}$ be the running average of volume and the residual ratio of active nodes $\textstyle \mathcal{S_t}$ during the process.
__label__learning_theory In this paper, we provide a detailed analysis of how the disagreement and the polarization (a notion we introduce and define in this paper) among classifiers relate to the performance gain achieved by aggregating individual classifiers, for majority vote strategies in classification tasks.
__label__reinforcement_learning DUSDi decomposes skills into disentangled components, where each skill component only affects one factor of the state space.
__label__optimization_for_deep_networks Our code is available at https://github.com/StefanosPert/Equivariant_Optimization_CR
__label__generative_models Significant progress has been made in text-to-video generation through the use of powerful generative models and large-scale internet data.
__label__interpretability_and_explainability Finally, we present a detailed case study where entropy neurons actively manage confidence: the setting of induction, i.e.
__label__machine_vision To resolve this challenge, we present a comprehensive regulation framework that allows the learnable prompts to actively interact with the well-learned general knowledge in large 3D models to maintain good generalization.
__label__neuroscience_and_cognitive_science We analytically reduce the learning dynamics to an effective eigenspace, revealing a virtuous cycle: fast adapting gates drive weight specialization by protecting previous knowledge, while weight specialization in turn increases the update rate of the gating layer.
__label__other However, deep learning approaches cannot reliably interpret such deictic representations due to their lack of reasoning capabilities in complex scenarios.
__label__privacy Notably, our findings suggest that the utility upper bound could be independent of the dimension $d$, even when $d \gg N$.
__label__probabilistic_methods We illustrate the validity of our findings as $N$ gets larger,  in a teacher-student experimental setting, training a student NN to learn from a WI, SI  or arbitrary teacher model through various SL schemes.
__label__optimization For this goal, we propose novel near-optimal methods for smooth and nonsmooth problems by reformulating them into functionally constrained problems.
__label__generative_models Additionally, we introduce a sigmoid time discretization schedule for diffusion sampling, which achieves high-quality generation in 5-10 steps combined with higher-order ODE solvers.
__label__graph_neural_networks In contrast, global attention-based models such as graph transformers demonstrate strong performance in practice, but comparing their expressive power with the $k$-WL hierarchy remains challenging, particularly since these architectures rely on positional or structural encodings for their expressivity and predictive performance.
__label__other The novelty of this score stems from the fact that it just assumes a finite second moment of the data and uses the characteristic function to evaluate the quality of the estimated mixing matrix without any knowledge of the parameters of the noise distribution.
__label__machine_vision We propose a drop-in replacement for the position encoding of plain ViTs that restricts attention heads to fixed fields of view, pointed in different directions, using 2D attention masks.
__label__optimization_for_deep_networks The $\alpha$-sparsity prototype loss aligns samples from underrepresented domains, enhancing intra-class similarity and reducing inter-class similarity.
__label__optimization Many zeroth-order (ZO) optimization algorithms have been developed to solve nonconvex minimax problems in machine learning and computer vision areas.
__label__learning_theory Adversarial examples have raised several open questions, such as why they can deceive classifiers and transfer between different models.
__label__privacy The source code needed to reproduce our experiments is available from https://github.com/spalabucr/bb-audit-dpsgd.
__label__generative_models Parameter-Efficient Fine-Tuning (PEFT) has become the standard for customising Foundation Models (FMs) to user-specific downstream tasks.
__label__safety_in_machine_learning Experiments show that our method has high power and false alarm control under various distribution shifts, including covariate and label shifts and natural shifts over geography and time.
__label__machine_learning_for_healthcare These findings underscore the significant impact of our method on healthcare applications, such as diagnosing Myocardial Infarction, Alzheimer's, and Parkinson's disease.
__label__reinforcement_learning One of the core tasks in the field of DRL is distributional policy evaluation, which involves estimating the return distribution $\eta^\pi$ for a given policy $\pi$.
__label__robotics Moving to the experimental side, we demonstrate that CON reaches SoA performance when learning complex nonlinear dynamics of mechanical systems directly from images.
__label__causal_inference Our design goal is to improve the estimation efficiency of the average treatment effect (ATE) of the primary outcome utilizing surrogate outcomes.
__label__neuroscience_and_cognitive_science However, despite previous efforts to optimize the learning algorithm of SNNs through various methods, SNNs still lag behind ANNs in terms of performance.
__label__robotics On the other line, diffusion models have also shown promise in robotic control tasks by denoising actions, known as diffusion policy.
__label__optimization_for_deep_networks Despite the widespread success of Transformers across various domains, their optimization guarantees in large-scale model settings are not well-understood.
__label__machine_vision Vision-language tracking (VLT) enhances traditional visual object tracking by integrating language descriptions, requiring the tracker to flexibly understand complex and diverse text in addition to visual information.
__label__robotics To make use of this extra flexibility, QueST imparts causal inductive bias from the action sequence data into the latent space, leading to more semantically useful and transferable representations.
__label__other Through imitative contingency learning, BeTop also effectively manages behavioral uncertainty for prediction and planning.
__label__causal_inference This approach, on the other hand, can suffer from low compliance, i.e., IV weakness.
__label__deep_learning_architectures Recently proposed subquadratic architectures, such as Mamba, have shown promise, but have been pretrained with substantially less computational resources than the strongest Transformer models.
__label__safety_in_machine_learning Our work provides a new perspective on understanding the effectiveness of backdoor safety tuning and highlights the importance of faithfully assessing the model's safety.
__label__diffusion_based_models In our experiments on the HRS and DrawBench benchmarks, we achieve state-of-the-art performance compared to previous training-free approaches.
__label__active_learning Typically, extensive _experiments_ to obtain features and labels come with a significant acquisition cost, making it impractical to carry out all of them.
__label__machine_vision To address this, we propose a novel Prompt Contrastive Recovery approach, \textbf{PCRIL}, which progressively identifies promising positive classes from unknown label sets and recursively searches for other relevant labels.
__label__machine_vision We hope UniSeg3D can serve as a solid unified baseline and inspire future work.
__label__natural_language_processing Our key idea is to leverage the reasoning capabilities of LLMs to identify effective feature generation rules without manually specifying the search space and provide language-based reasoning information highlighting past experiments as feedback for iterative rule improvements.
__label__reinforcement_learning Finally, we present a numerical experiment to highlight the salient features of PASQL and demonstrate the benefit of learning periodic policies over stationary policies.
__label__human-AI_interaction With only 10 minutes of human feedback, our algorithm achieves up to 30\% increase in success rate compared to its RL baseline.
__label__optimization_for_deep_networks Forward gradients, solely based on directional derivatives computed from two forward calls, have been recently used for model training, with substantial savings in computation and memory.
__label__deep_learning_architectures In this paper, we take a step forward to close the gap between the linear and Softmax attention with novel theoretical analyses, which demystify the core factors behind the performance deviations.
__label__safety_in_machine_learning Furthermore, by establishing bounds for reductions in recommendation error during ACF's optimization process, we find that applying personalized magnitudes of perturbation for different users based on their embedding scales can further improve ACF's effectiveness.
__label__probabilistic_methods We formally derive a cumulative regret bound of LB and compare it with the regret of an oracle BO algorithm using the optimal length scale.
__label__optimization_for_deep_networks Several challenges make it difficult for sparse neural networks to compete with dense models.
__label__machine_vision Additionally, we propose the first RID-oriented iterative mean-teacher framework, termed the Coherence-based Label Generator, to generate high-quality pseudo labels for network training.
__label__reinforcement_learning The empirical results demonstrate that our proposed CTR yields significant performance improvement over the state-of-the-art methods.
__label__natural_language_processing Linear attention Transformers and their gated variants, celebrated for enabling parallel training and efficient recurrent inference, still fall short in recall-intensive tasks compared to traditional Transformers and demand significant resources for training from scratch.
__label__reinforcement_learning We consider kernel-based function approximation for RL in the infinite horizon average reward setting, also referred to as the undiscounted setting.
__label__machine_learning_for_healthcare It is challenging to identify an inductive bias that consistently maintains the integrity of molecular activity during data augmentation.
__label__graph_neural_networks We show that the resulting collection of subgraphs can be viewed as the product of coarsened and original graphs, unveiling a new connectivity structure on which we perform generalized message passing.
__label__machine_learning_for_other_sciences_and_fields Previous works either store a larger FP32 model to switch between different precision models for higher accuracy or store a smaller INT8 model but compromise accuracy due to using shared quantization parameters.
__label__safety_in_machine_learning We propose a method that combines training in a higher precision than the target model, rounding after intermediate computation steps, and storing rounding decisions based on an adaptive thresholding procedure, to successfully control for  nondeterminism.
__label__machine_vision In general, soft labels can naturally represent ambiguous targets in classification.
__label__natural_language_processing Instead of developing a new importance criterion, MaskLLM explicitly models N:M patterns as a learnable distribution through Gumbel Softmax sampling.
__label__graph_neural_networks Thus, we propose a novel and simple GLAD method, named MUSE.
__label__evaluation To close this gap, we propose two metrics for localizing memorization in SSL encoders on a per-layer (LayerMem) and per-unit basis (UnitMem).
"__label__optimization A well-known theoretical tool in
maximal monotone operator theory, the Fitzpatrick function naturally leads to a
refined Fenchel-Young inequality, making Fitzpatrick losses tighter than
Fenchel-Young losses, while maintaining the same link
function for prediction."
__label__learning_theory This algorithm is proven to achieve a regret of $\tilde{O} (\sqrt{T \dim_E(\mathcal{F}) \log N(\mathcal{F})})$ and a communication complexity of $\tilde{O} (M^2 \dim_E(\mathcal{F}))$, where $M$ is the total number of agents and $T$ is the number of rounds, while $\dim_E(\mathcal{F})$ and $N(\mathcal{F})$ are  the Eluder dimension and the covering number of function space $\mathcal{F}$ respectively.
__label__machine_vision UMFC estimates image-level biases from domain-specific features and text-level biases from the direction of domain transition.
__label__deep_learning_architectures Recent work shows how to use stochastic Gumbel annealing (SGA) to refine the latents of pre-trained neural image compression models.
__label__machine_learning_for_healthcare In this paper, we introduce Uni-Med, a novel medical generalist foundation model which consists of a universal visual feature extraction module, a connector mixture-of-experts (CMoE) module, and an LLM.
__label__machine_learning_for_physical_sciences However, these methods often rely on explicit knowledge, such as pre-defined Lie groups, which are typically restricted to linear or affine transformations.
__label__natural_language_processing Extensive experiments on multiple benchmarks demonstrate our effectiveness, where Kangaroo achieves walltime speedups up to 2.04$\times$, outperforming Medusa-1 with 88.7\% fewer additional parameters.
__label__generative_models This new perspective allows us to unify many existing methods under a common umbrella.
"__label__optimization_for_deep_networks Compared to LoRA, PiSSA **updates the principal components** while **freezing the ""residual"" parts**, allowing faster convergence and enhanced performance."
__label__machine_vision Manual assignment of materials using graphic software is a tedious and time-consuming task.
__label__fairness We formally prove that (1) samples that exhibit spurious correlations lie on a lower rank manifold relative to the ones that do not; and (2) the depth of a network acts as an implicit regularizer on the rank of the attribute subspace that is encoded in its representations.
__label__machine_learning_for_other_sciences_and_fields The resulting FL ecosystem has two features: (i) self-interest, and (ii) competition among FL-PTs.
__label__machine_learning_for_other_sciences_and_fields However, while protein function is intricately tied to structure, most existing PLMs do not incorporate protein structure information.
__label__neuroscience_and_cognitive_science Although such results are thought to reflect shared computational principles between LMs and human brains, there are also clear differences in how LMs and humans represent and use language.
__label__optimization Extensive experiments are conducted to validate our theoretical results.
__label__natural_language_processing However, although useful in simple NLP tasks, these non-learnable methods fail to handle complex multimodal tasks like Visual Question Answering (VQA).
__label__optimization_for_deep_networks However, each SAM update requires _sequentially_ computing two gradients, effectively doubling the per-iteration cost compared to base optimizers like SGD.
__label__optimization It remains unclear how the other building blocks of the transformer contribute to ICL.
__label__natural_language_processing Then, we utilize the re-organized information in the reasoning process.
__label__diffusion_based_models CAT3D can create entire 3D scenes in as little as one minute, and outperforms existing methods for single image and few-view 3D scene creation.
__label__natural_language_processing By increasing the vocabulary size from the conventional 32K to 43K, we improve performance on ARC-Challenge from 29.1 to 32.0 with the same 2.3e21 FLOPs.
__label__machine_learning_for_other_sciences_and_fields This requires the desirable FL-PT selection strategy to simultaneously mitigate the problems of free riders and conflicts of interest among competitors.
__label__generative_models The code is released at https://github.com/magic-research/vector_quantization.
__label__optimization We specifically investigate critic-based losses derived from variational representations of statistical distances between probability measures.
__label__diffusion_based_models To this end, we propose motion consistency models (MCM), a single-stage video diffusion distillation method that disentangles motion and appearance learning.
__label__learning_theory First, existing methods in this setting primarily focus on optimizing transferability within their local client domains, thereby ignoring transferability over the global learning domain.
__label__causal_inference In this work, we address settings in which an important confounding variable is completely unobserved.
__label__deep_learning_architectures Theoretical discussions and intuitive analysis support the generalizability and interpretability of EpoD.
__label__active_learning This result is a generalization of the well-known $k$-means++ guarantee to a broad problem class which is also of independent interest.
__label__diffusion_based_models This paper proposes a fast and general-purpose image restoration method.
__label__machine_vision However, in the DDIM inversion, using the $t-1$ time step to approximate the noise prediction at time step $t$ introduces errors between the restored image and the reference image.
__label__deep_learning_architectures These findings highlight the potential of CherryQ for enabling efficient deployment of LLMs by taking advantage of parameter heterogeneity.
__label__active_learning We validate the effectiveness of our pipeline on benchmark datasets such as SCAPE, TOSCA, TOPKIDS, and others.
__label__reinforcement_learning While Li et al.
__label__safety_in_machine_learning Applications of this control abound in drug discovery, forensics, anomaly detection, and, in particular, machine learning, ranging from nonparametric outlier detection to out-of-distribution detection and one-class classification methods.
__label__online_learning Here, we develop a linear teacher-student model with latent structure and show analytically that high input feature similarity coupled with low readout similarity is catastrophic for both knowledge transfer and retention.
__label__learning_theory supervised learning*) with two classes.
__label__other Given different instructions, large vision-language models (LVLMs) exhibit different degrees of object hallucinations, posing a significant challenge to the evaluation of object hallucinations.
__label__other Existing offboard methods focus on 3D object detection with closed-set taxonomy and fail to match human-level recognition capability on the rapidly evolving perception tasks.
__label__machine_vision We propose perception prior embedding to better integrate perception priors with image features.
__label__algorithmic_game_theory In this work, we take the initial step of learning the optimal tax that can maximize social welfare with limited feedback in congestion games.
__label__machine_vision When depth maps are better refined, the labels also become more noise-free.
__label__safety_in_machine_learning Moreover, when the fine-tuning dataset contains a mixture of both benign and malicious data,  $\textsf{Safe LoRA}$ mitigates the negative effect made by malicious data while preserving performance on downstream tasks.
__label__deep_learning_architectures Furthermore, they typically sacrifice essential temporal correlation among consecutive training samples by shuffling them into mini-batches.
__label__natural_language_processing We point out that most approaches for modeling, correcting, and measuring the symmetry of an embedding space implicitly assume that the word frequencies are *uniform*; in reality, word frequencies follow a highly non-uniform distribution, known as *Zipf's law*.
__label__interpretability_and_explainability We create 102 different computer-assisted proof strategies and assess their length and tightness of bound on each of our models.
__label__interpretability_and_explainability To address these questions, we propose analyzing a model’s learning dynamics via a framework we call the concept space, where each axis represents an independent concept underlying the data generating process.
__label__optimization_for_deep_networks However, during the multimodal training process, the model tends to rely on only one modality based on which it could learn faster, thus leading to inadequate use of other modalities.
__label__reinforcement_learning In this paper, we theoretically analyze the online-finetuning of the decision transformer,  showing that the commonly used Return-To-Go (RTG) that's far from the expected return hampers the online fine-tuning process.
__label__graph_neural_networks Extensive experiments on 10 public benchmarks reveal that our learning framework outperforms not only previous non-BP methods but also the standard BP methods, and it exhibits excellent robustness against various types of noise and attacks.
__label__machine_vision Code is in https://lwpyh.github.io/ProMaC/.
__label__machine_learning_for_social_sciences To address this limitation, we propose the **Fed**erated model heterogeneous **M**atryoshka **R**epresentation **L**earning (**FedMRL**) approach for supervised learning tasks.
__label__safety_in_machine_learning Due to the impressive zero-shot capabilities, pre-trained vision-language models (e.g.
__label__machine_learning_for_social_sciences In this work, we study whether language models (LMs) can forecast at the level of competitive human forecasters.
__label__infrastructure We validate that FP8 FlashAttention-3 achieves 2.6$\times$ lower numerical error than a baseline FP8 attention.
__label__probabilistic_methods However, naive similarity metrics lack permutation invariance and are inappropriate for comparing networks.
__label__diffusion_based_models Text-to-image (T2I) diffusion models, when fine-tuned on a few personal images, can generate visuals with a high degree of consistency.
__label__machine_vision Extensive experiments validate the rationality and effectiveness of our distribution choice and network design.
__label__machine_vision (4) Due to the slowly changing embeddings across layers, and the high overlap between textual and multimodal activated weights, we compress LLMs by keeping only 1 subnetwork (called alpha-SubNet) that works well across a wide range of multimodal tasks.
__label__natural_language_processing Critique ability, i.e., the capability of Large Language Models (LLMs) to identify and rectify flaws in responses, is crucial for their applications in self-improvement and scalable oversight.
__label__machine_vision Motivated by applications in sports coaching and motor skill learning, we investigate the inverse problem: generating corrective instructional text, leveraging motion editing and generation models.
__label__causal_inference Empirical studies on both synthetic and real-world datasets validate our identifiability theory and the effectiveness of the proposed method in the finite-sample regime.
__label__reinforcement_learning We integrate this technique into existing off-policy RL methods, resulting in the RESeL algorithm.
__label__machine_vision Identifying unknowns is nontrivial due to the fixed and long-tailed patterns of positive label sets in training data, which hampers the discovery of new label combinations.
__label__deep_learning_architectures DEPrune is optimized by analyzing the computation of DSConv on GPUs.
__label__machine_vision In this work, we address shape recovery when garments are being manipulated instead of worn, which gives rise to an even larger range of possible shapes.
__label__machine_learning_for_other_sciences_and_fields Our code is here.
__label__optimization To do so, we bring classical optimization arguments for saddle-point algorithms to the geometry of Wasserstein space.
__label__machine_vision Finally, in the Refinement stage, in-context inpainting is designed to further improve rendering quality on the less observed human body parts.
__label__diffusion_based_models In this paper, we introduce a new perspective and framework, demonstrating that diffusion models with cross-attention can serve as a powerful inductive bias to facilitate the learning of disentangled representations.
__label__learning_theory This method is designed to explore the impact of varying eigenfunction orders.
__label__causal_inference To yield context-specific insights, conditional independence is tested on context-specific data.
__label__deep_learning_architectures Accordingly, a few of recent works not only discarded label correlation modeling, but also advocated to remove contextual information for multi-label image recognition.
__label__machine_vision However, they fail to establish reliable optimization objectives for unsupervised training, either relying on overly strong geometric assumptions, or suffering from poor-quality pseudo-labels due to inadequate integration of low-level geometric and high-level contextual information.
__label__graph_neural_networks The performance gain increases with a larger task graph size.
__label__diffusion_based_models We also construct a benchmark to facilitate further research.
__label__algorithmic_game_theory To address this shortcoming, we examine the iteration complexity of several gradient-based algorithms in the celebrated framework of smoothed analysis, and we show that they have polynomial smoothed complexity, in that their number of iterations grows as a polynomial in the dimensions of the game, $\text{log}(1/\epsilon)$, and $1/\sigma$, where $\sigma$ measures the magnitude of the smoothing perturbation.
__label__learning_theory When training deep neural networks, a model's generalization error is often observed to follow a power scaling law dependent both on the model size and the data size.
__label__other We innovatively treat the sequential recommendation task as a form of multi-task learning, integrating LoRA with the Mixture of Experts (MoE) framework.
__label__machine_vision Secondly, finetuning CLIP models (e.g., prompt learning) on seen base classes usually sacrifices the model's original generalization capability on unseen novel classes.
__label__neuroscience_and_cognitive_science This regularization improves both predictive performance and how consistently neuronal embeddings cluster across model fits  compared to uniform regularization.
__label__other To achieve these goals, we devise a practical gradient update procedure that can work under both single-source and multi-source UDA.
__label__other How do these characteristics affect different state-of-the-art algorithms?
__label__machine_learning_for_other_sciences_and_fields Leveraging the dataset's ultra-high resolution, which details proof states at the sub-type level, we propose a novel neural architecture targeted at faithfully representing dependently-typed programs on the basis of structural rather than nominal principles.
__label__natural_language_processing Finaly, the above process evolves in a dynamic refinement phase, where more effective LLMs are prioritized, enhancing the overall data quality.
__label__deep_learning_architectures Various linear complexity models, such as Linear Transformer (LinFormer), State Space Model (SSM), and Linear RNN (LinRNN), have been proposed to replace the conventional softmax attention in Transformer structures.
__label__diffusion_based_models Our approach requires minimal assumptions, is agnostic to the manifold hypothesis and avoids absolute continuity assumptions for the target distribution.
__label__safety_in_machine_learning Firstly, we show that both OOD classification and OOD calibration errors have a shared upper bound consisting of two terms of ID data: 1) ID calibration error and 2) the smallest singular value of the ID input covariance matrix.
__label__deep_learning_architectures The $G$-Bispectrum extracts every characteristic of a given signal up to group action: for example, the shape of an object in an image, but not its orientation.
__label__privacy In this paper, we study differentially private (DP) algorithms for computing the geometric median (GM) of a dataset: Given $n$ points, $x_1,\dots,x_n$ in $\mathbb{R}^d$, the goal is to find a point $\theta$ that minimizes the sum of the Euclidean distances to these points, i.e., $\sum_{i=1}^{n} \lVert|\theta - x_i\rVert_2$.
__label__natural_language_processing Motivated by this, we propose a new approach to calibrate reasoning by up-weighting reasoning paths with high internal consistency, resulting in a significant boost in reasoning performance.
__label__machine_learning_for_other_sciences_and_fields However, mainstream TSC models rely on the assumption of independent and identically distributed (i.i.d.
__label__learning_theory We study the gradient Expectation-Maximization (EM) algorithm for Gaussian Mixture Models (GMM) in the over-parameterized setting, where a general GMM with $n>1$ components learns from data that are generated by a single ground truth Gaussian distribution.
__label__diffusion_based_models The proposed strategy significantly reduces inference latency while minimally impacting the generative quality.
__label__optimization For the smooth and monotone case, we establish a lower bound with explicit dependence on the level of Jacobian inaccuracy and propose an optimal algorithm for this key setting.
__label__learning_theory Most existing works compute this quantity by assuming continuous- or infinite-time training dynamics, complicating the development of practical estimators capable of accurately predicting generalization without access to test data.
__label__machine_vision Similar to the birth of CNN inspired by receptive fields in the biological visual system, we draw inspiration from the information subsystem pathways in the biological visual system and propose Model Disassembling and Assembling (MDA).
__label__probabilistic_methods To address this, we present a method for extracting the inductive bias from a nonparametric Bayesian model and transferring it to an artificial neural network.
__label__diffusion_based_models Existing methods, though effective in various individual applications, often lack theoretical grounding and rigorous testing on extensive benchmarks.
__label__machine_learning_for_other_sciences_and_fields Hence, it is equally important to build the forecasting models from both perspectives.
__label__generative_models The MLLM is then jointly instruction-tuned with a pretrained image-editing model to unlock capabilities of simultaneous reasoning of language instructions and generation of imagined subgoals.
__label__algorithmic_game_theory (ICML'19) and relate it to the area of multiwinner voting in computational social choice.
__label__safety_in_machine_learning The Rashomon effect is a mixed blessing in responsible machine learning.
__label__diffusion_based_models This makes it challenging to achieve precise and disentangled image generation without additional training, especially in an unsupervised way.
__label__probabilistic_methods To this end, statistical methods which describe high-dimensional neural time series in terms of low-dimensional latent dynamics have played a fundamental role in characterizing neural systems.
__label__optimization Our experiments indicate that our algorithms may open the door to using ASQ more extensively in a variety of ML applications.
__label__other To answer this question, we introduce a diagnostic setting - **recurring TTA** where environments not only change but also recur over time, creating an extensive data stream.
__label__generative_models More precisely, it establishes bounds on the Kullback-Leibler divergence between the target distribution and the one generated by such DFM models under moment conditions on the score of $\nu^\star$, $\mu$ and $\pi$, and a standard $\mathrm{L}^2$-drift-approximation error assumption.
__label__diffusion_based_models Diffusion models have revolutionized customized text-to-image generation, allowing for efficient synthesis of photos from personal data with textual descriptions.
__label__neuroscience_and_cognitive_science Current state-of-the-art synchrony-based models encode object bindings with complex-valued activations and compute with real-valued weights in feedforward architectures.
__label__machine_vision The expanding variety of Pre-Trained VLMs enhances the likelihood of identifying a suitable VLM for specific tasks.
__label__optimization Furthermore, we incorporate communication compression into the method to enhance communication efficiency.
__label__safety_in_machine_learning This allows us to reweight client parameter updates and identify those with large discrepancies as backdoor attackers.
__label__machine_learning_for_physical_sciences Our memory-efficient solver operates on a new continuous coordinate-based surface representation called neural deformation fields (NDFs); it supervises NDF equilibria with the laws of the non-linear Kirchhoff-Love shell theory with a non-linear anisotropic material model.
__label__safety_in_machine_learning Moreover, to set up a comprehensive and discriminative feature representation, we devise a multi-layer semantic feature extraction strategy.
__label__evaluation Based on the ConBench tool, we are the first to reveal the tapestry and get the following findings: (1) In the discriminate realm, the larger the solution space of the prompt, the lower the accuracy of the answers.
__label__natural_language_processing Beyond traditional binary relational facts, n-ary relational knowledge graphs (NKGs) are comprised of n-ary relational facts containing more than two entities, which are closer to real-world facts with broader applications.
__label__safety_in_machine_learning It uses the losses from a trained model to construct a balanced dataset of high-loss and low-loss samples in which the training data group imbalance is mitigated.
__label__natural_language_processing Traditional methods rely heavily on extensive data for Reinforcement Learning from Human Feedback (RLHF), but representation engineering offers a new, training-free approach.
__label__optimization The influence maximization (IM) problem aims to identify a budgeted set of nodes with the highest potential to influence the largest number of users in a cascade model, a key challenge in viral marketing.
__label__machine_vision We introduce CogVLM, a powerful open-source visual language foundation model.
__label__machine_learning_for_other_sciences_and_fields To tackle this issue, methods based on sensitivity analysis and leveraging a few randomized controlled trial (RCT) data for model calibration are proposed.
__label__diffusion_based_models We demonstrate that the proposed PaDIS approach enables solving various inverse problems in both natural and medical image domains, including CT reconstruction, deblurring, and superresolution, given only patch-based priors.
__label__machine_learning_for_physical_sciences To address the problem of new materials design and fasten the process of new materials search, we have applied latest generative approaches to the problem of crystal structure design, trying to solve the inverse problem: by given properties generate a structure that satisfies them without utilizing supercomputer powers.
__label__machine_vision Domain generalization (DG) methods aim to maintain good performance in an unseen target domain by using training data from multiple source domains.
__label__interpretability_and_explainability We first prove theoretically that arbitrary features (not just knowledge) satisfy the consistency structure of a popular unsupervised knowledge-elicitation method: contrast-consistent search.
__label__diffusion_based_models This work develops an algorithmic framework for employing score-based diffusion models as an expressive data prior in nonlinear inverse problems with general forward models.
__label__graph_neural_networks Most advanced GCL methods fall into three main frameworks: node discrimination, group discrimination, and bootstrapping schemes, all of which achieve comparable performance.
__label__safety_in_machine_learning However, watermarks that keep the image semantically similar can be an alternative defense against our attacks.
__label__interpretability_and_explainability However, most GNNs operate as black-box models and require post-hoc explanations, which may not suffice in high-stakes scenarios where transparency is crucial.
__label__natural_language_processing LayTextLLM not only streamlines the interaction of layout and textual data but also shows enhanced performance in Key Information Extraction (KIE) and Visual Question Answering (VQA).
__label__machine_vision While images typically require 2D non-causal modeling, texts utilize 1D causal modeling.
__label__machine_vision We validate our approach on a cross-dataset setting with three skeleton action datasets, outperforming other domain generalization approaches by a considerable margin.
__label__natural_language_processing To relieve this problem, we tailor an abstract reasoning dataset (AbsR) together with a meaningful learning paradigm to teach LLMs how to leverage generic facts for reasoning purposes.
__label__machine_vision We show that with a minimal drop in performance, GL-NeRF can significantly reduce the number of MLP calls, showing the potential to speed up any NeRF model.
__label__safety_in_machine_learning Extensive experimental results on multiple benchmarks built upon various datasets demonstrate that our method achieves state-of-the-art performance in terms of detection accuracy and speed.
__label__learning_theory For the squared loss, $\mathsf{DLQ}$ matches the complexity of Correlation Statistical Queries $(\mathsf{CSQ})$—potentially much worse than $\mathsf{SQ}$.
__label__machine_vision However, compared to a multiview setup, the reduction in the number of cameras increases uncertainty in 3D reconstruction.
__label__deep_learning_architectures In this paper, we introduce SlotSSMs, a novel framework for incorporating independent mechanisms into SSMs to preserve or encourage separation of information.
__label__learning_theory Furthermore, we consider model selection performed by minimization of the validation error and provide a concentration bound for the regret.
__label__learning_theory Anchor-based strategies have been treated as effective ways to alleviate such efficiency problems by propagation on representative entities instead of the whole graph.
__label__reinforcement_learning Code and pre-trained models are available at https://thuml.github.io/iVideoGPT.
__label__fairness Trained on enormous amounts of data, these models have been shown to contain harmful biases which can hurt their performance when adapted for a downstream classification task.
__label__reinforcement_learning This technique reveals itself as a powerful analytical tool to better understand learning rate schedules in deep reinforcement learning, and as a means of improving robustness to nonstationarity in synthetic plasticity loss benchmarks along with both the single-task and sequential variants of the Arcade Learning Environment.
__label__online_learning While we obtain those positive results for the stochastic setting, another main result of our work is an impossibility in the adversarial setting.
__label__machine_learning_for_other_sciences_and_fields Extensive testing on the dataset demonstrates that our approach significantly outperforms state-of-the-art methods.
__label__machine_learning_for_other_sciences_and_fields While versatile LLMs appear to have demonstrated capabilities in understanding contextual causality and providing responses that obey the laws of causality, it remains unclear whether they perform genuine causal reasoning akin to humans.
__label__safety_in_machine_learning \textbf{CeTaD} strategically fine-tunes the normalization layer parameters within the defender using a limited set of clean and adversarial examples.
__label__machine_vision In this paper, we recognize the different degradation patterns in nighttime images and propose N2D3 (Night to Day via Degradation Disentanglement).
__label__machine_learning_for_other_sciences_and_fields This paper demonstrates that pre-trained language models (PLMs) are strong foundation models for on-device meteorological variable modeling.
__label__natural_language_processing We confirm this on datasets of natural language representations.
__label__machine_vision In the computer vision domain, state-of-the-art approaches utilize transformations like random crop and color jitter to achieve invariant representations, embedding semantically the same inputs despite transformations.
__label__deep_learning_architectures We qualitatively and quantitatively motivate the need for such a constraint, showing its benefits when merging homogeneous sets of models in scenarios spanning varying architectures and datasets.
__label__natural_language_processing To test our approach in an offline RL setting, we introduce the Code World Models Benchmark (CWMB), a suite of program synthesis and planning tasks comprised of 18 diverse RL environments paired with corresponding textual descriptions and curated trajectories.
__label__causal_inference In this paper, we propose a unified ITR estimation framework formulated as a constrained, weighted, and smooth convex optimization problem.
__label__generative_models Overall, our work systematically elucidates the synergy between Hamiltonian dynamics, force fields, and generative models, thereby opening new avenues for applications of machine learning in physical sciences and dynamical systems.
__label__optimization To the best of our knowledge, ZO-GDEGA is the first ZO algorithm with complexity guarantees to solve stochastic NC-C problems.
__label__neuroscience_and_cognitive_science We propose EEGPT, a novel 10-million-parameter pretrained transformer model designed for universal EEG feature extraction.
__label__machine_vision This approach has garnered substantial interest within the community, as seen in SRe$^2$L, G-VBSM, WMDD, and CDA.
__label__probabilistic_methods To counter, we develop Sketched Lanczos Uncertainty (SLU): an architecture-agnostic uncertainty score that can be applied to pre-trained neural networks with minimal overhead.
__label__online_learning Some of our results are proved via a reduction to prediction with expert advice under bandit feedback, a problem interesting on its own right.
__label__machine_learning_for_physical_sciences We experiment with the model on both global and limited area forecasting.
__label__generative_models Instead, current approaches for training these models rely on minimizing the log-squared difference between a proposal (forward policy) and a target (backward policy) distributions.
__label__neuroscience_and_cognitive_science To guide the SNN to fire spikes at optimal time steps, we propose the Spiking Position-aware Loss (SPA-Loss) to modulate the neuron's state.
__label__machine_vision However, plane-based methods rely on the inappropriate low-rank assumption and excessively decompose the space-time 4D encoding, resulting in overmuch feature overlap and unsatisfactory rendering quality.
__label__privacy Lastly, we present a new metric called MaxRényi-K%, which is based on the confidence of the model output and applies to both text and image data.
__label__optimization Ensuring that the outputs of neural networks satisfy specific constraints is crucial for applying neural networks to real-life decision-making problems.
__label__machine_learning_for_other_sciences_and_fields Learning subset-valued functions with neural networks has achieved great success by incorporating permutation invariance symmetry into the architecture.
__label__optimization This paper reveals the presence of topological obstruction in the loss landscape of shallow ReLU neural networks trained using gradient flow.
__label__machine_learning_for_other_sciences_and_fields We also repurpose linker design methods as strong baselines for this task.
__label__machine_learning_for_healthcare Experiments on a pharmacokinetic Warfarin dataset reveal that D3 identifies a new plausible model that is well-fitting, highlighting its potential for precision dosing in clinical applications.
__label__deep_learning_architectures Instead, we propose Meta Linear Attention (MetaLA) as a solution that satisfies these conditions.
__label__safety_in_machine_learning The security threat of backdoor attacks is a central concern for deep neural networks (DNNs).
__label__learning_theory In particular, the dependence on the number of input points can be reduced to polylogarithmic.
__label__optimization Moreover, using the proposed approach, _we provide the first accelerated optimization algorithm using the Order Oracle_.
__label__machine_vision To fully align compositional representations with continuous vector spaces, we extend Smolensky's Tensor Product Representation (TPR) and propose a new type of inherently *continuous* compositional representation, *Soft TPR*, along with a theoretically-principled architecture, *Soft TPR Autoencoder*, designed specifically for learning Soft TPRs.
__label__learning_theory Understanding the power of parameterized quantum circuits (PQCs) in accomplishing machine learning tasks is one of the most important questions in quantum machine learning.
__label__other We demonstrate that the efficiency of the conversion can be significantly enhanced by a proper regularization of the activation sparsity of the base model.
__label__safety_in_machine_learning As for the data quality indicator, we compute the per-sample gradients with respect to the private data and the anchor dataset, and use the trace of the accumulated inner products as a measurement of data quality.
__label__machine_vision MapUnveiler runs efficiently with the proposed clip-level pipeline by avoiding redundant computation with temporal stride while building a global map relationship.
__label__graph_neural_networks To this end, we propose an innovative framework called TFGDA.
__label__neuroscience_and_cognitive_science We show that this goal can be achieved through a neural network controller that injects currents (actions) into a recurrent neural network of fixed random weights to maximize future cumulative action-state entropy.
__label__machine_vision We show this by introducing a simple method called attention transfer, where only the attention patterns from a pre-trained teacher ViT are transferred to a student, either by copying or distilling the attention maps.
__label__natural_language_processing Empirically, we demonstrate that LLM agents can outperform crowdsourced human annotators—on a set of∼16k individual facts, SAFE agrees with crowdsourced human annotators 72% of the time, and on a random subset of 100 disagreement cases, SAFE wins 76% of the time.
__label__reinforcement_learning The question of computational efficiency under our assumptions remains open.
__label__optimization We show that FedAWE converges to a stationary point of even non-convex objectives while achieving the desired linear speedup property.
__label__machine_vision Radar signal interpretation plays a crucial role in remote detection and ranging.
__label__machine_vision Based on a simple SAT benchmark, we find that SAT still faces the problem of large robust generalization gap and degradation on natural samples.
__label__machine_vision By translating all multilingual image-text pairs from a raw web crawl to English and re-filtering them, we increase the prevalence of (translated) multilingual data in the resulting training set.
__label__machine_learning_for_other_sciences_and_fields Elaborately, symbolic equivalence identifies the logical homogeneity among autoformalization candidates using automated theorem provers, and semantic consistency evaluates the preservation of the original meaning by informalizing the candidates and computing the similarity between the embeddings of the original and informalized texts.
__label__natural_language_processing First, in practice, a new paper may select its citations from gigantic existing papers, where the combined texts far exceed the context length of LLMs.
__label__optimization The goal in this framework is for algorithms to achieve an improved performance when the predictions are accurate while maintaining acceptable guarantees when the predictions are erroneous.
__label__machine_vision Our innovative design not only for the first time demonstrates unarguable performance advantages over supervised counterparts, but also enjoys unprecedented efficiency in data, training, and inference.
__label__generative_models LoRA also exhibits concept-loss when multiple adapters are used concurrently.
__label__neuroscience_and_cognitive_science This inference is only valid if the subset of neural activity predicted by LLMs reflects core elements of language processing.
__label__natural_language_processing While parameter-efficient fine-tuning (PEFT) considerably reduces the training memory associated with parameters, it does not address the significant computational costs and activation memory.
__label__reinforcement_learning However, state-of-the-art offline RL algorithms perform sub-optimally when confronted with limited data confined to specific regions within the state space.
__label__machine_learning_for_other_sciences_and_fields We further demonstrate the utility of gRNAde on a new benchmark of multi-state design for structurally flexible RNAs, as well as zero-shot ranking of mutational fitness landscapes in a retrospective analysis of a recent RNA polymerase ribozyme structure.
__label__natural_language_processing Our scheme outperforms SOTA detectors under multiple metrics.
__label__other Time-series data in real-world settings typically exhibit long-range dependencies and are observed at non-uniform intervals.
__label__other However, many real-world datasets with low intrinsic dimensionality reside in an ambient space of much higher dimensionality.
__label__learning_theory This last step has substantial independent interest as it is grounded in a generalization of Leibniz-Newton's fundamental Theorem of calculus.
__label__interpretability_and_explainability We consider the dataset valuation problem, that is the problem of quantifying the incremental gain, to some relevant pre-defined utility of a machine learning task, of aggregating an individual dataset to others.
__label__machine_learning_for_healthcare We showcase the advantages of our representation compared to existing methods using synthetic data and real-world examples motivated by biomedical applications.
__label__optimization However, few existing methods can optimize the Sharpe ratio with the m-sparse constraint, due to the nonconvexity and the complexity of this constraint.
__label__optimization To estimate the changes in residual error after swaps, we propose a matched swap pair construction method to bound the approximation loss, ensuring a constant probability of loss reduction in each local search step.
__label__generative_models 2) Based on MV-VDM, we introduce a framework combining reconstruction and 4D Score Distillation Sampling (4D-SDS) to leverage the multi-view video diffusion priors for animating 3D objects.
__label__optimization The noise model is motivated by applications in overparametrized machine learning.
__label__learning_theory 3.
__label__algorithmic_game_theory We then investigate, both theoretically and empirically, approaches for mitigating polarization and promoting diversity in recommender systems.
__label__machine_vision To tackle these issues, we present a dual strategy: GenIR, an innovative data curation pipeline, and DreamClear, a cutting-edge Diffusion Transformer (DiT)-based image restoration model.
"__label__learning_theory We find that the variance quantities depend on the
non-linearity w.r.t."
__label__optimization We also give low-space streaming algorithms for John ellipsoids using similar ideas.
__label__machine_learning_for_physical_sciences We consider the problem of crystal materials generation using language models (LMs).
__label__diffusion_based_models Addressing these imperfections, we present Face2QR—a novel pipeline specifically designed for generating personalized QR codes that harmoniously blend aesthetics, face identity, and scannability.
__label__probabilistic_methods We demonstrate the applicability of our method on ambiguous classification tasks for uncertainty quantification.
__label__learning_theory Imagine a smart camera trap selectively clicking pictures to understand animal movement patterns within a particular habitat.
__label__active_learning To show the generality of our ideas, we study both absolute and ranking feedback models on items in the list.
__label__generative_models This gap can be attributed to the lack of semantic information provided by labels.
__label__other The LC factorization has multiple  advantages for low-rank OT including decoupling the problem into three OT problems and greater flexibility and interpretability.
__label__machine_learning_for_healthcare We introduce cell-type coherence loss and ontology alignment loss, which are minimized along with the masked gene expression prediction loss during the pre-training.
__label__optimization The various clustering problems that have been studied intensively include, e.g., the $k$-means problem and the $k$-center problem.
__label__robotics Visual imitation learning (VIL) provides an efficient and intuitive strategy for robotic systems to acquire novel skills.
__label__deep_learning_architectures In particular, we verify the advantages of MomentumSMoE over SMoE on a variety of practical tasks including ImageNet-1K object recognition and WikiText-103 language modeling.
__label__generative_models By removing vector quantization, our image generator achieves strong results while enjoying the speed advantage of sequence modeling.
__label__machine_vision To mitigate this issue, we present a new topology-preserving objective that can enforce feature topology structures of the combined base and novel classes to resemble the topology of CLIP.
__label__machine_vision The underlying reason behind this obstacle is that image restoration is sensitive to the spatial shift that occurs due to severe region-aware information loss, which exhibits a different behavior from high-level tasks.
__label__deep_learning_architectures Our proposed scaling shows behavior akin to the Maximal Update Parameterization, such as improved stability, better generalization, and transferability of optimal hyper-parameters from small to large scale SSMs.
__label__generative_models Internet of Things (IoT) sensing models often suffer from overfitting due to data distribution shifts between training dataset and real-world scenarios.
"__label__natural_language_processing The emergence of Large Language Models (LLMs) has significantly advanced natural language processing, but these models often generate factually incorrect information, known as ""hallucination."""
__label__algorithmic_game_theory We also show that the ratio of the learning rates of different bidders can qualitatively affect the convergence of the bidders.
__label__robotics A fundamental objective in robot manipulation is to enable models to comprehend visual scenes and execute actions.
__label__neuroscience_and_cognitive_science Recently, bio-inspired sensors with high dynamic range and ultra-high temporal resolution have been widely used in extreme vision scenarios.
__label__reinforcement_learning In the launching phase, the upper-level agents take the lead in making decisions and then communicate their actions with the lower-level agents.
__label__safety_in_machine_learning This issue is exacerbated for the setting of class-conditional coverage on imbalanced classification tasks with many and/or imbalanced classes.
__label__reinforcement_learning To achieve the distinguishability among trajectory representations of different agents, we introduce contrastive learning to maximize the mutual information between the trajectory representations and learnable identity representations of different agents.
__label__learning_theory We prove online consistency for all measurable functions in doubling metric spaces under the mild assumption that instances are generated by a process that is uniformly absolutely continuous with respect to an underlying finite, upper doubling measure.
__label__diffusion_based_models Moreover, we design a confusion-resistant strategy leveraging an indicator function and adaptive learning rate adjustment to mitigate the adverse effects of data heterogeneity and model inconsistency.
__label__optimization We then propose two variants of the Frank-Wolfe algorithm for solving the PGW problem and show that they are mathematically and computationally equivalent.
__label__machine_learning_for_other_sciences_and_fields We innovatively reframe the pocket prediction task as a pocket-ligand alignment problem rather than direct prediction in the first stage.
__label__diffusion_based_models We provide extensive experiments to demonstrate strong performance gains of the proposed method over contemporary baselines in the context of conditional graph generation, underscoring the potential of Twigs in challenging generative tasks such as inverse molecular design and molecular optimization.
__label__machine_vision To address these limitations, we generate the parameters of 3D Gaussians from a single image in a single forward pass.
__label__diffusion_based_models Additionally, the proposed PaDIS model is highly flexible and can be plugged in with different diffusion inverse solvers (DIS).
__label__natural_language_processing Using PV-Tuning, we achieve the first Pareto-optimal quantization for Llama-2 family models at  2 bits per parameter.
__label__machine_vision Besides diversity, it also provides better controllability for probabilistic trajectory generation.
__label__reinforcement_learning This paper proposes a \textit{flipping-based policy} for Chance-Constrained Markov Decision Processes (CCMDPs).
__label__algorithmic_game_theory Addressing this challenge, we introduce a tractable theoretical model of algorithmic monoculture in a two-sided matching market with many participants.
__label__machine_learning_for_healthcare Such entities, or *agents,* may *game* model decisions by manipulating their inputs to the model to obtain better outcomes and maximize some utility.
__label__machine_vision In this paper, we identify a main bottleneck in the state-of-the-art video text spotter: the limited recognition capability.
__label__other It iteratively optimizes the image by leveraging pre-trained weights, focusing on alternate reconstruction of different image parts, and gradually assembles fully denoised image within limited number of iterations.
__label__machine_vision Existing Neural Radiance Fields (NeRF) based works typically require extensive training and rendering time due to the adopted implicit representations.
__label__graph_neural_networks To mitigate this drawback, we introduce a novel algorithm, Fast Graph Sharpness-Aware Minimization (FGSAM), that integrates the rapid training of Multi-Layer Perceptrons (MLPs) with the superior performance of GNNs.
__label__other The key elements of our approach involve quantifying the uncertainty of response predictions via predictive inference and addressing individual and interactive constraints in a sequential manner.
__label__machine_vision To this end, we propose a Locally constrained Compact point cloud Model (LCM) consisting of a locally constrained compact encoder and a locally constrained Mamba-based decoder.
__label__neuroscience_and_cognitive_science At a cocktail party, humans exhibit an impressive ability to direct their attention.
__label__reinforcement_learning In this setting, *policy-based* methods are widely used since they come with several advantages when dealing with continuous-control problems.
__label__causal_inference The optimal ITR can be robustly and effectively computed by projected gradient descent.
__label__machine_vision Demos and code will be available at https://distillnerf.github.io/.
__label__optimization_for_deep_networks SAMPa achieves a twofold speedup of SAM under the assumption that communication costs between devices are negligible.
__label__optimization In this paper, we introduce MUVERA (MUlti-VEctor Retrieval Algorithm), a retrieval mechanism which reduces multi-vector similarity search to single-vector similarity search.
__label__machine_vision On the other hand, we introduce supplemental constraints from the 3D space by using a 3D detector to guide a further merging process.
__label__machine_learning_for_healthcare To address this challenge, we propose a novel medical VLP framework, named **Global to Dense level representation learning (G2D)**, which aims to learn global and dense visual features simultaneously using only image-text pairs without extra annotations.
__label__reinforcement_learning Such observations are available in many applications, including transactions, navigation and more.
__label__safety_in_machine_learning By sampling the optimal texture image from the diffusion model with a user-specific text prompt, our method can generate natural and customizable adversarial camouflage while maintaining high attack performance.
__label__privacy However, their measurement method requires training two models – one to estimate dataset-level correlations and the other to estimate memorization.
__label__machine_vision This gap hinders accurate sensory grounding in real-world scenarios.
__label__learning_theory Our work gives a converse to the polynomial method in algorithm design.
__label__safety_in_machine_learning Existing research has mainly focused on unimodal scenarios on image data.
__label__evaluation (1) Existing editing methods lead to inevitable performance deterioration on general benchmarks, indicating that existing editing methods maintain the general abilities of the model within only a few dozen edits.
__label__machine_vision However, video understanding remains a challenge despite the availability of substantial web video-text data.
__label__learning_theory All of our results explicitly allow the case of infinite-dimensional output variables, proving consistency of recent practical applications.
__label__machine_vision Continual learning aims to incrementally acquire new concepts in data streams while resisting forgetting previous knowledge.
__label__learning_theory Self-attention is the key mechanism of transformers, which are the essential building blocks of modern foundation models.
__label__optimization However, Unbalanced GW (UGW) can only be regarded as a discrepancy rather than a rigorous metric/distance between two metric measure spaces (mm-spaces).
__label__optimization Stochastic Gradient Descent (SGD) with adaptive steps is widely used to train deep neural networks and generative models.
"__label__machine_learning_for_other_sciences_and_fields In experiments, we show that our geometry-forward approach achieves higher-fidelity predictions with less
data than various baselines."
__label__generative_models To reconstruct a VoxSplat from images, we employ a hierarchical voxel latent diffusion model conditioned on the input images followed by a feedforward appearance prediction model.
__label__probabilistic_methods Next, we propose an iterative algorithm that, although greedily, searches the optimal histogram for every possible node split.
__label__other In this paper, we develop a nonparametric score to adaptively pick the right algorithm for ICA with arbitrary Gaussian noise.
__label__generative_models GDPO is based on an eager policy gradient tailored for graph diffusion models, developed through meticulous analysis and promising improved performance.
__label__machine_learning_for_other_sciences_and_fields Unlike natural language processing and computer vision, the development of Foundation Models (FMs) for time series forecasting is blocked due to data scarcity.
"__label__learning_theory The balance between these two forces offers an intuitive understanding
of feature learning in ResNets."
__label__graph_neural_networks It further benefits GNNs in regression tasks as well, reducing the mean squared error compared to all baselines.
__label__machine_vision By allowing relatively atypical samples to be adequately fitted while preserving reliable logit direction, the problem of overconfidence can be mitigated.
__label__robotics We conducted extensive validation of our proposed unified modality embodied agent using several simulation benchmarks, including Franka Kitchen, Meta-World, and Maniskill2, as well as in our real-world settings.
__label__robotics To address this problem, we propose DG-SLAM, the first robust dynamic visual SLAM system grounded in 3D Gaussians, which provides precise camera pose estimation alongside high-fidelity reconstructions.
__label__machine_vision We make three contributions: First, we introduce the first open-world counting model, CountGD,  where the prompt can be specified by a text description or visual exemplars or both; Second, we show that the performance of the model significantly improves the state of the art on multiple counting benchmarks -- when using text only, CountGD outperforms all previous text-only works, and when using both text and visual exemplars, we outperform all previous models; Third, we carry out a preliminary study into different interactions between the text and visual exemplar prompts, including the cases where they reinforce each other and where one restricts the other.
__label__optimization Moreover, HyperPrism could adaptively choose different mapping for different layers of the local model with a dedicated hypernetwork per device, achieving automatic optimization of DML in high divergence settings.
__label__active_learning Instead, we consider the calibration process from the perspective of Bayesian adaptive experimental design and propose a data-efficient algorithm to run maximally informative simulations within a batch-sequential process.
__label__learning_theory These spectral algorithms include kernel ridge regression, kernel principal component regression and various implementations of gradient descent.
__label__learning_theory It then applies the learned $\widehat{W}$ for next-token prediction, thereby verifying the mesa-optimization hypothesis.
__label__diffusion_based_models Adding additional guidance to pretrained diffusion models has become an increasingly popular research area, with extensive applications in computer vision, reinforcement learning, and AI for science.
__label__machine_vision Our dataset, SARDet-100K, is a result of intense surveying, collecting, and standardizing 10 existing SAR detection datasets, providing a large-scale and diverse dataset for research purposes.
__label__optimization_for_deep_networks Despite advances using low-rank adapters and quantization, pretraining of large models on consumer hardware has not been possible without model sharding, offloading during training, or per-layer gradient updates.
__label__safety_in_machine_learning While Large Language Models (LLMs) display versatile functionality, they continue to generate harmful, biased, and toxic content, as demonstrated by the prevalence of human-designed *jailbreaks*.
__label__machine_vision Visible-infrared person re-identification (VIReID) is widely used in fields such as video surveillance and intelligent transportation, imposing higher demands on model security.
__label__machine_vision Meanwhile, to enhance the infrared modality with extracted textual representations, we leverage modality alignment capabilities of VLMs and VLM-generated feature-level filters.
__label__robotics To further evaluate on vehicle-to-everything (V2X) scenario, we construct the first real-world V2X motion forecasting dataset V2X-Traj, which contains multiple autonomous vehicles and infrastructure in every scenario.
__label__neuroscience_and_cognitive_science We investigate how to generate maximal neural variability while at the same time having high network performance.
__label__safety_in_machine_learning We also show that RLbreaker is robust against three SOTA defenses and its trained agents can transfer across different LLMs.
__label__machine_vision Moreover, we introduce an alignment mechanism that utilizes reinforcement learning techniques to enhance the reverse generation trajectory of the diffusion model, ensuring improved performance and accuracy.
__label__machine_learning_for_other_sciences_and_fields Firstly, prior methods struggle to identify amino acids with Post-Translational Modifications (PTMs) due to their lower frequency in training data compared to canonical amino acids, further resulting in unsatisfactory peptide sequencing performance.
__label__optimization But this is not the final answer.
__label__deep_learning_architectures In a controlled head-to-head comparison with LLAMA2, MEGALODON achieves better efficiency than Transformer in the scale of 7 billion parameters and 2 trillion training tokens.
__label__bandits Bayesian optimisation (BO) is a powerful framework for global optimisation of costly functions, using predictions from Gaussian process models (GPs).
__label__optimization_for_deep_networks While most FL methods assume identical data domains across clients, real-world scenarios often involve heterogeneous data domains.
__label__machine_vision This paper presents a novel neural framework, LoSF-UDF, for reconstructing surfaces from 3D point clouds by leveraging local shape functions to learn UDFs.
__label__neuroscience_and_cognitive_science Comprehensive experiments on various dynamics in terms of grounded particle accuracy, dynamic rendering quality, and generalization ability demonstrate that NeuMA can accurately capture intrinsic dynamics.
__label__reinforcement_learning We provide empirical evidence demonstrating the efficacy of our approach compared to baselines in both stochastic and deterministic environments.
__label__probabilistic_methods Large Language Models (LLMs) provide a useful starting point for designing such a tool since they 1) provide an interface where users can incorporate expert insights in natural language and 2) provide an opportunity for leveraging latent problem-relevant knowledge encoded in LLMs that users may not have themselves.
__label__machine_vision To address the client gap, we design a local-synergistic contrastive learning approach that helps single-view clients and multi-view clients achieve consistency for mitigating heterogeneity among all clients.
__label__optimization_for_deep_networks In this work, we introduce a sharper ambient dimension-independent convergence analysis for sketch-DL using the second-order geometry specified by the loss Hessian.
__label__natural_language_processing This approach often yields a low-quality fine-tuning training set (e.g., incorrect plans or intermediate reasoning).
__label__graph_neural_networks This severely limits their potential to solve complex tasks.
__label__neuroscience_and_cognitive_science We make testable predictions: a) rapidly changing sensory context will disrupt place fields, b) place fields will form even if recurrent connections are blocked, but reversion to previously learned representations upon remapping will be abolished, c) the dimension of temporally smooth experience sets the dimensionality of place fields, including during virtual navigation of abstract spaces.
__label__safety_in_machine_learning We also show that our approach significantly outperforms the State-of-the-Art tools in closed-loop NNV
__label__robotics Given a few seed task descriptions and the robot APIs, Robo-Instruct is capable of generating a training dataset using only a small open-weight model.
__label__machine_vision The code and model are available at https://github.com/XingyuCuii/Virtual-Scanning-NLOS.
__label__machine_learning_for_other_sciences_and_fields To tackle these challenges, we introduce ProtGO, a unified model that harnesses a teacher network equipped with a customized graph neural network (GNN) and a Gene Ontology (GO) encoder to learn hybrid embeddings.
__label__algorithmic_game_theory However, collaboration in groups of strategic learners is not a given.
__label__machine_learning_for_healthcare In multi-sequence Magnetic Resonance Imaging (MRI), the accurate segmentation of the kidney and tumor based on traditional supervised methods typically necessitates detailed annotation for each sequence, which is both time-consuming and labor-intensive.
__label__optimization The omnipresence of NP-hard combinatorial optimization problems (COPs) compels domain experts to engage in trial-and-error heuristic design.
__label__active_learning Prior research shows that the outcomes of learning dynamics, which comprise both the services' adjustments and users' service selections, hinge significantly on the initial conditions.
__label__machine_vision We experimentally validate BIRD on several image restoration tasks and show that it achieves state of the art performance.
__label__interpretability_and_explainability However, an open question still remains: what is the degree of model simplification we can expect under different noise levels?
__label__optimization In-context learning (ICL) is a cornerstone of large language model (LLM) functionality, yet its theoretical foundations remain elusive due to the complexity of transformer architectures.
__label__optimization_for_deep_networks We propose a Regularized Adaptive Momentum Dual Averaging (RAMDA) algorithm for training structured neural networks.
__label__deep_learning_architectures Notably, UNIT retains the original vision encoder architecture, making it cost-free in terms of inference and deployment.
__label__safety_in_machine_learning Typically, these purified models exhibit low Attack Success Rates (ASR), rendering them resistant to backdoored inputs.
__label__deep_learning_architectures For the tasks considered, our results show separations based on the size of the model required for different architectures.
__label__machine_vision We present a novel paradigm that unifies anomaly segmentation into change segmentation.
__label__optimization Our empirical results show that the selected CGF can outperform the GMI cuts for certain distributions.
__label__active_learning Our algorithms consider how to acquire information for both the machine learning model and the human discretion model.
__label__graph_neural_networks GSIP achieves a 10\% increase in terms of the forgetting metric compared to prior methods on large-scale datasets.
__label__deep_learning_architectures The code is available on GitHub: \href{https://github.com/Ronglong-Fang/AddressingSpectralBiasviaMGDL}{\texttt{Addressing Spectral Bias via MGDL}}.
__label__machine_vision To validate RCDN, we create OPV2V-N, a new large-scale dataset with manual labelling under different camera failed scenarios.
__label__natural_language_processing On the practical side, when used for 1-2 bit vector quantization, PV-Tuning outperforms prior techniques for highly-performant models such as Llama and Mistral.
__label__privacy Differential privacy (DP) is a formal notion that restricts the privacy leakage of an algorithm when running on sensitive data, in which privacy-utility trade-off is one of the central problems in private data analysis.
__label__fairness In high-stakes domains such as healthcare and hiring, the role of machine learning (ML) in decision-making raises significant fairness concerns.
__label__natural_language_processing For the latter, we use the instruction data from the fine-tuning task, such as math or coding, to orientate the decomposition and train the largest $r$ components that most correspond to the task to learn.
__label__machine_vision Our approach not only sets a new standard for state-of-the-art works but also significantly enhances attack performance, exceeding the baseline method by over 16\%.
__label__diffusion_based_models Experiments demonstrate that our model achieves accurate unsupervised image segmentation and high-quality synthetic image generation across multiple datasets.
__label__reinforcement_learning (2022) is whether hybrid RL can improve upon the existing lower bounds established in purely offline and purely online RL without relying on the single-policy concentrability assumption.
__label__algorithmic_game_theory Previous works assume that the sender has knowledge about either the prior distribution over states of nature or receiver's utilities, or both.
__label__machine_vision Reducing to 16 tokens (8x less TFLOPs) only sacrifices the performance by 2.4 points on MMBench.
__label__optimization To stabilize Newton's method, we leverage a connection between Newton's method damped with trust regions and Kalman smoothing.
__label__natural_language_processing However, while code generation with Large Language Models (LLMs) is an extraordinarily active research area, test generation remains relatively unexplored.
__label__natural_language_processing Through experiments conducted on both newly introduced and established CKL benchmarks, TAALM proves the state-of-the-art performance upon the baselines, and also shows synergistic compatibility when integrated with previous CKL approaches.
__label__machine_vision The code is available at https://github.com/boyuh/AUCSeg.
__label__generative_models By leveraging signal provided by data attribution methods such as influence functions, SPA partitions data into subsets, each targeting unique aspects of the data, and trains multiple model adaptations optimized for these subsets.
__label__deep_learning_architectures A wide array of sequence models are built on a framework modeled after Transformers, comprising alternating sequence mixer and channel mixer layers.
__label__algorithmic_game_theory This model is often too pessimistic and does not adequately represent real-world online selection processes.
__label__machine_vision Extensive experiments on multiple downstream tasks show that SNELL achieves state-of-the-art performance with low memory usage, endowing PEFT with sparse tuning to large-scale models.
__label__learning_theory To optimize this loss function, we propose two families of surrogate losses: cost-sensitive comp-sum losses and cost-sensitive constrained losses.
__label__learning_theory The existing literature on rank collapse, however, has mostly overlooked other critical components in transformers that may alleviate the rank collapse issue.
__label__learning_theory We tackle it through a novel tensor decomposition perspective, proposing the Functional t-Singular Value Decomposition (Ft-SVD) theorem which extends the classical tensor SVD to infinite and continuous feature domains, providing a natural tool for representing and analyzing multi-output functions.
__label__learning_theory While most existing works in Markov games focus on external regret as the  learning objective, external regret becomes inadequate when the adversaries are adaptive.
"__label__generative_models To further achieve graph manipulation while keeping the visual content consistent, we introduce a Multi-Layered Sampler (MLS) for an ""isolated"" image editing effect."
__label__neuroscience_and_cognitive_science The experimental results indicate that our EEG-based visual zero-shot framework achieves SOTA performance in classification, retrieval and reconstruction, highlighting the portability, low cost, and high temporal resolution of EEG, enabling a wide range of BCI applications.
__label__graph_neural_networks Finally, SGA introduces a novel data augmentation perspective to enhance the training process of SGNNs.
__label__optimization We focus on one of the simplest yet fundamental settings: binary search (or searching a sorted array).
__label__other Existing AUC maximization methods usually assume that training and test distributions are identical.
__label__other Currently, KV cache quantization is performed per-channel or per-token independently.
__label__machine_vision Additionally, we introduce a LoRA composing strategy based on the degradation similarity, which adaptively combines trained LoRAs and enables our model to be applicable for mixed degradation restoration.
"__label__optimization This improves upon the current best rate of
$\sqrt{\frac{\Tr(\Sigma)\ln(\tfrac{1}{\delta})}{T}}$ for Clipped-SGD, known \emph{only} for smooth and strongly convex objectives."
__label__machine_vision Tracking points in video frames is essential for understanding video content.
__label__reinforcement_learning To this end, we present a novel framework called Cross-Task Policy Guidance (CTPG), which trains a guide policy for each task to select the behavior policy interacting with the environment from all tasks' control policies, generating better training trajectories.
__label__reinforcement_learning Reinforcement Learning (RL) encompasses diverse paradigms, including model-based RL, policy-based RL, and value-based RL, each tailored to approximate the model, optimal policy, and optimal value function, respectively.
__label__machine_vision Consequently, we achieve state-of-the-art performance in unsupervised low-light image enhancement across various benchmark datasets.
__label__generative_models We prove that, if the data is curated according to a reward model, then the expected reward of the iterative retraining procedure is maximized.
__label__machine_vision However, visual representations are inherently independent of language.
__label__privacy These include approximate certified unlearning for non-convex problems, complexity saving compared to retraining, sequential and batch unlearning for multiple unlearning requests.
__label__natural_language_processing The experimental results show that our optimization method effectively enhances the state vector and achieves the state-of-the-art performance on diverse tasks.
__label__generative_models We prove that the gradient flow induced by Fisher-FLow is optimal in reducing the forward KL divergence.
__label__reinforcement_learning In addition, we propose two gating mechanisms to improve the learning efficiency of CTPG: one gate filters out control policies that are not beneficial for guidance, while the other gate blocks tasks that do not necessitate guidance.
__label__natural_language_processing Extensive experimental results show that CPO significantly improves LLM performance in solving a variety of complex problems, including question answering, fact verification, and arithmetic reasoning, demonstrating its effectiveness.
__label__other Starting from a simple intuition that channels with larger weights would have larger gradients and the difference in weight norm enlarges between channels with similar weight, we empirically validate that wide and narrow layers show two different patterns with experiments across different data modalities and network architectures.
__label__online_learning Our experiment demonstrates the superiority of \sname in multiple aspects, including online adaptation performance, time, and memory efficiency.
__label__probabilistic_methods To address this issue, we present Symile, a simple contrastive learning approach that captures higher-order information between any number of modalities.
__label__causal_inference The theory is corroborated by experiments.
__label__reinforcement_learning The uncertainty measure is based on the conditional Q-value distribution, which is learned via a high-fidelity and efficient consistency model.
__label__algorithmic_game_theory This phenomenon is formulated by a decision-dependent distribution mapping within the recently proposed framework of performative prediction (PP) Perdomo et al.
__label__machine_vision Ultimately, our approach significantly outperforms the existing state-of-the-art methods on the mainstream benchmark OpenLane-V2 (23.9 v.s.
__label__safety_in_machine_learning By labeling within this region, we can maximally disambiguate the two types of OOD data, thereby maximizing the utility of the fixed labeling budget.
__label__speech_and_audio While various aspects of song generation have been explored by previous works, such as singing voice, vocal composition and instrumental arrangement, etc., generating songs with both vocals and accompaniment given lyrics remains a significant challenge, hindering the application of music generation models in the real world.
__label__safety_in_machine_learning Backdoor attacks aim to inject a backdoor into a classifier such that it predicts any input with an attacker-chosen backdoor trigger as an attacker-chosen target class.
"__label__optimization We provide
our code for evaluation at https://github.com/amackenzie1/highline2024."
__label__probabilistic_methods Offline model-based optimization seeks to optimize against a learned surrogate model without querying the true oracle objective function during optimization.
__label__diffusion_based_models We exploit the unique construction of diffusion SDEs to further simplify the formulation of the continuous adjoint equations using *exponential integrators*.
__label__natural_language_processing We carry out our study by constructing a 1.4 trillion-token datastore named MassiveDS, which is the largest and the most diverse open-sourced datastore for retrieval-based LMs to date, and designing an efficient pipeline for studying datastore scaling in an accessible manner.
__label__graph_neural_networks In this paper, we explore graph learning-based methods for task planning, a direction that is orthogonal to the prevalent focus on prompt design.
__label__safety_in_machine_learning [1] further enhanced by Canales-Martínez et al.
__label__interpretability_and_explainability It also enables the analysis of previous speculations regarding the long-range capabilities of Mamba models.
__label__generative_models Sparse points are then uniformly sampled across the mesh surface, and are used to build a deformation graph to drive the motion of the 3D object for the sake of computational efficiency and providing additional constraint.
__label__fairness Using CulturePark, we generated 41,000 cultural samples to fine-tune eight culture-specific LLMs.
__label__machine_vision Furthermore, we have annotated over 8,000 anomaly videos with language descriptions, enabling effective training across diverse open-world scenarios, and also created 8,000 question-answering pairs for users' open-world questions.
__label__optimization The natural question is: must we randomize?
__label__machine_vision All resources including the benchmark, the training data, and the finetuned model checkpoints will be released.
__label__probabilistic_methods They remain stable and are most sample-efficient when estimated with just a single sample.
__label__machine_vision We also analyze several critical design choices of Zeroverse that contribute to LRM-Zero's capability and training stability.
__label__graph_neural_networks Further investigation reveals that the soft labels mitigate overfitting during training, leading to better generalization performance.
__label__learning_theory At the source stage, we transform a pre-trained deterministic model into a Bayesian Neural Network (BNN) via a variational warm-up strategy, injecting uncertainties into the model.
__label__generative_models Low Rank Adaptation (LoRA) has gained massive attention in the recent generative AI research.
__label__interpretability_and_explainability However, SAEs may learn more about the structure of the datatset than the computational structure of the network.
__label__deep_learning_architectures Experimental results on language modeling show that RetNet achieves favorable scaling results, parallel training, low-cost deployment, and efficient inference.
__label__diffusion_based_models We show that policies learned using Stable Control Representations are competitive with state-of-the-art representation learning approaches across a broad range of simulated control settings, encompassing challenging manipulation and navigation tasks.
__label__probabilistic_methods Unlike many existing formalisms, LCNs have the ability to represent cycles and allow specifying marginal and conditional probability bounds on logic formulae which may be important in many realistic scenarios.
__label__diffusion_based_models The final trained model Edgen is demonstrated to outperform these advanced models.
__label__interpretability_and_explainability Our project is available at https://anonymous.4open.science/r/12345-DFCC.
__label__machine_vision In this study, we propose Regression-based Analytic Incremental Learning (RAIL), which utilizes a recursive ridge regression-based adapter to learn from a sequence of domains in a non-forgetting manner and decouple the cross-domain correlations by projecting features to a higher-dimensional space.
__label__learning_theory From the learning theory perspective, the learning guarantees of SCO algorithms with known derivatives have been studied in the literature.
__label__other However, identifying and assessing evolving patterns in learning tasks often relies on subjective judgments rooted in the prior knowledge of human experts, lacking a standardized quantitative measure.
__label__causal_inference Our approach tests conditional independence on the pooled datasets to infer the dependence between system variables, including the context, to avoid introducing selection bias.
__label__bandits Unlike traditional MAB problems, the reward of each unit depends on the treatments assigned to other units, i.e., there is *interference* across the underlying network of units.
__label__machine_learning_for_other_sciences_and_fields One type of study that has quickly gained popularity employs ML to predict unobserved outcomes in massive samples, and then uses predicted outcomes in downstream statistical inference.
__label__deep_learning_architectures D3 employs discrete, learnable key-value dictionaries trained to capture symbolic features essential for decomposition operations.
__label__deep_learning_architectures By controlling temperature, SSA adapts the contextual sparsity of the attention map to the query embedding and its position in the context window.
__label__generative_models On ImageNet 256x256 benchmark, VAR significantly improve AR baseline by improving Frechet inception distance (FID) from 18.65 to 1.73, inception score (IS) from 80.4 to 350.2, with around 20x faster inference speed.
__label__privacy Experiment results on distributed mean estimation show that PPR consistently gives a better trade-off between communication, accuracy and central differential privacy compared to the coordinate subsampled Gaussian mechanism, while also providing local differential privacy.
__label__generative_models The recent introduction of Diffusion Transformers (DiTs) has demonstrated exceptional capabilities in image generation by using a different backbone architecture, departing from traditional U-Nets and embracing the scalable nature of transformers.
__label__optimization_for_deep_networks Temporal Domain Generalization (TDG) addresses the challenge of training predictive models under temporally varying data distributions.
__label__machine_vision Finally, we integrate the results from both branches to achieve WSI classification.
__label__reinforcement_learning Our results uncover that function approximation in robust offline RL is essentially distinct from and probably harder than that in standard offline RL.
__label__machine_vision Recently, there have been explorations of generalist segmentation models that can effectively tackle a variety of image segmentation tasks within a unified in-context learning framework.
__label__machine_vision Our code is available at https://github.com/xxyzll/UMB.
__label__machine_vision However, applying a similar approach to real-world objects and scenes is difficult due to a lack of large-scale data.
__label__online_learning [2023].
__label__deep_learning_architectures CherryQ outperforms existing quantization approaches in terms of perplexity and downstream task performance.
__label__machine_vision This method performs latent compression learning by maximizing the mutual information between the inputs and outputs of a causal attention model.
__label__probabilistic_methods We then propose several approximate schemes that allow us to scale MAP and Marginal MAP inference to larger problem instances.
__label__safety_in_machine_learning This oversight leads to instability and challenges in reproducing these attacks.
__label__machine_vision We also propose a point query mechanism to enable AnyChange's zero-shot object-centric change detection capability.
__label__diffusion_based_models Recent research in tabular data synthesis has focused on single tables, whereas real-world applications often involve complex data with tens or hundreds of interconnected tables.
"__label__interpretability_and_explainability In-context learning (ICL) allows transformer-based language models that are pre-trained on general text to quickly learn a specific task with a few ""task demonstrations""  without updating their parameters, significantly boosting their flexibility and generality."
__label__safety_in_machine_learning DeepFake can be bifurcated into entertainment applications like face swapping and illicit uses such as lip-syncing fraud.
__label__deep_learning_architectures In this work we consider the problem of discretization of neural operators in a general setting.
__label__learning_theory The notion of optimistically universal online learning was defined in [Hanneke, 2021] in order to understand learnability under minimal assumptions.
__label__machine_vision However, target domain data in the downstream tasks can be highly different from the pre-training phase, which makes it hard for such a single model to generalize well.
__label__optimization This is a parallel to the optimization of an *acquisition function in policy space*.
"__label__speech_and_audio Our experiments show that when VATT is compared to existing
video-to-audio generation methods in objective metrics, such as VGGSound audiovisual dataset, it achieves competitive performance when the audio caption is
not provided."
__label__optimization Our algorithm (AGNES) provably achieves acceleration for smooth convex and strongly convex minimization tasks with noisy gradient estimates if the noise intensity is proportional to the magnitude of the gradient at every point.
__label__optimization_for_deep_networks Theoretical insights from the aspects of ReLU activation and softmax function could explain the interesting phenomenon.
__label__machine_vision Our approach introduces the Mixed-SSM Block (MSB), which initially rearranges tokens from adjacent frames in an interleaved fashion and subsequently applies multi-directional S6 modeling.
__label__machine_vision Since MMDE can be viewed as the composition of MRDE and metric scale recovery, we attribute this difficulty to scene dependency, where MMDE models rely on scenes observed during supervised training for predicting scene scales during inference.
__label__optimization We study a class of offline learning algorithms for CLO with bandit feedback, which we term induced empirical risk minimization (IERM), where we fit a predictive model to directly optimize the downstream performance of the policy it induces.
__label__probabilistic_methods Markov jump processes are continuous-time stochastic processes which describe dynamical systems evolving in discrete state spaces.
__label__machine_vision Single-image relighting is a challenging task that involves reasoning about the complex interplay between geometry, materials, and lighting.
__label__safety_in_machine_learning Prior works on physical adversarial camouflage against vehicle detectors mainly focus on the effectiveness and robustness of the attack.
__label__interpretability_and_explainability To address this challenge, Concept Bottleneck Models (CBMs) have made significant progress by incorporating human-interpretable concepts into deep learning architectures.
__label__infrastructure In experiments with the Llama3-8B model, with MsT, we measure no degradation in throughput or convergence even with 12x longer sequences than standard implementations.
__label__causal_inference We use Causal Maximum Entropy (CMAXENT) as inductive bias to merge the predictors, however, we expect similar differences to hold also when we use other merging methods that take into account asymmetries between cause and effect.
__label__causal_inference We substantiate our theoretical claims with synthetic data experiments.
"__label__online_learning When compared to prior work,
the size of our discretization schemes scales gracefully with the approximation parameter, which translates to better regret in online learning."
__label__machine_learning_for_other_sciences_and_fields Presently, the task of protocol translation predominantly requires the manual and labor-intensive involvement of domain experts and information technology specialists, rendering the process time-intensive.
__label__neuroscience_and_cognitive_science We investigate a $\textit{unified}$ neurosymbolic system where transformations in the network can be interpreted simultaneously as both symbolic and neural computation.
__label__optimization We propose a novel family of decision-aware surrogate losses, called Perturbation Gradient (PG) losses, for the predict-then-optimize framework.
__label__deep_learning_architectures Moreover, when confined to a model architecture consisting of a single Neural ODE followed by a linear layer, TA-BN achieves 91.1\% test accuracy on CIFAR-10 with 2.2 million parameters, making it the first \texttt{unmixed} Neural ODE architecture to approach MobileNetV2-level parameter efficiency.
__label__reinforcement_learning Unsupervised Environment Design (UED) formalizes the problem of autocurricula through interactive training between a teacher agent and a student agent.
__label__machine_vision This network accurately predicts the complete object-centric occupancy volume for inaccurate object proposals by leveraging temporal information from long sequences.
__label__learning_theory By generating high-fidelity coresets of size significantly smaller than the input points, KT is known to speed up unsupervised tasks like Monte Carlo integration, uncertainty quantification, and non-parametric hypothesis testing, with minimal loss in statistical accuracy.
__label__human-AI_interaction We therefore hypothesized that human autonomy and SA performance improves through dynamic and selective copilot intervention.
__label__machine_learning_for_other_sciences_and_fields Until now, no tabular method has consistently outperformed classical supervised learning, which ignores these shifts.
__label__machine_vision With both differentiable pulling and splatting, we jointly optimize 3D Gaussians and the neural SDF with both RGB and geometry constraints, which recovers more accurate, smooth, and complete surfaces with more geometry details.
__label__machine_vision To address these challenges, we propose the first self-supervised framework for the task of spike-guided motion deblurring.
__label__reinforcement_learning Code is available at https://github.com/FanmingL/Recurrent-Offpolicy-RL.
__label__neuroscience_and_cognitive_science This speed is faster than the typical 30 Hz framerate, leaving critical computation time for the computation of feedback in a closed-loop setting.
__label__generative_models Our key insight in achieving this is that interaction semantics and dynamics can be decoupled.
__label__probabilistic_methods At the same time, it stays competitive for black-box supervised learning problems, where neural networks typically excel.
__label__other Here, we propose the method of contrastive denoising with noisy chain-of-thought (CD-CoT).
__label__machine_learning_for_physical_sciences Motivated by this, we look at deep equilibrium models (DEQs), which mimic an infinite-depth, weight-tied network using a fraction of the memory by employing a root solver to find the fixed points of the network.
__label__machine_learning_for_other_sciences_and_fields The reason lies in its inability to capture the intricate nonlinear spatio-temporal relationship found in large-scale, highly dynamic traffic data.
"__label__probabilistic_methods After training, this ""neural circuit"" has distilled the corresponding inductive bias and can successfully perform sequential inference over an open set of classes."
__label__machine_vision UDON's distillation approach is not only effective, but also very efficient, by sharing most model parameters between the student and all teachers, where all models are jointly trained in an online manner.
__label__generative_models Adapting large language models (LLMs) for specific tasks usually involves fine-tuning through reinforcement learning with human feedback (RLHF) on preference data.
__label__safety_in_machine_learning Large Language Models (LLMs) can elicit unintended and even harmful content when misaligned with human values, posing severe risks to users and society.
__label__deep_learning_architectures We present Parametric Piecewise Linear Networks (PPLNs) for temporal vision inference.
__label__generative_models Our preliminary observation has found that directly applying the short video diffusion model to generate long videos can lead to severe video quality degradation.
__label__machine_vision Prevailing image reconstruction methods, generating intermediate frames from these spike streams, often rely on complex step-by-step network architectures that overlook the intrinsic collaboration of spatio-temporal complementary information.
__label__optimization_for_deep_networks Comprehensive experimental results on diverse benchmarks demonstrate our method's performance superiority with even lower computational costs.
__label__other This limits current VQ-based PTQ works to low VQ dimensions ($\le 8$) that in turn limit quantization quality.
__label__machine_vision In this paper, we propose an unsupervised learning-based framework for NLOS imaging from irregularly undersampled transients~(IUT).
__label__deep_learning_architectures In this work, we introduce the Hadamard-derived linear Binding (HLB), which is designed to have favorable computational efficiency, and efficacy in classic VSA tasks, and perform well in differentiable systems.
__label__graph_neural_networks We complement our analysis of the graph domain with the study of general unitary convolutions and analyze their role in enhancing stability in general group convolutional architectures.
__label__causal_inference Finally, compatibility has deep connections to information theory.
__label__diffusion_based_models To bridge this gap, we propose a novel framework, termed MoMu-Diffusion, for long-term and synchronous motion-music generation.
__label__machine_vision To address depth ambiguity, we propose the Binary Depth Coordinates (BDC), which simplifies depth estimation into a binary classification of joint positions (front or back).
__label__natural_language_processing We study how information propagates in decoder-only Transformers, which are the architectural foundation of most existing frontier large language models (LLMs).
__label__graph_neural_networks We first present GED as a quadratic assignment problem (QAP) that incorporates these four costs.
__label__evaluation Simultaneously, the likelihood ratio is adjusted according to the density ratio by reducing the generalization error of the distribution discrepancy as transformed through the two ratios.
__label__learning_theory Machine learning for node classification on graphs is a prominent area driven by applications such as recommendation systems.
__label__natural_language_processing Experiments reveal that PaDeLLM-NER significantly increases inference speed that is 1.76 to 10.22 times faster than the autoregressive approach for both English and Chinese.
"__label__optimization_for_deep_networks These models break the stabilization of the sharpness, which we explain using a
simplified model of the joint dynamics of the learning rate and the curvature."
__label__diffusion_based_models We also introduce a cross-attention map regularization term to enhance the learning of the attention map.
__label__neuroscience_and_cognitive_science The vector binding operation can also be used to bind different contexts to spatial representations, yielding a model for entorhinal cortex and hippocampus.
__label__diffusion_based_models MotionBooth excels in preserving the appearance of subjects while simultaneously controlling the motions in generated videos.
__label__reinforcement_learning Both the algorithm and the upper bound are extended to multi-agent decentralized DT extractions by an iteratively-grow-DT procedure guided by an action-value function conditioned on the current DTs of other agents.
__label__graph_neural_networks As a result, extending the advantage of GNTK to temporal graphs becomes a critical problem.
__label__optimization_for_deep_networks However, LLMs often struggle with catastrophic forgetting when engaged in sequential task learning.
__label__natural_language_processing Our experimental results demonstrate that the introduced regularization technique markedly improves the accuracy of learned reward models across a variety of out-of-distribution (OOD) tasks and effectively alleviates the over-optimization issue in RLHF, offering a more reliable and robust preference learning paradigm.
__label__infrastructure These models are commonly fine-tuned locally and then uploaded by users to cloud platforms such as HuggingFace for secure storage.
__label__bandits For the multi-task linear bandit, we first analyze the preexisting hierarchical Thompson sampling (HierTS) algorithm, and improve its gap-independent Bayes regret bound from $O(m\sqrt{n\log{n}\log{(mn)}})$ to $O(m\sqrt{n\log{n}})$ in the case of infinite action set, with $m$ being the number of tasks and $n$ the number of iterations per task.
__label__optimization_for_deep_networks This result leads us to a cheap and memory-efficient algorithm for both fine-tuning and pre-training LLMs.
__label__bandits Our lower bound shows that how the geometry of the preferences and reward vectors changes the hardness of this problem.
__label__probabilistic_methods While typical ParVIs such as Stein Variational Gradient Descent (SVGD) approximate the gradient flow within a reproducing kernel Hilbert space (RKHS), many attempts have been made recently to replace RKHS with more expressive function spaces, such as neural networks.
__label__privacy }, average default rate in a transaction network), leading to severe consequences for data owners.
__label__machine_vision The pre-trained text-to-image diffusion models have been increasingly employed to tackle the real-world image super-resolution (Real-ISR) problem due to their powerful generative image priors.
__label__learning_theory Experimental results on three datasets demonstrate the method's effectiveness in mitigating error accumulation within the CTTA framework.
__label__neuroscience_and_cognitive_science We explore this accumulation by constructing two models under two distinct notions of a generation: episodic generations, in which accumulation occurs via in-context learning and train-time generations, in which accumulation occurs via in-weights learning.
__label__machine_vision Social relation reasoning aims to identify relation categories such as friends, spouses, and colleagues from images.
__label__other RankUp achieves this by converting the original regression task into a ranking problem and training it concurrently with the original regression objective.
__label__machine_vision The advent of 3D Gaussian Splatting (3DGS) enables real-time rendering and faster training.
__label__machine_vision Especially in the ImageNet-1k classification, DTEM achieves a 37.2\% reduction in FLOPs while maintaining a top-1 accuracy of 79.85\% with DeiT-small.
__label__safety_in_machine_learning Leveraging this insight, we propose a novel white-box jailbreak method that surgically disables a model's ability to refuse, with minimal effect on other capabilities.
__label__learning_theory Structured state space models (SSMs), the core engine behind prominent neural networks such as S4 and Mamba, are linear dynamical systems adhering to a specified structure, most notably diagonal.
__label__online_learning Unlike approaches for RL, MetaCURL handles full adversarial losses, not just stochastic ones.
__label__machine_vision The best hypothesis is selected to perform registration.
__label__machine_vision Meanwhile, we introduce a symmetric interactive attention block and a multi-motion field estimation block to further enhance the interaction capability of the overall network.
__label__machine_learning_for_physical_sciences Extensive experiments on a range of benchmark datasets validate the superiority of the proposed EGODE compared to various state-of-the-art baselines.
__label__machine_vision This modification ensures that the $\mathbf{w}$MSE component is always effective during training, providing extra constructive cues.
__label__natural_language_processing This avoids fine-tuning the agent, which is costly, challenging to scale with the number of users, and may even degrade its performance on other tasks.
__label__machine_vision Prior work has often approached this problem by using the principle of common fate, namely the fact that the motion of points that belong to the same object is strongly correlated.
__label__machine_vision We divide anchors into different levels and the anchors that are not coded yet can be predicted based on the already coded ones in all the coarser levels, leading to more accurate modeling and higher coding efficiency.
__label__machine_learning_for_healthcare This is followed by a uniquely designed Sparse MoE framework.
__label__online_learning [1997] and Hanneke et al.
__label__other The code and dataset will be made available upon publication.
__label__generative_models The accurate prediction of geometric state evolution in complex systems is critical for advancing scientific domains such as quantum chemistry and material modeling.
__label__graph_neural_networks Our code is available at https://github.com/W-rudder/TEA-GLM.
__label__natural_language_processing To ensure that the model focuses more on the information in the middle, we introduce a truncated Gaussian to encourage sampling from the middle part of the context during fine-tuning, thus alleviating the ''Lost-in-the-Middle'' problem faced by long-context LLMs.
__label__natural_language_processing Most existing prompting methods suffer from the issues of generalizability and consistency, as they often rely on instance-specific solutions that may not be applicable to other instances and lack task-level consistency across the selected few-shot examples.
__label__graph_neural_networks Our code is available at https://github.com/ipsitmantri/DiGRAF.
__label__machine_vision Image pyramids are commonly used in modern computer vision tasks to obtain multi-scale features for precise understanding of images.
__label__evaluation To unveil the inner workings of modern neural architectures, a recent work proposed an information-theoretic objective function called Sparse Rate Reduction (SRR) and interpreted its unrolled optimization as a Transformer-like model called Coding Rate Reduction Transformer (CRATE).
__label__probabilistic_methods We release an implementation that can be easily adapted to different data types and statistical models.
__label__machine_vision Particularly, to handle hand variations, we disentangle the 3D presentation of hands into optimization-based identity maps and learning-based latent geometric features and neural texture maps.
__label__interpretability_and_explainability We propose a dictionary learning based approach, applied to the representation of tokens.
__label__causal_inference Causal interactions among a group of variables are often modeled by a single causal graph.
__label__machine_vision Recent top-performing approaches are prompt-based methods that utilize a set of learnable parameters (i.e., prompts) to encode task knowledge, from which appropriate ones are selected to guide the fixed pre-trained model in generating features tailored to a certain task.
__label__machine_vision To address these issues, in this paper, we propose a novel framework called Generated and Pseudo Content guided Prototype Refinement (GPCPR), which explicitly leverages LLM-generated content and reliable query context to enhance prototype quality.
__label__natural_language_processing Specifically, IRCAN first identifies neurons that significantly contribute to context processing, utilizing a context-aware attribution score derived from integrated gradients.
__label__other Yet, it is unclear whether TTA methods can maintain their adaptability over prolonged periods.
__label__robotics Codes will be made publicly available.
__label__machine_vision Visual relationship understanding has been studied separately in human-object interaction(HOI) detection, scene graph generation(SGG),  and referring relationships(RR) tasks.
__label__other Code is publicly available at https://github.com/BIT-DA/DUSA.
__label__machine_vision Our approach involves the creation of two supplementary training tasks GenQA and EvalQA, aiming at fostering the skills of asking and assessing questions in the context of images.
__label__optimization Moreover, it is at least 10 times faster than state-of-the-art algorithms across all large-scale datasets.
__label__machine_vision However, implementing RAW image-based view synthesis directly using 3DGS is challenging due to its inherent drawbacks: 1) in nighttime scenes, extremely low SNR leads to poor structure-from-motion (SfM) estimation in dis- tant views; 2) the limited representation capacity of the spherical harmonics (SH) function is unsuitable for RAW linear color space; and 3) inaccurate scene structure hampers downstream tasks such as refocusing.
__label__deep_learning_architectures They are well known to emerge during standard transformer training and have the undesirable effect of hindering quantisation in afflicted models.
__label__probabilistic_methods Encouraging empirical results have been obtained with the family of alpha-divergences, but few works have focused on the asymptotic properties of the proposed algorithms, especially as the number of iterations goes to infinity.
__label__probabilistic_methods First, probabilistic inference is inherently hard, #P-hard to be precise.
__label__active_learning Three main data selection approaches are: (1) leveraging external non-CLIP models to aid data selection, (2) training new CLIP-style embedding models that are more effective at selecting high-quality data than the original OpenAI CLIP model, and (3) designing better metrics or strategies universally applicable to any CLIP embedding without requiring specific model properties (e.g., CLIPScore is one popular metric).
__label__diffusion_based_models Diffusion-based image generation models have achieved great success in recent years by showing the capability of synthesizing high-quality content.
__label__learning_theory We derive a logarithmic regret bound, which translates into a $O(\frac{1}{\Delta\cdot\epsilon}\cdot\log^2(1/\epsilon))$ sample complexity bound, with $\Delta$ being a problem-dependent parameter, yet independent of $\epsilon$.
__label__graph_neural_networks Secondly, the process of identifying and generating optimal augmentations generally involves substantial computational overhead.
__label__other Extensive experiments demonstrate that USIM outperforms traditional generative models in OOV item recommendation performance across traditional collaborative filtering and GNN-based collaborative filtering models.
__label__generative_models We propose a new mixture-of-experts prior, softly guiding each modality's latent representation towards a shared aggregate posterior.
"__label__machine_vision Experiments
on the ImageNet-1K dataset for classification show that RFPAR outperformed
state-of-the-art query-based pixel attacks."
__label__interpretability_and_explainability We find that Leela internally represents future optimal moves and that these representations are crucial for its final output in certain board states.
__label__learning_theory In this paper, we give the first theoretical analysis of this topic.
__label__causal_inference Further, we discuss our theory's implications for understanding the underlying mechanisms of latent diffusion models and provide corresponding empirical evidence for our theoretical insights.
__label__generative_models 1.
__label__causal_inference To discover causal graphs, we introduce the Cascade algorithm, which adds edges in topological order.
__label__machine_learning_for_healthcare In this paper, we introduce Medformer, a multi-granularity patching transformer tailored specifically for MedTS classification.
__label__diffusion_based_models This work not only pushes the boundaries of generative image compositing but also reduces reliance on expensive annotated datasets by effectively utilizing existing resources in innovative ways.
__label__learning_theory Specifically, for inadequately constrained distributions, the error can exponentially escalate as we progress through the gradual shifts.
__label__causal_inference We validate our theory on standard systems, and we demonstrate improved causal inference performance across a number of benchmark tasks.
__label__other Our theoretical and empirical analysis reveals an unexpected finding: for a given task, utilizing a publicly available, task- and architecture-agnostic model (referred to as the `prior model' in this paper) can effectively produce efficient data.
__label__safety_in_machine_learning Extensive experiments show that diverseMix achieves superior performance on commonly used and recent challenging large-scale benchmarks, which further confirm the importance of the diversity of auxiliary outliers.
__label__evaluation Model editing has become an increasingly popular alternative for efficiently updating knowledge within language models.
__label__deep_learning_architectures Furthermore, a Spatial-Temporal Transformer Encoder is proposed to model the spatial-temporal relationships and learn discriminative features for each candidate through guidance of the appearance-motion embeddings.
__label__machine_learning_for_other_sciences_and_fields Its application is expected standard in silicon design and the EDA industry has invested decades into the development of performant symbolic model checking algorithms.
__label__deep_learning_architectures Therefore, we employ the entropy as the proxy to partition blocks optimally, which aims to achieve satisfying trade-offs between discretization errors and the search cost.
__label__optimization_for_deep_networks Such an equivalence is crucial in GCIL settings as data distributions among different tasks no longer pose challenges to adopting our GACL.
__label__learning_theory We consider the problem of hypothesis testing for discrete distributions.
__label__diffusion_based_models Instead, we seek to understand the properties of these activations, such that the activations that are clearly inferior can be filtered out in advance via simple qualitative evaluation.
__label__online_learning Moreover, we demonstrate that FTPL-A also attains an $\tilde{O}(T^\frac{2}{3}(V_T+1)^\frac{1}{3})$ dynamic regret bound.
__label__machine_vision We introduce an optimization framework based on a conditional variational autoencoder, which jointly models the prompt and the granularity of the object with a latent probability distribution.
__label__machine_learning_for_other_sciences_and_fields Importantly, we utilize a domain adaptation method to facilitate distribution approximation for guiding the training of the teacher-student framework.
__label__safety_in_machine_learning harmful) examples, which are much easier and cheaper to collect (e.g.
__label__optimization_for_deep_networks Unlike distributed training, it typically orchestrates resource-constrained edge devices to communicate via a low-bandwidth communication network with a central server.
__label__other In this paper, we reveal that existing detectors suffer from substantial Accuracy drops in such cross-scene generalization.
__label__machine_vision Contrastive image-to-LiDAR knowledge transfer, commonly used for learning 3D representations with synchronized images and point clouds, often faces a self-conflict dilemma.
__label__machine_vision To this end, we propose a simple yet powerful 3D focusing-and-matching network for multi-instance point cloud registration by learning the multiple pair-wise point cloud registration.
__label__reinforcement_learning The source code is available in the supplementary.
__label__reinforcement_learning Our method enables the calculation of the soft value function used in the policy evaluation target without Monte Carlo approximation.
__label__neuroscience_and_cognitive_science A self-supervised learning objective is arguably a more biologically plausible organizing principle, as the optimization does not require a large number of labeled examples.
__label__deep_learning_architectures Neural Cellular Automata (NCA) enables the modeling of global visual-token representations through local interactions, with its training strategies and architecture design conferring strong generalization ability and robustness against noisy input.
"__label__learning_theory For this SSL setting, we analyze information theoretic lower bounds for accurate feature selection as well as computational lower bounds, 
assuming the low-degree likelihood hardness conjecture."
__label__machine_learning_for_healthcare We also show that the approach scales to very large fragment libraries, further increasing the number of potential molecules.
__label__learning_theory This typically necessitates computationally demanding methods such as neighborhood or ensemble-based label corrections.
__label__other However, these methods are not directly applicable to regression tasks.
__label__generative_models To fill this gap, we introduce Atlas3D, an automatic and easy-to-implement method that enhances existing Score Distillation Sampling (SDS)-based text-to-3D tools.
__label__learning_theory Moreover, we also demonstrate that deeper over-parameterization can further enhance the generalization capability of the model.
__label__safety_in_machine_learning $\texttt{SGen}^{\texttt{Sup}}$, a direct modification of the selective prediction, is a supervised learning algorithm which exploits entailment-labeled data, annotated by humans.
__label__machine_vision We propose Concentric Causal Attention (CCA), a simple yet effective positional alignment strategy that mitigates the impact of RoPE long-term decay in LVLMs by naturally reducing relative distance between visual and instruction tokens.
__label__generative_models Second, they induce a latent style space that we can interpret and manipulate algorithmically.
__label__reinforcement_learning Such formulations are applicable when there are separate sets of optimization criteria and constraints on a system's behavior.
__label__speech_and_audio To alleviate these issues, we introduce a new ADD model that explicitly uses the Style-LInguistics Mismatch (SLIM) in fake speech to separate them from real speech.
__label__privacy Finally, we provide an extensive empirical evaluation of our private and non-private algorithms under varying levels of statistical and size heterogeneity on the Reddit, StackOverflow, and Amazon Reviews datasets.
__label__graph_neural_networks This surprisingly simple method obtains SOTA performance in GLAD, performing best overall among 14 methods across 10 datasets.
__label__machine_vision This paper introduces Voxel Proposal Network (VPNet) that completes scenes from 3D and Bird's-Eye-View (BEV) perspectives.
__label__interpretability_and_explainability Selective explanations allow practitioners to specify the fraction of samples that receive explanations with initial guess, offering a principled way to bridge the gap between amortized explainers (one inference) and more computationally costly approximations (multiple inferences).
__label__machine_learning_for_other_sciences_and_fields Traditional spatiotemporal models mostly focus on a specific task by assuming a same distribution between training and testing sets.
__label__learning_theory On the negative side, we show that in many of these settings, _normalized_ spectral bisection outputs a partitioning that makes a classification mistake on a constant fraction of the vertices.
"__label__safety_in_machine_learning The key contribution is the novel notion of Parametric Linear Relaxation,
which enables PREPARED to construct tight output bounds of the DNN that are parameterized
by the new parameters $\theta'$."
__label__machine_vision To address these drawbacks, we propose a novel Prototypical Hash Encoding (PHE) framework consisting of Category-aware Prototype Generation (CPG) and Discriminative Category Encoding (DCE) to mitigate the sensitivity of hash code while preserving rich discriminative information contained in high-dimension feature space, in a two-stage projection fashion.
__label__diffusion_based_models Despite the analogy of NODEs as continuous-depth residual networks, their application in typical supervised learning tasks has not been popular, mainly due to the large number of function evaluations required by ODE solvers and numerical instability in gradient estimation.
__label__reinforcement_learning By directly modeling long-term outcomes in the dataset, DiSPOs avoid compounding error while enabling a simple scheme for zero-shot policy optimization across reward functions.
__label__optimization Empirical conclusions are made to guide the application of our FSEO framework.
__label__interpretability_and_explainability By contrast, most research into LLMs' internal mechanisms focuses on models at one snapshot in time (the end of pre-training), raising the question of whether their results generalize to real-world settings.
__label__optimization_for_deep_networks With these factors corrected, we obtain excellent agreement with the Hoffmann et al.
__label__safety_in_machine_learning While neural networks (NNs) have a large potential as autonomous controllers for Cyber-Physical Systems, verifying the safety of neural network based control systems (NNCSs) poses significant challenges for the practical use of NNs— especially when safety is needed for unbounded time horizons.
__label__learning_theory As a consequence, existing strategies to address the dynamic nature of data and goals exhibit poor real-world performance.
__label__learning_theory Especially, throughout the past few years, the concept of *bi-Lipschitzness* has been proved as a beneficial inductive bias in many areas.
__label__generative_models Notably, our method directly supervises the decoded geometry using a semi-continuous surface sampling strategy, diverging from previous methods relying on rendered images as supervision signals.
__label__interpretability_and_explainability Our experiments on various models and datasets demonstrate that feature attributions via selective explanations strike a favorable balance between explanation quality and computational efficiency.
"__label__optimization We demonstrate the effectiveness of Fitzpatrick losses for
label proportion estimation."
__label__learning_theory Bounds on the smallest eigenvalue of the neural tangent kernel (NTK) are a key ingredient in the analysis of neural network optimization and memorization.
__label__interpretability_and_explainability Specifically, we find that models write into low-rank subspaces of the residual stream to represent features which are then read out by later layers, forming low-rank *communication channels* (Elhage et al., 2021) between layers.
__label__generative_models Extensive experiments demonstrate the effectiveness of LPR, improving the EM score by up to 18.75 on common benchmarks with noisy annotations.
__label__machine_vision In this paper, we introduce DeBaRA, a score-based model specifically tailored for precise, controllable and flexible arrangement generation in a bounded environment.
__label__learning_theory In our setting, the conditional edge probability matrices given the latent variables are represented by $P$ for the source and $Q$ for the target.
__label__reinforcement_learning Diffusion planning has been recognized as an effective decision-making paradigm in various domains.
__label__reinforcement_learning We quantify the efficiency of temporal difference (TD) learning over the direct, or Monte Carlo (MC), estimator for policy evaluation in reinforcement learning, with an emphasis on estimation of quantities related to rare events.
__label__deep_learning_architectures In contrast, linear attention naturally enjoys linear complexity and has great potential to scale up to higher-resolution images.
__label__generative_models Despite the astronomical number of potential personalized tasks (e.g., $1.73\times10^{13}$), by our design, Tina demonstrates remarkable in-distribution and out-of-distribution generalization even trained on small datasets ($\sim 1000$).
__label__infrastructure When employing a grid search over hybrid parallelism hyperparameters in practical scenarios, our methods demonstrate a 16\% throughput improvement over the 1F1B baseline for large language models.
__label__learning_theory This includes establishing rigorous theoretical guarantees, coupled with finite sample bounds, regarding the coverage and width of our prediction intervals.
__label__interpretability_and_explainability Specifically, we propose a $\beta$-VAE based module to extract factors as the initial nodes of the graph, and leverage the multimodal large language model (MLLM) to discover and rank latent correlations, thereby updating the weighted edges.
__label__safety_in_machine_learning Crucially, the poisoned samples demonstrate transferability across different VLM architectures, posing a significant concern in black-box settings.
__label__privacy Moreover, for the case of unknown covariance, we present an algorithm whose sample complexity has improved dependence on the dimension, from $d^{1/2}$ to $d^{1/4}$.
__label__machine_vision Such decomposition permits rendering error gradients and object view-predictive models to recover object 3D completions and deformations while bounding box tracks guide the large object movements in the scene.
__label__machine_vision Our work highlights the remarkably accurate semantic knowledge embedded within diffusion UNet encoders that could then serve as foundation vision encoders for downstream tasks.
__label__learning_theory In this paper, we introduce imprecise label learning (ILL), a framework for the unification of learning with various imprecise label configurations.
__label__machine_learning_for_healthcare This eliminates the need for invasive procedures or uncertain treatment decisions.
__label__other We propose Any2graph, a generic framework for end-to-end Supervised Graph Prediction (SGP) i.e.
__label__generative_models While such knowledge offers high-level control over interaction semantics, it cannot grasp the intricacies of low-level interaction dynamics.
__label__learning_theory Using a localization technique, we also develop a fast-rate generalization bound.
__label__machine_learning_for_other_sciences_and_fields In this paper, we pioneer the exploration of explicitly modeling this periodicity to enhance the performance of models in long-term time series forecasting (LTSF) tasks.
__label__reinforcement_learning In addition, a reward predictive function is further introduced to filter task-irrelevant information in both modality-consistent and inconsistent features, ensuring information integrity while avoiding potential distractions.
__label__machine_vision By using HHRT, we extend the SW into Hierarchical Hybrid Sliced Wasserstein (H2SW) distance which is designed specifically for comparing heterogeneous joint distributions.
"__label__learning_theory We study the problem of learning a single neuron with respect to the $L_2^2$-loss in the presence of adversarial distribution shifts, where the labels can be arbitrary, and the goal is to find a ""best-fit"" function."
__label__causal_inference The model's inference capabilities allow for the immediate identification of intervention targets on unseen samples and novel causal graphs, circumventing the need for retraining.
__label__diffusion_based_models Additionally, different makeup styles generally have varying effects on the person face, but existing methods struggle to deal with this diversity.
__label__machine_learning_for_physical_sciences To mitigate this inherent deficiency of the default scatter-point optimization, this paper proposes and theoretically studies a new training paradigm as region optimization.
__label__bandits The key idea is to sample from a chain of approximate conditional posteriors, one for each stage of the reverse diffusion process, which are obtained by the Laplace approximation.
__label__diffusion_based_models Our thorough toy experiments thus contribute a deeper understanding of how diffusion models capture compositional structure in data, paving the way for future research aimed at enhancing factorization and compositional generalization in generative models for real-world applications.
__label__fairness We assess the fairness of an algorithm by comparing the GPI of different groups, and say that it achieves perfect *Perceptual Fairness* (PF) if the GPIs of all groups are identical.
__label__probabilistic_methods Latent Bayesian optimization (LBO) approaches have successfully adopted Bayesian optimization over a continuous latent space by employing an encoder-decoder architecture to address the challenge of optimization in a high dimensional or discrete input space.
__label__interpretability_and_explainability Transformers have shown impressive capabilities across various tasks, but their performance on compositional problems remains a topic of debate.
__label__safety_in_machine_learning In this paper, we revisit existing DOV methods and find that they all mainly focused on the first stage by designing different types of dataset watermarks and directly exploiting watermarked samples as the verification samples for ownership verification.
__label__reinforcement_learning However, the lack of multimodal data in such domains represents an obstacle to developing foundation models for embodied applications.
__label__natural_language_processing These losses are designed to effectively cluster related entities (input as texts) and organise them hierarchically.
__label__interpretability_and_explainability To tackle this challenge, we introduce a bidirectional weighted graph-based framework, to learn factorized attributes and their interrelations within complex data.
__label__optimization_for_deep_networks For small-scale initialization, we show that the learned weight matrices are approximately rank-one and that their singular vectors align.
__label__machine_learning_for_physical_sciences Although it has been empirically observed that PINNs encounter difficulties in convergence when dealing with high-order or high-dimensional PDEs, a comprehensive theoretical understanding of this issue remains elusive.
__label__other However, clients may also need to make real-time predictions on streaming data in non-stationary environments.
__label__machine_learning_for_other_sciences_and_fields We also developed a multimodal data collection system that combines a pressure glove, an sEMG armband, and a markerless finger-tracking module.
__label__machine_learning_for_healthcare It has inspired various applications in molecular property prediction and drug design.
__label__machine_vision Specifically, **$\text{Di}^2\text{Pose}$** employs a two-stage process: it first converts 3D poses into a discrete representation through a pose quantization step, which is subsequently modeled in latent space through a discrete diffusion process.
__label__safety_in_machine_learning Code is available at https://github.com/xiasong0501/GRAT.
__label__other Furthermore, we deploy ITR on the industrial recommender and achieve promising results.
__label__reinforcement_learning When solving long-horizon tasks, it is intriguing to decompose the high-level task into subtasks.
__label__reinforcement_learning However, they are designed for artificial agents that sense, think, and react much faster than the brain, and they tend to fail when operating under human-like sensory and reaction times.
__label__machine_vision This pathway first bootstraps the semantics of individual objects and then modulates the model to prioritize features relevant to these semantics.
__label__privacy Our code and dataset are available at: https://github.com/tsinghua-fib-lab/NeurIPS2024_SPV-MIA
__label__machine_vision Extensive experiments demonstrate that our AdaptIR achieves stable performance on single-degradation tasks, and excels in hybrid-degradation tasks, with training only 0.6% parameters for 8 hours.
__label__diffusion_based_models Mass transport problems arise in many areas of machine learning whereby one wants to compute a map transporting one distribution to another.
__label__neuroscience_and_cognitive_science Our findings shed new light on the formation and encoding properties of place cells, and also demonstrate an interesting case of representational reuse.
__label__reinforcement_learning Our framework is a semi-supervised approach that leverages expert demonstrations as weak supervision to derive a set of candidate reward functions that align with the task rather than only with the data.
__label__probabilistic_methods For challenging state estimation problems arising in domains like vision and robotics, particle-based representations attractively enable temporal reasoning about multiple posterior modes.
__label__graph_neural_networks However, graph sizes often become unwieldy, leading to storage, computation, and analysis challenges.
__label__reinforcement_learning However, current results indicate that offline RL often performs worse than imitation learning, and it is often unclear what holds back the performance of offline RL.
__label__reinforcement_learning Here $d$ is the dimension of the feature space and $H$ is the horizon.
__label__safety_in_machine_learning While the Denoising Diffusion Probabilistic Model (DDPM) offers an efficient single-step purification, it falls short in ensuring purified images reside on the data manifold.
__label__deep_learning_architectures Large language models (LLMs) have shown impressive performance on language tasks but face challenges when deployed on resource-constrained devices due to their extensive parameters and reliance on dense multiplications, resulting in high memory demands and latency bottlenecks.
__label__diffusion_based_models Consequently, each component is facilitated to compute in parallel on separate devices.
__label__reinforcement_learning This approach seeks to modify other agents' $Q$-values by increasing their return following beneficial actions (with respect to the Reciprocator) and decreasing it after detrimental actions, guiding them towards mutually beneficial actions without directly differentiating through a model of their policy.
__label__optimization This enables the definition of topological loss functions, which assert to which extent a given object exhibits some topological properties.
__label__causal_inference While the general formulation consists of a bi-level optimization procedure, we derive an efficiently estimable analytical kernel-based estimator of the bounds for the causal effect, a query of key interest, and demonstrate the efficacy of our approach in confounded, multivariate, nonlinear synthetic settings.
__label__graph_neural_networks IntraMix efficiently tackles both issues faced by graphs and challenges the prior notion of the limited effectiveness of Mixup in node classification.
__label__interpretability_and_explainability However, the scarcity of data coupled with the high expenses involved in sensor deployment results in notable data imbalances.
__label__generative_models By employing invertible transforms such as the delay embedding and the short-time Fourier transform, we unlock three main advantages: i) We can exploit advanced diffusion vision models; ii) We can remarkably process short- and long-range inputs within the same framework; and iii) We can harness recent and established tools proposed in the time series to image literature.
__label__machine_vision This paradigm enables us to leverage large-scale synthetic image pairs, featuring object-level and local region changes, derived from existing image datasets, which are independent of target anomaly datasets.
__label__diffusion_based_models Point cloud diffusion refines the distribution of 3D points fitted to both the multi-hypothesis shape condition and pixel-aligned image features, offering detailed clothed shapes and inpainting occluded parts of human bodies.
__label__bandits This feedback interpolates between the full-information and bandit scenarios depending on the auctions' results.
__label__deep_learning_architectures Moreover, compared to conventional change-of-variable models, the introduced NCoV exhibits augmented expressiveness for pdf modeling, especially in high-dimensional spaces.
__label__optimization_for_deep_networks CTDG tackles critical challenges including: 1) Characterizing the continuous dynamics of both data and models, 2) Learning complex high-dimensional nonlinear dynamics, and 3) Optimizing and controlling the generalization across continuous temporal domains.
__label__learning_theory More precisely, we first propose an improved minimax lower bound for the kernel regression problem in large dimensional settings and show that the gradient flow with early stopping strategy will result in an estimator achieving this lower bound (up to a logarithmic factor).
__label__reinforcement_learning We study reinforcement learning with _multinomial logistic_ (MNL) function approximation where the underlying transition probability kernel of the _Markov decision processes_ (MDPs) is parametrized by an unknown transition core with features of state and action.
__label__reinforcement_learning Despite the advancements in large language models (LLMs), recent agents built with these often neglect the control over discussion tactics, which are essential in communication scenarios and games.
__label__other In addition, we propose some new contrast functions and algorithms that enjoy the same fast computability as existing algorithms like FASTICA and JADE but work in domains where the former may fail.
__label__interpretability_and_explainability In this paper, we introduce LMAC-ZS (Listenable Maps for Zero-Shot Audio Classifiers), which, to the best of our knowledge, is the first decoder-based post-hoc explanation method for explaining the decisions of zero-shot audio classifiers.
__label__machine_vision Our extensive experiments demonstrate KptLLM's superiority in various keypoint detection benchmarks and its unique semantic capabilities in interpreting keypoints.
__label__privacy We propose a simple heuristic privacy analysis of noisy clipped stochastic gradient descent (DP-SGD) in the setting where only the last iterate is released and the intermediate iterates remain hidden.
__label__bandits Lastly, we provide experimental results to demonstrate the performance of our algorithms.
__label__speech_and_audio ELSA is a single model that is competitive with state-of-the-art for both semantic retrieval and 3D source localization.
__label__graph_neural_networks In this work, we propose Scale Equivariant Graph MetaNetworks - ScaleGMNs, a framework that adapts the Graph Metanetwork (message-passing) paradigm by incorporating scaling symmetries and thus rendering neuron and edge representations equivariant to valid scalings.
__label__machine_learning_for_healthcare This paper presents the **H**ybrid **E**arly-fusion **A**ttention **L**earning **Net**work (HEALNet) – a flexible multimodal fusion architecture, which: a) preserves modality-specific structural information, b) captures the cross-modal interactions and structural information in a shared latent space, c) can effectively handle missing modalities during training and inference, and d) enables intuitive model inspection by learning on the raw data input instead of opaque embeddings.
__label__neuroscience_and_cognitive_science Task switching in the gating layer accelerates as a function of curriculum block size and task training, mirroring key findings in cognitive neuroscience.
__label__learning_theory If other $f$ exist and are quickly computable, they can be used in place of softmax for fast subquadratic attention algorithms.
__label__optimization In this work, we address this gap by providing the first high-probability complexity guarantees for nonconvex/PL minimax problems corresponding to a smooth function that satisfies the PL-condition in the dual variable.
__label__learning_theory Specializing our results to deterministic, stationary policies, we show that the gap between offline and online IL is not fundamental: (i) it is possible to achieve linear dependence on horizon in offline IL under dense rewards (matching what was previously only known to be achievable in online IL); and (ii) without further assumptions on the policy class, online IL cannot improve over offline IL with the logarithmic loss, even in benign MDPs.
__label__evaluation However, these metrics are not rigorously compared and benchmarked, instead presented with correlation to human Likert scores over a set of easy-to-discriminate images against seemingly weak baselines.
__label__deep_learning_architectures Extensive experiments demonstrate that our method not only achieves higher performance ceilings but also effectively overcomes distribution shift while allowing controllable adjustments according to user preferences.
__label__generative_models It first extracts diverse coding concepts from high-quality seed snippets to generate new tasks.
__label__bandits Efficient exploration in contextual bandits is crucial due to their large action space, where uninformed exploration can lead to computational and statistical inefficiencies.
__label__interpretability_and_explainability Furthermore, the universal feature alignment and the clustering of channels produce a picture and quantification of how visual information is processed through the different network layers, which produces precise comparisons between the networks.
__label__natural_language_processing Meanwhile, the orthogonal weight vector pairs are more flexible regarding the relative position, which may correspond to high-level syntactic information.
__label__reinforcement_learning While PVRs have been extensively studied in the context of model-free RL, their potential in MBRL remains largely unexplored.
__label__learning_theory Additionally, we show that ARDTs can be used on top of transformer representations to solve complex reasoning tasks.
__label__machine_learning_for_healthcare In this paper, we present the **Data-Driven Discovery (D3)** framework, a novel approach leveraging Large Language Models (LLMs) to iteratively discover and refine interpretable models of dynamical systems, demonstrated here with pharmacological applications.
__label__learning_theory In particular, we show that the square root-link admits an $O(\log(y_{\max}))$ dependence, where $y_{\max}$ denotes the largest count presented in the data, while the ID-link requires a $\Theta(\sqrt{y_{\max}/\log(y_{\max})})$ dependence.
__label__machine_vision We propose DistillNeRF, a self-supervised learning framework addressing the challenge of understanding 3D environments from limited 2D observations in outdoor autonomous driving scenes.
__label__natural_language_processing In this study, we aim to reduce generation latency for Named Entity Recognition (NER) with Large Language Models (LLMs).
__label__interpretability_and_explainability Despite their widespread use, the mechanisms by which large language models (LLMs) represent and regulate uncertainty in next-token predictions remain largely unexplored.
__label__safety_in_machine_learning Despite there are extensive successful attacks for image, designing a stealthy and effect attack for EEG is a non-trivial task.
__label__natural_language_processing We also benchmark thirteen language models on LongFact across four model families (Gemini, GPT, Claude, and PaLM-2), finding that larger language models generally achieve better long-form factuality.
__label__optimization_for_deep_networks We discover that using a single model's weights can hardly simulate all the models' performance.
__label__machine_learning_for_other_sciences_and_fields Guided by theoretical bounds on loss values, a sample selection criterion is introduced and modified to be more robust against potentially problematic values.
__label__generative_models It is also empirically verified that VAR outperforms the Diffusion Transformer (DiT) in multiple dimensions including image quality, inference speed, data efficiency, and scalability.
__label__machine_vision We conduct extensive experiments to verify the superiority of our CION in terms of efficiency and performance.
__label__graph_neural_networks First, we design a positive knowledge transfer module that ensures privacy during inter-domain knowledge transmission.
__label__optimization_for_deep_networks In this paper, we introduce an approximated Gauss-Newton (AGN) method for tackling the non-convex LRMS problem.
__label__reinforcement_learning We empirically evaluate C-LAP on the D4RL and V-D4RL benchmark, and show that C-LAP is competitive to state-of-the-art methods, especially outperforming on datasets with visual observations.
__label__bandits Finally, we provide numerical simulations to assess our theoretical findings.
__label__machine_learning_for_social_sciences We also explore the intrinsic properties of agent trust under conditions including external manipulations and advanced reasoning strategies.
__label__optimization The current state-of-the-art is a recent $0.401$-approximation algorithm, but its computational complexity makes it highly impractical.
__label__machine_vision Our approach utilizes homography to model global motion and employs multi-layer perceptrons (MLPs) to capture local residual deformations, enhancing the model’s ability to handle complex video dynamics.
__label__machine_vision It is trained on a curated set of 3D shapes and works on novel shape instances during testing.
__label__natural_language_processing Our approach involves (1) recalling relevant documents, (2) extracting instruction-response pairs, and (3) refining the extracted pairs using open-source LLMs.
__label__generative_models After converting to a graph representation, the RFM model takes samples from the LLM and iteratively refines the coordinates and lattice parameters.
__label__reinforcement_learning In this paper, we bring the concept of Distributional RL to MCTS, focusing on modeling value functions as categorical and particle distributions.
__label__machine_vision The growing interest in vision-language models (VLMs) has been driven by improvements in large language models and vision transformers.
__label__learning_theory Yet, a theoretical characterization of how such mechanisms emerge remains elusive.
__label__machine_vision However, due to not being specifically trained on 3D data, their application to multi-view data often exacerbates inconsistency, hence impacting the overall quality of the 3D output.
__label__machine_vision In response to this issue, we propose to efficiently turn an off-the-shelf query-based image text spotter into a specialist on video and present a simple baseline termed GoMatching, which focuses the training efforts on tracking while maintaining strong recognition performance.
__label__online_learning While existing prompt-based continual learning methods excel in leveraging prompts for state-of-the-art performance, they often lack a theoretical explanation for the effectiveness of prompting.
__label__optimization This paper introduces the notion of upper-linearizable/quadratizable functions, a class that extends concavity and DR-submodularity in various settings, including monotone and non-monotone cases over different types of convex sets.
__label__natural_language_processing LAIF first performs supervised fine-tuning (SFT) using demonstrations from a teacher model and then further fine-tunes the model with reinforcement learning (RL) or direct preference optimization (DPO), using feedback from a critic model.
__label__privacy However, simultaneously addressing both concerns is challenging; secure aggregation facilitates poisoning attacks as most anomaly detection techniques require access to unencrypted local model updates, which are obscured by secure aggregation.
__label__natural_language_processing Previous work addresses this challenge by independently training multiple dense expert models and using them to initialize an MoE.
__label__machine_learning_for_healthcare In this paper we propose Reaction-GFlowNet (RGFN), an extension of the GFlowNet framework that operates directly in the space of chemical reactions, thereby allowing out-of-the-box synthesizability while maintaining comparable quality of generated candidates.
__label__machine_vision Our method achieves a superior balance between data fidelity and visual quality compared to existing methods, as demonstrated with two popular lensless systems, PhlatCam and DiffuserCam.
__label__graph_neural_networks Graph neural networks (GNNs) are recognized for their strong performance across various applications, with the backpropagation (BP) algorithm playing a central role in the development of most GNN models.
__label__diffusion_based_models This paper studies the challenging task of makeup transfer, which aims to apply diverse makeup styles precisely and naturally to a given facial image.
__label__bandits Especially, the cost incurred during the selection (e.g., accessing LLM and evaluating the responses) is rarely explicitly considered.
__label__causal_inference The proposed DRM is nuisance-free, eliminating the need to fit models for nuisance parameters, and it effectively prioritizes the selection of a distributionally robust CATE estimator.
__label__optimization_for_deep_networks Our work thereby underscores the generality of $\mathcal{NC}$ as it extends to the novel and more challenging setting of language modeling.
__label__reinforcement_learning Notably, the proposed framework eliminates the requirement of any expert demonstration or domain knowledge at test time.
__label__natural_language_processing The code and demo are available at https://github.com/sail-sg/scaling-with-vocab and https://hf.co/spaces/sail/scaling-with-vocab-demo.
__label__graph_neural_networks This complexity might quickly become  prohibitive for large graphs provided they are not very sparse.
__label__diffusion_based_models To overcome this challenge, we introduce a novel diffusion-based box refinement approach.
__label__natural_language_processing From an optimization perspective, our SLED framework leverages the latent knowledge embedded within the LLM by contrasting the output logits from the final layer with those from early layers.
__label__causal_inference Finally, we validate our design using synthetic data from a clinical trial on cirrhosis.
__label__safety_in_machine_learning For the first time in the literature, we show that the jail-break effect can be mitigated by separating two states in the fine-tuning stage to respectively optimize over the alignment and user datasets.
__label__deep_learning_architectures We then formulate the exact formula for the value matrix in self-attention, theoretically and empirically demonstrating that this value matrix captures the eigenvectors of the Gram matrix of the key vectors in self-attention.
__label__natural_language_processing To address this problem, this paper introduces SnapKV, an innovative and fine-tuning-free approach that efficiently minimizes KV cache size while still delivering comparable performance in real-world applications.
__label__safety_in_machine_learning However, it suffers from high computational overhead, and we also find that it overly relies on prominent backdoor features that are highly distinguishable from benign features.
__label__privacy We empirically demonstrate that calibrating noise to attack sensitivity/specificity, rather than ε, when training privacy-preserving ML models substantially improves model accuracy for the same risk level.
__label__reinforcement_learning Deep reinforcement learning agents achieve state-of-the-art performance in a wide range of simulated control tasks.
__label__reinforcement_learning As a variant of the famous communication game Werewolf, *One Night Ultimate Werewolf* (ONUW) requires players to develop strategic discussion policies due to the potential role changes that increase the uncertainty and complexity of the game.
__label__optimization In this work, focusing on the central case of topological optimization for point clouds, we propose to overcome this limitation using diffeomorphic interpolation, turning sparse gradients into smooth vector fields defined on the whole space.
__label__machine_learning_for_other_sciences_and_fields While tabular classification has traditionally relied on from-scratch training, a recent breakthrough called prior-data fitted networks (PFNs) challenges this approach.
__label__deep_learning_architectures Extensive evaluations on the challenging nuScenes dataset demonstrate that CRT-Fusion achieves state-of-the-art performance for radar-camera-based 3D object detection.
__label__generative_models Our method ensures the consistency between visual and tactile textures while preserving photorealism.
__label__optimization More recently, Newton-type methods using sparsified Hessian matrices have demonstrated promising results on OT computation, but there still remain a lot of unresolved open questions.
__label__machine_vision Prior methods primarily focus on the stochastic nature of human motion, while neglecting the specific impact of external environment, leading to the pronounced artifacts in prediction when applied to real-world scenarios.
__label__machine_learning_for_other_sciences_and_fields Recently, representation-based boundary detection has gained popularity, but its emphasis on consecutive distance difference backfires, especially when the changes are gradual.
__label__interpretability_and_explainability While our results focus on synthetically defined toy datasets, we hypothesize a general claim on emergence of hidden capabilities may hold: generative models possess latent capabilities that emerge suddenly and consistently during training, though a model might not exhibit these capabilities under naive input prompting.
__label__optimization_for_deep_networks Kronecker) approximations used or any damping-based interpolation towards first-order updates.
__label__deep_learning_architectures We identify the main bottleneck in transformer structures caused by data transformation and propose a hardware-friendly core-periphery guided self-attention to decrease computation demands.
__label__causal_inference We prove, under certain assumptions about the instances of text and accuracy of the zero-shot predictions, that our method of inferring text-based proxies satisfies identification conditions of the proximal g-formula while other seemingly reasonable proposals do not.
__label__machine_learning_for_healthcare Experiments on multiple real-world clinical datasets demonstrate that our method outperforms state-of-the-art deep survival models in both discrimination and calibration.
__label__generative_models Large language models (LLMs) have shown impressive performance on downstream tasks by in-context learning (ICL), which heavily relies on the quality of demonstrations selected from a large set of annotated examples.
__label__reinforcement_learning In this work, we explore the use of Reward Machines for Deep RL in noisy and uncertain environments.
__label__safety_in_machine_learning The Benjamini-Hochberg (BH) procedure is widely used to control the false detection rate (FDR) in multiple testing.
__label__reinforcement_learning In this paper, we introduce *episodic future thinking (EFT) mechanism* for a reinforcement learning (RL) agent, inspired by the cognitive processes observed in animals.
__label__machine_vision We term our method Variance Reduction Fine-tuning (VRF), as it effectively reduces the variance in ensemble predictions, thereby decreasing residual error.
"__label__probabilistic_methods These embeddings can then be used as features for
tasks such as community detection/node clustering or link prediction, where they achieve state of the art
performance."
__label__diffusion_based_models It is also equipped with an innovative multi-resolution modeling and generation paradigm to capture the complex relationships between time series and their attributes.
__label__optimization Finally, we extend our results to stochastic and adversarially robust variants of our meta-learning algorithm.
__label__machine_vision Thus, in continual learning, SGG models are often required to expand, modify, retain, and reason scene graphs within the process of adaptive visual scene understanding.
__label__safety_in_machine_learning However, the theoretical relationship between the transferability of adversarial examples and their flatness has not been well established, making the belief questionable.
__label__machine_learning_for_other_sciences_and_fields Specifically, we extend the traditional binary angular margin loss to a balanced extension with feature angle distribution transformations under the Gaussian assumption, where the distributions are iteratively updated during classifier training.
__label__neuroscience_and_cognitive_science We find that $L_1$ regularization to be an important ingredient for structured embeddings and develop an adaptive regularization that adjusts the strength of regularization per neuron.
__label__optimization We provide rigorous sample complexity bounds for the selection of an effective CGF from certain parameterized families that provably performs well for any specified distribution on the problem instances.
__label__optimization_for_deep_networks We propose a series of algorithm enhancements that further reduce the memory footprint, and the accuracy gap compared to backpropagation.
__label__machine_learning_for_other_sciences_and_fields In this work, we introduce \emph{Feedforward-tied Energy-based Models} (ff-EBMs), a hybrid model comprised of feedforward and energy-based blocks housed on digital and analog circuits.
__label__generative_models We propose a significantly more efficient approach that can, at the same time, maintain both soundness and completeness.
__label__generative_models To further validate Lumina-Next as a versatile generative framework, we instantiate it on diverse tasks including visual recognition, multi-views, audio, music, and point cloud generation, showcasing strong performance across these domains.
__label__graph_neural_networks While existing graph-specific normalization layers have been proposed, they often struggle to offer substantial and consistent benefits.
__label__graph_neural_networks While tokenized graph Transformers have demonstrated strong performance in node classification tasks, their reliance on a limited subset of nodes with high similarity scores for constructing token sequences overlooks valuable information from other nodes, hindering their ability to fully harness graph information for learning optimal node representations.
__label__diffusion_based_models To address this issue, we propose **S**imple and **F**ast **D**istillation (SFD) of diffusion models, which simplifies the paradigm used in existing methods and largely shortens their fine-tuning time up to $1000\times$.
__label__probabilistic_methods In particular, variational methods are known to produce overconfident estimates of posterior uncertainty and are typically non-identifiable, with many latent variable configurations generating equivalent predictions.
__label__safety_in_machine_learning The task of distinguishing individuals of interest from a vast pool of candidates using predictive models has garnered significant attention in recent years.
__label__machine_vision To achieve this, existing methods use the agent model to extract information from the target dataset and embed it into the distilled dataset.
__label__causal_inference Such causal graphs delineate the relations among alarms and can significantly aid engineers in identifying and rectifying faults.
__label__machine_vision Our method employs a two-phase alternating optimization framework, similar to Expectation-Maximization (EM), where one phase reduces the geometry gap and the other addresses the modality gap.
__label__diffusion_based_models Furthermore, we design a semantic and depth conditioned diffusion model to generate multi-view images, which are used to learn a neural radiance field (NeRF) as the final scene representation.
"__label__machine_learning_for_other_sciences_and_fields To build SocraticLM, we first propose a novel ""Dean-Teacher-Student"" multi-agent pipeline to construct a new dataset, SocraTeach, which contains $35$K meticulously crafted Socratic-style multi-round (equivalent to $208$K single-round) teaching dialogues grounded in fundamental mathematical problems."
__label__graph_neural_networks Among the designs we investigate, the Word-frequency-based Text-level GIA (WTGIA) is particularly notable for its balance between performance and interpretability.
__label__reinforcement_learning While there exist many methods that allow for such savings, they generally trade computational efficiency for approximations of the exact Jacobian.
__label__machine_vision In zero-shot classification, OpenDlign surpasses previous models by 8.0\% on ModelNet40 and 16.4\% on OmniObject3D.
__label__other We examine the theoretical properties of the latent embedding space induced by our objective, demonstrating that its geometric structure is well-suited for solving downstream discriminative tasks.
__label__diffusion_based_models However, DDBM's sampling process typically requires hundreds of network evaluations to achieve decent performance, which may impede their practical deployment due to high computational demands.
__label__graph_neural_networks These challenges primarily arise from the violation of the independent and identically distributed (i.i.d.)
__label__neuroscience_and_cognitive_science MP solely relies on patch-level optical flows from video clips as inputs.
__label__machine_vision Discriminative training, exemplified by models like CLIP, excels in zero-shot image-text classification and retrieval, yet struggles with complex scenarios requiring fine-grained semantic differentiation.
__label__deep_learning_architectures However, the reliance on the non-maximum suppression (NMS) for post-processing hampers the end-to-end deployment of YOLOs and adversely impacts the inference latency.
__label__neuroscience_and_cognitive_science This trade-off is tuned by place cell width, which might explain the change in firing field scale along the dorsal-ventral axis of the hippocampus.
__label__graph_neural_networks Conversely, empirical experiments have shown that relatively small GNNs can solve LPs effectively, revealing a significant discrepancy between theoretical predictions and practical observations.
__label__graph_neural_networks Based on this theoretical insight, we propose HEGNN, a high-degree version of EGNN to increase the expressivity by incorporating high-degree steerable vectors while maintaining EGNN's efficiency through the scalarization trick.
__label__generative_models However, their approach only employs successful paths from decision trees (also called inference trees) for supervised fine-tuning (SFT), missing out on the potential learning opportunities from failed paths.
__label__generative_models Sufficient scene-level experiments on both object-centric and forward-facing datasets verify the effectiveness of MVInpainter, including diverse tasks, such as multi-view object removal, synthesis, insertion, and replacement.
__label__machine_learning_for_other_sciences_and_fields Although LLMs have been used to develop agent systems that surpass human teams and yield impressive investment returns, opportunities to enhance multi-source information synthesis and optimize decision-making outcomes through timely experience refinement remain unexplored.
__label__reinforcement_learning To tackle this challenge, we propose a robust RLHF approach -- $R^3M$, which models the potentially corrupted preference label as sparse outliers.
__label__diffusion_based_models Learning is driven entirely by the denoising diffusion objective, without any annotation or prior knowledge about regions during training.
__label__natural_language_processing We investigate how vocabulary size impacts LLM scaling laws by training models ranging from 33M to 3B parameters on up to 500B characters with various vocabulary configurations.
"__label__machine_vision Currently,
most of existing works have focused on addressing the distortion issues in 2D
panoramic images without considering spatial properties of indoor scene."
__label__optimization Finally, we probe performance.
__label__reinforcement_learning (2) Online Self-Curriculum Learning, which first estimates the policy's goal-achieving capability based on historical evaluation information and then selects progressively challenging goals for learning based on its current capability.
__label__machine_vision During training, our DoGaussian maintains one global 3DGS model on the master node and $K$ local 3DGS models on the slave nodes.
__label__deep_learning_architectures Therefore, it is only effective when the textual expressions are relatively simple.
__label__online_learning In practice, predictive features may be expensive, so we allow the decision maker to issue at most $k$ such queries.
__label__natural_language_processing For training scenario, we simulate arena battles among various state-of-the-art models on a large scale of instruction data, subsequently leveraging the battle results to constantly enhance target model in both the supervised fine-tuning and reinforcement learning .
__label__learning_theory We provide, (i) a non-adaptive algorithm making $O(n \log^2 n \log k)$ queries which improves to $O(n \log k)$ when the cluster sizes are within any constant factor of each other, (ii) for constant $k$, a non-adaptive algorithm making $O(n \log{\log{n}})$ queries.
__label__bandits Our experiments validate our theory and demonstrate dTS's favorable performance.
__label__diffusion_based_models Finally, we generate styled facial images from noise directly to complete the facial stylization task.
__label__learning_theory In particular, it is common practice to pretrain neural networks on a large auxiliary task before finetuning on a downstream task with fewer samples.
__label__optimization Empirical experiments show that our proposed algorithm achieves better quality and time compared to previous algorithms on both small and large datasets.
__label__bandits With an extra assumption on well-separated models, we can further improve the regret to $ O(d_0 \sqrt{T\log(d)} )$.
__label__interpretability_and_explainability Code: \url{https://github.com/jugechengzi/Rationalization-MRD}.
__label__reinforcement_learning To tackle this problem, we propose a novel robust variational Bayesian inference for offline RL (TRACER).
__label__machine_vision LiT enables state-of-the-art zero-shot and unified domain detection across diverse LiDAR datasets, marking a step toward data-driven domain unification for autonomous driving systems.
__label__privacy We also study PA-DP supervised learning with \textit{unlabeled} public samples.
__label__learning_theory In this paper, we shed light on the efficacy of ICL from the viewpoint of statistical learning theory.
__label__reinforcement_learning Inverse Reinforcement Learning (IRL) deals with the problem of deducing a reward function that explains the behavior of an expert agent who is assumed to act *optimally* in an underlying unknown task.
__label__natural_language_processing Our algorithm uses prior sample outputs to soundly overapproximate the future grammaticality of different output prefixes.
__label__optimization Through fine-grained analysis in the lens of primal-dual cyclic coordinate methods and the introduction of novel smoothness parameters, we present several results for shuffled SGD on smooth and non-smooth convex losses, where our novel analysis framework provides tighter convergence bounds over all popular shuffling schemes (IG, SO, and RR).
__label__machine_vision It is vital to infer a signed distance function (SDF) for multi-view based surface reconstruction.
__label__machine_vision Recently, ensemble-based models (ESM) have been shown to offer significant robustness improvement, while preserving high ID accuracy.
__label__neuroscience_and_cognitive_science NER also reveals distinct latent dynamics in S1 during consistent movements and in M1 during curved reaching tasks.
__label__machine_vision Lumen first promotes fine-grained vision-language concept alignment, which is the fundamental capability for various visual tasks.
__label__machine_vision SaSPA consistently outperforms all established baselines across multiple settings, including full dataset training and contextual bias.
__label__neuroscience_and_cognitive_science As these models grow increasingly popular, their use in studying neural activity provides insights into how our brains respond to such multi-modal naturalistic stimuli, i.e., where it separates and integrates information from different sensory modalities.
__label__privacy To reduce the communication cost of differential privacy mechanisms, we introduce a novel construction, called Poisson private representation (PPR), designed to compress and simulate any local randomizer while ensuring local differential privacy.
__label__interpretability_and_explainability Our explanatory framework generalizes PDPs, including them as a special case, as well as a variety of other interpretive plots that show, for example, the total, direct, and indirect effects of causal mediation.
__label__interpretability_and_explainability These channels can be clustered into recurring sets, corresponding to distinct brain regions, indicating the formation of visual concepts.
__label__other Finally, extensive experiments conducted on two datasets demonstrate the effectiveness of $\textit{FedMKD}$ which outperforms state-of-the-art baselines 4.78\% under linear evaluation on average.
__label__graph_neural_networks Our experimental results confirm that unitary graph convolutional networks achieve competitive performance on benchmark datasets compared to state-of-the-art graph neural networks.
__label__learning_theory As a result, LLMs can exhibit unintended or even harmful behaviours, such as leaking information, fake news or hate speech.
__label__generative_models To address this limitation, we propose a novel active viewpoint selection strategy.
__label__learning_theory We verify our predictions in simulations with synthetic data and experimentally study the importance of implicit bias in robust ERM with deep neural networks.
__label__machine_vision To address this issue, we propose the Tree of Attributes Prompt learning (TAP), which first instructs LLMs to generate a tree of attributes with a ``concept - attribute - description'' structure for each associated category name, and then learn the hierarchy with vision and text prompt tokens.
__label__machine_vision This paper investigates the 3D domain generalization (3DDG) ability of large 3D models based on prevalent prompt learning.
__label__safety_in_machine_learning We evaluate our approach on numerous neural networks used for image or text classification and show that it significantly enhances existing calibration methods.
__label__diffusion_based_models We next develop the *tilted transport* technique, which leverages the quadratic structure of the log-likelihood in linear inverse problems in combination with the prior denoising oracle to exactly transform the original posterior sampling problem into a new one that is provably easier to sample from.
__label__natural_language_processing In automatic quality evaluations simulating real-life interaction, the proposed system reduces the average conversation response latency by more than threefold compared with LLM-based half-duplex dialogue systems while responding within less than 500 milliseconds in more than 50% of evaluated interactions.
__label__generative_models We conduct experiments on tasks such as response detoxification, faithfulness enhancement, and sentiment revising, and show that PaCE achieves state-of-the-art alignment performance while maintaining linguistic capabilities.
__label__graph_neural_networks We compare TACO with a wide range of state-of-the-art baselines, proving its superiority and the necessity of preserving high-quality topological information for effective replaying.
__label__algorithmic_game_theory We emphasise the solution concept (i.e.
__label__generative_models Project page is available at https://GenWarp-NVS.github.io.
__label__reinforcement_learning In recent years, the application of reinforcement learning (RL) involving interactions with individuals has seen significant growth.
__label__algorithmic_game_theory These results provide an in-depth understanding of no-regret learning in harmonic games, nesting prior work on $2$-player zero-sum games, and showing at a high level that potential and harmonic games are complementary not only from the strategic but also from the dynamic viewpoint.
__label__learning_theory As a practical consequence of the path equivalences, we develop an efficient cross-validation method for tuning and apply it to subsampled pretrained representations across several models (e.g., ResNet-50) and datasets (e.g., CIFAR-100).
__label__reinforcement_learning Given the scarcity of motion capture data on multi-humanoid collaboration and the efficiency challenges associated with multi-agent learning, these tasks cannot be straightforwardly addressed using training paradigms designed for single-agent scenarios.
__label__safety_in_machine_learning Our empirical results show that 87% of the query-specific adversarial suffixes generated by the developed combination can induce Llama-2-7B-Chat to produce the output that exactly matches the target string on AdvBench.
__label__diffusion_based_models However, the expensive computation and massive parameters of DMs hinder their practical use in resource-constrained scenarios.
__label__generative_models Subjective and objective experiments demonstrate that our method surpasses the performance of state-of-the-art models.
__label__fairness In the basic recommendation paradigm, the most (predicted) relevant item is recommended to each user.
__label__diffusion_based_models This framework interacts with advanced models through their public APIs to obtain text-image data pairs to train a base model.
__label__machine_vision GL-NeRF significantly reduces the number of MLP calls needed for volume rendering, introducing no additional data structures or neural networks.
__label__graph_neural_networks Our framework could also facilitate more efficient local message-passing mechanisms for GNNs.
__label__deep_learning_architectures We propose a new perspective to reconsider the Fourier transform from a basis functions perspective.
__label__machine_vision both NDS and mAP.
__label__optimization By utilizing bilevel optimization, boundary subproblems are optimized and adjusted alternately, thereby refining their optimal solutions to align with the nadir objective vector.
__label__probabilistic_methods We further extend our result to show how to sample from a $d$-dimensional spectrahedron, the constrained set of a semidefinite program, specified by the set $\{x\in \mathbb{R}^d: \sum_{i=1}^d x_i A_i \succeq C \}$ where $A_1,\ldots,A_d, C$ are $n\times n$ real symmetric matrices.
__label__other Further validations on the proposed interactive scenario benchmark showcase planning compliance in interactive cases.
__label__machine_vision In this work, we introduce a novel approach that utilizes pseudo LiDAR point clouds generated from low-cost miniatures or real-world videos, which is called Pseudo Ground Truth augmentation (PGT-Aug).
__label__diffusion_based_models This unifies the frameworks of feature extraction and denoising, where the former progressively embeds features from low-level to high-level, and the latter recursively denoises features step-by-step.
"__label__machine_vision The key idea to unification is to train the model to effectively handle both visible and invisible parts of the sequence in an integrated manner;
the visible part is for temporal segmentation, and the invisible part is for future anticipation."
__label__generative_models Extensive experiments conducted on unconditional and class-conditioned object generation, digital avatar creation, and text-to-3D synthesis all show that our model achieves state-of-the-art generation results both qualitatively and quantitatively, underscoring the potential of GaussianCube as a highly accurate and versatile radiance representation for 3D generative modeling.
__label__machine_vision In the current MLLM rat race, the focus seems to be predominantly on the linguistic side.
__label__causal_inference We experiment with a wind simulator with partially known factors of variation.
__label__other The proposed ST$_k$ Loss outperforms AT$_k$ Loss and achieves the best average performance on multiple benchmarks, with the lowest standard deviation.
__label__machine_vision When fine-tuning zero-shot models like CLIP, our desideratum is for the fine-tuned model to excel in both in-distribution (ID) and out-of-distribution (OOD).
__label__reinforcement_learning The project page can be found at https://oswinso.xyz/rcppo.
__label__deep_learning_architectures These adversaries learn the marginal class probability functions over different data subspaces, while a single generator in the full space models the entire distribution of the inlier class.
__label__generative_models Then, it determines how to select data and schedule the adaptor training based on this scoring system.
__label__safety_in_machine_learning Existing backdoor defense methods typically require either homogeneous assumption, validation datasets, or client optimization conflicts.
__label__reinforcement_learning It is difficult to scale towards more general behavior without confronting challenges in multi-task optimization, and few solutions are compatible with meta-RL's goal of learning from large training sets of unlabeled tasks.
__label__diffusion_based_models Specifically, content shift is related to the information drift during the process of recovering an image from the noisy input, pointing out the possibility of turning off-the-shelf generation techniques into tools for content shift suppression.
__label__machine_vision To enable processing of arbitrarily sampled point-cloud textures and ensure long-distance texture consistency we introduce a fast re-sampling of the mesh spectral properties used during the heat diffusion and introduce a novel heat-diffusion-based self-attention mechanism.
__label__machine_vision In this paper, we propose MVSDet which utilizes plane sweep for geometry-aware 3D object detection.
__label__machine_vision A novel regularized within-prototype entropy (RWE) is introduced to achieve CoSW and stable prototype update.
__label__diffusion_based_models Additionally, we derive the corresponding sample complexity bound and show that the data distribution generated from the estimated score function converges toward a proximate area of the original one.
__label__machine_learning_for_other_sciences_and_fields Trained on a sequence-to-sequence task using an automatically constructed dataset, MSA-Generator employs protein-specific attention mechanisms to harness large-scale protein databases, generating virtual MSAs that enrich existing ones and boost prediction accuracy.
__label__machine_learning_for_physical_sciences These methods can become computationally expensive, especially when relying on solvers like Newton's method, which may struggle with ill-posedness near bifurcation points.
__label__optimization Finally, we provide extensive empirical studies, including synthetic functions, reinforcement learning tasks, and neural network training on various datasets, to underscore the substantial efficiency improvements achieved by our OptEx in practice.
__label__machine_learning_for_other_sciences_and_fields Inspired by recent progress in large multi-modal models, we propose that it is possible to harness the strengths of uni-modal code models from both sides to bridge the semantic gap effectively.
__label__learning_theory Furthermore, we investigate the excess generalization error of the ASGD algorithm, revealing the effects of asynchronous delay, model initialization, number of training samples and iterations on generalization performance.
__label__optimization al.
__label__safety_in_machine_learning Additionally, we highlight the scalability of our method to 100 LLMs, ranging from 0.5B to 236B parameters, incurring no additional cost as the number of parameters increases.
__label__safety_in_machine_learning Digital watermarking techniques are crucial for copyright protection and source identification of images, especially in the era of generative AI models.
__label__graph_neural_networks This task is nontrivial as the molecular pre-trained models are non-generative and exhibit a diversity of model architectures, which differs significantly from language and image models.
__label__natural_language_processing In this paper, we unveil the intrinsic capacity of LLMs for understanding extremely long sequences without any fine-tuning.
__label__graph_neural_networks Adversarial attacks against graph neural networks (GNNs) through perturbations of the graph structure are increasingly common in social network tasks like rumor detection.
__label__deep_learning_architectures We argue in this article that one has to move away from this basic tenet to obtain generalisation across distributions.
__label__machine_vision We experimentally show the effectiveness of BITR in practical tasks.
__label__natural_language_processing Through iterative refinement, we develop an optimized tokenizer.
__label__probabilistic_methods We introduce a novel architecture, the Transformer Neural Decision Process (TNDP), capable of instantly proposing the next experimental design, whilst inferring the downstream decision, thus effectively amortizing both tasks within a unified workflow.
__label__deep_learning_architectures VB-LoRA achieves extreme parameter efficiency while maintaining comparable or better performance compared to state-of-the-art PEFT methods.
__label__machine_learning_for_other_sciences_and_fields Structure-based drug design (SBDD), which aims to generate 3D ligand molecules binding to target proteins, is a fundamental task in drug discovery.
__label__machine_learning_for_other_sciences_and_fields The temporal encoder captures temporal dependencies within ETS data, taking into account exogenous variables.
__label__deep_learning_architectures D2R2 leverages the powerful expression ability of diffusion models to extract essential semantic knowledge crucial for denoising process.
__label__safety_in_machine_learning We complement our findings with empirical evaluations demonstrating rising steganographic capabilities in frontier single and multi-agent LLM setups and examining potential scenarios where collusion may emerge, revealing limitations in countermeasures such as monitoring, paraphrasing, and parameter optimization.
"__label__machine_vision We posit that enhancing MLLMs with the capabilities to answer, ask, and assess questions 
will enhance their multimodal comprehension, ultimately improving overall performance."
__label__natural_language_processing Despite recent progress made by large language models in code generation, they still struggle with programs that meet complex requirements.
__label__machine_learning_for_other_sciences_and_fields It provides substantial improvements in reducing errors and maintaining robustness, especially for larger networks.
__label__machine_vision Extensive experiments have been conducted on two public benchmarks, showing the superiority of HMNet.
__label__online_learning As applications of this framework, we consider three major problems with a minimax regret of $\Theta(T^{2/3})$: partial monitoring, graph bandits, and multi-armed bandits with paid observations.
__label__learning_theory Empirically, numerical results further validate the theoretical findings, showcasing the efficiency and accuracy of the proposed framework.
__label__machine_learning_for_physical_sciences We first show that incorporating conditioning mechanisms for learning parametric PDEs is essential and that among them, \textit{adaptive conditioning}, allows stronger generalization.
__label__machine_vision Combined, our findings challenge conventional wisdom in dataset distillation, underscore the importance of soft labels in learning, and suggest new directions for improving distillation methods.
__label__machine_learning_for_physical_sciences To address this, we present a framework that achieves high-precision modeling for a wide range of dynamical systems from the numerical aspect, by enforcing Time-Reversal Symmetry (TRS) via a novel regularization term.
__label__machine_vision We first introduce a general mask editing method that combines rigid and non-rigid editing techniques to generate high-quality synthetic masks.
__label__natural_language_processing Current chain-of-thought and tool-use paradigms only use text as intermediate reasoning steps.
__label__active_learning Learning an ordering of items based on pairwise comparisons is useful when items are difficult to rate consistently on an absolute scale, for example, when annotators have to make subjective assessments.
__label__generative_models Grounded in the 2D motion quantization, we build a spatial-temporal modeling framework, where 2D joint VQVAE, temporal-spatial 2D masking technique, and spatial-temporal 2D attention are proposed to take advantage of spatial-temporal signals among the 2D tokens.
__label__learning_theory In this work, we propose a new query language called threshold statistical queries and study their power for learning under various noise models.
__label__probabilistic_methods These processes find wide application in the natural sciences and machine learning, but their inference is known to be far from trivial.
__label__algorithmic_game_theory From this perspective, we can evaluate different aggregation methods via established axioms, examining whether these methods meet or fail well-known standards.
__label__neuroscience_and_cognitive_science To address this challenge, we introduce ActSort, an active-learning algorithm for sorting large-scale datasets that integrates features engineered by domain experts together with data formats with minimal memory requirements.
__label__machine_vision There are two challenges in this direction: First, rendering error gradients are often insufficient to recover fast object motion, and second,  view predictive generative models work much better for objects than whole scenes, so, score distillation objectives cannot currently be applied at the scene level directly.
__label__privacy The standard practice is to select the noise scale to satisfy a given privacy budget ε.
__label__natural_language_processing The problem is that severe local quantization errors, caused by excessive magnitudes of activation in GLU variants, significantly degrade the performance of the quantized LLM.
__label__natural_language_processing Despite the advanced intelligence abilities of large language models (LLMs) in various applications, they still face significant computational and storage demands.
__label__machine_learning_for_physical_sciences Theoretically, we analyze the convergence properties of DCGD algorithms in a non-convex setting.
__label__neuroscience_and_cognitive_science Finally, we numerically compare SGL in networks with sign activation function and finite width to kernel regression with the surrogate gradient NTK; the results confirm that the surrogate gradient NTK provides a good characterization of SGL.
__label__machine_learning_for_other_sciences_and_fields The DEQH model inherently captures the self-consistency nature of Hamiltonian, a critical aspect often overlooked by traditional machine learning approaches for Hamiltonian prediction.
__label__other Additionally, we extend our analysis with the NTK to the low-rank adaptation (LoRA) method and validate its effectiveness.
__label__safety_in_machine_learning However, such generative methods face the key dilemma, $i.e.$, improving the reconstruction power of the generative model, while keeping compact representation of the ID data.
__label__other We study the embedding dimension of distance comparison data in two settings: contrastive learning and $k$-nearest neighbors ($k$-NN).
__label__machine_vision By comparing SDF-based volume rendering to density-based volume rendering, we identify two main factors within the SDF-based approach that degrade surface quality: SDF-to-density representation and geometric regularization.
__label__generative_models Recently, reinforcement learning (RL) has been employed to fine-tune diffusion models with human preference data, but it requires at least two images (``winner'' and ``loser'' images) for each text prompt.
__label__generative_models Extensive experiments demonstrate that MVGamba outperforms state-of-the-art baselines in all 3D content generation scenarios with approximately only $0.1\times$ of the model size.
__label__learning_theory (2020) sets a threshold for the average surrogate loss at training time; above the threshold, gradient descent is run as usual, but below the threshold, a switch to gradient *ascent* is made.
__label__reinforcement_learning SeqComm treats agents asynchronously (the upper-level agents make decisions before the lower-level ones) and has two communication phases.
__label__diffusion_based_models Our approach is remarkably simple, requiring only *one line of code* to restrict the diffuse-able area for each image while preserving the Gaussian distribution of noise.
__label__neuroscience_and_cognitive_science However, typical self-supervised objectives may result in network representations that are overly invariant to changes in the input.
__label__machine_vision Our approach tunes a mixture of temporal experts to learn multiple task views with various degrees of data fitting.
__label__machine_learning_for_physical_sciences Physics-Informed Neural Networks (PINNs) are infamous for being hard to train.
__label__machine_learning_for_other_sciences_and_fields An obstacle in applying supervised paradigms to such problems is the need for costly target solutions often produced with exact solvers.
__label__safety_in_machine_learning Meanwhile, by discussing mechanisms behind the gains, new insights are drawn, and proper combinations of these methods are also developed.
__label__machine_learning_for_physical_sciences As a proof of concept, we conduct experiments on two FDEs and demonstrate that our model can successfully achieve typical $L^1$ relative error orders of PINNs $\sim 10^{-3}$.
__label__machine_vision USVI-ReID is a challenging yet underexplored task.
__label__machine_learning_for_other_sciences_and_fields To resolve these limitations, we propose **G3**, a novel framework based on Retrieval-Augmented Generation (RAG).
__label__machine_learning_for_physical_sciences Among them, Parareal computes the solution sequentially using an inaccurate (fast) solver, and then ``corrects'' it using an accurate (slow) integrator that runs in  parallel across temporal subintervals.
__label__learning_theory Kégl shows empirically that AdaBoost.MH works better when the classical one-against-all base classifiers are replaced by factorized base classifiers containing a binary classifier and a vote (or code) vector.
__label__machine_learning_for_other_sciences_and_fields We provide extensive and in-depth analyses experiments, which verify that LM-Weather can (1) indeed leverage sequential knowledge from natural language to accurately handle meteorological sequence, (2) allows each devices obtain highly customized models under significant heterogeneity, and (3) generalize under data-limited and out-of-distribution (OOD) scenarios.
__label__safety_in_machine_learning Furthermore, we prioritize rescaling important parameters to expedite adaptation to the target distribution, encouraging significant elements to contribute more while diminishing the influence of trivial ones.
__label__diffusion_based_models UltraPixel leverages semantics-rich representations of lower-resolution images in a later denoising stage to guide the whole generation of highly detailed high-resolution images, significantly reducing complexity.
__label__learning_theory We study the generalization error of statistical learning algorithms in a non-i.i.d.
__label__interpretability_and_explainability Remarkably, we demonstrate that for this problem linear transformers discover an intricate and highly effective optimization algorithm, surpassing or matching in performance many reasonable baselines.
__label__machine_learning_for_physical_sciences Recent progress of deep generative models in the vision and language domain has stimulated significant interest in more structured data generation such as molecules.
__label__deep_learning_architectures Additionally, we propose a strategy of linking parameterized Gaussian splats, forming a Triangle Soup with the estimated mesh.
__label__learning_theory Additionally, we verify our approach with real data using the MNIST dataset.
__label__reinforcement_learning To address these challenges, we propose the **Pa**rallelized **Mo**del-based **R**einforcement **L**earning (**PaMoRL**) framework.
__label__diffusion_based_models LID can be understood as the number of local factors of variation: the more factors of variation a datum has, the more complex it tends to be.
__label__learning_theory To fill this gap, in this paper, we explore whether consecutive layers collaborate to strengthen adversarial robustness during gradient descent.
__label__optimization To address this gap, we develop a method for fast exact conformalization of generalized statistical estimation.
__label__interpretability_and_explainability We propose InversionView, which allows us to practically inspect this subset by sampling from a trained decoder model conditioned on activations.
__label__deep_learning_architectures We introduce a novel calibration method, called LaSCal, which uses the estimator in conjunction with a post-hoc calibration strategy, to perform unsupervised calibration on the target distribution.
__label__reinforcement_learning Second, we find that a big barrier to improving offline RL performance is often imperfect policy generalization on test-time states out of the support of the training data, rather than policy learning on in-distribution states.
__label__generative_models However, from a mobile deployment standpoint, we can either avoid inference overhead in the fused mode but lose the ability to switch adapters rapidly, or suffer significant (up to 30% higher) inference latency while enabling rapid switching in the unfused mode.
__label__evaluation Building generative models that meaningfully capture the underlying logic of the domains they model would be immensely valuable; our results suggest new ways to assess how close a given model is to that goal.
__label__diffusion_based_models The proposed U-DiT could outperform DiT-XL with only 1/6 of its computation cost.
__label__machine_vision Code is available at https://github.com/alibaba/imood.
__label__deep_learning_architectures Through a detailed signal propagation analysis in SSMs, both forward and backward, we identify the appropriate scaling necessary for non-trivial feature evolution in the infinite-width limit.
__label__learning_theory Our main result derives the precise information-theoretic threshold for exact community recovery using any constant number of correlated graphs, answering a question of Gaudio, Rácz, and Sridhar (COLT 2022).
"__label__diffusion_based_models This ensures that the object, its attributes and sub-objects all share
the same cross-attention map."
__label__generative_models In this way, the input of LLM does not require visual tokens, which reduces the length of the input sequence and greatly improves efficiency.
__label__safety_in_machine_learning We have mediocre success in password-locking a model to mimic the answers a weaker model would give.
__label__learning_theory Furthermore, simulations on standard data sets and popular model architectures show that 2sED correlates well with the training error.
__label__machine_vision To further improve performance, we enhance the OT formulation by introducing two regularization terms.
__label__diffusion_based_models Diffusion models have gained prominence for their effectiveness in high-fidelity image generation.
__label__reinforcement_learning Then, we use sub-goals to prompt the transformer, establishing high-quality predictions.
__label__safety_in_machine_learning Its performance is both theoretically proven and experimentally validated.
__label__active_learning This work introduces REDUCR, a robust and efficient data downsampling method that uses class priority reweighting.
__label__diffusion_based_models VLM continuously evaluates the base model during training and dynamically updates and refines the training dataset by the discrimination, expansion, deletion, and mutation operations.
__label__safety_in_machine_learning The assignment of values to the constructed matrix is guided by Bayesian conditional probability, considering the joint distribution of the downstream labels and the labels predicted by the pretrained model on downstream samples.
__label__machine_vision the joint angles of a predefined skeleton.
__label__reinforcement_learning Consequently, the replay buffer is overwhelmed with failed trajectories, impeding the establishment of an applicable curriculum.
__label__privacy Specifically, AdaSCP evaluates the importance of parameters with the gradients in dominant timesteps of the diffusion model.
__label__privacy Our method only requires training a small set of models on graphs, while generating a sufficient number of approximated shadow models for attacks.
__label__machine_vision We found that this inferiority primarily results from their heavy reliance on manual textual annotations, which include the frequent provision of ambiguous language descriptions.
__label__probabilistic_methods In our experimental validation we show that tensorising probabilistic integer linear arithmetics and leveraging the fast Fourier transform allows us to push the state of the art by several orders of magnitude in terms of inference and learning times.
__label__machine_vision Our approach demonstrates its effectiveness in instructional scenarios, offering text-based guidance to correct and enhance user performance.
__label__safety_in_machine_learning Protecting the copyright of large language models (LLMs) has become crucial due to their resource-intensive training and accompanying carefully designed licenses.
__label__safety_in_machine_learning Our approach directly incorporates the adversary into the defensive objective and optimizes a lightweight and transferable suffix, enabling RPO to adapt to worst-case adaptive attacks.
__label__machine_vision In this way, VQ-Prompt can optimize the prompt selection process with task loss and meanwhile achieve effective abstraction of task knowledge for continual learning.
__label__safety_in_machine_learning In this framework, the objective is to find the encoder and decoder functions that minimize the loss function, defined as the mean squared error between the estimated and true values.
__label__machine_vision Our codes/dataset/demo will be released at https://github.com/jqtangust/hawk.
__label__active_learning Evaluation on a realistic case study with gas compressors confirms that TVSAFEOPT ensures safety when solving time-varying optimization problems with unknown reward and safety functions.
__label__diffusion_based_models Colloquially speaking, image generation models based upon diffusion processes are frequently said to exhibit ''hallucinations'' samples that could never occur in the training data.
__label__learning_theory We also empirically demonstrate that our algorithm can outperform SGD in our setting.
__label__speech_and_audio Conventionally, the feature is separated into speaker-specific ones at the final stage of the network.
__label__generative_models We provide a new perspective on behavioral stylometry that addresses these limitations, by drawing a connection to the vast literature of transfer learning in NLP.
__label__causal_inference However, these methods often require access to underlying causal structures that might not always be available, posing practical challenges.
__label__machine_learning_for_physical_sciences The spatio-temporal relations of impacts of extreme events and their drivers in climate data are not fully understood and there is a need of machine learning approaches to identify such spatio-temporal relations from data.
__label__optimization The first baseline algorithm is a natural extension of prior work, training a single NN which makes a single prediction of unknown parameters.
__label__other To overcome this limitation, orthogonal to existing approaches, we propose a novel perspective that views the CL model ability in preserving old knowledge and performing well in new task as a matter of model sensitivity to parameter updates.
__label__evaluation MR-Ben comprises 5,975 questions curated by human experts across a wide range of subjects, including physics, chemistry, logic, coding, and more.
__label__learning_theory Statistical learning theory is the foundation of machine learning, providing theoretical bounds for the risk of models learned from a (single) training set, assumed to issue from an unknown probability distribution.
__label__natural_language_processing We define different types of indirect knowledge, and carefully study the available sources to obtain it, methods to encode the structure of direct demonstrations, and finally methods to transform indirect knowledge into direct demonstrations.
__label__reinforcement_learning In this work, we develop an alternative perspective by considering couplings between a ``flattened'' version of the joint distributions that we call discounted occupancy couplings, and show that calculating optimal transport distances in the full space of joint distributions can be equivalently formulated as solving a linear program (LP) in this reduced space.
__label__reinforcement_learning While reinforcement learning (RL) holds great potential for decision making in the real world, it suffers from a number of unique difficulties which often need specific consideration.
__label__generative_models Our experimental results demonstrate that Wings outperforms equally-scaled MLLMs in both text-only and visual question-answering tasks.
__label__optimization We first develop a distributed variant of random reshuffling with gradient compression (Q-RR), and show how to reduce the variance coming from gradient quantization through the use of control iterates.
__label__natural_language_processing Previous works usually synthesize data from proprietary models to augment existing datasets, followed by instruction tuning to achieve top-tier results.
__label__natural_language_processing By doing so, the context of the representative samples is captured through deciding the factorizing orientation.
__label__robotics The former performs primitive action parsing and primitive-driven waypoint prediction, while the latter focuses on decoding low-level actions.
__label__machine_vision First, we capture the structural features using persistence homology by homological evolution across different scales in vision data, where the multi-scale characteristic established its stability under noise interference.
__label__natural_language_processing We are motivated to combine LLMs and prior small models on knowledge graphs (KGMs) for both inferential accuracy and cost saving.
__label__natural_language_processing To cope with these challenges, we propose Assistant-based Retrieval-Augmented Generation (AssistRAG), integrating an intelligent information assistant within LLMs.
"__label__learning_theory $g$ could be the modulo map of the symmetries of $f^{*}$),
so that $h$ can be learned in spite of its low regularity."
__label__machine_vision To overcome this issue, we propose a novel network architecture known as the Parameter-Inverted Image Pyramid Networks (PIIP).
__label__optimization We study a class of optimization problems in the Wasserstein space (the space of probability measures) where the objective function is nonconvex along generalized geodesics.
__label__safety_in_machine_learning These models incorporate a black-box safety filter to prevent the generation of unsafe or unethical content, such as violent, criminal, or hateful imagery.
__label__online_learning It employs a meta-algorithm running multiple black-box algorithms instances over different intervals, aggregating outputs via a sleeping expert framework.
__label__natural_language_processing In this paper, we introduce SHED, an automated dataset refinement framework based on Shapley value for instruction fine-tuning.
__label__optimization To address this issue, we propose a new Accelerated Bilevel Optimization algorithm named AccBO.
__label__machine_vision We evaluate 16 leading LVLMs on MMStar to assess their multi-modal capabilities, and on 7 benchmarks with the proposed metrics to investigate their data leakage and actual multi-modal gain.
__label__probabilistic_methods Bayesian optimization is a popular framework for efficiently tackling black-box search problems.
__label__privacy For MobileNetV2 on ImageNet, PrivCirNet achieves $1.7\times$ lower latency and $4.2$\% better accuracy over Bolt and SpENCNN, respectively.
__label__other This framework is especially relevant for applications that require joint compression and retrieval, and in scenarios involving distributional shifts due to processing.
__label__learning_theory Construction of secluded partitions with better parameters (small $k$ and large $\varepsilon$) will lead to replicable learning algorithms with small list and sample complexities.
__label__diffusion_based_models However, prior works on TS diffusion models often borrow the framework of existing works proposed in other domains without considering the characteristics of TS data, leading to suboptimal performance.
__label__natural_language_processing While large language models (LLMs) process input contexts through a causal and sequential perspective, this approach can potentially limit their ability to handle intricate and complex inputs effectively.
__label__machine_vision Our approach re-parameterizes surface colors as the product of normals and a designed Integrated Directional Illumination Vector (IDIV).
__label__neuroscience_and_cognitive_science While humans excel at these tasks \textit{without any prior training}, current AI models struggle with poor generalization performance.
__label__infrastructure Driven by this limitation, we propose SDP4Bit (Toward 4Bit Communication Quantization in Sharded Data Parallelism for LLM Training), which effectively reduces the communication of weights and gradients to nearly 4 bits via two novel techniques: quantization on weight differences, and two-level gradient smooth quantization.
__label__diffusion_based_models Further in-depth analysis reveals how our dual learning strategy advances.
__label__machine_vision This growth presents scalability challenges and hinders splatting efficiency.
__label__learning_theory Recent advances in algorithmic design show how to utilize predictions obtained by machine learning models from past and present data.
"__label__probabilistic_methods We also  show how to compute an
 efficient deterministic 
 approximation to the VB objective,
  as well as our simplified objective,
 when the variational distribution is
 Gaussian or a sub-family, including the case of
 a diagonal plus low-rank
precision matrix."
__label__privacy We focus on addressing two central challenges: statistical heterogeneity and protection of user privacy.
__label__deep_learning_architectures In this paper, we delve into the point-based method and develop a simpler, faster, stronger variant model, dubbed as LinNet.
__label__other Our method, Random Cycle Coding (RCC), encodes data sequentially and sends assignment information as cycles of the permutation defined by the order of encoded elements.
__label__generative_models In short, the barycenter task is to take the average of a collection of probability distributions w.r.t.
__label__machine_vision This paper introduces R$^2$-Gaussian, the first 3DGS-based framework for sparse-view tomographic reconstruction.
__label__graph_neural_networks Experiments across diverse domains, including node and graph property prediction, 3D object recognition, and large-scale semantic parsing, demonstrate that the proposed DuMCC effectively enables training-free knowledge transfer, yielding results on par with those of pre-trained models.
__label__graph_neural_networks Conventional methods typically require numerous labels for node classification.
__label__safety_in_machine_learning To mitigate the threat, existing solutions attempted to search for backdoor triggers, which can be time-consuming when handling a large search space.
__label__reinforcement_learning We also propose oracle-efficient model-free algorithms with $poly(d, A, H)T^{4/5}$ regret.
__label__neuroscience_and_cognitive_science We then identify a set of sufficient conditions ensuring the existence of pathwise gradients of solution trajectories and event times with respect to the network's parameters and show how these gradients satisfy a recursive relation.
__label__machine_vision Firstly, an agent must deal with a goal specification in one of multiple modalities (e.g., through a natural language description) while the search cues are provided in other modalities (aerial imagery).
__label__reinforcement_learning To enable effective online-to-offline knowledge transfer, we introduce CoWorld, a model-based RL approach that mitigates cross-domain discrepancies in state and reward spaces.
__label__probabilistic_methods However, when applied to machine learning tasks, this family of algorithms has yet to perform on par with other variational approaches in high-dimensional, structured inference problems.
__label__neuroscience_and_cognitive_science This one-shot drawing task requires powerful inductive biases that have not been systematically investigated.
__label__generative_models Through extensive experiments and analysis, we show that FouRA successfully solves the problems related to data copying and distribution collapse while significantly improving the generated image quality.
__label__learning_theory To investigate this relative gain, we propose using the simple but canonical task of linear regression.
__label__safety_in_machine_learning However, traditional fine-tuning methods are computationally intensive and require updating billions of model parameters.
__label__generative_models We compare our method with the state-of-the-art Text2Video-Zero reporting qualitative and quantitative improvements, demonstrating the effectiveness of our approach to generate videos with finely-prescribed complex motion dynamics.
__label__safety_in_machine_learning In this study, we prove that diffusion classifiers possess $O(1)$ Lipschitzness, and establish their certified robustness, demonstrating their inherent resilience.
__label__deep_learning_architectures We also find certain hybrids showing optimistic performance improvements, informing potential future ICL-focused architecture modifications.
__label__machine_vision Our encoder replaces self-attention with our local aggregation layers to achieve an elegant balance between performance and efficiency.
__label__other Our data and code are available at https://github.com/AkaliKong/iLoRA.
__label__natural_language_processing In this paper, we investigate the integration of LLMs with KGs by introducing a specialized KG Language (KGL), where a sentence precisely consists of an entity noun, a relation verb, and ends with another entity noun.
__label__diffusion_based_models Extensive experiments demonstrate that our approach significantly reduces peak memory and computational overhead, making it feasible to generate high-quality videos on a single consumer GPU (e.g., reducing peak memory of Animatediff from 42GB to 11GB, featuring faster inference on 2080Ti).
__label__diffusion_based_models The proposed method recast the traditional sampling process of generative diffusion models as a constrained optimization problem, steering the generated data distribution to remain within a specified region to ensure adherence to the given constraints.
__label__natural_language_processing A prominent issue with such methods is reward over-optimization or reward hacking, where the performance as measured by the learned proxy reward model increases, but the true model quality plateaus or even deteriorates.
__label__learning_theory Contemporary machine learning methods will try to approach the Bayes error, as it is the lowest possible error any model can achieve.
__label__safety_in_machine_learning Finally, we demonstrate the efficacy of the $\texttt{SGen}$ family in achieving a desired FDR-E level with comparable selection efficiency to those from baselines on both open and closed source GLMs.
__label__generative_models Similar to how IRL trains a policy based on the reward function learned from expert demonstrations, we train (or fine-tune) a diffusion model using the log probability density estimated from training data.
__label__causal_inference To address these challenges, recent advancements in causal imitation learning have been pursued.
__label__diffusion_based_models In this work, we present UniEdit, a tuning-free framework that supports both video motion and appearance editing by harnessing the power of a pre-trained text-to-video generator within an inversion-then-generation framework.
__label__machine_vision Moreover, their working mechanisms are typically restricted to certain simple topologies, thus relying on cumbersome manual efforts (e.g., surface cutting, part segmentation) for pre-processing.
__label__optimization Then we follow recent works to pursue the weak approximate solutions.
__label__machine_learning_for_other_sciences_and_fields While recent work demonstrates the advantage of generative models in this realm, the exploration of different probability paths are still insufficient, and hallucinations during sampling are persistently occurring.
__label__evaluation We conduct an extensive evaluation of Elo behavior across simulated and real-world scenarios, demonstrating that individual Elo computations can exhibit significant volatility.
__label__natural_language_processing Pre-trained Language Models (LMs) exhibit strong zero-shot and in-context learning capabilities; however, their behaviors are often difficult to control.
__label__learning_theory A distribution shift occurs when the underlying data-generating process changes, leading to a deviation in the model's performance.
__label__reinforcement_learning These methods often result in knowledge tightly coupled with those tasks and fail to adequately capture the distinct characteristics of different embodiments.
__label__active_learning However, many algorithms ignore shared structure between items, limiting their sample efficiency and precluding generalization to new items.
__label__machine_learning_for_physical_sciences We address this shortcoming with a new quantum-physics-aware neural network architecture for learning capability models.
__label__reinforcement_learning Code is available at https://github.com/hmhuy0/SPRINQL .
__label__machine_vision Thanks to its simplicity and comparatively negligible computation, ZERO can serve as a strong baseline for future work in this field.
__label__human-AI_interaction We further show in simulation that when using this model for preference learning, we can significantly improve a utility in a range of real-world tasks.
__label__evaluation Going beyond this study, we derive different implementations by analyzing layer-wise behaviors of CRATE, both theoretically and empirically.
__label__optimization_for_deep_networks In OPTO, an optimizer receives an execution trace along with feedback on the computed output and updates parameters iteratively.
__label__natural_language_processing }$, Codex@1, GSM-COT, BBH-COT).
__label__safety_in_machine_learning Our theoretical framework supports analyzing conformal prediction methods that involve calibrating model predictions and subsequently constructing conditionally valid prediction intervals on the same data, where the conditioning set or conformity scores may depend on the calibrated predictions.
__label__natural_language_processing We design a dynamic decision module for each transformer layer that decides whether a network unit should be executed or skipped.
__label__optimization_for_deep_networks During model optimization, the expected calibration error tends to overfit  earlier than classification accuracy, indicating distinct optimization objectives for classification error and calibration error.
__label__machine_vision Through our investigation, two quantifiable errors—light source instability and exposure time mismatches—significantly impact the prediction performance of ONN.
__label__natural_language_processing As Archimedes famously said, ``Give me a lever long enough and a fulcrum on which to place it, and I shall move the world'', in this study, we propose to use a tiny Language Model (LM), \eg, a Transformer with 67M parameters, to lever much larger Vision-Language Models (LVLMs) with 9B parameters.
__label__learning_theory This work lays a preliminary theoretical foundation for multi-output regression under CDS.
__label__reinforcement_learning Our theoretical results demonstrate the benefits of using the learned representation from the upstream offline task instead of directly learning the representation of the low-rank model.
__label__machine_vision Instead, we propose constructing a mixed Gaussian prior for a normalizing flow model for trajectory prediction.
__label__graph_neural_networks Extensive experiments, spanning diverse models and real-world datasets subjected to various adversarial attacks, validate our findings.
__label__deep_learning_architectures This semantic knowledge proves beneficial in few-shot downstream tasks.
__label__learning_theory Our core new tool is called the representation theory of the hyperrectangle.
__label__optimization Conventional nonlinear RNNs are not naturally parallelizable across the sequence length, unlike transformers and linear RNNs.
__label__reinforcement_learning We empirically show our method's performance in tabular and nonlinear function approximation settings, including Mujoco environments, with stationary and non-stationary reward signals, optimizing data usage and reducing prediction errors across multiple GVFs.
__label__natural_language_processing Enabled by this efficient uncertainty quantification method, we formulate AdvPO, a distributionally robust optimization procedure to tackle the reward overoptimization problem in RLHF.
__label__machine_vision We consider the training of the first layer of vision models and notice the clear relationship between pixel values and gradient update magnitudes: the gradients arriving at the weights of a first layer are by definition directly proportional to (normalized) input pixel values.
__label__graph_neural_networks Graph Neural Networks (GNNs) are non-Euclidean deep learning models for graph-structured data.
"__label__deep_learning_architectures The recurrent representation enables low-cost 
 inference, which improves decoding throughput, latency, and GPU memory without sacrificing performance."
__label__learning_theory In this work, we investigate and propose a novel framework for bi-Lipschitzness that can achieve such a clear and tight control based on convex neural networks and the Legendre-Fenchel duality.
__label__optimization_for_deep_networks Despite their success, training and fine-tuning these models is still far too computationally and memory intensive.
__label__machine_vision Unsigned distance fields (UDFs) provide a versatile framework for representing a diverse array of 3D shapes, encompassing both watertight and non-watertight geometries.
__label__graph_neural_networks Concretely, we pretrain a GNN, aligning its representations with token embeddings of an LLM.
__label__optimization_for_deep_networks Quantization emerges as one of the most promising compression technologies for deploying efficient large models in recent years.
__label__machine_learning_for_other_sciences_and_fields We start by parameterizing the allocation of each good to each buyer using a neural network, which depends solely on the context of the buyer and the good.
__label__online_learning One effective strategy is to use a model ensemble, which leverages the diverse expertise of different models to transfer knowledge to evolving data distributions.
__label__reinforcement_learning In this context, our sample complexity bound can be seen as a strict improvement on the previous bounds under the single-policy concentrability and the single-policy realizability.
__label__machine_learning_for_other_sciences_and_fields Experimental results indicate that our proposed method offers more accurate reconstructions and posterior estimation compared to existing DM-based imaging inverse methods.
__label__other We give an explicit algorithm to compute the provably minimal set of entries, and demonstrate empirically that one can train node embedding models with greater efficiency and performance, provided the energy function has an appropriate inductive bias.
__label__other Traditional multi-label learning studies primarily focus on closed set scenario, i.e.
__label__optimization We further extend the analysis to non-convex objectives and where some clients may be unavailable during training.
__label__interpretability_and_explainability The core question is how should we compare the values of data distributions from their samples?
__label__natural_language_processing By Interpreting the contribution of individual FFN vectors and attention heads, we identify the biased LLM components that skew LLMs' prediction toward specific labels.
__label__machine_learning_for_physical_sciences To improve out-of-distribution performance, we further assist neural operators in flexibly leveraging a similarity-based method that learns in-context examples, without incurring extra training costs or designs.
__label__optimization_for_deep_networks This leads to, for the first time, $2-3\times$ speedup per training epoch compared with standard training.
__label__other Our code is publicly available at https://github.com/cuong-dm/IGNITE.
__label__generative_models On synthetic and real-data benchmarks, we provide strong empirical evidence for the un-identifiability of existing hybrid-DGMs using unconditional priors, and strong identifiability results of the presented meta-formulations of hybrid-DGMs.
__label__generative_models We evaluated FreeLong on multiple base video diffusion models and observed significant improvements.
__label__generative_models DEFT is much faster than existing baselines while achieving state-of-the-art performance across a variety of linear and non-linear benchmarks.
__label__privacy We evaluate dèjá vu memorization at both sample and population level, and show that it is significant for OpenCLIP trained on as many as 50M image-caption pairs.
__label__optimization Finally, we identify which neural network architectures give rise to such star body gauges and when do such regularizers have favorable properties for optimization.
__label__diffusion_based_models In addition, we conducted a thorough analysis that sheds light on how it improves diffusion training speed while improving fidelity.
__label__evaluation We do so by making use of the tools of psychometrics which are designed to perform meaningful measurement in test taking.
__label__machine_vision The objective of this paper is to address the persistent bottleneck challenges from three perspectives: model, dataset, and benchmark.
__label__generative_models Additionally, our method supports coherent multi-prompt generation, ensuring both visual coherence and seamless transitions between scenes.
__label__other In this paper we propose to solve this through _semantic RSMs_, which are invariant to spatial permutation.
"__label__diffusion_based_models Our
method is particularly effective in complex scenarios that involve multiple objects
and attributes, which previous methods often fail to address."
__label__machine_vision 3) Benchmark-level: we develop a comprehensive benchmark of 19 methods based on IEA40K.
__label__probabilistic_methods However, the design of methods for learning credal set predictors remains a challenging problem.
__label__safety_in_machine_learning Our code is available at [PerpCorrect](https://github.com/luxinyayaya/PerpCorrect).
__label__neuroscience_and_cognitive_science We demonstrate that this approach results in a significantly lower overall cost than current state-of-the-art solutions, particularly in the presence of internal noise, though the improvement is present in other circumstances as well, with theoretical explanations for this enhanced performance.
__label__algorithmic_game_theory Sequences with this property are called satisficing paths, and arise naturally in many MARL algorithms.
__label__machine_vision TransCLIP is applicable as a plug-and-play module on top of popular inductive zero- and few-shot models, consistently improving their performances.
__label__probabilistic_methods Experiments on both diverse ensembles and hypernetworks show that our approach significantly outperforms baselines in terms of uncertainty quantification in both synthetic and realistic outlier detection tasks.
__label__machine_learning_for_other_sciences_and_fields Such UNN-based methods are appealing as they have the potential of avoiding the computationally intensive retraining required for different source models and different measurement scenarios.
__label__safety_in_machine_learning The first is a novel curriculum learning scheme that iteratively increases the verified safe horizon.
__label__machine_vision However, every synthetic image ultimately originates from the upstream data used to train the generator.
__label__machine_vision For instance, DyT achieves superior performance compared to existing PEFT methods while evoking only 71% of their FLOPs on the VTAB-1K benchmark.
__label__machine_vision Instead, we propose a Prospective Representation Learning (PRL) approach to prepare the model for handling conflicts in advance.
__label__machine_vision Task graphs learned with our approach are also shown to significantly enhance online mistake detection in procedural egocentric videos, achieving notable gains of +19.8% and +7.5% on the Assembly101-O and EPIC-Tent-O datasets.
__label__deep_learning_architectures In this work, we propose Retentive Network (RetNet) as a foundation architecture for large language models, simultaneously achieving training parallelism, low-cost inference, and good performance.
__label__other Surprisingly, these projected models can perform reasonably well even without finetuning.
__label__learning_theory We theoretically show that this method guarantees the classification error across all $P_i$s can be suitably bounded.
"__label__graph_neural_networks Graph Neural Networks (GNNs) experience ""catastrophic forgetting"" in continual learning setups, where they tend to lose previously acquired knowledge and perform poorly on old tasks."
__label__machine_vision Our source code is available at: https://github.com/Jinec98/PCoTTA.
__label__causal_inference These scores quantify the strength of the average causal effect, helping to accelerate the pruning process and correct inaccurate predictions in the pruning step.
__label__machine_learning_for_healthcare To facilitate efficient and precise inference on volumetric images, we design a zoom-out-zoom-in mechanism.
__label__probabilistic_methods In this work, we propose and study a GP model that achieves robustness against sparse outliers by inferring data-point-specific noise levels with a sequential selection procedure maximizing the log marginal likelihood that we refer to as relevance pursuit.
__label__safety_in_machine_learning Using a set of textual action descriptions, we collect value judgments from humans, as well as similarity judgments from both humans and multiple language models, and demonstrate that representational alignment enables both safe exploration and improved generalization when learning human values.
__label__natural_language_processing However, such actions are missing in current multimodal language models (LMs).
__label__reinforcement_learning Under a generative model, we establish a sample complexity for $h$-PMD which improves over prior work.
__label__reinforcement_learning Recently, Sherman et al.
__label__reinforcement_learning Furthermore, we show that data diversity and network architecture are the most important contributors to OOD generalization performance.
__label__natural_language_processing We define a strong instance of the ReFT family, Low-rank Linear Subspace ReFT (LoReFT), and we identify an ablation of this method that trades some performance for increased efficiency.
__label__natural_language_processing Our results suggest that unstructured text data is a rich source of causal effect information, and NATURAL is a first step towards an automated pipeline to tap this resource.
__label__machine_learning_for_physical_sciences Results demonstrate our superiority across metrics from distribution learning to condition control for molecular properties.
__label__machine_vision We carry out extensive experiments to show our superior robustness, and tho rough ablations to dissect the effectiveness of each component.
__label__graph_neural_networks By incorporating adaptive similar nodes information, SNAPS can generate compact prediction sets and increase the singleton hit ratio (correct prediction sets of size one).
"__label__machine_vision To further minimize the additional bitrate overhead introduced by the
entropy models, NVRC also compresses all the network, quantization and entropy
model parameters hierarchically."
__label__neuroscience_and_cognitive_science In this study, we designed a BMI decoder based on KalmanNet, an extension of the KF that augments its operation with recurrent neural networks to compute the Kalman gain.
__label__machine_vision Our work can be seamlessly combined with models pretrained by different SSL frameworks without revising the learning objectives and helps to bridge the gap between SAT and AT.
__label__deep_learning_architectures However, constrained by the inherent geometric constraints of sequences, it still falls short in modeling long-range dependencies.
__label__machine_learning_for_physical_sciences We show that combining Monte Carlo sampling with higher-order quadrature rules is critical for accurately estimating the training objective from sample data and for stabilizing the training process.
__label__natural_language_processing Yet, in the context of LLMs, current methods for adversarial training are hindered by the high computational costs required to perform discrete adversarial attacks at each training iteration.
__label__machine_vision With knowledge distillation and re-blurring loss, we further design a lightweight deblur network to generate high-quality sequences with brightness and texture consistency with the original input.
__label__safety_in_machine_learning Altogether, our work aims to consolidate the foundations of scalable oversight, formalizing and studying the various challenges thereof.
"__label__online_learning Using the improved discretization schemes previously developed, we are able to achieve 
$\widetilde{O}(m\sqrt{T})$ regret in the stochastic setting and $\widetilde{\mathcal{O}}(m^{3/2}\sqrt{T})$ regret in the adversarial setting."
__label__other We develop a novel framework, namely *corssing sparse proximity graph (CSPG)*, based on random partitioning of the dataset.
__label__generative_models Driven by these findings, we propose EfficientNAT (ENAT), a NAT model that explicitly encourages these critical interactions inherent in NATs.
__label__machine_learning_for_other_sciences_and_fields By co-working with a foundation model, a global adapter and a local adapter jointly tackle the test-time distribution shifts and client-specific personalization.
__label__natural_language_processing Code and dataset are available at https://github.com/zou-group/avatar.
__label__graph_neural_networks Convolutional neural networks (CNNs) have led to a revolution in analyzing array data.
"__label__machine_learning_for_healthcare We demonstrate its versatility and robustness through applications in cell development studies, cellular drug response modeling, and cross-modality cell translation,
illustrating significant potential for enhancing therapeutic strategies."
__label__learning_theory Notably, the persistent homology dimension has been proposed to correlate with the generalization gap.
__label__machine_learning_for_other_sciences_and_fields This integration allows our model to infuse geometric information into node features while preserving the spatial semantics of coordinates, leading to greater expressive power than standard FA models.
__label__optimization This provides a much-needed routine to create synthetic problems where the ground-truth OT map is known, by analogy to the Brenier theorem, which states that the gradient of any convex potential is always a valid Monge map for the $\ell_2^2$ cost; *(ii)* We propose a loss to *learn* the parameter $\theta$ of a parameterized regularizer $\tau_\theta$, and apply it in the case where $\tau_{A}({\bf z}):=\|A^\perp {\bf z}\|^2_2$.
__label__probabilistic_methods We introduce a novel contextual bandit algorithm for top-k recommendations, leveraging a Gaussian process with a Kendall kernel to model the reward function.
__label__natural_language_processing Despite efforts to ethically align LLMs, these are often fragile and can be circumvented by jailbreaking attacks through optimized or manual adversarial prompts.
__label__machine_vision Recently, an approach for learning logic gate networks directly via a differentiable relaxation was proposed.
__label__interpretability_and_explainability This begs the question: how do ViTs attempt to perform tasks that require computing visual relations between objects?
__label__machine_vision We propose the domain-aware partitioning in scatter and context, guided by a routing mechanism, to address the data interference issue, and further incorporate the text modality for a language-guided classification to unify the multi-dataset label spaces and mitigate the category interference issue.
__label__deep_learning_architectures This achievement represents a significant step towards more efficient and scalable deep learning models for sequence modeling.
__label__natural_language_processing To solve the lack of quantification, we first define a reasoning boundary (RB) to quantify the upper-bound of CoT and establish a combination law for RB, enabling a practical quantitative approach applicable to various real-world CoT tasks.
__label__reinforcement_learning Source code is available at \url{https://anonymous.4open.science/r/SMG/}.
__label__probabilistic_methods We show that marginalization is always beneficial when applicable and highlight improvements in various models, especially ones from cognitive sciences.
__label__machine_learning_for_other_sciences_and_fields We have open-sourced LLMDFA at https://github.com/chengpeng-wang/LLMDFA.
__label__deep_learning_architectures Moreover, TTMs are lightweight and can be executed even on CPU-only machines, enhancing usability and fostering wider adoption in resource-constrained environments.
"__label__machine_vision Extensive experiments showcase that task-aware components disassembled from CNN classifiers or new models assembled using these components closely match or even surpass the performance of the baseline,
demonstrating its promising results for model reuse."
__label__generative_models Moreover, we show that this transformation yields the best possible trade-off between win-rate against the base model vs KL distance from the base model.
__label__safety_in_machine_learning Certified robustness analysis by randomized smoothing has not been performed for deep regression networks where the output variable is continuous and unbounded.
__label__machine_learning_for_social_sciences Our code is released at https://github.com/jiaruzouu/PRB.
__label__natural_language_processing We introduce a novel attack based on a previously overlooked source of information — byte-pair encoding (BPE) tokenizers, used by the vast majority of modern language models.
__label__other In this paper, we propose a novel conceptual framework to detect outliers using optimal transport with a concave cost function.
__label__graph_neural_networks We propose **U**niversal **G**raph **C**oarsening (UGC), a framework equally suitable for homophilic and heterophilic datasets.
__label__optimization_for_deep_networks We show an implicit regularization towards flat minima: the sharpness of the minimizer is no more than a constant times the lower bound.
__label__machine_vision In this work, we propose a novel open-vocabulary monocular 3D object detection framework, dubbed OVM3D-Det, which trains detectors using only RGB images, making it both cost-effective and scalable to publicly available data.
__label__optimization_for_deep_networks Unlike prior works on weight-centric tensor decomposition, ESPACE projects activations onto a pre-calibrated set of principal components.
__label__machine_vision Our proposed TinyLUT enables superior inference speed on edge devices with new state-of-the-art accuracy on both of image super-resolution and denoising, showcasing the potential of applying this method to various image restoration tasks at the edge.
__label__machine_vision Not only can our approach achieve excellent visual fidelity, but it also allows for the real-time rendering of high-resolution images.
__label__generative_models Recent advancements in 3D content generation have been significant, primarily due to the visual priors provided by pretrained diffusion models.
__label__privacy An extension GReM-LNN (Gaussian Residuals-to-Marginals with Local Non-negativity) reconstructs marginals under Gaussian noise satisfying consistency and non-negativity, which often reduces error on reconstructed answers.
__label__machine_vision We rethink the common practice of using binary preferences (*i.e.
__label__learning_theory Recently Aamand et al.
__label__robotics More specifically, (i) we show analytically that CON is a Lagrangian system - i.e., it possesses well-defined potential and kinetic energy terms.
__label__reinforcement_learning For weakly communicating MDPs, we establish the complexity bound $\widetilde{O}\left(SA\frac{\mathsf{H}}{\varepsilon^2} \right)$, where $\mathsf{H}$ is the span of the bias function of the optimal policy and $SA$ is the cardinality of the state-action space.
"__label__learning_theory The hope is that, if the source and target distributions are ""close"", then the fine-tuned model will perform well on the target distribution even though it has seen only a few samples from it."
__label__machine_learning_for_other_sciences_and_fields Our framework also achieves efficient transfer knowledge from the encoders pre-training as well as in between modalities.
__label__reinforcement_learning However, none of the algorithms available in literature can scale to problems with large state spaces.
__label__robotics BAKU builds upon recent advancements in offline imitation learning and meticulously combines observation trunks, action chunking, multi-sensory observations, and action heads to substantially improve upon prior work.
__label__machine_learning_for_social_sciences Moreover, we propose intent-assisted contrastive learning by using cluster centers as self-supervision signals, further enhancing mutual promotion.
__label__active_learning A key requirement for training an accurate 3D object detector is the availability of a large amount of LiDAR-based point cloud data.
__label__natural_language_processing Notably, PPO outperforms DPO by up to 2.5% in math and 1.2% in general domains.
__label__machine_vision Subsequently, we employ conditional diffusion models to generate plausible full-body motions based on these temporally dense trajectories of the head and hands, guided by the uncertainty estimates from the imputation.
__label__generative_models In response to these challenges, we propose a new strategy that targets synthetic data created by DGMs for specific data analyses.
__label__neuroscience_and_cognitive_science Inspired by these experimental findings, we develop a self-supervised learning (SSL) objective that explicitly quantifies and promotes straightening.
__label__causal_inference This task, however, becomes challenging in areas like social sciences and online marketplaces, where treating one experimental unit can influence outcomes for others through direct or indirect interactions.
__label__reinforcement_learning To this end, we propose SCAS, a simple yet effective approach that unifies OOD state correction and OOD action suppression in offline RL.
__label__reinforcement_learning The resulting algorithm is easy to implement, requiring only a few lines of code modification to existing methods.
__label__interpretability_and_explainability Besides, existing adversarial training (AT) associated with intensive computations may lead to an arms race.
__label__neuroscience_and_cognitive_science We theoretically analyze the nonlinear circuit dynamics and analytically identify the Bayesian sampling algorithm performed by the circuit dynamics.
__label__machine_learning_for_healthcare We introduce a novel multi-agent framework, named **M**edical **D**ecision-making **Agents** (**MDAgents**) that helps to address this gap by automatically assigning a collaboration structure to a team of LLMs.
__label__machine_vision Our method employs a mix of sequence compression strategies, including randomized token dropout and flexible patch scaling, to reduce the cost of gradient estimation and accelerate convergence.
__label__optimization_for_deep_networks In empirical studies, we find that OptoPrime is capable of first-order numerical optimization, prompt optimization, hyper-parameter tuning, robot controller design, code debugging, etc., and is often competitive with specialized optimizers for each domain.
__label__neuroscience_and_cognitive_science The inferred switches are aligned with the behavior, and the reconstructions show that the recovered neural dynamics are distinct across different stages of the behavior.
__label__other In this context, given two large point clouds of sizes $n$ and $m$ in $\mathbb{R}^d$, entropic OT (EOT) solvers have emerged as the most reliable tool to either solve the Kantorovich problem and output a $n\times m$ coupling matrix, or to solve the Monge problem and learn a vector-valued push-forward map.
__label__machine_vision Although most existing SAOD methods rely on fixed thresholding to filter pseudo-labels for enhancing detector performance, adapting to aerial objects proves challenging due to the imbalanced probabilities/confidences associated with predicted aerial objects.
__label__diffusion_based_models This approach directly addresses on-device memory constraints and substantially reduces GPU memory requirements for training, in contrast to previous methods that primarily focus on minimizing training steps and reducing the number of parameters to update.
__label__generative_models Despite its compact nature, TiTok achieves competitive performance to state-of-the-art approaches.
__label__natural_language_processing Our experiments simulate the soft preference labels with AI feedback from LLMs and demonstrate that geometric averaging consistently improves performance on standard benchmarks for alignment research.
__label__privacy We develop new lower bounds for this setting which shows that this rate cannot be improved with more public samples, and any fewer public samples leads to a worse rate.
